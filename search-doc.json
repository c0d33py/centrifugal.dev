{"searchDocs":[{"title":"Million connections with Centrifugo","type":0,"sectionRef":"#","url":"/blog/2020/02/10/million-connections-with-centrifugo","content":"In order to get an understanding about possible hardware requirements for reasonably massive Centrifugo setup we made a test stand inside Kubernetes. Our goal was to run server based on Centrifuge library (the core of Centrifugo server) with one million WebSocket connections and send many messages to connected clients. While sending many messages we have been looking at delivery time latency. In fact we will see that about 30 million messages per minute (500k messages per second) will be delivered to connected clients and latency won't be larger than 200ms in 99 percentile. Server nodes have been run on machines with the following configuration: CPU Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHzLinux Debian 4.9.65-3+deb9u1 (2017-12-23) x86_64 GNU/Linux Some sysctl values: fs.file-max = 3276750 fs.nr_open = 1048576 net.ipv4.tcp_mem = 3086496\t4115330\t6172992 net.ipv4.tcp_rmem = 8192\t8388608\t16777216 net.ipv4.tcp_wmem = 4096\t4194394\t16777216 net.core.rmem_max = 33554432 net.core.wmem_max = 33554432 Kubernetes used these machines as its nodes. We started 20 Centrifuge-based server pods. Our clients connected to server pods using Centrifuge Protobuf protocol. To scale horizontally we used Redis Engine and sharded it to 5 different Redis instances (each Redis instance consumes 1 CPU max). To achieve many client connections we used 100 Kubernetes pods each generating about 10k client connections to server. Here are some numbers we achieved: 1 million WebSocket connectionsEach connection subscribed to 2 channels: one personal channel and one group channel (with 10 subscribers in it), i.e. we had about 1.1 million active channels at each moment.28 million messages per minute (about 500k per second) delivered to clients200k per minute constant connect/disconnect rate to simulate real-life situation where clients connect/disconnect from server200ms delivery latency in 99 percentileThe size of each published message was about 100 bytes And here are some numbers about final resource usage on server side (we don't actually interested in client side resource usage here): 40 CPU total for server nodes when load achieved values claimed above (20 pods, ~2 CPU each)27 GB of RAM used mostly to handle 1 mln WebSocket connections, i.e. about 30kb RAM per connection0.32 CPU usage on every Redis instance100 mbit/sec rx и 150 mbit/sec tx of network used on each server pod The picture that demonstrates experiment (better to open image in new tab): This also demonstrates that to handle one million of WebSocket connections without many messages sent to clients you need about 10 CPU total for server nodes and about 5% of CPU on each of Redis instances. In this case CPU mostly spent on connect/disconnect flow, ping/pong frames, subscriptions to channels. If we enable history and history message recovery features we see an increased Redis CPU usage: 64% instead of 32% on the same workload. Other resources usage is pretty the same. The results mean that one can theoretically achieve the comparable numbers on single modern server machine. But numbers can vary a lot in case of different load scenarios. In this benchmark we looked at basic use case where we only connect many clients and send Publications to them. There are many features in Centrifuge library and in Centrifugo not covered by this artificial experiment. Also note that though benchmark was made for Centrifuge library for Centrifugo you can expect similar results. Read and write buffer sizes of websocket connections were set to 512 kb on server side (sizes of buffers affect memory usage), with Centrifugo this means that to reproduce the same configuration you need to set: { ... &quot;websocket_read_buffer_size&quot;: 512, &quot;websocket_write_buffer_size&quot;: 512 } ","keywords":"","version":null},{"title":"Experimenting with QUIC and WebTransport","type":0,"sectionRef":"#","url":"/blog/2020/10/16/experimenting-with-quic-transport","content":"","keywords":"","version":null},{"title":"Overview​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#overview","content":" WebTransport is a new browser API offering low-latency, bidirectional, client-server messaging. If you have not heard about it before I suggest to first read a post called Experimenting with QuicTransport published recently on web.dev – it gives a nice overview to WebTransport and shows client-side code examples. Here we will concentrate on implementing server side.  Some key points about WebTransport spec:  WebTransport standard will provide a possibility to use streaming client-server communication using modern transports such as QUIC and HTTP/3It can be a good alternative to WebSocket messaging, standard provides some capabilities that are not possible with current WebSocket spec: possibility to get rid of head-of-line blocking problems using individual streams for different data, the possibility to reuse a single connection to a server in different browser tabsWebTransport also defines an unreliable stream API using UDP datagrams (which is possible since QUIC is UDP-based) – which is what browsers did not have before without a rather complex WebRTC setup involving ICE, STUN, etc. This is sweet for in-browser real-time games.  To help you figure out things here are links to current WebTransport specs:  WebTransport overview – this spec gives an overview of WebTransport and provides requirements to transport layerWebTransport over QUIC – this spec describes QUIC-based transport for WebTransportWebTransport over HTTP/3 – this spec describes HTTP/3-based transport for WebTransport (actually HTTP/3 is a protocol defined on top of QUIC)  At moment Chrome only implements trial possibility to try out WebTransport standard and only implements WebTransport over QUIC. Developers can initialize transport with code like this:  const transport = new QuicTransport('quic-transport://localhost:4433/path');   In case of HTTP/3 transport one will use URL like 'https://localhost:4433/path' in transport constructor. All WebTransport underlying transports should support instantiation over URL – that's one of the spec requirements.  I decided that this is a cool possibility to finally play with QUIC protocol and its Go implementation github.com/lucas-clemente/quic-go.  danger Please keep in mind that all things described in this post are work in progress. WebTransport drafts, Quic-Go library, even QUIC protocol itself are subjects to change. You should not use it in production yet.  Experimenting with QuicTransport post contains links to a client example and companion Python server implementation.    We will use a linked client example to connect to a server that runs on localhost and uses github.com/lucas-clemente/quic-go library. To make our example work we need to open client example in Chrome, and actually, at this moment we need to install Chrome Canary. The reason behind this is that the quic-go library supports QUIC draft-29 while Chrome &lt; 85 implements QuicTransport over draft-27. If you read this post at a time when Chrome stable 85 already released then most probably you don't need to install Canary release and just use your stable Chrome.  We also need to generate self-signed certificates since WebTransport only works with a TLS layer, and we should make Chrome trust our certificates. Let's prepare our client environment before writing a server and first install Chrome Canary.  ","version":null,"tagName":"h2"},{"title":"Install Chrome Canary​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#install-chrome-canary","content":" Go to https://www.google.com/intl/en/chrome/canary/, download and install Chrome Canary. We will use it to open client example.  note If you have Chrome &gt;= 85 then most probably you can skip this step.  ","version":null,"tagName":"h2"},{"title":"Generate self-signed TLS certificates​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#generate-self-signed-tls-certificates","content":" Since WebTransport based on modern network transports like QUIC and HTTP/3 security is a keystone. For our experiment we will create a self-signed TLS certificate using openssl.  Make sure you have openssl installed:  $ which openssl /usr/bin/openssl   Then run:  openssl genrsa -des3 -passout pass:x -out server.pass.key 2048 openssl rsa -passin pass:x -in server.pass.key -out server.key rm server.pass.key openssl req -new -key server.key -out server.csr   Set localhost for Common Name when asked.  The self-signed TLS certificate generated from the server.key private key and server.csr files:  openssl x509 -req -sha256 -days 365 -in server.csr -signkey server.key -out server.crt   After these manipulations you should have server.crt and server.key files in your working directory.  To help you with process here is my console output during these steps (click to open):  ??? example &quot;My console output generating self-signed certificates&quot;  $ openssl genrsa -des3 -passout pass:x -out server.pass.key 2048 Generating RSA private key, 2048 bit long modulus ...........................................................................................+++ .....................+++ e is 65537 (0x10001) $ ls server.pass.key $ openssl rsa -passin pass:x -in server.pass.key -out server.key writing RSA key $ ls server.key server.pass.key $ rm server.pass.key $ openssl req -new -key server.key -out server.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) []:RU State or Province Name (full name) []: Locality Name (eg, city) []: Organization Name (eg, company) []: Organizational Unit Name (eg, section) []: Common Name (eg, fully qualified host name) []:localhost Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: $ openssl x509 -req -sha256 -days 365 -in server.csr -signkey server.key -out server.crt Signature ok subject=/C=RU/CN=localhost Getting Private key $ ls server.crt server.csr server.key   ","version":null,"tagName":"h2"},{"title":"Run client example​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#run-client-example","content":" Now the last step. What we need to do is run Chrome Canary with some flags that will allow it to trust our self-signed certificates. I suppose there is an alternative way making Chrome trust your certificates, but I have not tried it.  First let's find out a fingerprint of our cert:  openssl x509 -in server.crt -pubkey -noout | openssl pkey -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64   In my case base64 fingerprint was pe2P0fQwecKFMc6kz3+Y5MuVwVwEtGXyST5vJeaOO/M=, yours will be different.  Then run Chrome Canary with some additional flags that will make it trust out certs (close other Chrome Canary instances before running it):  $ /Applications/Google\\ Chrome\\ Canary.app/Contents/MacOS/Google\\ Chrome\\ Canary \\ --origin-to-force-quic-on=localhost:4433 \\ --ignore-certificate-errors-spki-list=pe2P0fQwecKFMc6kz3+Y5MuVwVwEtGXyST5vJeaOO/M=   This example is for MacOS, for your system see docs on how to run Chrome/Chromium with custom flags.  Now you can open https://googlechrome.github.io/samples/quictransport/client.html URL in started browser and click Connect button. What? Connection not established? OK, this is fine since we need to run our server :)  ","version":null,"tagName":"h2"},{"title":"Writing a QUIC server​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#writing-a-quic-server","content":" Maybe in future we will have libraries that are specified to work with WebTransport over QUIC or HTTP/3, but for now we should implement server manually. As said above we will use github.com/lucas-clemente/quic-go library to do this.  ","version":null,"tagName":"h2"},{"title":"Server skeleton​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#server-skeleton","content":" First, let's define a simple skeleton for our server:  package main import ( &quot;errors&quot; &quot;log&quot; &quot;github.com/lucas-clemente/quic-go&quot; ) // Config for WebTransportServerQuic. type Config struct { // ListenAddr sets an address to bind server to. ListenAddr string // TLSCertPath defines a path to .crt cert file. TLSCertPath string // TLSKeyPath defines a path to .key cert file TLSKeyPath string // AllowedOrigins represents list of allowed origins to connect from. AllowedOrigins []string } // WebTransportServerQuic can handle WebTransport QUIC connections according // to https://tools.ietf.org/html/draft-vvv-webtransport-quic-02. type WebTransportServerQuic struct { config Config } // NewWebTransportServerQuic creates new WebTransportServerQuic. func NewWebTransportServerQuic(config Config) *WebTransportServerQuic { return &amp;WebTransportServerQuic{ config: config, } } // Run server. func (s *WebTransportServerQuic) Run() error { return errors.New(&quot;not implemented&quot;) } func main() { server := NewWebTransportServerQuic(Config{ ListenAddr: &quot;0.0.0.0:4433&quot;, TLSCertPath: &quot;server.crt&quot;, TLSKeyPath: &quot;server.key&quot;, AllowedOrigins: []string{&quot;localhost&quot;, &quot;googlechrome.github.io&quot;}, }) if err := server.Run(); err != nil { log.Fatal(err) } }   ","version":null,"tagName":"h3"},{"title":"Accept QUIC connections​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#accept-quic-connections","content":" Let's concentrate on implementing Run method. We need to accept QUIC client connections. This can be done by creating quic.Listener instance and using its .Accept method to accept incoming client sessions.  // Run server. func (s *WebTransportServerQuic) Run() error { listener, err := quic.ListenAddr(s.config.ListenAddr, s.generateTLSConfig(), nil) if err != nil { return err } for { sess, err := listener.Accept(context.Background()) if err != nil { return err } log.Printf(&quot;session accepted: %s&quot;, sess.RemoteAddr().String()) go func() { defer func() { _ = sess.CloseWithError(0, &quot;bye&quot;) log.Println(&quot;close session&quot;) }() s.handleSession(sess) }() } } func (s *WebTransportServerQuic) handleSession(sess quic.Session) { // Not implemented yet. }   An interesting thing to note is that QUIC allows closing connection with specific application-level integer code and custom string reason. Just like WebSocket if you worked with it.  Also note, that we are starting our Listener with TLS configuration returned by s.generateTLSConfig() method. Let's take a closer look at how this method can be implemented.  // https://tools.ietf.org/html/draft-vvv-webtransport-quic-02#section-3.1 const alpnQuicTransport = &quot;wq-vvv-01&quot; func (s *WebTransportServerQuic) generateTLSConfig() *tls.Config { cert, err := tls.LoadX509KeyPair(s.config.TLSCertPath, s.config.TLSKeyPath) if err != nil { log.Fatal(err) } return &amp;tls.Config{ Certificates: []tls.Certificate{cert}, NextProtos: []string{alpnQuicTransport}, } }   Inside generateTLSConfig we load x509 certs from cert files generated above. WebTransport uses ALPN (Application-Layer Protocol Negotiation to prevent handshakes with a server that does not support WebTransport spec. This is just a string wq-vvv-01 inside NextProtos slice of our *tls.Config.  ","version":null,"tagName":"h3"},{"title":"Connection Session handling​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#connection-session-handling","content":" At this moment if you run a server and open a client example in Chrome then click Connect button – you should see that connection successfully established in event log area:    Now if you try to send data to a server nothing will happen. That's because we have not implemented reading data from session streams.  Streams in QUIC provide a lightweight, ordered byte-stream abstraction to an application. Streams can be unidirectional or bidirectional.  Streams can be short-lived, streams can also be long-lived and can last the entire duration of a connection.  Client example provides three possible ways to communicate with a server:  Send a datagramOpen a unidirectional streamOpen a bidirectional stream  Unfortunately, quic-go library does not support sending UDP datagrams at this moment. To do this quic-go should implement one more draft called An Unreliable Datagram Extension to QUIC. There is already an ongoing pull request that implements it. This means that it's too early for us to experiment with unreliable UDP WebTransport client-server communication in Go. By the way, the interesting facts about UDP over QUIC are that QUIC congestion control mechanism will still apply and QUIC datagrams can support acknowledgements.  Implementing a unidirectional stream is possible with quic-go since the library supports creating and accepting unidirectional streams, but I'll leave this for a reader (though we will need accepting one unidirectional stream for parsing client indication anyway – see below).  Here we will only concentrate on implementing a server for a bidirectional case. We are in the Centrifugo blog, and this is the most interesting type of stream for me personally.  ","version":null,"tagName":"h3"},{"title":"Parsing client indication​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#parsing-client-indication","content":" According to section-3.2 of Quic WebTransport spec in order to verify that the client's origin allowed connecting to the server, the user agent has to communicate the origin to the server. This is accomplished by sending a special message, called client indication, on stream 2, which is the first client-initiated unidirectional stream.  Here we will implement this. In the beginning of our session handler we will accept a unidirectional stream initiated by a client.  At moment spec defines two client indication keys: Origin and Path. In our case an origin value will be https://googlechrome.github.io and path will be /counter.  Let's define some constants and structures:  // client indication stream can not exceed 65535 bytes in length. // https://tools.ietf.org/html/draft-vvv-webtransport-quic-02#section-3.2 const maxClientIndicationLength = 65535 // define known client indication keys. type clientIndicationKey int16 const ( clientIndicationKeyOrigin clientIndicationKey = 0 clientIndicationKeyPath = 1 ) // ClientIndication container. type ClientIndication struct { // Origin client indication value. Origin string // Path client indication value. Path string }   Now what we should do is accept unidirectional stream inside session handler:  func (s *WebTransportServerQuic) handleSession(sess quic.Session) { stream, err := sess.AcceptUniStream(context.Background()) if err != nil { log.Println(err) return } log.Printf(&quot;uni stream accepted, id: %d&quot;, stream.StreamID()) indication, err := receiveClientIndication(stream) if err != nil { log.Println(err) return } log.Printf(&quot;client indication: %+v&quot;, indication) if err := s.validateClientIndication(indication); err != nil { log.Println(err) return } // this method blocks. if err := s.communicate(sess); err != nil { log.Println(err) } } func receiveClientIndication(stream quic.ReceiveStream) (ClientIndication, error) { return ClientIndication{}, errors.New(&quot;not implemented yet&quot;) } func (s *WebTransportServerQuic) validateClientIndication(indication ClientIndication) error { return errors.New(&quot;not implemented yet&quot;) } func (s *WebTransportServerQuic) communicate(sess quic.Session) error { return errors.New(&quot;not implemented yet&quot;) }   As you can see to accept a unidirectional stream with data we can use .AcceptUniStream method of quic.Session. After accepting a stream we should read client indication data from it.  According to spec it will contain a client indication in the following format:  0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Key (16) | Length (16) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Value (*) ... +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+   The code below parses client indication out of a stream data, we decode key-value pairs from uni stream until an end of stream (indicated by EOF):  func receiveClientIndication(stream quic.ReceiveStream) (ClientIndication, error) { var clientIndication ClientIndication // read no more than maxClientIndicationLength bytes. reader := io.LimitReader(stream, maxClientIndicationLength) done := false for { if done { break } var key int16 err := binary.Read(reader, binary.BigEndian, &amp;key) if err != nil { if err == io.EOF { done = true } else { return clientIndication, err } } var valueLength int16 err = binary.Read(reader, binary.BigEndian, &amp;valueLength) if err != nil { return clientIndication, err } buf := make([]byte, valueLength) n, err := reader.Read(buf) if err != nil { if err == io.EOF { // still need to process indication value. done = true } else { return clientIndication, err } } if int16(n) != valueLength { return clientIndication, errors.New(&quot;read less than expected&quot;) } value := string(buf) switch clientIndicationKey(key) { case clientIndicationKeyOrigin: clientIndication.Origin = value case clientIndicationKeyPath: clientIndication.Path = value default: log.Printf(&quot;skip unknown client indication key: %d: %s&quot;, key, value) } } return clientIndication, nil }   We also validate Origin inside validateClientIndication method of our server:  var errBadOrigin = errors.New(&quot;bad origin&quot;) func (s *WebTransportServerQuic) validateClientIndication(indication ClientIndication) error { u, err := url.Parse(indication.Origin) if err != nil { return errBadOrigin } if !stringInSlice(u.Host, s.config.AllowedOrigins) { return errBadOrigin } return nil } func stringInSlice(a string, list []string) bool { for _, b := range list { if b == a { return true } } return false }   Do you have stringInSlice function in every Go project? I do :)  ","version":null,"tagName":"h3"},{"title":"Communicating over bidirectional streams​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#communicating-over-bidirectional-streams","content":" The final part here is accepting a bidirectional stream from a client, reading it, and sending responses back. Here we will just echo everything a client sends to a server back to a client. You can implement whatever bidirectional communication you want actually.  Very similar to unidirectional case we can call .AcceptStream method of session to accept a bidirectional stream.  func (s *WebTransportServerQuic) communicate(sess quic.Session) error { for { stream, err := sess.AcceptStream(context.Background()) if err != nil { return err } log.Printf(&quot;stream accepted: %d&quot;, stream.StreamID()) if _, err := io.Copy(stream, stream); err != nil { return err } } }   When you press Send button in client example it creates a bidirectional stream, sends data to it, then closes stream. Thus our code is sufficient. For a more complex communication that involves many concurrent streams you will have to write a more complex code that allows working with streams concurrently on server side.    ","version":null,"tagName":"h3"},{"title":"Full server example​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#full-server-example","content":" Full server code can be found in a Gist. Again – this is a toy example based on things that all work in progress.  ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Experimenting with QUIC and WebTransport","url":"/blog/2020/10/16/experimenting-with-quic-transport#conclusion","content":" WebTransport is an interesting technology that can open new possibilities in modern Web development. At this moment it's possible to play with it using QUIC transport – here we looked at how one can do that. Though we still have to wait a bit until all these things will be suitable for production usage.  Also, even when ready we will still have to think about WebTransport fallback options – since wide adoption of browsers that support some new technology and infrastructure takes time. Actually WebTransport spec authors consider fallback options in design. This was mentioned in IETF slides (PDF, 2.6MB), but I have not found any additional information beyond that.  Personally, I think the most exciting thing about WebTransport is the possibility to exchange UDP datagrams, which can help a lot to in-browser gaming. Unfortunately, we can't test it at this moment with Go (but it's already possible using Python as server as shown in the example).  WebTransport could be a nice candidate for a new Centrifugo transport next to WebSocket and SockJS – time will show. ","version":null,"tagName":"h2"},{"title":"Scaling WebSocket in Go and beyond","type":0,"sectionRef":"#","url":"/blog/2020/11/12/scaling-websocket","content":"","keywords":"","version":null},{"title":"WebSocket server tasks​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#websocket-server-tasks","content":" Speaking about scalable servers that work with many persistent WebSocket connections – I found several important tasks such a server should be able to do:  Maintain many active connectionsSend many messages to clientsSupport WebSocket fallback to scale to every clientAuthenticate incoming connections and invalidate connectionsSurvive massive reconnect of all clients without loosing messages  note Of course not all of these points equally important in various situations.  Below we will look at some tips which relate to these points.    ","version":null,"tagName":"h2"},{"title":"WebSocket libraries​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#websocket-libraries","content":" In Go language ecosystem we have several libraries which can be used as a building block for a WebSocket server.  Package golang.org/x/net/websocket is considered deprecated.  The default choice in the community is gorilla/websocket library. Made by Gary Burd (who also gifted us an awesome Redigo package to communicate with Redis) – it's widely used, performs well, has a very good API – so in most cases you should go with it. Some people think that library not actively maintained at moment – but this is not quite true, it implements full WebSocket RFC, so actually it can be considered done.  In 2018 my ex-colleague Sergey Kamardin open-sourced gobwas/ws library. It provides a bit lower-level API than gorilla/websocket thus allows reducing RAM usage per connection and has nice optimizations for WebSocket upgrade process. It does not support WebSocket permessage-deflate compression but otherwise a good alternative you can consider using. If you have not read Sergey's famous post A Million WebSockets and Go – make a bookmark!  One more library is nhooyr/websocket. It's the youngest one and actively maintained. It compiles to WASM which can be a cool thing for someone. The API is a bit different from what gorilla/websocket offers, and one of the big advantages I see is that it solves a problem with a proper WebSocket closing handshake which is a bit hard to do right with Gorilla WebSocket.  You can consider all listed libraries except one from x/net for your project. Take a library, follow its examples (make attention to goroutine-safety of various API operations). Personally I prefer Gorilla WebSocket at moment since it's feature-complete and battle tested by tons of projects around Go world.  ","version":null,"tagName":"h2"},{"title":"OS tuning​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#os-tuning","content":" OK, so you have chosen a library and built a server on top of it. As soon as you put it in production the interesting things start happening.  Let's start with several OS specific key things you should do to prepare for many connections from WebSocket clients.  Every connection will cost you an open file descriptor, so you should tune a maximum number of open file descriptors your process can use. An errors like too many open files raise due to OS limit on file descriptors which is usually 256-1024 by default (see with ulimit -n on Unix). A nice overview on how to do this on different systems can be found in Riak docs. Wanna more connections? Make this limit higher.  Nice tip here is to limit a maximum number of connections your process can serve – making it less than known file descriptor limit:  // ulimit -n == 65535 if conns.Len() &gt;= 65500 { return errors.New(&quot;connection limit reached&quot;) } conns.Add(conn)   – otherwise you have a risk to not even able to look at pprof when things go bad. And you always need monitoring of open file descriptors.  You can also consider using netutil.LimitListener for this task, but don't forget to put pprof on another port with another HTTP server instance in this case.  Keep attention on Ephemeral ports problem which is often happens between your load balancer and your WebSocket server. The problem arises due to the fact that each TCP connection uniquely identified in the OS by the 4-part-tuple:  source ip | source port | destination ip | destination port   On balancer/server boundary you are limited in 65536 possible variants by default. But actually due to some OS limits and sockets in TIME_WAIT state the number is even less. A very good explanation and how to deal with it can be found in Pusher blog.  Your possible number of connections also limited by conntrack table. Netfilter framework which is part of iptables keeps information about all connections and has limited size for this information. See how to see its limits and instructions to increase in this article.  One more thing you can do is tune your network stack for performance. Do this only if you understand that you need it. Maybe start with this gist, but don't optimize without full understanding why you are doing this.  ","version":null,"tagName":"h2"},{"title":"Sending many messages​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#sending-many-messages","content":" Now let's speak about sending many messages. The general tips follows.  Make payload smaller. This is obvious – fewer data means more effective work on all layers. BTW WebSocket framing overhead is minimal and adds only 2-8 bytes to your payload. You can read detailed dedicated research in Dissecting WebSocket's Overhead article. You can reduce an amount of data traveling over network with permessage-deflate WebSocket extension, so your data will be compressed. Though using permessage-deflate is not always a good thing for server due to poor performance of flate, so you should be prepared for a CPU and RAM resource usage on server side. While Gorilla WebSocket has a lot of optimizations internally by reusing flate writers, overhead is still noticeable. The increase value heavily depends on your load profile.  Make less system calls. Every syscall will have a constant overhead, and actually in WebSocket server under load you will mostly see read and write system calls in your CPU profiles. An advice here – try to use client-server protocol that supports message batching, so you can join individual messages together.  Use effective message serialization protocol. Maybe use code generation for JSON to avoid extensive usage of reflect package done by Go std lib. Maybe use sth like gogo/protobuf package which allows to speedup Protobuf marshalling and unmarshalling. Unfortunately Gogo Protobuf is going through hard times at this moment. Try to serialize a message only once when sending to many subscribers.  Have a way to scale to several machines - more power, more possible messages. We will talk about this very soon.  ","version":null,"tagName":"h2"},{"title":"WebSocket fallback transport​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#websocket-fallback-transport","content":"   Even in 2020 there are still users which cannot establish connection with WebSocket server. Actually the problem mostly appears with browsers. Some users still use old browsers. But they have a choice – install a newer browser. Still, there could also be users behind corporate proxies. Employees can have a trusted certificate installed on their machine so company proxy can re-encrypt even TLS traffic. Also, some browser extensions can block WebSocket traffic.  One ready solution to this is Sockjs-Go library. This is a mature library that provides fallback transport for WebSocket. If client does not succeed with WebSocket connection establishment then client can use some of HTTP transports for client-server communication: EventSource aka Server-Sent Events, XHR-streaming, Long-Polling etc. The downside with those transports is that to achieve bidirectional communication you should use sticky sessions on your load balancer since SockJS keeps connection session state in process memory. We will talk about many instances of your WebSocket server very soon.  You can implement WebSocket fallback yourself, this should be simple if you have a sliding window message stream on your backend which we will discuss very soon.  Maybe look at GRPC, depending on application it could be better or worse than WebSocket – in general you can expect a better performance and less resource consumption from WebSocket for bidirectional communication case. My measurements for a bidirectional scenario showed 3x win for WebSocket (binary + GOGO protobuf) in terms of server CPU consumption and 4 times less RAM per connection. Though if you only need RPC then GRPC can be a better choice. But you need additional proxy to work with GRPC from a browser.  ","version":null,"tagName":"h2"},{"title":"Performance is not scalability​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#performance-is-not-scalability","content":" You can optimize client-server protocol, tune your OS, but at some point you won't be able to use only one process on one server machine. You need to scale connections and work your server does over different server machines. Horizontal scaling is also good for a server high availability. Actually there are some sort of real-time applications where a single isolated process makes sense - for example multiplayer games where limited number of players play independent game rounds.    As soon as you distribute connections over several machines you have to find a way to deliver a message to a certain user. The basic approach here is to publish messages to all server instances. This can work but this does not scale well. You need a sort of instance discovery to make this less painful.  Here comes PUB/SUB, where you can connect WebSocket server instances over central PUB/SUB broker. Clients that establish connections with your WebSocket server subscribe to topics (channels) in a broker, and as soon as you publish a message to that topic it will be delivered to all active subscribers on WebSocket server instances. If server node does not have interested subscriber then it won't get a message from a broker thus you are getting effective network communication.  Actually the main picture of this post illustrates exactly this architecture:    Let's think about requirements for a broker for real-time messaging application. We want a broker:  with reasonable performance and possibility to scalewhich maintains message order in topicscan support millions of topics, where each topic should be ephemeral and lightweight – topics can be created when user comes to application and removed after user goes awaypossibility to keep a sliding window of messages inside channel to help us survive massive reconnect scenario (will talk about this later below, can be a separate part from broker actually)  Personally when we talk about such brokers here are some options that come into my mind:  RabbitMQKafka or PulsarNats or Nats-StreamingTarantoolRedis  Sure there are more exist including libraries like ZeroMQ or nanomsg.  Below I'll try to consider these solutions for the task of making scalable WebSocket server facing many user connections from Internet.  If you are looking for unreliable at most once PUB/SUB then any of solutions mentioned above should be sufficient. Many real-time messaging apps are ok with at most once guarantee delivery.  If you don't want to miss messages then things are a bit harder. Let's try to evaluate these options for a task where application has lots of different topics from which it wants to receive messages with at least once guarantee (having a personal topic per client is common thing in applications). A short analysis below can be a bit biased, but I believe thoughts are reasonable enough. I did not found enough information on the internet about scaling WebSocket beyond a single server process, so I'll try to fill the gap a little based on my personal knowledge without pretending to be absolutely objective in these considerations.  In some posts on the internet about scaling WebSocket I saw advices to use RabbitMQ for PUB/SUB stuff in real-time messaging server. While this is a great messaging server, it does not like a high rate of queue bind and unbind type of load. It will work, but you will need to use a lot of server resources for not so big number of clients (imagine having millions of queues inside RabbitMQ). I have an example from my practice where RabbitMQ consumed about 70 CPU cores to serve real-time messages for 100k online connections. After replacing it with Redis keeping the same message delivery semantics we got only 0.3 CPU consumption on broker side.  Kafka and Pulsar are great solutions, but not for this task I believe. The problem is again in dynamic ephemeral nature of our topics. Kafka also likes a more stable configuration of its topics. Keeping messages on disk can be an overkill for real-time messaging task. Also your consumers on Kafka server should pull from millions of different topics, not sure how well it performs, but my thoughts at moment - this should not perform very well. Kafka itself scales perfectly, you will definitely be able to achieve a goal but resource usage will be significant. Here is a post from Trello where they moved from RabbitMQ to Kafka for similar real-time messaging task and got about 5x resource usage improvements. Note also that the more partitions you have the more heavy failover process you get.  Nats and Nats-Streaming. Raw Nats can only provide at most once guarantee. BTW recently Nats developers released native WebSocket support, so you can consider it for your application. Nats-Streaming server as broker will allow you to not lose messages. To be fair I don't have enough information about how well Nats-Streaming scales to millions of topics. An upcoming Jetstream which will be a part of Nats server can also be an interesting option – like Kafka it provides a persistent stream of messages for at least once delivery semantics. But again, it involves disk storage, a nice thing for backend microservices communication but can be an overkill for real-time messaging task.  Sure Tarantool can fit to this task well too. It's fast, im-memory and flexible. Some possible problems with Tarantool are not so healthy state of its client libraries, complexity and the fact that it's heavily enterprise-oriented. You should invest enough time to benefit from it, but this can worth it actually. See an article on how to do a performant broker for WebSocket applications with Tarantool.  Building PUB/SUB system on top of ZeroMQ will require you to build separate broker yourself. This could be an unnecessary complexity for your system. It's possible to implement PUB/SUB pattern with ZeroMQ and nanomsg without a central broker, but in this case messages without active subscribers on a server will be dropped on a consumer side thus all publications will travel to all server nodes.  My personal choice at moment is Redis. While Redis PUB/SUB itself provides at most once guarantee, you can build at least once delivery on top of PUB/SUB and Redis data structures (though this can be challenging enough). Redis is very fast (especially when using pipelining protocol feature), and what is more important – very predictable. It gives you a good understanding of operation time complexity. You can shard topics over different Redis instances running in HA setup - with Sentinel or with Redis Cluster. It allows writing LUA procedures with some advanced logic which can be uploaded over client protocol thus feels like ordinary commands. You can use Redis to keep sliding window event stream which gives you access to missed messages from a certain position. We will talk about this later.  OK, the end of opinionated thoughts here :)  Depending on your choice the implementation of your system will vary and will have different properties – so try to evaluate possible solutions based on your application requirements. Anyway, whatever broker will be your choice, try to follow this rules to build effective PUB/SUB system:  take into account message delivery guarantees of your system: at most once or at least once, ideally you should have an option to have both for different real-time features in your appmake sure to use one or pool of connections between your server and a broker, don't create new connection per each client or topic that comes to your WebSocket serveruse effective serialization format between your WebSocket server and broker  ","version":null,"tagName":"h2"},{"title":"Massive reconnect​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#massive-reconnect","content":"   Let's talk about one more problem that is unique for Websocket servers compared to HTTP. Your app can have thousands or millions of active WebSocket connections. In contract to stateless HTTP APIs your application is stateful. It uses push model. As soon as you deploying your WebSocket server or reload your load balancer (Nginx maybe) – connections got dropped and all that army of users start reconnecting. And this can be like an avalanche actually. How to survive?  First of all - use exponential backoff strategies on client side. I.e. reconnect with intervals like 1, 2, 4, 8, 16 seconds with some random jitter.  Turn on various rate limiting strategies on your WebSocket server, some of them should be turned on your backend load balancer level (like controlling TCP connection establishment rate), some are application specific (maybe limit an amount of requests from certain user).  One more interesting technique to survive massive reconnect is using JWT (JSON Web Token) for authentication. I'll try to explain why this can be useful.    As soon as your client start reconnecting you will have to authenticate each connection. In massive setups with many persistent connection this can be a very significant load on your Session backend. Since you need an extra request to your session storage for every client coming back. This can be a no problem for some infrastructures but can be really disastrous for others. JWT allows to reduce this spike in load on session storage since it can have all required authentication information inside its payload. When using JWT make sure you have chosen a reasonable JWT expiration time – expiration interval depends on your application nature and just one of trade-offs you should deal with as developer.  Don't forget about making an effective connection between your WebSocket server and broker – as soon as all clients start reconnecting you should resubscribe your server nodes to all topics as fast as possible. Use techniques like smart batching at this moment.  Let's look at a small piece of code that demonstrates this technique. Imagine we have a source channel from which we get items to process. We don’t want to process items individually but in batch. For this we wait for first item coming from channel, then try to collect as many items from channel buffer as we want without blocking and timeouts involved. And then process slice of items we collected at once. For example build Redis pipeline from them and send to Redis in one connection write call.  maxBatchSize := 50 for { select { case item := &lt;-sourceCh: batch := []string{item} loop: for len(batch) &lt; maxBatchSize { select { case item := &lt;-sourceCh: batch = append(batch, item) default: break loop } } // Do sth with collected batch of items. println(len(batch)) } }   Look at a complete example in a Go playground: https://play.golang.org/p/u7SAGOLmDke.  I also made a repo where I demonstrate how this technique together with Redis pipelining feature allows to fully utilize connection for a good performance https://github.com/FZambia/redigo-smart-batching.  Another advice for those who run WebSocket services in Kubernetes. Learn how your ingress behaves – for example Nginx ingress can reload its configuration on every change inside Kubernetes services map resulting into closing all active WebSocket connections. Proxies like Envoy don't have this behaviour, so you can reduce number of mass disconnections in your system. You can also proxy WebSocket without using ingress at all over configured WebSocket service NodePort.  ","version":null,"tagName":"h2"},{"title":"Message event stream benefits​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#message-event-stream-benefits","content":" Here comes a final part of this post. Maybe the most important one.  Not only mass client re-connections could create a significant load on a session backend but also a huge load on your main application database. Why? Because WebSocket applications are stateful. Clients rely on a stream of messages coming from a backend to maintain its state actual. As soon as connection dropped client tries to reconnect. In some scenarios it also wants to restore its actual state. What if client reconnected after 3 seconds? How many state updates it could miss? Nobody knows. So to make sure state is actual client tries to get it from application database. This is again a significant spike in load on your main database in massive reconnect scenario. In can be really painful with many active connections.  So what I think is nice to have for scenarios where we can't afford to miss messages (like in chat-like apps for example) is having effective and performant stream of messages inside each channel. Keep this stream in fast in-memory storage. This stream can have time retention and be limited in size (think about it as a sliding window of messages). I already mentioned that Redis can do this – it's possible to keep messages in Redis List or Redis Stream data structures. Other broker solutions could give you access to such a stream inside each channel out of the box.  So as soon as client reconnects it can restore its state from fast in-memory event stream without even querying your database. Actually to survive mass reconnect scenario you don't need to keep such a stream for a long time – several minutes should be enough. You can even create your own Websocket fallback implementation (like Long-Polling) utilizing event stream with limited retention.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Scaling WebSocket in Go and beyond","url":"/blog/2020/11/12/scaling-websocket#conclusion","content":" Hope advices given here will be useful for a reader and will help writing a more robust and more scalable real-time application backends.  Centrifugo server and Centrifuge library for Go language have most of the mechanics described here including the last one – message stream for topics limited by size and retention period. Both also have techniques to prevent message loss due to at most once nature of Redis PUB/SUB giving at least once delivery guarantee inside message history window size and retention period. ","version":null,"tagName":"h2"},{"title":"Centrifuge – real-time messaging with Go","type":0,"sectionRef":"#","url":"/blog/2021/01/15/centrifuge-intro","content":"","keywords":"","version":null},{"title":"How it's all started​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#how-its-all-started","content":" I wrote several blog posts before (for example this one – yep, it's on Medium...) about an original motivation of Centrifugo server.  danger Centrifugo server is not the same as Centrifuge library for Go. It's a full-featured project built on top of Centrifuge library. Naming can be confusing, but it's not too hard once you spend some time with ecosystem.  In short – Centrifugo was implemented to help traditional web frameworks dealing with many persistent connections (like WebSocket or SockJS HTTP transports). So frameworks like Django or Ruby on Rails, or frameworks from the PHP world could be used on a backend but still provide real-time messaging features like chats, multiplayer browser games, etc for users. With a little help from Centrifugo.  Now there are cases when Centrifugo server used in conjunction even with a backend written in Go. While Go mostly has no problems dealing with many concurrent connections – Centrifugo provides some features beyond simple message passing between a client and a server. That makes it useful, especially since design is pretty non-obtrusive and fits well microservices world. Centrifugo is used in some well-known projects (like ManyChat, Yoola.io, Spot.im, Badoo etc).  At the end of 2018, I released Centrifugo v2 based on a real-time messaging library for Go language – Centrifuge – the subject of this post.  It was a pretty hard experience to decouple Centrifuge out of the monolithic Centrifugo server – I was unable to make all the things right immediately, so Centrifuge library API went through several iterations where I introduced backward-incompatible changes. All those changes targeted to make Centrifuge a more generic tool and remove opinionated or limiting parts.  ","version":null,"tagName":"h2"},{"title":"So what is Centrifuge?​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#so-what-is-centrifuge","content":" This is ... well, a framework to build real-time messaging applications with Go language. If you ever heard about socket.io – then you can think about Centrifuge as an analogue. I think the most popular applications these days are chats of different forms, but I want to emphasize that Centrifuge is not a framework to build chats – it's a generic instrument that can be used to create different sorts of real-time applications – real-time charts, multiplayer games.  The obvious choice for real-time messaging transport to achieve fast and cross-platform bidirectional communication these days is WebSocket. Especially if you are targeting a browser environment. You mostly don't need to use WebSocket HTTP polyfills in 2021 (though there are still corner cases so Centrifuge supports SockJS polyfill).  Centrifuge has its own custom protocol on top of plain WebSocket or SockJS frames.  The reason why Centrifuge has its own protocol on top of underlying transport is that it provides several useful primitives to build real-time applications. The protocol described as strict Protobuf schema. It's possible to pass JSON or binary Protobuf-encoded data over the wire with Centrifuge.  note GRPC is very handy these days too (and can be used in a browser with a help of additional proxies), some developers prefer using it for real-time messaging apps – especially when one-way communication needed. It can be a bit better from integration perspective but more resource-consuming on server side and a bit trickier to deploy.  note Take a look at WebTransport – a brand-new spec for web browsers to allow fast communication between a client and a server on top of QUIC – it may be a good alternative to WebSocket in the future. This in a draft status at the moment, but it's already possible to play with in Chrome.  Own protocol is one of the things that prove the framework status of Centrifuge. This dictates certain limits (for example, you can't just use an alternative message encoding) and makes developers use custom client connectors on a front-end side to communicate with a Centrifuge-based server (see more about connectors in ecosystem part).  But protocol solves many practical tasks – and here we are going to look at real-time features it provides for a developer.  ","version":null,"tagName":"h2"},{"title":"Centrifuge Node​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#centrifuge-node","content":" To start working with Centrifuge you need to start Centrifuge server Node. Node is a core of Centrifuge – it has many useful methods – set event handlers, publish messages to channels, etc. We will look at some events and channels concept very soon.  Also, Node abstracts away scalability aspects, so you don't need to think about how to scale WebSocket connections over different server instances and still have a way to deliver published messages to interested clients.  For now, let's start a single instance of Node that will serve connections for us:  node, err := centrifuge.New(centrifuge.DefaultConfig) if err != nil { log.Fatal(err) } if err := node.Run(); err != nil { log.Fatal(err) }   It's also required to serve a WebSocket handler – this is possible just by registering centrifuge.WebsocketHandler in HTTP mux:  wsHandler := centrifuge.NewWebsocketHandler(node, centrifuge.WebsocketConfig{}) http.Handle(&quot;/connection/websocket&quot;, wsHandler)   Now it's possible to connect to a server (using Centrifuge connector for a browser called centrifuge-js):  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket'); centrifuge.connect();   Though connection will be rejected by the server since we also need to provide authentication details – Centrifuge expects explicitly provided connection Credentials to accept connection.  ","version":null,"tagName":"h2"},{"title":"Authentication​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#authentication","content":" Let's look at how we can tell Centrifuge details about connected user identity, so it could accept an incoming connection.  There are two main ways to authenticate client connection in Centrifuge.  The first one is over the native middleware mechanism. It's possible to wrap centrifuge.WebsocketHandler or centrifuge.SockjsHandler with middleware that checks user authentication and tells Centrifuge current user ID over context.Context:  func auth(h http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { cred := &amp;centrifuge.Credentials{ UserID: &quot;42&quot;, } newCtx := centrifuge.SetCredentials(r.Context(), cred) r = r.WithContext(newCtx) h.ServeHTTP(w, r) }) }   So WebsocketHandler can be registered this way (note that a handler now wrapped by auth middleware):  wsHandler := centrifuge.NewWebsocketHandler(node, centrifuge.WebsocketConfig{}) http.Handle(&quot;/connection/websocket&quot;, auth(wsHandler))   Another authentication way is a bit more generic – developers can authenticate connection based on custom token sent from a client inside first WebSocket/SockJS frame. This is called connect frame in terms of Centrifuge protocol. Any string token can be set – this opens a way to use JWT, Paceto, and any other kind of authentication tokens. For example see an authenticaton with JWT.  note BTW it's also possible to pass any information from client side with a first connect message from client to server and return custom information about server state to a client. This is out of post scope though.  Nothing prevents you to integrate Centrifuge with OAuth2 or another framework session mechanism – like Gin for example.  ","version":null,"tagName":"h2"},{"title":"Channel subscriptions​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#channel-subscriptions","content":" As soon as a client connected and successfully authenticated it can subscribe to channels. Channel (room or topic in other systems) is a lightweight and ephemeral entity in Centrifuge. Channel can have different features (we will look at some channel features below). Channels created automatically as soon as the first subscriber joins and destroyed as soon as the last subscriber left.  The application can have many real-time features – even on one app screen. So sometimes client subscribes to several channels – each related to a specific real-time feature (for example one channel for chat updates, one channel likes notification stream, etc).  Channel is just an ASCII string. A developer is responsible to find the best channel naming convention suitable for an application. Channel naming convention is an important aspect since in many cases developers want to authorize subscription to a channel on the server side – so only authorized users could listen to specific channel updates.  Let's look at a basic subscription example on the client-side:  centrifuge.subscribe('example', function(msgCtx) { console.log(msgCtx) })   On the server-side, you need to define subscribe event handler. If subscribe event handler not set then the connection won't be able to subscribe to channels at all. Subscribe event handler is where a developer may check permissions of the current connection to read channel updates. Here is a basic example of subscribe event handler that simply allows subscriptions to channel example for all authenticated connections and reject subscriptions to all other channels:  node.OnConnect(func(client *centrifuge.Client) { client.OnSubscribe(func(e centrifuge.SubscribeEvent, cb centrifuge.SubscribeCallback) { if e.Channel != &quot;example&quot; { cb(centrifuge.SubscribeReply{}, centrifuge.ErrorPermissionDenied) return } cb(centrifuge.SubscribeReply{}, nil) }) })   You may notice a callback style of reacting to connection related things. While not being very idiomatic for Go it's very practical actually. The reason why we use callback style inside client event handlers is that it gives a developer possibility to control operation concurrency (i.e. process sth in separate goroutines or goroutine pool) and still control the order of events. See an example that demonstrates concurrency control in action.  Now if some event published to a channel:  // Here is how we can publish data to a channel. node.Publish(&quot;example&quot;, []byte(`{&quot;input&quot;: &quot;hello&quot;}`))   – data will be delivered to a subscribed client, and message will be printed to Javascript console. PUB/SUB in its usual form.  note Though Centrifuge protocol based on Protobuf schema in example above we published a JSON message into a channel. By default, we can only send JSON to connections since default protocol format is JSON. But we can switch to Protobuf-based binary protocol by connecting to ws://localhost:8000/connection/websocket?format=protobuf endpoint – then it's possible to send binary data to clients.  ","version":null,"tagName":"h2"},{"title":"Async message passing​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#async-message-passing","content":" While Centrifuge mostly shines when you need channel semantics it's also possible to send any data to connection directly – to achieve bidirectional asynchronous communication, just what a native WebSocket provides.  To send a message to a server one can use the send method on the client-side:  centrifuge.send({&quot;input&quot;: &quot;hello&quot;});   On the server-side data will be available inside a message handler:  client.OnMessage(func(e centrifuge.MessageEvent) { log.Printf(&quot;message from client: %s&quot;, e.Data) })   And vice-versa, to send data to a client use Send method of centrifuge.Client:  client.Send([]byte(`{&quot;input&quot;: &quot;hello&quot;}`))   To listen to it on the client-side:  centrifuge.on('message', function(data) { console.log(data); });   ","version":null,"tagName":"h2"},{"title":"RPC​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#rpc","content":" RPC is a primitive for sending a request from a client to a server and waiting for a response (in this case all communication still happens via asynchronous message passing internally, but Centrifuge takes care of matching response data to request previously sent).  On client side it's as simple as:  const resp = await centrifuge.namedRPC('my_method', {});   On server side RPC event handler should be set to make calls available:  client.OnRPC(func(e centrifuge.RPCEvent, cb centrifuge.RPCCallback) { if e.Method == &quot;my_method&quot; { cb(centrifuge.RPCReply{Data: []byte(`{&quot;result&quot;: &quot;42&quot;}`)}, nil) return } cb(centrifuge.RPCReply{}, centrifuge.ErrorMethodNotFound) })   Note, that it's possible to pass the name of RPC and depending on it and custom request params return different results to a client – just like a regular HTTP request but over asynchronous WebSocket (or SockJS) connection.  ","version":null,"tagName":"h2"},{"title":"Server-side subscriptions​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#server-side-subscriptions","content":" In many cases, a client is a source of knowledge which channels it wants to subscribe to on a specific application screen. But sometimes you want to control subscriptions to channels on a server-side. This is also possible in Centrifuge.  It's possible to provide a slice of channels to subscribe connection to at the moment of connection establishment phase:  node.OnConnecting(func(ctx context.Context, e centrifuge.ConnectEvent) (centrifuge.ConnectReply, error) { return centrifuge.ConnectReply{ Subscriptions: map[string]centrifuge.SubscribeOptions{ &quot;example&quot;: {}, }, }, nil })   Note, that OnConnecting does not follow callback-style – this is because it can only happen once at the start of each connection – so there is no need to control operation concurrency.  In this case on the client-side you will have access to messages published to channels by listening to on('publish') event:  centrifuge.on('publish', function(msgCtx) { console.log(msgCtx); });   Also, centrifuge.Client has Subscribe and Unsubscribe methods so it's possible to subscribe/unsubscribe client to/from channel somewhere in the middle of its long WebSocket session.  ","version":null,"tagName":"h2"},{"title":"Windowed history in channel​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#windowed-history-in-channel","content":" Every time a message published to a channel it's possible to provide custom history options. For example:  node.Publish( &quot;example&quot;, []byte(`{&quot;input&quot;: &quot;hello&quot;}`), centrifuge.WithHistory(300, time.Minute), )   In this case, Centrifuge will maintain a windowed Publication cache for a channel - or in other words, maintain a publication stream. This stream will have time retention (one minute in the example above) and the maximum size will be limited to the value provided during Publish (300 in the example above).  Every message inside a history stream has an incremental offset field. Also, a stream has a field called epoch – this is a unique identifier of stream generation - thus client will have a possibility to distinguish situations where a stream is completely removed and there is no guarantee that no messages have been lost in between even if offset looks fine.  Client protocol provides a possibility to paginate over a stream from a certain position with a limit:  const streamPosition = {'offset': 0, epoch: 'xyz'} resp = await sub.history({since: streamPosition, limit: 10});   Iteration over history stream is a new feature which is just merged into Centrifuge master branch and can only be used from Javascript client at the moment.  Also, Centrifuge has an automatic message recovery feature. Automatic recovery is very useful in scenarios when tons of persistent connections start reconnecting at once. I already described why this is useful in one of my previous posts about Websocket scalability. In short – since WebSocket connections are stateful then at the moment of mass reconnect they can create a very big spike in load on your main application database. Such mass reconnects are a usual thing in practice - for example when you reload your load balancers or re-deploying the Websocket server (new code version).  Of course, recovery can also be useful for regular short network disconnects - when a user travels in the subway for example. But you always need a way to load an actual state from the main application database in case of an unsuccessful recovery.  To enable automatic recovery you can provide the Recover flag in subscribe options:  client.OnSubscribe(func(e centrifuge.SubscribeEvent, cb centrifuge.SubscribeCallback) { cb(centrifuge.SubscribeReply{ Options: centrifuge.SubscribeOptions{ Recover: true, }, }, nil) })   Obviously, recovery will work only for channels where history stream maintained. The limitation in recovery is that all missed publications sent to client in one protocol frame – pagination is not supported during recovery process. This means that recovery is mostly effective for not too long offline time without tons of missed messages.  ","version":null,"tagName":"h2"},{"title":"Online presence and presence stats​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#online-presence-and-presence-stats","content":" Another cool thing Centrifuge exposes to developers is online presence information for channels. Presence information contains a list of active channel subscribers. This is useful to show the online status of players in a game for example.  Also, it's possible to turn on Join/Leave message feature inside channels: so each time connection subscribes to a channel all channel subscribers receive a Join message with client information (client ID, user ID). As soon as the client unsubscribes Leave message is sent to remaining channel subscribers with information who left a channel.  Here is how to enable both online presence and join/leave features for a subscription to channel:  client.OnSubscribe(func(e centrifuge.SubscribeEvent, cb centrifuge.SubscribeCallback) { cb(centrifuge.SubscribeReply{ Options: centrifuge.SubscribeOptions{ Presence: true, JoinLeave: true, }, }, nil) })   On a client-side then it's possible to call for the presence and setting event handler for join/leave messages.  The important thing to be aware of when using Join/Leave messages is that this feature can dramatically increase CPU utilization and overall traffic in channels with a big number of active subscribers – since on every client connect/disconnect event such Join or Leave message must be sent to all subscribers. The advice here – avoid using Join/Leave messages or be ready to scale (Join/Leave messages scale well when adding more Centrifuge Nodes – more about scalability below).  One more thing to remember is that online presence information can also be pretty expensive to request in channels with many active subscribers – since it returns information about all connections – thus payload in response can be large. To help a bit with this situation Centrifuge has a presence stats client API method. Presence stats only contain two counters: the number of active connections in the channel and amount of unique users in the channel.  If you still need to somehow process online presence in rooms with a massive number of active subscribers – then I think you better do it in near real-time - for example with fast OLAP like ClickHouse.  ","version":null,"tagName":"h2"},{"title":"Scalability aspects​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#scalability-aspects","content":" To be fair it's not too hard to implement most of the features above inside one in-memory process. Yes, it takes time, but the code is mostly straightforward. When it comes to scalability things tend to be a bit harder.  Centrifuge designed with the idea in mind that one machine is not enough to handle all application WebSocket connections. Connections should scale over application backend instances, and it should be simple to add more application nodes when the amount of users (connections) grows.  Centrifuge abstracts scalability over the Node instance and two interfaces: Broker interface and PresenceManager interface.  A broker is responsible for PUB/SUB and streaming semantics:  type Broker interface { Run(BrokerEventHandler) error Subscribe(ch string) error Unsubscribe(ch string) error Publish(ch string, data []byte, opts PublishOptions) (StreamPosition, error) PublishJoin(ch string, info *ClientInfo) error PublishLeave(ch string, info *ClientInfo) error PublishControl(data []byte, nodeID string) error History(ch string, filter HistoryFilter) ([]*Publication, StreamPosition, error) RemoveHistory(ch string) error }   See full version with comments in source code.  Every Centrifuge Node subscribes to channels via a broker. This provides a possibility to scale connections over many node instances – published messages will flow only to nodes with active channel subscribers.  It's and important thing to combine PUB/SUB with history inside a Broker implementation to achieve an atomicity of saving message into history stream and publishing it to PUB/SUB with generated offset.  PresenceManager is responsible for online presence information management:  type PresenceManager interface { Presence(ch string) (map[string]*ClientInfo, error) PresenceStats(ch string) (PresenceStats, error) AddPresence(ch string, clientID string, info *ClientInfo, expire time.Duration) error RemovePresence(ch string, clientID string) error }   Full code with comments.  Broker and PresenceManager together form an Engine interface:  type Engine interface { Broker PresenceManager }   By default, Centrifuge uses MemoryEngine that does not use any external services but limits developers to using only one Centrifuge Node (i.e. one server instance). Memory Engine is fast and can be suitable for some scenarios - even in production (with configured backup instance) – but as soon as the number of connections grows – you may need to load balance connections to different server instances. Here comes the Redis Engine.  Redis Engine utilizes Redis for Broker and PresenceManager parts.  History cache saved to Redis STREAM or Redis LIST data structures. For presence, Centrifuge uses a combination of HASH and ZSET structures.  Centrifuge tries to fully utilize the connection between Node and Redis by using pipelining where possible and smart batching technique. All operations done in a single RTT with the help of Lua scripts loaded automatically to Redis on engine start.  Redis is pretty fast and will allow your app to scale to some limits. When Redis starts being a bottleneck it's possible to shard data over different Redis instances. Client-side consistent sharding is built-in in Centrifuge and allows scaling further.  It's also possible to achieve Redis's high availability with built-in Sentinel support. Redis Cluster supported too. So Redis Engine covers many options to communicate with Redis deployed in different ways.  At Avito we served about 800k active connections in the messenger app with ease using a slightly adapted Centrifuge Redis Engine, so an approach proved to be working for rather big applications. We will look at some more concrete numbers below in the performance section.  Both Broker and PresenceManager are pluggable, so it's possible to replace them with alternative implementations. Examples show how to use Nats server for at most once only PUB/SUB together with Centrifuge. Also, we have an example of full-featured Engine for Tarantool database – Tarantool Engine shows even better throughput for history and presence operations than Redis-based Engine (up to 10x for some ops).  ","version":null,"tagName":"h2"},{"title":"Order and delivery properties​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#order-and-delivery-properties","content":" Since Centrifuge is a messaging system I also want to describe its order and message delivery guarantees.  Message ordering in channels supported. As soon as you publish messages into channels one after another of course.  Message delivery model is at most once by default. This is mostly comes from PUB/SUB model – message can be dropped on Centrifuge level if subscriber is offline or simply on broker level – since Redis PUB/SUB also works with at most once guarantee.  Though if you maintain history stream inside a channel then things become a bit different. In this case you can tell Centrifuge to check client position inside stream. Since every publication has a unique incremental offset Centrifuge can track that client has correct offset inside a channel stream. If Centrifuge detects any missed messages it disconnects a client with special code – thus make it reconnect and recover messages from history stream. Since a message first saved to history stream and then published to PUB/SUB inside broker these mechanisms allow achieving at least once message delivery guarantee.    Even if stream completely expired or dropped from broker memory Centrifuge will give a client a tip that messages could be lost – so client has a chance to restore state from a main application database.  ","version":null,"tagName":"h2"},{"title":"Ecosystem​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#ecosystem","content":" Here I want to be fair with my readers – Centrifuge is not ideal. This is a project maintained mostly by one person at the moment with all consequences. This hits an ecosystem a lot, can make some design choices opinionated or non-optimal.  I mentioned in the first post that Centrifuge built on top of the custom protocol. The protocol is based on a strict Protobuf schema, works with JSON and binary data transfer, supports many features. But – this means that to connect to the Centrifuge-based server developers have to use custom connectors that can speak with Centrifuge over its custom protocol.  The difficulty here is that protocol is asynchronous. Asynchronous protocols are harder to implement than synchronous ones. Multiplexing frames allows achieving good performance and fully utilize a single connection – but it hurts simplicity.  At this moment Centrifuge has client connectors for:  centrifuge-js - Javascript client for a browser, NodeJS and React Nativecentrifuge-go - for Go languagecentrifuge-mobile - for mobile development based on centrifuge-go and gomobile projectcentrifuge-swift - for iOS native developmentcentrifuge-java - for Android native development and general Javacentrifuge-dart - for Dart and Flutter  Not all clients support all protocol features. Another drawback is that all clients do not have a persistent maintainer – I mostly maintain everything myself. Connectors can have non-idiomatic and pretty dumb code since I had no previous experience with mobile development, they lack proper tests and documentation. This is unfortunate.  The good thing is that all connectors feel very similar, I am quickly releasing new versions when someone sends a pull request with improvements or bug fixes. So all connectors are alive.  I maintain a feature matrix in connectors to let users understand what's supported. Actually feature support is pretty nice throughout all these connectors - there are only several things missing and not so much work required to make all connectors full-featured. But I really need help here.  It will be a big mistake to not mention Centrifugo as a big plus for Centrifuge library ecosystem. Centrifugo is a server deployed in many projects throughout the world. Many features of Centrifuge library and its connectors have already been tested by Centrifugo users.  One more thing to mention is that Centrifuge does not have v1 release. It still evolves – I believe that the most dramatic changes have already been made and backward compatibility issues will be minimal in the next releases – but can't say for sure.  ","version":null,"tagName":"h2"},{"title":"Performance​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#performance","content":" I made a test stand in Kubernetes with one million connections.  I can't call this a proper benchmark – since in a benchmark your main goal is to destroy a system, in my test I just achieved some reasonable numbers on limited hardware. These numbers should give a good insight into a possible throughput, latency, and estimate hardware requirements (at least approximately).  Connections landed on different server pods, 5 Redis instances have been used to scale connections between pods.  The detailed test stand description can be found in Centrifugo documentation.    Some quick conclusions are:  One connection costs about 30kb of RAMRedis broker CPU utilization increases linearly with more messages traveling around1 million connections with 500k delivered messages per second with 200ms delivery latency in 99 percentile can be served with hardware amount equal to one modern physical server machine. The possible amount of messages can vary a lot depending on the number of channel subscribers though.  ","version":null,"tagName":"h2"},{"title":"Limitations​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#limitations","content":" Centrifuge does not allow subscribing on the same channel twice inside a single connection. It's not simple to add due to design decisions made – though there was no single user report about this in seven years of Centrifugo/Centrifuge history.  Centrifuge does not support wildcard subscriptions. Not only because I never needed this myself but also due to some design choices made – so be aware of this.  SockJS fallback does not support binary data - only JSON. If you want to use binary in your application then you can only use WebSocket with Centrifuge - there is no built-in fallback transport in this case.  SockJS also requires sticky session support from your load balancer to emulate a stateful bidirectional connection with its HTTP fallback transports. Ideally, Centrifuge will go away from SockJS at some point, maybe when WebTransport becomes mature so users will have a choice between WebTransport or WebSocket.  Websocket permessage-deflate compression supported (thanks to Gorilla WebSocket), but it can be pretty expensive in terms of CPU utilization and memory usage – the overhead depends on usage pattern, it's pretty hard to estimate in numbers.  As said above you cannot only rely on Centrifuge for state recovery – it's still required to have a way to fully load application state from the main database.  Also, I am not very happy with current error and disconnect handling throughout the connector ecosystem – this can be improved though, and I have some ideas for the future.  ","version":null,"tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#examples","content":" I am adding examples to _examples folder of Centrifuge repo. These examples completely cover Centrifuge API - including things not mentioned here.  Check out the tips &amp; tricks section of README – it contains some additional insights about an implementation.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Centrifuge – real-time messaging with Go","url":"/blog/2021/01/15/centrifuge-intro#conclusion","content":" I think Centrifuge could be a nice alternative to socket.io - with a better performance, main server implementation in Go language, and even more builtin features to build real-time apps.  Centrifuge ecosystem definitely needs more work, especially in client connectors area, tutorials, community, stabilizing API, etc.  Centrifuge fits pretty well proprietary application development where time matters and deadlines are close, so developers tend to choose a ready solution instead of writing their own. I believe Centrifuge can be a great time saver here.  For Centrifugo server users Centrifuge package provides a way to write a more flexible server code adapted for business requirements but still use the same real-time core and have the same protocol features. ","version":null,"tagName":"h2"},{"title":"Centrifugo v3 released","type":0,"sectionRef":"#","url":"/blog/2021/08/31/hello-centrifugo-v3","content":"","keywords":"","version":null},{"title":"Centrifugo v2 flashbacks​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#centrifugo-v2-flashbacks","content":" Centrifugo v2 life cycle has come to an end. Before discussing v3 let's look back at what has been done during the last three years.  Centrifugo v2 was a pretty huge refactoring of v1. Since the v2 release, Centrifugo is built on top of new Centrifuge library for Go language. Centrifuge library evolved significantly since its initial release and now powers Grafana v8 real-time streaming among other things.  Here is an awesome demo made by my colleague Alexander Zobnin that demonstrates real-time telemetry of Assetto Corsa sports car streamed in real-time to Grafana dashboard:      Centrifugo integrated with Redis Streams, got Redis Cluster support, can now work with Nats server as a PUB/SUB broker. Notable additions of Centrifugo v2 were server-side subscriptions with some interesting features on top – like maintaining a single global connection from one user and automatic personal channel subscription upon user connect.  A very good addition which increased Centrifugo adoption a lot was introduction of proxy to backend. This made Centrifugo fit many setups where JWT authentication and existing subscription permission model did not suit well before.  Client ecosystem improved significantly. The fact that client protocol migrated to a strict Protobuf schema allowed to introduce binary protocol format (in addition to JSON) and simplify building client connectors. We now have much better and complete client libraries (compared to v1 situation).  We also have an official Helm chart, Grafana dashboard for Prometheus datasource, and so on.    Centrifugo is becoming more noticeable in a wider real-time technology community. For example, it was included in a periodic table of real-time created by Ably.com (one of the most powerful real-time messaging cloud services at the moment):    Of course, there are many aspects where Centrifugo can be improved. And v3 addresses some of them. Below we will look at the most notable features and changes of the new major Centrifugo version.  ","version":null,"tagName":"h3"},{"title":"Backwards compatibility​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#backwards-compatibility","content":" Let's start with the most important thing – backwards compatibility concerns.  In Centrifugo v3 client protocol mostly stayed the same. We expect that most applications will be able to update without any change on a client-side. This was an important concern for v3 given how painful the update cycle can be on mobile devices and lessons learned from v1 to v2 migration. There is one breaking change though which can affect users who use history API manually from a client-side (we provide a temporary workaround to give apps a chance to migrate smoothly).  On a server-side, much more changes happened, especially in the configuration: some options were renamed, some were removed. We provide a v2 to v3 configuration converter which can help dealing with changes. In most cases, all you should do is adapt Centrifugo configuration to match v3 changes and redeploy Centrifugo using v3 build instead of v2. All features are still there (or a replacement exists, like for channels API).  For more details, refer to the v3 migration guide.  ","version":null,"tagName":"h3"},{"title":"License change​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#license-change","content":" As some of you know we considered changing Centrifugo license to AGPL v3 for a new release. After thinking a lot about this we decided to not step into this area.  But the license has been changed: the license of OSS Centrifugo is now Apache 2.0 instead of MIT. Apache 2.0 is also a permissive OSS license, it's just a bit more concrete in some aspects.    ","version":null,"tagName":"h3"},{"title":"Unidirectional real-time transports​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#unidirectional-real-time-transports","content":" Server-side subscriptions introduced in Centrifugo v2 and recent improvements in the underlying Centrifuge library opened a road for a unidirectional approach.  This means that Centrifugo v3 provides a set of unidirectional real-time transports where messages flow only in one direction – from a server to a client. Why is this change important?  Centrifugo originally concentrated on using bidirectional transports for client-server communication. Like WebSocket and SockJS. Bidirectional transports allow implementing some great protocol features since a client can communicate with a server in various ways after establishing a persistent connection. While this is a great opportunity this also leads to an increased complexity.  Centrifugo users had to use special client connector libraries which abstracted underlying work into a simple public API. But internally connectors do many things: matching requests to responses, handling timeouts, handling an ordering, queuing operations, error handling. So the client connector is a pretty complex piece of software.  But what if a user just needs to receive real-time updates from a stable set of channels known in connection time? Can we simplify everything and avoid using custom software on a client-side?  With unidirectional transports, the answer is yes. Clients can now connect to Centrifugo using a bunch of unidirectional transports. And the greatest thing is that in this case, developers should not depend on Centrifugo client connectors at all – just use native browser APIs or GRPC-generated code. It's finally possible to consume events from Centrifugo using CURL (see an example).  Using unidirectional transports you can still benefit from Centrifugo built-in scalability with various engines, utilize built-in authentication over JWT or the connect proxy feature.  With subscribe server API (see below) it's even possible to subscribe unidirectional client to server-side channels dynamically. With refresh server API or the refresh proxy feature it's possible to manage a connection expiration.  Centrifugo supports the following unidirectional transports:  EventSource (SSE)HTTP streamingUnidirectional WebSocketUnidirectional GRPC stream  We expect that introducing unidirectional transports will significantly increase Centrifugo adoption.  ","version":null,"tagName":"h3"},{"title":"History iteration API​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#history-iteration-api","content":"   There was a rather important limitation of Centrifugo history API – it was not very suitable for keeping large streams because a call to a history could only return the entire channel history.  Centrifugo v3 introduces an API to iterate over a stream. It's possible to do from the current stream beginning or end, in both directions – forward and backward, with configured limit. Also with certain starting stream position if it's known.  This, among other things, can help to implement manual missed message recovery on a client-side to reduce the load on the application backend.  Here is an example program in Go which endlessly iterates over stream both ends (using gocent API library), upon reaching the end of stream the iteration goes in reversed direction (not really useful in real world but fun):  // Iterate by 10. limit := 10 // Paginate in reversed order first, then invert it. reverse := true // Start with nil StreamPosition, then fill it with value while paginating. var sp *gocent.StreamPosition for { historyResult, err = c.History( ctx, channel, gocent.WithLimit(limit), gocent.WithReverse(reverse), gocent.WithSince(sp), ) if err != nil { log.Fatalf(&quot;Error calling history: %v&quot;, err) } for _, pub := range historyResult.Publications { log.Println(pub.Offset, &quot;=&gt;&quot;, string(pub.Data)) sp = &amp;gocent.StreamPosition{ Offset: pub.Offset, Epoch: historyResult.Epoch, } } if len(historyResult.Publications) &lt; limit { // Got all pubs, invert pagination direction. reverse = !reverse log.Println(&quot;end of stream reached, change iteration direction&quot;) } }   caution This new API does not remove the need in having the main application database – that's still mandatory for idiomatic Centrifugo usage.  ","version":null,"tagName":"h3"},{"title":"Redis Streams by default​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#redis-streams-by-default","content":" In Centrifugo v3 Redis engine uses Redis Stream data structure by default for keeping channel history. Before v3 Redis Streams were supported by not enabled by default so almost nobody used them. This change is important in terms of introducing history iteration API described above – since Redis Streams allow doing iteration effectively.  ","version":null,"tagName":"h3"},{"title":"Tarantool engine​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#tarantool-engine","content":" As you may know, Centrifugo has several built-in engines that allow scaling Centrifugo nodes (using PUB/SUB) and keep shared history and presence state. Before v3 Centrifugo had in-memory and Redis (or KeyDB) engines available.  Introducing a new engine to Centrifugo is pretty hard since the engine should provide a very robust PUB/SUB performance, fast history and presence operations, possibility to publish a message to PUB/SUB and save to history atomically. It also should allow dealing with ephemeral frequently changing subscriptions. It's typical for Centrifugo use case to have millions of users each subscribed to a unique channel and constantly connecting/disconnecting (thus subscribing/unsubscribing).    In v3 we added experimental support for the Tarantool engine. It fits nicely all the requirements above and provides a huge performance speedup for history and presence operations compared to Redis. According to our benchmarks, the speedup can be up to 4-10x depending on operation. The PUB/SUB performance of Tarantool is comparable with Redis (10-20% worse according to our internal benchmarks to be exact, but that's pretty much the same).  For example, let's look at Centrifugo benchmark where we recover zero messages (i.e. emulate a situations when many connections disconnected for a very short time interval due to load balancer reload).  For Redis engine:  Redis engine, single Redis instance BenchmarkRedisRecover 26883 ns/op 1204 B/op 28 allocs/op   Compare it with the same operation measured with Tarantool engine:  Tarantool engine, single Tarantool instance BenchmarkTarantoolRecover 6292 ns/op 563 B/op 10 allocs/op   Tarantool can provide new storage properties (like synchronous replication), new adoption. We are pretty excited about adding it as an option.  The reason why Tarantool support is experimental is because Tarantool integration involves one more moving piece – the Centrifuge Lua module which should be run by a Tarantool server.  This increases deployment complexity and given the fact that many users have their own best practices in Tarantool deployment we are still evaluating a sufficient way to distribute Lua part. For now, we are targeting standalone (see examples in centrifugal/tarantool-centrifuge) and Cartridge Tarantool setups (with centrifugal/rotor).  Refer to the Tarantool Engine documentation for more details.  ","version":null,"tagName":"h3"},{"title":"GRPC proxy​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#grpc-proxy","content":" Centrifugo can now transform events received over persistent connections from users into GRPC calls to the application backend (in addition to the HTTP proxy available in v2).  GRPC support should make Centrifugo ready for today's microservice architecture where GRPC is a huge player for inter-service communication.  So we mostly just provide more choices for Centrifugo users here. GRPC has some good advantages – for example an application backend RPC layer which is responsible for communication with Centrifugo can now be generated from Protobuf definitions for all popular programming languages.  ","version":null,"tagName":"h3"},{"title":"Server API improvements​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#server-api-improvements","content":"   Centrifugo v3 has some valuable server API improvements.  The new subscribe API method allows subscribing connection to a channel at any point in time. This works by utilizing server-side subscriptions. So it's not only possible to subscribe connection to a list of server-side channels during the connection establishment phase – but also later during the connection lifetime. This may be very useful for the unidirectional approach - by emulating client-side subscribe call over request to application backend which in turn calls subscribe Centrifugo server API.  Publish API now returns the current top stream position (offset and epoch) for channels with history enabled.  Server history API inherited iteration possibilities described above.  Channels command now returns a number of clients in a channel, also supports channel filtering by a pattern. Since we changed how channels call implemented internally there is no limitation anymore to call it when using Redis cluster.  Admin web UI has been updated too to support new API methods, so you can play with new API from its actions tab.  ","version":null,"tagName":"h3"},{"title":"Better clustering​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#better-clustering","content":" Centrifugo behaves a bit better in cluster mode: as soon as a node leaves a cluster gracefully (upon graceful termination) it sends a shutdown signal to the control channel thus giving other nodes a chance to immediately delete that node from the local registry.  ","version":null,"tagName":"h3"},{"title":"Client improvements​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#client-improvements","content":" While preparing the v3 release we improved client connectors too. All existing client connectors now actualized to the latest protocol, support server-side subscriptions, history API.  One important detail is that it's not required to set ?format=protobuf URL param now when connecting to Centrifugo from mobile devices - this is now managed internally by using the WebSocket subprotocol mechanism (requires using the latest client connector version and Centrifugo v3).  ","version":null,"tagName":"h3"},{"title":"New documentation site​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#new-documentation-site","content":" You are reading this post on a new project site. It's built with amazing Docusaurus.  A lot of documents were actualized, extended, and rewritten. We also now have new chapters like:  Main highlightsDesign overviewHistory and recoveryError and disconnect codes.  Server API and proxy documentation have been improved significantly.  ","version":null,"tagName":"h3"},{"title":"Performance improvements​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#performance-improvements","content":"   Centrifugo v3 has some notable performance improvements.  JSON client protocol now utilizes a couple of libraries (easyjson for encoding and segmentio/encoding for unmarshaling). Actually we use a slightly customized version of easyjson library to achieve even faster performance than it provides out-of-the-box. Changes allowed to speed up JSON encoding and decoding up to 4-5x for small messages. For large payloads speed up can be even more noticeable – we observed up to 30x performance boost when serializing 5kb messages.  For example, let's look at a JSON serialization benchmark result for 256 byte payload. Here is what we had before:  Centrifugo v2 JSON encoding/decoding cpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz BenchmarkMarshal-12 5883 ns/op 1121 B/op 6 allocs/op BenchmarkMarshalParallel-12 1009 ns/op 1121 B/op 6 allocs/op BenchmarkUnmarshal-12 1717 ns/op 1328 B/op 16 allocs/op BenchmarkUnmarshalParallel-12 492.2 ns/op 1328 B/op 16 allocs/op   And what we have now with mentioned JSON optimizations:  Centrifugo v3 JSON encoding/decoding cpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz BenchmarkMarshal-12 461.3 ns/op 928 B/op 3 allocs/op BenchmarkMarshalParallel-12 250.6 ns/op 928 B/op 3 allocs/op BenchmarkUnmarshal-12 476.5 ns/op 136 B/op 3 allocs/op BenchmarkUnmarshalParallel-12 107.2 ns/op 136 B/op 3 allocs/op   tip Centrifugo Protobuf protocol is still faster than JSON for encoding/decoding on a server-side.  Of course, JSON encoding is only one part of Centrifugo – so you should not expect overall 4x performance improvement. But loaded setups should notice the difference and this should also be a good thing for reducing garbage collection pauses.  Centrifugo inherited a couple of other improvements from the Centrifuge library.  In-memory connection hub is now sharded – this should reduce lock contention between operations in different channels. In our artificial benchmarks we noticed a 3x better hub throughput, but in reality the benefit is heavily depends on the usage pattern.  Centrifugo now allocates less during message broadcasting to a large number of subscribers.  Also, an upgrade to Go 1.17 for builds results in ~5% performance boost overall, thanks to a new way of passing function arguments and results using registers instead of the stack introduced in Go 1.17.  ","version":null,"tagName":"h3"},{"title":"Centrifugo PRO​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#centrifugo-pro","content":" The final notable thing is an introduction of Centrifugo PRO. This is an extended version of Centrifugo built on top of the OSS version. It provides some unique features targeting business adopters.  Those who followed Centrifugo for a long time know that there were some attempts to make project development sustainable. Buy me a coffee and Opencollective approaches were not successful, during a year we got ~300$ of total contributions. While we appreciate these contributions a lot - this does not fairly justify a time spent on Centrifugo maintenance these days and does not allow bringing it to the next level. So here is an another attempt to monetize Centrifugo.  Centrifugo PRO details and features described here in docs. Let's see how it goes. We believe that a set of additional functionality can provide great advantages for both small and large-scale Centrifugo setups. PRO features can give useful insights on a system, protect from client API misusing, reduce server resource usage, and more.  PRO version will be released soon after Centrifugo v3 OSS.  ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Centrifugo v3 released","url":"/blog/2021/08/31/hello-centrifugo-v3#conclusion","content":" There are some other changes introduced in v3 but not mentioned here. The full list can be found in the release notes and the migration guide.  Hope we stepped into an exciting time of the v3 life cycle and many improvements will follow. Join our communities in Telegram and Discord if you have questions or want to follow Centrifugo development:     Enjoy Centrifugo v3, and let the Centrifugal force be with you.  Special thanks Special thanks to Anton Silischev for the help with v3 tests, examples and CI. To Leon Sorokin for the spinning CSS Centrifugo logo. To Michael Filonenko for the help with Tarantool. To German Saprykin for Dart magic. Thanks to the community members who tested out Centrifugo v3 beta, found bugs and sent improvements. Icons used here made by wanicon from www.flaticon.com ","version":null,"tagName":"h3"},{"title":"Centrifugo integration with NodeJS tutorial","type":0,"sectionRef":"#","url":"/blog/2021/10/18/integrating-with-nodejs","content":"","keywords":"","version":null},{"title":"What we are building​","type":1,"pageTitle":"Centrifugo integration with NodeJS tutorial","url":"/blog/2021/10/18/integrating-with-nodejs#what-we-are-building","content":" Not a super-cool app to be honest. Our goal here is to give a reader an idea how integration with Centrifugo could look like. There are many possible apps which could be built on top of this knowledge.  The end result here will allow application user to authenticate and once authenticated – connect to Centrifugo. Centrifugo will proxy connection requests to NodeJS backend and native ExpressJS session middleware will be used for connection authentication. We will also send some periodical real-time messages to a user personal channel.  The full source code of this tutorial located on Github. You can clone examples repo and run this demo by simply writing:  docker compose up   ","version":null,"tagName":"h2"},{"title":"Creating Express.js app​","type":1,"pageTitle":"Centrifugo integration with NodeJS tutorial","url":"/blog/2021/10/18/integrating-with-nodejs#creating-expressjs-app","content":" Start new NodeJS app:  npm init   Install dependencies:  npm install express express-session cookie-parser axios morgan   Create index.js file.  index.js const express = require('express'); const cookieParser = require(&quot;cookie-parser&quot;); const sessions = require('express-session'); const morgan = require('morgan'); const axios = require('axios'); const app = express(); const port = 3000; app.use(express.json()); const oneDay = 1000 * 60 * 60 * 24; app.use(sessions({ secret: &quot;this_is_my_secret_key&quot;, saveUninitialized: true, cookie: { maxAge: oneDay }, resave: false })); app.use(cookieParser()); app.use(express.urlencoded({ extended: true })) app.use(express.json()) app.use(express.static('static')); app.use(morgan('dev')); app.get('/', (req, res) =&gt; { if (req.session.userid) { res.sendFile('views/app.html', { root: __dirname }); } else res.sendFile('views/login.html', { root: __dirname }) }); app.listen(port, () =&gt; { console.log(`Example app listening at http://localhost:${port}`); });   Create login.html file in views folder:  views/login.html &lt;html&gt; &lt;body&gt; &lt;form action=&quot;/login&quot; method=&quot;post&quot;&gt; &lt;h2&gt;Login (username: demo-user, password: demo-pass)&lt;/h2&gt; &lt;div class=&quot;input-field&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot; id=&quot;username&quot; placeholder=&quot;Enter Username&quot;&gt; &lt;/div&gt; &lt;div class=&quot;input-field&quot;&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot; id=&quot;password&quot; placeholder=&quot;Enter Password&quot;&gt; &lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;Log in&quot;&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt;   Also create app.html file in views folder:  views/app.html &lt;html&gt; &lt;head&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;app.css&quot;&gt; &lt;script src=&quot;https://cdn.jsdelivr.net/gh/centrifugal/centrifuge-js@2.8.3/dist/centrifuge.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;a href='/logout'&gt;Click to logout&lt;/a&gt; &lt;/div&gt; &lt;div id=&quot;log&quot;&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt;   Make attention that we import centrifuge-js client here which abstracts away Centrifugo bidirectional WebSocket protocol.  Let's write an HTTP handler for login form:  index.js const myusername = 'demo-user' const mypassword = 'demo-pass' app.post('/login', (req, res) =&gt; { if (req.body.username == myusername &amp;&amp; req.body.password == mypassword) { req.session.userid = req.body.username; res.redirect('/'); } else { res.send('Invalid username or password'); } });   In this example we use hardcoded username and password for out single user. Of course in real app you will have a database with user credentials. But since our goal is only show integration with Centrifugo – we are skipping these hard parts here.  Also create a handler for a logout request:  index.js app.get('/logout', (req, res) =&gt; { req.session.destroy(); res.redirect('/'); });   Now if you run an app with node index.js you will see a login form using which you can authenticate. At this point this is a mostly convenient NodeJS application, let's add Centrifugo integration.  ","version":null,"tagName":"h2"},{"title":"Starting Centrifugo​","type":1,"pageTitle":"Centrifugo integration with NodeJS tutorial","url":"/blog/2021/10/18/integrating-with-nodejs#starting-centrifugo","content":" Run Centrifugo with config.json like this:  config.json { &quot;token_hmac_secret_key&quot;: &quot;secret&quot;, &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;password&quot;, &quot;admin_secret&quot;: &quot;my_admin_secret&quot;, &quot;api_key&quot;: &quot;my_api_key&quot;, &quot;allowed_origins&quot;: [ &quot;http://localhost:9000&quot; ], &quot;user_subscribe_to_personal&quot;: true, &quot;proxy_connect_endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;proxy_http_headers&quot;: [ &quot;Cookie&quot; ] }   I.e.:  ./centrifugo -c config.json   Create app.js file in static folder:  static/app.js function drawText(text) { const div = document.createElement('div'); div.innerHTML = text; document.getElementById('log').appendChild(div); } const centrifuge = new Centrifuge('ws://localhost:9000/connection/websocket'); centrifuge.on('connect', function () { drawText('Connected to Centrifugo'); }); centrifuge.on('disconnect', function () { drawText('Disconnected from Centrifugo'); }); centrifuge.on('publish', function (ctx) { drawText('Publication, time = ' + ctx.data.time); }); centrifuge.connect();   ","version":null,"tagName":"h2"},{"title":"Adding Nginx​","type":1,"pageTitle":"Centrifugo integration with NodeJS tutorial","url":"/blog/2021/10/18/integrating-with-nodejs#adding-nginx","content":" Since we are going to use native session auth of ExpressJS we can't just connect from localhost:3000 (where our NodeJS app is served) to Centrifugo running on localhost:8000 – browser won't send a Cookie header to Centrifugo in this case. Due to this reason we need a reverse proxy which will terminate a traffic from frontend and proxy requests to NodeJS process or to Centrifugo depending on URL path. In this case both browser and NodeJS app will share the same origin – so Cookie will be sent to Centrifugo in WebSocket Upgrade request.  tip Alternatively, we could also use JWT authentication of Centrifugo but that's a topic for another tutorial. Here we are using connect proxy feature for auth.  Nginx config will look like this:  server { listen 9000; server_name localhost; location / { proxy_pass http://localhost:3000; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } location /connection { proxy_pass http://localhost:8000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_cache_bypass $http_upgrade; } }   Run Nginx and open http://localhost:9000. After authenticating in app you should see an attempt to connect to a WebSocket endpoint. But connection will fail since we need to implement connect proxy handler in NodeJS app.  index.js app.post('/centrifugo/connect', (req, res) =&gt; { if (req.session.userid) { res.json({ result: { user: req.session.userid } }); } else res.json({ disconnect: { code: 1000, reason: &quot;unauthorized&quot;, reconnect: false } }); });   Restart NodeJS process and try opening an app again. Application should now successfully connect to Centrifugo.  ","version":null,"tagName":"h2"},{"title":"Send real-time messages​","type":1,"pageTitle":"Centrifugo integration with NodeJS tutorial","url":"/blog/2021/10/18/integrating-with-nodejs#send-real-time-messages","content":" Let's also periodically publish current server time to a client's personal channel. In Centrifugo configuration we set a user_subscribe_to_personal option which turns on automatic subscription to a personal channel for each connected user. We can use axios library and send publish API requests to Centrifugo periodically (according to API docs):  index.js const centrifugoApiClient = axios.create({ baseURL: `http://centrifugo:8000/api`, headers: { Authorization: `apikey my_api_key`, 'Content-Type': 'application/json', }, }); setInterval(async () =&gt; { try { await centrifugoApiClient.post('', { method: 'publish', params: { channel: '#' + myusername, // construct personal channel name. data: { time: Math.floor(new Date().getTime() / 1000), }, }, }); } catch (e) { console.error(e.message); } }, 5000);   After restarting NodeJS you should see periodical updates on application web page.  You can also log in into Centrifugo admin web UI http://localhost:8000 using password password - and play with other available server API from within web interface.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Centrifugo integration with NodeJS tutorial","url":"/blog/2021/10/18/integrating-with-nodejs#conclusion","content":" While not being super useful this example can help understanding core concepts of Centrifugo - specifically connect proxy feature and server API.  It's possible to use unidirectional Centrifugo transports instead of bidrectional WebSocket used here – in this case you can go without using centrifuge-js at all.  This application scales perfectly if you need to handle more connections – thanks to Centrifugo builtin PUB/SUB engines.  It's also possible to use client-side subscriptions, keep channel history cache, enable channel presence and more. All the power of Centrifugo is in your hands. ","version":null,"tagName":"h2"},{"title":"Centrifugo integration with Django – building a basic chat application","type":0,"sectionRef":"#","url":"/blog/2021/11/04/integrating-with-django-building-chat-application","content":"","keywords":"","version":null},{"title":"Why integrate Django with Centrifugo​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#why-integrate-django-with-centrifugo","content":" Why would Django developers want to integrate a project with Centrifugo for real-time messaging functionality? This is a good question especially since there is a popular Django Channels project which solves the same task.  I found several points which could be a good motivation:  Centrifugo is fast and scales well. We have an optimized Redis Engine with client-side sharding and Redis Cluster support. Centrifugo can also scale with KeyDB, Nats, or Tarantool. So it's possible to handle millions of connections distributed over different server nodes.Centrifugo provides a variety of features out-of-the-box – some of them are unique, especially for real-time servers that scale to many nodes. Check out our doc!With Centrifugo you don't need to rewrite the existing application to introduce real-time messaging features to your users.Centrifugo works as a separate service – so can be a universal tool in the developer's pocket, can migrate from one project to another, no matter what programming language or framework is used for business logic.  ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#prerequisites","content":" We assume that you are already familiar with basic Django concepts. If not take a look at the official Django tutorial first and then come back to this tutorial.  Also, make sure you read a bit about Centrifugo – introduction and quickstart tutorial.  We also assume that you have Django installed already.  One possible way to quickly install Django locally is to create virtualenv, activate it, and install Django:  python3 -m venv env . env/bin/activate pip install django   Alos, make sure you have Centrifugo v3 installed already.  This tutorial also uses Docker to run Redis. We use Redis as a Centrifugo engine – this allows us to have a scalable solution in the end. Using Redis is optional actually, Centrifugo uses a Memory engine by default (but it does not allow scaling Centrifugo nodes). We will also run Nginx with Docker to serve the entire app. Install Docker from its official website but I am sure you already have one.  ","version":null,"tagName":"h2"},{"title":"Creating a project​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#creating-a-project","content":" First, let's create a Django project.  From the command line, cd into a directory where you’d like to store your code, then run the following command:  django-admin startproject mysite   This will create a mysite directory in your current directory with the following contents:  ❯ tree mysite mysite ├── manage.py └── mysite ├── __init__.py ├── asgi.py ├── settings.py ├── urls.py └── wsgi.py   ","version":null,"tagName":"h2"},{"title":"Creating the chat app​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#creating-the-chat-app","content":" We will put the code for the chat server inside chat app.  Make sure you’re in the same directory as manage.py and type this command:  python3 manage.py startapp chat   That’ll create a directory chat, which is laid out like this:  ❯ tree chat chat ├── __init__.py ├── admin.py ├── apps.py ├── migrations │ └── __init__.py ├── models.py ├── tests.py └── views.py   For this tutorial, we will only be working with chat/views.py and chat/__init__.py. Feel free to remove all other files from the chat directory.  After removing unnecessary files, the chat directory should look like this:  ❯ tree chat chat ├── __init__.py └── views.py   We need to tell our project that the chat app is installed. Edit the mysite/settings.py file and add 'chat' to the INSTALLED_APPS setting. It’ll look like this:  # mysite/settings.py INSTALLED_APPS = [ 'chat', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', ]   ","version":null,"tagName":"h2"},{"title":"Add the index view​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#add-the-index-view","content":" We will now create the first view, an index view that lets you type the name of a chat room to join.  Create a templates directory in your chat directory. Within the templates directory, you have just created, create another directory called chat, and within that create a file called index.html to hold the template for the index view.  Your chat directory should now look like this:  ❯ tree chat chat ├── __init__.py ├── templates │ └── chat │ └── index.html └── views.py   Put the following code in chat/templates/chat/index.html:  chat/templates/chat/index.html &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;Select a chat room&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;center&quot;&gt; &lt;div class=&quot;input-wrapper&quot;&gt; &lt;input type=&quot;text&quot; id=&quot;room-name-input&quot; /&gt; &lt;/div&gt; &lt;div class=&quot;input-help&quot;&gt; Type a room name to &lt;a id=&quot;room-name-submit&quot; href=&quot;#&quot;&gt;JOIN&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;script&gt; const nameInput = document.querySelector('#room-name-input'); const nameSubmit = document.querySelector('#room-name-submit'); nameInput.focus(); nameInput.onkeyup = function (e) { if (e.keyCode === 13) { // enter, return nameSubmit.click(); } }; nameSubmit.onclick = function (e) { e.preventDefault(); var roomName = nameInput.value; if (!roomName) { return; } window.location.pathname = '/chat/room/' + roomName + '/'; }; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt;   Create the view function for the room view. Put the following code in chat/views.py:  chat/views.py from django.shortcuts import render def index(request): return render(request, 'chat/index.html')   To call the view, we need to map it to a URL - and for this, we need a URLconf.  To create a URLconf in the chat directory, create a file called urls.py. Your app directory should now look like this:  ❯ tree chat chat ├── __init__.py ├── templates │ └── chat │ └── index.html └── views.py └── urls.py   In the chat/urls.py file include the following code:  chat/urls.py from django.urls import path from . import views urlpatterns = [ path('', views.index, name='index'), ]   The next step is to point the root URLconf at the chat.urls module. In mysite/urls.py, add an import for django.conf.urls.include and insert an include() in the urlpatterns list, so you have:  mysite/urls.py from django.conf.urls import include from django.urls import path from django.contrib import admin urlpatterns = [ path('chat/', include('chat.urls')), path('admin/', admin.site.urls), ]   Let’s verify that the index view works. Run the following command:  python3 manage.py runserver   You’ll see the following output on the command line:  Watching for file changes with StatReloader Performing system checks... System check identified no issues (0 silenced). You have 18 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): admin, auth, contenttypes, sessions. Run 'python manage.py migrate' to apply them. October 21, 2020 - 18:49:39 Django version 3.1.2, using settings 'mysite.settings' Starting development server at http://localhost:8000/ Quit the server with CONTROL-C.   Go to http://localhost:8000/chat/ in your browser and you should see the a text input to provide a room name.  Type in &quot;lobby&quot; as the room name and press Enter. You should be redirected to the room view at http://localhost:8000/chat/room/lobby/ but we haven’t written the room view yet, so you’ll get a &quot;Page not found&quot; error page.  Go to the terminal where you ran the runserver command and press Control-C to stop the server.  ","version":null,"tagName":"h2"},{"title":"Add the room view​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#add-the-room-view","content":" We will now create the second view, a room view that lets you see messages posted in a particular chat room.  Create a new file chat/templates/chat/room.html. Your app directory should now look like this:  chat ├── __init__.py ├── templates │ └── chat │ ├── index.html │ └── room.html ├── urls.py └── views.py   Create the view template for the room view in chat/templates/chat/room.html:  chat/templates/chat/room.html &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;Chat Room&lt;/title&gt; &lt;script src=&quot;https://cdn.jsdelivr.net/gh/centrifugal/centrifuge-js@2.8.3/dist/centrifuge.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;ul id=&quot;chat-thread&quot; class=&quot;chat-thread&quot;&gt;&lt;/ul&gt; &lt;div class=&quot;chat-message&quot;&gt; &lt;input id=&quot;chat-message-input&quot; class=&quot;chat-message-input&quot; type=&quot;text&quot; autocomplete=&quot;off&quot; autofocus /&gt; &lt;/div&gt; {{ room_name|json_script:&quot;room-name&quot; }} &lt;script&gt; const roomName = JSON.parse(document.getElementById('room-name').textContent); const chatThread = document.querySelector('#chat-thread'); const messageInput = document.querySelector('#chat-message-input'); const centrifuge = new Centrifuge(&quot;ws://&quot; + window.location.host + &quot;/connection/websocket&quot;); centrifuge.on('connect', function (ctx) { console.log(&quot;connected&quot;, ctx); }); centrifuge.on('disconnect', function (ctx) { console.log(&quot;disconnected&quot;, ctx); }); const sub = centrifuge.subscribe('rooms:' + roomName, function (ctx) { const chatNewThread = document.createElement('li'); const chatNewMessage = document.createTextNode(ctx.data.message); chatNewThread.appendChild(chatNewMessage); chatThread.appendChild(chatNewThread); chatThread.scrollTop = chatThread.scrollHeight; }); centrifuge.connect(); messageInput.focus(); messageInput.onkeyup = function (e) { if (e.keyCode === 13) { // enter, return e.preventDefault(); const message = messageInput.value; if (!message) { return; } sub.publish({ 'message': message }); messageInput.value = ''; } }; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt;   Create the view function for the room view in chat/views.py:  chat/views.py from django.shortcuts import render def index(request): return render(request, 'chat/index.html') def room(request, room_name): return render(request, 'chat/room.html', { 'room_name': room_name })   Create the route for the room view in chat/urls.py:  # chat/urls.py from django.urls import path, re_path from . import views urlpatterns = [ path('', views.index, name='index'), re_path('room/(?P&lt;room_name&gt;[A-z0-9_-]+)/', views.room, name='room'), ]   Start the development server:  python3 manage.py runserver   Go to http://localhost:8000/chat/ in your browser and to see the index page.  Type in &quot;lobby&quot; as the room name and press enter. You should be redirected to the room page at http://localhost:8000/chat/lobby/ which now displays an empty chat log.  Type the message &quot;hello&quot; and press Enter. Nothing happens! In particular, the message does not appear in the chat log. Why?  The room view is trying to open a WebSocket connection with Centrifugo using the URL ws://localhost:8000/connection/websocket but we haven’t started Centrifugo to accept WebSocket connections yet. If you open your browser’s JavaScript console, you should see an error that looks like this:  WebSocket connection to 'ws://localhost:8000/connection/websocket' failed   And since port 8000 has already been allocated we will start Centrifugo at a different port actually.  ","version":null,"tagName":"h2"},{"title":"Starting Centrifugo server​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#starting-centrifugo-server","content":" As promised we will use Centrifugo with Redis engine. So first thing to do before running Centrifugo is to start Redis:  docker run -it --rm -p 6379:6379 redis:6   Then create a configuration file for Centrifugo:  { &quot;port&quot;: 8001, &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;redis://localhost:6379&quot;, &quot;allowed_origins&quot;: &quot;http://localhost:9000&quot;, &quot;proxy_connect_endpoint&quot;: &quot;http://localhost:8000/chat/centrifugo/connect/&quot;, &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:8000/chat/centrifugo/publish/&quot;, &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:8000/chat/centrifugo/subscribe/&quot;, &quot;proxy_http_headers&quot;: [&quot;Cookie&quot;], &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;rooms&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true, &quot;proxy_subscribe&quot;: true } ] }   And run Centrifugo with it like this:  centrifugo -c config.json   Let's describe some options we used here:  port - sets the port Centrifugo runs on since we are running everything on localhost we make it different (8001) from the port allocated for the Django server (8000).engine - as promised we are using Redis engine so we can easily scale Centrifigo nodes to handle lots of WebSocket connectionsredis_address allows setting Redis addressallowed_origins - we will connect from http://localhost:9000 so we need to allow itnamespaces – we are using rooms: prefix when subscribing to a channel, i.e. using Centrifugo rooms namespace. Here we define this namespace and tell Centrifigo to proxy subscribe and publish events for channels in the namespace.  tip It's a good practice to use different namespaces in Centrifugo for different real-time features as this allows enabling only required options for a specific task.  Also, config has some options related to Centrifugo proxy feature. This feature allows proxying WebSocket events to the configured endpoints. We will proxy three types of events:  Connect (called when a user establishes WebSocket connection with Centrifugo)Subscribe (called when a user wants to subscribe on a channel)Publish (called when a user tries to publish data to a channel)  ","version":null,"tagName":"h2"},{"title":"Adding Nginx​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#adding-nginx","content":" In Centrifugo config we set endpoints which we will soon implement inside our Django app. You may notice that the allowed origin has a URL with port 9000. That's because we want to proxy Cookie headers from a persistent connection established with Centrifugo to the Django app and need Centrifugo and Django to share the same origin (so browsers can send Django session cookies to Centrifugo).  While not used in this tutorial (we will use fake tutorial-user as user ID here) – this can be useful if you decide to authenticate connections using Django native sessions framework later. To achieve this we should also add Nginx with a configuration like this:  nginx.conf events { worker_connections 1024; } error_log /dev/stdout info; http { access_log /dev/stdout; server { listen 9000; server_name localhost; location / { proxy_pass http://host.docker.internal:8000; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } location /connection/websocket { proxy_pass http://host.docker.internal:8001; proxy_http_version 1.1; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_cache_bypass $http_upgrade; } } }   Start Nginx (replace the path to nginx.conf to yours):  docker run -it --rm -v /path/to/nginx.conf:/etc/nginx/nginx.conf:ro -p 9000:9000 --add-host=host.docker.internal:host-gateway nginx   Note that we are exposing port 9000 to localhost and use a possibility to use host.docker.internal host to communicate from inside Docker network with services which are running on localhost (on the host machine). See this answer on SO.  Open http://localhost:9000. Nginx should now properly proxy requests to Django server and to Centrifugo, but we still need to do some things.  ","version":null,"tagName":"h2"},{"title":"Implementing proxy handlers​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#implementing-proxy-handlers","content":" Well, now if you try to open a chat page with Nginx, Centrifugo, Django, and Redis running you will notice some errors in Centrifugo logs. That's because Centrifugo tries to proxy WebSocket connect events to Django to authenticate them but we have not created event handlers in Django yet. Let's fix this.  Extend chat/urls.py:  chat/urls.py from django.urls import path, re_path from . import views urlpatterns = [ path('', views.index, name='index'), re_path('room/(?P&lt;room_name&gt;[A-z0-9_-]+)/', views.room, name='room'), path('centrifugo/connect/', views.connect, name='connect'), path('centrifugo/subscribe/', views.subscribe, name='subscribe'), path('centrifugo/publish/', views.publish, name='publish'), ]   Extend chat/views.py:  chat/views.py from django.http import JsonResponse from django.views.decorators.csrf import csrf_exempt @csrf_exempt def connect(request): # In connect handler we must authenticate connection. # Here we return a fake user ID to Centrifugo to keep tutorial short. # More details about connect result format can be found in proxy docs: # https://centrifugal.dev/docs/server/proxy#connect-proxy logger.debug(request.body) response = { 'result': { 'user': 'tutorial-user' } } return JsonResponse(response) @csrf_exempt def publish(request): # In publish handler we can validate publication request initialted by a user. # Here we return an empty object – thus allowing publication. # More details about publish result format can be found in proxy docs: # https://centrifugal.dev/docs/server/proxy#publish-proxy response = { 'result': {} } return JsonResponse(response) @csrf_exempt def subscribe(request): # In subscribe handler we can validate user subscription request to a channel. # Here we return an empty object – thus allowing subscription. # More details about subscribe result format can be found in proxy docs: # https://centrifugal.dev/docs/server/proxy#subscribe-proxy response = { 'result': {} } return JsonResponse(response)   connect view will accept all connections and return user ID as tutorial-user. In real app you most probably want to use Django sessions and return real authenticated user ID instead of tutorial-user. Since we told Centrifugo to proxy connection Cookie headers native Django user authentication will work just fine.  Restart Django and try the chat app again. You should now successfully connect. Open a browser tab to the room page at http://localhost:9000/chat/room/lobby/. Open a second browser tab to the same room page.  In the second browser tab, type the message &quot;hello&quot; and press Enter. You should now see &quot;hello&quot; echoed in the chat log in both the second browser tab and in the first browser tab.  You now have a basic fully-functional chat server!  ","version":null,"tagName":"h2"},{"title":"What could be improved​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#what-could-be-improved","content":" The list is large, but it's fun to do. To name some possible improvements:  Replace tutorial-user used here with native Django session framework. We already proxying the Cookie header to Django from Centrifugo, so you can reuse native Django authentication. Only allow authenticated users to join rooms.Create Room model and add users to it – thus you will be able to check permissions inside subscribe and publish handlers.Create Message model to display chat history in Room.Replace Django devserver with something more suitable for production like Gunicorn.Check out Centrifugo possibilities like presence to display online users.Use cent Centrifugo HTTP API library to publish something to a user on behalf of a server. In this case you can avoid using publish proxy, publish messages to Django over convinient AJAX call - and then call Centrifugo HTTP API to publish message into a channel.You can replace connect proxy (which is an HTTP call from Centrifugo to Django on each connect) with JWT authentication. JWT authentication may result in a better application performance (since no additional proxy requests will be issued on connect). It can allow your Django app to handle millions of users on a reasonably small hardware and survive mass reconnects from all those users. More details can be found in Scaling WebSocket in Go and beyond blog post.Instead of using subscribe proxy you can put channel into connect proxy result or into JWT – thus using server-side subscriptions and avoid subscribe proxy HTTP call.  One more thing I'd like to note is that if you aim to build a chat application like WhatsApp or Telegram where you have a screen with list of chats (which can be pretty long!) you should not create a separate channel for each room. In this case using separate channel per room does not scale well and you better use personal channel for each user to receive all user-related messages. And as soon as message published to a chat you can send message to each participant's channel. In this case, take a look at Centrifugo broadcast API.  ","version":null,"tagName":"h2"},{"title":"Tutorial source code with docker-compose​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#tutorial-source-code-with-docker-compose","content":" The full example which can run by issuing a single docker compose up can be found on Github. It also has some CSS styles so that the chat looks like shown in the beginning.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Centrifugo integration with Django – building a basic chat application","url":"/blog/2021/11/04/integrating-with-django-building-chat-application#conclusion","content":" Here we implemented a basic chat app with Django and Centrifugo.  While a chat still requires work to be suitable for production this example can help understand core concepts of Centrifugo - specifically channel namespaces and proxy features.  It's possible to use unidirectional Centrifugo transports instead of bidirectional WebSocket used here – in this case, you can go without using centrifuge-js at all.  Centrifugo scales perfectly if you need to handle more connections – thanks to Centrifugo built-in PUB/SUB engines.  It's also possible to use server-side subscriptions, keep channel history cache, use JWT authentication instead of connect proxy, enable channel presence, and more. All the power of Centrifugo is in your hands.  Hope you enjoyed this tutorial. And let the Centrifugal force be with you!  Join our community channels in case of any questions left after reading this. ","version":null,"tagName":"h2"},{"title":"Building a multi-room chat application with Laravel and Centrifugo","type":0,"sectionRef":"#","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial","content":"","keywords":"","version":null},{"title":"Application overview​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#application-overview","content":" The result will look like this:  Sorry, your browser doesn't support embedded video.  For the backend, we are using Laravel (version 8.65) as one of the most popular PHP frameworks. Centrifugo v3 will accept WebSocket client connections. And we will implement an integration layer between Laravel and Centrifugo.  For CSS styles we are using recently released Bootstrap 5. Also, some vanilla JS instead of frameworks like React/Vue/whatever to make frontend Javascript code simple – so most developers out there could understand the mechanics.  We are also using a bit old-fashioned server rendering here where server renders templates for different room routes (URLs) – i.e. our app is not a SPA app – mostly for the same reasons: to keep example short and let reader focus on Centrifugo and Laravel integration parts.  To generate fake user avatars we are requesting images from https://robohash.org/ which can generate unique robot puctures based on some input string (username in our case). Robots like to chat with each other!              tip We also have some ideas on further possible app improvements at the end of this post.  ","version":null,"tagName":"h2"},{"title":"Why integrate Laravel with Centrifugo?​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#why-integrate-laravel-with-centrifugo","content":" Why would Laravel developers want to integrate a project with Centrifugo for real-time messaging functionality? That's a good question. There are several points which could be a good motivation:  Centrifugo is open-source and self-hosted. So you can run it on your own infrastructure. Popular Laravel real-time broadcasting intergrations (Pusher and Ably) are paid cloud solutions. At scale Centrifugo will cost you less than cloud solutions. Of course cloud solutions do not require additional server setup – but everything is a trade-off right? So you should decide for youself.Centrifugo is fast and scales well. It has an optimized Redis Engine with client-side sharding and Redis Cluster support. Centrifugo can also scale with KeyDB, Nats, or Tarantool. So it's possible to handle millions of connections distributed over different Centrifugo nodes.Centrifugo provides a variety of features out-of-the-box – some of them are unique, especially for self-hosted real-time servers that scale to many nodes (like fast message history cache, or maintaining single user connection, both client-side and server-side subscriptions, etc).Centrifugo is lightweight, single binary server which works as a separate service – it can be a universal tool in the developer's pocket, can migrate with you from one project to another, no matter what programming language or framework is used for business logic.  Hope this makes sense as a good motivation to give Centrifugo a try in your Laravel project. Let's get started!  ","version":null,"tagName":"h2"},{"title":"Setup and start a project​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#setup-and-start-a-project","content":" For the convenience of working with the example, we wrapped the end result into docker compose.  To start the app clone examples repo, cd into v3/php_laravel_chat_tutorial directory and run:  docker compose up   At the first launch, the necessary images will be downloaded (will take some time and network bytes). When the main service is started, you should see something like this in container logs:  ... app | Database seeding completed successfully. app | [10-Dec-2021 12:25:05] NOTICE: fpm is running, pid 112 app | [10-Dec-2021 12:25:05] NOTICE: ready to handle connections   Then go to http://localhost/ – you should see:    Register (using some fake credentials) or sign up – and proceed to the chat rooms.  Pay attention to the configuration of Centrifugo and Nginx. Also, on entrypoint which does some things:  dependencies are installed via composercopying settings from .env.exampledb migrations are performed and the necessary npm packages are installedphp-fpm starts  ","version":null,"tagName":"h2"},{"title":"Application structure​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#application-structure","content":" We assume you already familar with Laravel concepts, so we will just point you to some core aspects of the Laravel application structure and will pay more attention to Centrifugo integration parts.  ","version":null,"tagName":"h2"},{"title":"Environment settings​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#environment-settings","content":" After the first launch of the application, all settings will be copied from the file .env.example to .env. Next, we will take a closer look at some settings.  ","version":null,"tagName":"h3"},{"title":"Database migrations and models​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#database-migrations-and-models","content":" You can view the database structure here.  We will use the following tables which will be then translated to the application models:  Laravel standard user authentication tables. See https://laravel.com/docs/8.x/authentication. In the service we are using Laravel Breeze. For more information see official docs.rooms table. Basically - describes different rooms in the app every user can create.rooms many-to-many relation to users. Allows to add users into rooms when join button clicked or automatically upon room creation.messages. Keeps message history in rooms.  ","version":null,"tagName":"h3"},{"title":"Broadcasting​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#broadcasting","content":" For broadcasting we are using laravel-centrifugo library. It helps to simplify interaction between Laravel and Centrifugo by providing some convenient wrappers.  Step-by-step configuration can be viewed in the readme file of this library.  Pay attention to the CENTRIFUGO_API_KEY setting. It is used to send API requests from Laravel to Centrifugo and must match in .env and centrifugo.json files. And we also telling laravel-centrifugo the URL of Centrifugo. That's all we need to configure for this example app.  See more information about Laravel broadcasting here.  tip As an alternative to laravel-centrifugo, you can use phpcent – it's an official generic API client which allows publishing to Centrifugo HTTP API. But it does know nothing about Laravel broadcasting specifics.  ","version":null,"tagName":"h3"},{"title":"Interaction with Centrifugo​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#interaction-with-centrifugo","content":" When user opens a chat app it connects to Centrifugo over WebSocket transport.  Let's take a closer look at Centrifugo server configuration file we use for this example app:  { &quot;port&quot;: 8000, &quot;engine&quot;: &quot;memory&quot;, &quot;api_key&quot;: &quot;some-long-api-key-which-you-should-keep-secret&quot;, &quot;allowed_origins&quot;: [ &quot;http://localhost&quot;, ], &quot;proxy_connect_endpoint&quot;: &quot;http://nginx/centrifugo/connect/&quot;, &quot;proxy_http_headers&quot;: [ &quot;Cookie&quot; ], &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot; } ] }   This configuration defines a connect proxy endpoint which is targeting Nginx and then proxied to Laravel. Centrifugo will proxy Cookie header of WebSocket HTTP Upgrade requests to Laravel – this allows using native Laravel authentication.  We also defined a &quot;personal&quot; namespace – we will subscribe each user to a personal channel in this namespace inside connect proxy handler. Using namespaces for different real-time features is one of Centrifugo best-practices.  Allowed origins must be properly set to prevent cross-site WebSocket connection hijacking.  ","version":null,"tagName":"h3"},{"title":"Connect proxy controller​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#connect-proxy-controller","content":" To use native Laravel user authentication middlewares, we will use Centrifugo proxy feature.  When user connects to Centrifugo it's connection attempt will be transformed into HTTP request from Centrifugo to Laravel and will hit the connect proxy controller:  class CentrifugoProxyController extends Controller { public function connect() { return new JsonResponse([ 'result' =&gt; [ 'user' =&gt; (string) Auth::user()-&gt;id, 'channels' =&gt; [&quot;personal:#&quot;.Auth::user()-&gt;id], ] ]); } }   This controller protected by auth middleware.  Since Centrifugo proxies Cookie header of initial WebSocket HTTP Upgrade request Laravel auth layer will work just fine. So in a controller you already has access to the current authenticated user.  In the response from controller we tell Centrifugo the ID of connecting user and subscribe user to its personal channel (using user-limited channel feature of Centrifugo). Returning a channel in such way will subscribe user to it using server-side subscriptions mechanism.  tip Note, that in our chat app we are using a single personal channel for each user to receive real-time updates from all rooms. We are not creating separate subscriptions for each room user joined too. This will allow us to scale more easily in the future, and basically the only viable solution in case of room list pagination in chat application like this. It does not mean you can not combine personal user channels and separate room channels for different tasks though. Some additional tips can be found in Centrifugo FAQ.  ","version":null,"tagName":"h3"},{"title":"Room controller​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#room-controller","content":" In RoomController we perform various actions with rooms:  displaying roomscreate roomsjoin users to roomspublish messages  When we publish a message in a room, we send a message to the personal channel of all users joined to the room using the broadcast method of Centrifugo API. It allows publishing the same message into many channels.  $message = Message::create([ 'sender_id' =&gt; Auth::user()-&gt;id, 'message' =&gt; $requestData[&quot;message&quot;], 'room_id' =&gt; $id, ]); $room = Room::with('users')-&gt;find($id); $channels = []; foreach ($room-&gt;users as $user) { $channels[] = &quot;personal:#&quot; . $user-&gt;id; } $this-&gt;centrifugo-&gt;broadcast($channels, [ &quot;text&quot; =&gt; $message-&gt;message, &quot;createdAt&quot; =&gt; $message-&gt;created_at-&gt;toDateTimeString(), &quot;roomId&quot; =&gt; $id, &quot;senderId&quot; =&gt; Auth::user()-&gt;id, &quot;senderName&quot; =&gt; Auth::user()-&gt;name, ]);   We also add some fields to the published message which will be used when dynamically displaying a message coming from a WebSocket connection (see Client side below).  ","version":null,"tagName":"h3"},{"title":"Client side​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#client-side","content":" Our chat is basically a one page with some variations dependng on the current route. So we use a single view for the entire chat app.  On the page we have a form for creating rooms. The user who created the room automatically joins it upon creation. Other users need to join manually (using join button in the room).  When sending a message (using the chat room message input), we make an AJAX request that hits RoomController shown above. A message saved into the database and then broadcasted to all users who joined this room. Here is a code that processes sending on ENTER:  messageInput.onkeyup = function(e) { if (e.keyCode === 13) { e.preventDefault(); const message = messageInput.value; if (!message) { return; } const xhttp = new XMLHttpRequest(); xhttp.open(&quot;POST&quot;, &quot;/rooms/&quot; + roomId + &quot;/publish&quot;); xhttp.setRequestHeader(&quot;X-CSRF-TOKEN&quot;, csrfToken); xhttp.send(JSON.stringify({ message: message })); messageInput.value = ''; } };   After the message is processed on the server and broadcasted to Centrifugo it instantly comes to client-side. To receive the message we are connecting to Centrifugo WebSocket endpoint and wait for a message in the publish event handler:  const url = &quot;ws://&quot; + window.location.host + &quot;/connection/websocket&quot;; const centrifuge = new Centrifuge(url); centrifuge.on('connect', function(ctx) { console.log(&quot;connected to Centrifugo&quot;, ctx); }); centrifuge.on('disconnect', function(ctx) { console.log(&quot;disconnected from Centrifugo&quot;, ctx); }); centrifuge.on('publish', function(ctx) { if (ctx.data.roomId.toString() === currentRoomId) { addMessage(ctx.data); scrollToLastMessage(); } addRoomLastMessage(ctx.data); }); centrifuge.connect();   We are using centrifuge-js client connector library to communicate with Centrifugo. This client abstracts away bidirectional asynchronous protocol complexity for us providing a simple way to listen connect, disconnect events and communicate with a server in various ways.  In publish event handler we check whether the message belongs to the room the user is currently in. If yes, then we add it to the message history of the room. We also add this message to the room in the list on the left as the last chat message in room. If necessary, we crop the text for normal display.  tip In our example we only subscribe each user to a single channel, but user can be subscribed to several server-side channels. To distinguish between them use ctx.channel inside publish event handler.  And that's it! We went through all the main parts of the integration.  ","version":null,"tagName":"h3"},{"title":"Possible improvements​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#possible-improvements","content":" As promised, here is a list with several possible app improvements:  Transform to a single page app, use productive Javascript frameworks like React or VueJS instead of vanilla JS.Add message read statuses - as soon as one of the chat participants read the message mark it read in the database.Introduce user-to-user chats.Support pagination for the message history, maybe for chat room list also.Don't show all rooms in the system – add functionality to search room by name.Horizontal scaling (using multiple nodes of Centrifugo, for example with Redis Engine) – mostly one line in Centrifugo config if you have Redis running.Gracefully handle temporary disconnects by loading missed messages from the database or Centrifugo channel history cache.Optionally replace connect proxy with JWT authentication to reduce HTTP calls from Centrifugo to Laravel. This may drastically reduce resources for Laravel backend at scale.Try using Centrifugo RPC proxy feature to use WebSocket connection for message publish instead of issuing AJAX request.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Building a multi-room chat application with Laravel and Centrifugo","url":"/blog/2021/12/14/laravel-multi-room-chat-tutorial#conclusion","content":" We built a chat app with Laravel and Centrifugo. While there is still an area for improvements, this example is not really the basic. It's already valuable in the current form and may be transformed into part of your production system with minimal tweaks.  Hope you enjoyed this tutorial. If you have any questions after reading – join our community channels. We touched only part of Centrifugo concepts here – take a look at detailed Centrifugo docs nearby. And let the Centrifugal force be with you! ","version":null,"tagName":"h2"},{"title":"Centrifugo v4 released – a little revolution","type":0,"sectionRef":"#","url":"/blog/2022/07/19/centrifugo-v4-released","content":"","keywords":"","version":null},{"title":"Centrifugo v3 flashbacks​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#centrifugo-v3-flashbacks","content":" Let's start from looking back a bit. Centrifugo v3 was released last year. It had a great list of improvements – like unidirectional transports support (EventSource, HTTP-streaming and GRPC), GRPC transport for proxy, history iteration API, faster JSON protocol, super-fast but experimental Tarantool engine implementation, and others.  During the Centrifugo v3 lifecycle we added even more JSON protocol optimizations and introduced a granular proxy mode. Experimental Tarantool engine has also evolved a bit.  But Centrifugo v3 did not contain anything... let's say revolutional. Revolutional for Centrifugo itself, community, or even the entire field of open-source real-time messaging.  With this release, we feel that we bring innovation to the ecosystem. Now let's talk about it and introduce all the major things of the brand new v4 release.    ","version":null,"tagName":"h2"},{"title":"Unified client SDK API​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#unified-client-sdk-api","content":" The most challenging part of Centrifugo project is not a server itself. Client SDKs are the hardest part of the ecosystem. We try to time additional improvements to the SDKs with each major release of the server. But this time the SDKs are the centerpiece of the v4 release.  Centrifugo uses bidirectional asynchronous protocol between client and server. On top of this protocol SDK provides a request-response over an asynchronous connection, reconnection logic, subscription management and multiplexing, timeout and error handling, ping-pong, token refresh, etc. Some of these things are not that trivial to implement. And all this should be implemented in different programming languages. As you may know, we have official real-time SDKs in Javascript, Dart, Swift, Java and Go.  While implementing the same protocol and same functions, all SDKs behaved slightly differently. That was the result of the missing SDK specification. Without a strict SDK spec, it was hard to document things, hard to explain the exact details of the real-time SDK behavior. What we did earlier in the Centrifugo documentation – was pointing users to specific SDK Github repo to look for behaviour details.  The coolest thing about Centrifugo v4 is the next generation SDK API. We now have a client SDK API specification. It's a source of truth for SDKs behavior which try to follow the spec closely.  The new SDK API is the result of several iterations and reflections on possible states, transitions, token refresh mechanism, etc. Users in our Telegram group may remember how it all started:    And after several iterations these prototypes turned into working mechanisms with well-defined behaviour:    A few things that have been revised from the ground up:  Client states, transitions, eventsSubscription states, transitions, eventsConnection and subscription token refresh behaviorPing-pong behavior (see details below)Resubscribe logic (SDKs can now resubscribe with backoff)Error handlingUnified backoff behavior (based on full jitter technique)  We now also have a separation between temporary and non-temporary protocol errors – this allows us to handle subscription internal server errors on the SDK level, making subscriptions more resilient, with automatic resubscriptions, and to ensure individual subscription failures do not affect the entire connection.  The mechanics described in the client SDK API specification are now implemented in all of our official SDKs. The SDKs now support all major client protocol features that currently exist. We believe this is a big step forward for the Centrifugo ecosystem and community.  ","version":null,"tagName":"h2"},{"title":"Modern WebSocket emulation in Javascript​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#modern-websocket-emulation-in-javascript","content":" WebSocket is supported almost everywhere these days. But there is a case that we believe is the last one preventing users to connect over WebSocket - corporate proxies. With the root certificate installed on employee computer machines, these proxies can block WebSocket traffic, even if it's wrapped in a TLS layer. That's really annoying, and often developers choose to not support clients connecting from such &quot;broken&quot; environments at all.  Prior to v4, Centrifugo users could use the SockJS polyfill library to fill this gap.  SockJS is great software – stable and field proven. It is still used by some huge real-time messaging players out there to polyfill the WebSocket transport.  But SockJS is an extra frontend dependency with a bunch of legacy transports, and the future of it is unknown.  SockJS comes with a notable overhead – it's an aditional protocol wrapper, consumes more memory per connection on a server (at least when using SockJS-Go library – the only choice for implementing SockJS server in Go language these days). When using SockJS, Centrifugo users were losing the ability to use our main pure WebSocket transport because SockJS uses its own WebSocket implementation on a server side.  SockJS does not support binary data transfer – only JSON format can be used with it. As you know, our main WebSocket transport works fine with binary in case of using Protobuf protocol format. So with SockJS we don't have fallback for WebSocket with a binary data transfer.  And finally, if you want to use SockJS with a distributed backend, you must enable sticky session support on the load-balancer level. This way you can point requests from the client to the server to the correct server node – the one which maintains a persistent unidirectional HTTP connection.  We danced around the idea of replacing SockJS for a long time. But only now we are ready to provide our alternative to it – meet Centrifugo own bidirectional emulation layer. It's based on two additional transports:  HTTP-streaming (using modern browser ReadableStream API in JavaScript, supports both binary Protobuf and JSON transfer)Eventsource (Server-Sent Events, SSE) – while a bit older choice and works with JSON only EventSource transport is loved by many developers and can provide fallback in slightly older browsers which don't have ReadableStream, so we implemented bidirectional emulation with it too.  So when the fallback is used, you always have a real-time, persistent connection in server -&gt; to -&gt; client direction. Requests in client -&gt; to -&gt; server direction are regular HTTP – similar to how SockJS works. But our bidirectional emulation layer does not require sticky sessions – Centrifugo can proxy client-to-server requests to the correct node in the cluster. Having sticky sessions is an optimization for Centrifugo bidirectional emulation layer, not a requirement. We believe that this is a game changer for our users – no need to bother about proper load balancing, especially since in most cases 95% or even more users will be able to connect using the WebSocket transport.  Here is a simplified diagram of how it works:    The bidirectional emulation layer is only supported by the Javascript SDK (centrifuge-js) – as we think fallbacks mostly make sense for browsers. If we find use cases where other SDKs can benefit from HTTP based transport – we can expand on them later.  Let's look at example of using this feature from the Javascript side. To use fallbacks, all you need to do is to set up a list of desired transports with endpoints:  const transports = [ { transport: 'websocket', endpoint: 'wss://your_centrifugo.com/connection/websocket' }, { transport: 'http_stream', endpoint: 'https://your_centrifugo.com/connection/http_stream' }, { transport: 'sse', endpoint: 'https://your_centrifugo.com/connection/sse' } ]; const centrifuge = new Centrifuge(transports); centrifuge.connect()   note We are using explicit transport endpoints in the above example due to the fact that transport endpoints can be configured separately in Centrifugo – there is no single entry point for all transports. Like the one in Socket.IO or SockJS when developer can only point client to the base address. In Centrifugo case, we are requesting an explicit transport/endpoint configuration from the SDK user.  By the way, a few advantages of HTTP-based transport over WebSocket:  Sessions can be automatically multiplexed within a single connection by the browser when the server is running over HTTP/2, while with WebSocket browsers open a separate connection in each browser tabBetter compression support (may be enabled on load balancer level)WebSocket requires special configuration in some load balancers to get started (ex. Nginx)  SockJS is still supported by Centrifugo and centrifuge-js, but it's now DEPRECATED.  ","version":null,"tagName":"h2"},{"title":"No layering in client protocol​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#no-layering-in-client-protocol","content":" Not only the API of client SDK has changed, but also the format of Centrifugo protocol messages. New format is more human-readable (in JSON case, of course), has a more compact ping message size (more on that below).  The client protocol is now one-shot encode/decode compatible. Previously, Centrifugo protocol had a layered structure and we had to encode some messages before appending them to the top-level message. Or decode two or three times to unwrap the message envelope. To achieve good performance when encoding and decoding client protocol messages, Centrifugo had to use various optimization techniques – like buffer memory pools, byte slice memory pools.  By restructuring the message format, we were able to avoid layering, which allowed us to slightly increase the performance of encoding/decoding without additional optimization tricks.    We also simplified the client protocol documentation overview a bit.  ","version":null,"tagName":"h2"},{"title":"Redesigned PING-PONG​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#redesigned-ping-pong","content":" In many cases in practice (when dealing with persistent connections like WebSocket), pings and pongs are the most dominant types of messages passed between client and server. Your application may have many concurrent connections, but only a few of them receive the useful payload. But at the same time, we still need to send pings and respond with pongs. Thus, optimizing the ping-pong process can significantly reduce server resource usage.  One optimization comes from the revised PING-PONG behaviour. Previous versions of Centrifugo and SDKs sent ping/pong in both &quot;client-&gt;to-&gt;server&quot; and &quot;server-&gt;to-&gt;client&quot; directions (for WebSocket transport). This allowed finding non-active connections on both client and server sides.  In Centrifugo v4 we only send pings from a server to a client and expect pong from a client. On the client-side, we have a timer which fires if there hasn't been a ping from the server within the configured time, so we still have a way to detect closed connections.  Sending pings only in one direction results in 2 times less ping-pong messages - and this should be really noticable for Centrifugo installations with thousands of concurrent connections. In our experiments with 10k connections, server CPU usage was reduced by 30% compared to Centrifugo v3.    Pings and pongs are application-level messages. Ping is just an empty asynchronous reply – for example in JSON case it's a 2-byte message: {}. Pong is an empty command – also, {} in JSON case. Having application-level pings from the server also allows unifying the PING format for all unidirectional transports.  Another improvement is that Centrifugo now randomizes the time it sends first ping to the client (but no longer than the configured ping interval). This allows to spread ping-pongs in time, providing a smoother CPU profile, especially after a massive reconnect scenario.  ","version":null,"tagName":"h2"},{"title":"Secure by default channel namespaces​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#secure-by-default-channel-namespaces","content":" Data security and privacy are more important than ever in today's world. And as Centrifugo becomes more popular and widely used, the need to be secure by default only increases.  Previously, by default, clients could subcribe to all channels in a namespace (except private channels, which are now revised – see details below). It was possible to use &quot;protected&quot;: true option to make namespace protected, but we are not sure if everyone did that. This is extra configuration and additional knowledge on how Centrifugo works.  Also, a common confusion we ran into: if server-side subscriptions were dictated by a connection JWT, many users would expect client-side subscriptions to those channels to not work. But without the protected option enabled, this was not the case.  In Centrifugo v4, by default, it is not possible to subscribe to a channel in a namespace. The namespace must be configured to allow subscriptions from clients, or token authorization must be used. There are a bunch of new namespace options to tune the namespace behavior. Also the ability to provide a regular expression for channels in the namespace.  The new permission-related channel option names better reflect the purpose of the option. For example, compare &quot;publish&quot;: true and &quot;allow_publish_for_client&quot;: true. The second one is more readable and provides a better understanding of the effect once turned on.  Centrifugo is now more strict when checking channel name. Only ASCII symbols allowed – it was already mentioned in docs before, but wasn't actually enforced. Now we are fixing this.  We understand that these changes will make running Centrifugo more of a challenge, especially when all you want is a public access to all the channels without worrying too much about permissions. It's still possible to achieve, but now the intent must be expicitly expressed in the config.  Check out the updated documentation about channels and namespaces. Our v4 migration guide contains an automatic converter for channel namespace options.  ","version":null,"tagName":"h2"},{"title":"Private channel concept revised​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#private-channel-concept-revised","content":" A private channel is a special channel starting with $ that could not be subscribed to without a subscription JWT. Prior to v4, having a known prefix allowed us to distinguish between public channels and private channels. But since namespaces are now non-public by default, this distinction is not really important.  This means 2 things:  it's now possible to subscribe to any channel by having a valid subscription JWT (not just those that start with $)channels beginning with $ can only be subscribed with a subscription JWT, even if they belong to a namespace where subscriptions allowed for all clients. This is for security compatibility between v3 and v4.  Another notable change in a subscription JWT – client claim is now DEPRECATED. There is no need to put it in the subscription token anymore. Centrifugo supports it only for backwards compatibility, but it will be completely removed in the future releases.  The reason we're removing client claim is actually interesting. Due to the fact that client claim was a required part of the subscription JWT applications could run into a situation where during the massive reconnect scenario (say, million connections reconnect) many requests for new subscription tokens can be generated because the subscription token must contain the client ID generated by Centrifugo for the new connection. That could make it unusually hard for the application backend to handle the load. With a connection JWT we had no such problem – as connections could simply reuse the previous token to reconnect to Centrifugo.  Now the subscription token behaves just like the connection token, so we get a scalable solution for token-based subscriptions as well.  What's more, this change paved the way for another big improvement...  ","version":null,"tagName":"h2"},{"title":"Optimistic subscriptions​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#optimistic-subscriptions","content":" The improvement we just mentioned is called optimistic subscriptions. If any of you are familiar with the QUIC protocol, then optimistic subscriptions are somewhat similar to the 0-RTT feature in QUIC. The idea is simple – we can include subscription commands to the first frame sent to the server.  Previously, we sent subscriptions only after receiving a successful Connect Reply to a Connect Command from a server. But with the new changes in token behaviour, it seems so logical to put subscribe commands within the initial connect frame. Especially since Centrifugo protocol always supported batching of commands. Even token-based subscriptions can now be included into the initial frame during reconnect process, since the previous token can be reused now.    The benefit is awesome – in most scenarios, we save one RTT of latency when connecting to Centrifugo and subscribing to channels (which is actually the most common way to use Centrifugo). While not visible on localhost, this is pretty important in real-life. And this is less syscalls for the server after all, resulting in less CPU usage.  Optimistic subscriptions are also great for bidirectional emulation with HTTP, as they avoid the long path of proxying a request to the correct Centrifugo node when connecting.  Optimistic subscriptions are now only part of centrifuge-js. At some point, we plan to roll out this important optimization to all other client SDKs.  ","version":null,"tagName":"h2"},{"title":"Channel capabilities​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#channel-capabilities","content":" The channel capabilities feature is introduced as part of Centrifugo PRO. Initially, we aimed to make it a part of the OSS version. But the lack of feedback on this feature made us nervous it's really needed. So adding it to PRO, where we still have room to evaluate the idea, seemed like the safer decision at the moment.  Centrifugo allows configuring channel permissions on a per-namespace level. When creating a new real-time feature, it is recommended to create a new namespace for it and configure permissions. But to achieve a better channel permission control within a namespace the Channel capabilities can be used now.  The channel capability feature provides a possibility to set capabilities on an individual connection basis, or an individual channel subscription basis.  For example, in a connection JWT developers can set sth like:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;, &quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;] } ] }   And this tells Centrifugo that the connection is able to subscribe on channels news or user_42 using client-side subscriptionsat any time while the connection is active. Centrifugo also supports wildcard and regex channel matches.  Subscription JWT can provide capabilities for the channel too, so permissions may be controlled on an individual subscription basis, ex. the ability to publish and call history API may be expressed with allow claim in subscription JWT:  { &quot;allow&quot;: [&quot;pub&quot;, &quot;hst&quot;] }   Read more about this mechanism in Channel capabilities chapter.  ","version":null,"tagName":"h2"},{"title":"Better connections API​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#better-connections-api","content":" Another addition to Centrifugo PRO is the improved connection API. Previously, we could only return all connections from a specific user.  The API now supports filtering all connections: by user ID, by subscribed channel, by additional meta information attached to the connection.  The filtering works by user ID or with a help of CEL expressions (Common Expression Language). CEL expressions provide a developer-friendly, fast and secure (as they are not Turing-complete) way to evaluate some conditions. They are used in some Google services (ex. Firebase), in Envoy RBAC configuration, etc. If you've never seen it before – take a look, cool project. We are also evaluating how to use CEL expressions for a dynamic and efficient channel permission checks, but that's an early story.  The connections API call result contains more useful information: a list of client's active channels, information about the tokens used to connect and subscribe, meta information attached to the connection.  ","version":null,"tagName":"h2"},{"title":"Javascript client moved to TypeScript​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#javascript-client-moved-to-typescript","content":" It's no secret that centrifuge-js is the most popular SDK in the Centrifugo ecosystem. We put additional love to it – and centrifuge-js is now fully written in Typescript ❤️  This was a long awaited improvement, and it finally happened! The entire public API is strictly typed. The cool thing is that even EventEmitter events and event handlers are the subject to type checks - this should drastically simplify and speedup development and also help to reduce error possibility.  ","version":null,"tagName":"h2"},{"title":"Experimenting with HTTP/3​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#experimenting-with-http3","content":" Centrifugo v4 has an experimental HTTP/3 support. Once TLS is enabled and &quot;http3&quot;: true option is set all the endpoints on an external port will be served by a HTTP/3 server based on lucas-clemente/quic-go implementation.  It's worth noting that WebSocket will still use HTTP/1.1 for its Upgrade request (there is an interesting IETF draft BTW about Bootstrapping WebSockets with HTTP/3). But HTTP-streaming and EventSource should work just fine with HTTP/3.  HTTP/3 does not currently work with our ACME autocert TLS - i.e. you need to explicitly provide paths to cert and key files as described here.  ","version":null,"tagName":"h2"},{"title":"Experimenting with WebTransport​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#experimenting-with-webtransport","content":" Having HTTP/3 on board allowed us to make one more thing. Some of you may remember the post Experimenting with QUIC and WebTransport published in our blog before. We danced around the idea to add WebTransport to Centrifugo since then. WebTransport IETF specification is still a draft, it changed a lot since our first blog post about it. But WebTransport object is already part of Chrome (since v97) and things seem to be very close to the release.  So we added experimental WebTransport support to Centrifugo v4. This is made possible with the help of the marten-seemann/webtransport-go library.  To use WebTransport you need to run HTTP/3 experimental server and enable WebTransport endpoint with &quot;webtransport&quot;: true option in the configuration. Then you can connect to that endpoint using centrifuge-js. For example, let's enable WebTransport and use WebSocket as a fallback option:  const transports = [ { transport: 'webtransport', endpoint: 'https://your_centrifugo.com/connection/webtransport' }, { transport: 'websocket', endpoint: 'wss://your_centrifugo.com/connection/websocket' } ]; const centrifuge = new Centrifuge(transports); centrifuge.connect()   Note, that we are using secure schemes here – https:// and wss://. While in WebSocket case you could opt for non-TLS communication, in HTTP/3 and specifically WebTransport non-TLS communication is simply not supported by the specification.  In Centrifugo case, we utilize the bidirectional reliable stream of WebTransport to pass our protocol between client and server. Both JSON and Protobuf communication formats are supported. There are some issues with the proper passing of the disconnect advice in some cases, otherwise it's fully functional.  Obviously, due to the limited WebTransport support in browsers at the moment, possible breaking changes in the WebTransport specification we can not recommended it for production usage for now. At some point in the future, it may become a reasonable alternative to WebSocket, now we are more confident that Centrifugo will be able to provide a proper support of it.  ","version":null,"tagName":"h2"},{"title":"Migration guide​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#migration-guide","content":" The migration guide contains steps to upgrade your Centrifugo from version 3 to version 4. While there are many changes in the v4 release, it should be possible to migrate to Centrifugo v4 without changing the code on the client side at all. And then, after updating the server, gradually update the client-side to the latest version of the stack.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#conclusion","content":"   To sum it up, here are some benefits of Centrifugo v4:  unified experience thoughout application frontend environmentsan optimized protocol which is generally faster, more compact and human-readable in JSON case, provides more resilient behavior for subscriptionsrevised channel namespace security model, more granular permission controlmore efficient and flexible use of subscription tokensbetter initial latency – thanks to optimistic subscriptions and the ability to pre-create subscription tokens (as the client claim not needed anymore)the ability to use more efficient WebSocket bidirectional emulation in the browser without having to worry about sticky sessions, unless you want to optimize the real-time infrastructure  That's it. We now begin the era of v4 and it is going to be awesome, no doubt.  ","version":null,"tagName":"h2"},{"title":"Join community​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#join-community","content":" The release contains many changes that strongly affect developing with Centrifugo. And of course you may have some questions or issues regarding new or changed concepts. Join our communities in Telegram (the most active) and Discord:     Enjoy Centrifugo v4, and let the Centrifugal force be with you.  ","version":null,"tagName":"h2"},{"title":"Special thanks​","type":1,"pageTitle":"Centrifugo v4 released – a little revolution","url":"/blog/2022/07/19/centrifugo-v4-released#special-thanks","content":" The refactoring of client SDKs and introducing unified behavior based on the common spec was the hardest part of Centrifugo v4 release. Many thanks to Vitaly Puzrin (who is the author of several popular open-source libraries such as markdown-it, fontello, and others). We had a series of super productive sessions with him on client SDK API design. Some great ideas emerged from these sessions and the result seems like a huge step forward for Centrifugal projects.  Also, thanks to Anton Silischev who helped a lot with WebTransport prototypes earlier this year, so we could quickly adopt WebTransport for v4.  tip As some of you know, Centrifugo server is built on top of the Centrifuge library for Go. Most of the optimizations and improvements described here are now also part of Centrifuge library. With its new unified SDK behavior and bidirectional emulation layer, it seems a solid alternative to Socket.IO in the Go language ecosystem. In some cases, Centrifuge library can be a more flexible solution than Centrifugo, since Centrifugo (as a standalone server) dictates some mechanics and rules that must be followed. In the case of Centrifugo, the business logic must live on the application backend side, with Centrifuge library it can be kept closer to the real-time transport layer.  Attributions This post used images from freepik.com: background by liuzishan. Also image by kenshinstock. ","version":null,"tagName":"h2"},{"title":"101 ways to subscribe user on a personal channel in Centrifugo","type":0,"sectionRef":"#","url":"/blog/2022/07/29/101-way-to-subscribe","content":"","keywords":"","version":null},{"title":"Setup​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#setup","content":" To make the post a bit easier to consume let's setup some things. Let's assume that the user for which we provide all the examples in this post has ID &quot;17&quot;. Of course in real-life the examples given here can be extrapolated to any user ID.  When you create a real-time connection to Centrifugo the connection is authenticated using the one of the following ways:  using connection JWTusing connection request proxy from Centrifugo to the configured endpoint of the application backend (connect proxy)  As soon as the connection is successfully established and authenticated Centrifugo knows the ID of connected user. This is important to understand.  And let's define a namespace in Centrifugo configuration which will be used for personal user channels:  { ... &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot;, &quot;presence&quot;: true } ] }   Defining namespaces for each new real-time feature is a good practice in Centrifugo. As an awesome improvement we also enabled presence in the personal namespace, so whenever users subscribe to a channel in this namespace Centrifugo will maintain online presence information for each channel. So you can find out all connections of the specific user existing at any moment. Defining presence is fully optional though - turn it of if you don't need presence information and don't want to spend additional server resources on maintaining presence.  ","version":null,"tagName":"h2"},{"title":"1 – user-limited channel​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#1--user-limited-channel","content":" tip Probably the most performant approach.  All you need to do is to extend namespace configuration with allow_user_limited_channels option:  { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot;, &quot;presence&quot;: true, &quot;allow_user_limited_channels&quot;: true } ] }   On the client side you need to have sth like this (of course the ID of current user will be dynamic in real-life):  const sub = centrifuge.newSubscription('personal:#17'); sub.on('publication', function(ctx) { console.log(ctx.data); }) sub.subscribe();   Here you are subscribing to a channel in personal namespace and listening to publications coming from a channel. Having # in channel name tells Centrifugo that this is a user-limited channel (because # is a special symbol that is treated in a special way by Centrifugo as soon as allow_user_limited_channels enabled).  In this case the user ID part of user-limited channel is &quot;17&quot;. So Centrifugo allows user with ID &quot;17&quot; to subscribe on personal:#17 channel. Other users won't be able to subscribe on it.  To publish updates to subscription all you need to do is to publish to personal:#17 using server publish API (HTTP or GRPC).  ","version":null,"tagName":"h2"},{"title":"2 - channel token authorization​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#2---channel-token-authorization","content":" tip Probably the most flexible approach, with reasonably good performance characteristics.  Another way we will look at is using subscription JWT for subscribing. When you create Subscription object on the client side you can pass it a subscription token, and also provide a function to retrieve subscription token (useful to automatically handle token refresh, it also handles initial token loading).  const token = await getSubscriptionToken('personal:17'); const sub = centrifuge.newSubscription('personal:17', { token: token }); sub.on('publication', function(ctx) { console.log(ctx.data); }) sub.subscribe();   Inside getSubscriptionToken you can issue a request to the backend, for example in browser it's possible to do with fetch API.  On the backend side you know the ID of current user due to the native session mechanism of your app, so you can decide whether current user has permission to subsribe on personal:17 or not. If yes – return subscription JWT according to our rules. If not - return empty string so subscription will go to unsubscribed state with unauthorized reason.  Here are examples for generating subscription HMAC SHA-256 JWTs for channel personal:17 and HMAC secret key secret:      PythonNodeJS import jwt import time claims = { &quot;sub&quot;: &quot;17&quot;, &quot;channel&quot;: &quot;personal:17&quot; &quot;exp&quot;: int(time.time()) + 30*60 } token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   Since we set expiration time for subscription JWT tokens we also need to provide a getToken function to a client on the frontend side:  const sub = centrifuge.newSubscription('personal:17', { getToken: async function (ctx) { const token = await getSubscriptionToken('personal:17'); return token; } }); sub.on('publication', function(ctx) { console.log(ctx.data); }) sub.subscribe();   This function will be called by SDK automatically to refresh subscription token when it's going to expire. And note that we omitted setting token option here – since SDK is smart enough to call provided getToken function to extract initial subscription token from the backend.  The good thing in using subscription JWT approach is that you can provide token expiration time, so permissions to subscribe on a channel will be validated from time to time while connection is active. You can also provide additional channel context info which will be attached to presence information (using info claim of subscription JWT). And you can granularly control channel permissions using allow claim of token – and give client capabilities to publish, call history or presence information (this is Centrifugo PRO feature at this point). Token also allows to override some namespace options on per-subscription basis (with override claim).  Using subscription tokens is a general approach for any channels where you need to check access first, not only for personal user channels.  ","version":null,"tagName":"h2"},{"title":"3 - subscribe proxy​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#3---subscribe-proxy","content":" tip Probably the most secure approach.  Subscription JWT gives client a way to subscribe on a channel, and avoid requesting your backend for permission on every resubscribe. Token approach is very good in massive reconnect scenario, when you have many connections and they all resubscribe at once (due to your load balancer reload, for example). But this means that if you unsubscribed client from a channel using server API, client can still resubscribe with token again - until token will expire. In some cases you may want to avoid this.  Also, in some cases you want to be notified when someone subscribes to a channel.  In this case you may use subscribe proxy feature. When using subscribe proxy every attempt of a client to subscribe on a channel will be translated to request (HTTP or GRPC) from Centrifugo to the application backend. Application backend can decide whether client is allowed to subscribe or not.  One advantage of using subscribe proxy is that backend can additionally provide initial channel data for the subscribing client. This is possible using data field of subscribe result generated by backend subscribe handler.  { &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:9000/centrifugo/subscribe&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot;, &quot;presence&quot;: true, &quot;proxy_subscribe&quot;: true } ] }   And on the backend side define a route /centrifugo/subscribe, check permissions of user upon subscription and return result to Centrifugo according to our subscribe proxy docs. Or simply run GRPC server using our proxy definitions and react on subscription attempt sent from Centrifugo to backend over GRPC.  On the client-side code is as simple as:  const sub = centrifuge.newSubscription('personal:17'); sub.on('publication', function(ctx) { console.log(ctx.data); }) sub.subscribe();   ","version":null,"tagName":"h2"},{"title":"4 - server-side channel in connection JWT​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#4---server-side-channel-in-connection-jwt","content":" tip The approach where you don't need to manage client-side subscriptions.  Server-side subscriptions is a way to consume publications from channels without even create Subscription objects on the client side. In general, client side Subscription objects provide a more flexible and controllable way to work with subscriptions. Clients can subscribe/unsubscribe on channels at any point. Client-side subscriptions provide more details about state transitions.  With server-side subscriptions though you are consuming publications directly from Client instance:  const client = new Centrifuge('ws://localhost:8000/connection/websocket', { token: 'CONNECTION-JWT' }); client.on('publication', function(ctx) { console.log('publication received from server-side channel', ctx.channel, ctx.data); }); client.connect();   In this case you don't have separate Subscription objects and need to look at ctx.channel upon receiving publication or to publication content to decide how to handle it. Server-side subscriptions could be a good choice if you are using Centrifugo unidirectional transports and don't need dynamic subscribe/unsubscribe behavior.  The first way to subscribe client on a server-side channel is to include channels claim into connection JWT:  { &quot;sub&quot;: &quot;17&quot;, &quot;channels&quot;: [&quot;personal:17&quot;] }   Upon successful connection user will be subscribed to a server-side channel by Centrifugo. One downside of using server-side channels is that errors in one server-side channel (like impossible to recover missed messages) may affect the entire connection and result into reconnects, while with client-side subscriptions individual subsription failures do not affect the entire connection.  But having one server-side channel per-connection seems a very reasonable idea to me in many cases. And if you have stable set of subscriptions which do not require lifetime state management – this can be a nice approach without additional protocol/network overhead involved.  ","version":null,"tagName":"h2"},{"title":"5 - server-side channel in connect proxy​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#5---server-side-channel-in-connect-proxy","content":" Similar to the previous one for cases when you are authenticating connections over connect proxy instead of using JWT.  This is possible using channels field of connect proxy handler result. The code on the client-side is the same as in Option #4 – since we only change the way how list of server-side channels is provided.  ","version":null,"tagName":"h2"},{"title":"6 - automatic personal channel subscription​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#6---automatic-personal-channel-subscription","content":" tip Almost no code approach.  As we pointed above Centrifugo knows an ID of the user due to authentication process. So why not combining this knowledge with automatic server-side personal channel subscription? Centrifugo provides exactly this with user personal channel feature.  { &quot;user_subscribe_to_personal&quot;: true, &quot;user_personal_channel_namespace&quot;: &quot;personal&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot;, &quot;presence&quot;: true } ] }   This feature only subscribes non-anonymous users to personal channels (those with non-empty user ID). The configuration above will subscribe our user &quot;17&quot; to channel personal:#17 automatically after successful authentication.  ","version":null,"tagName":"h2"},{"title":"7 – capabilities in connection JWT​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#7--capabilities-in-connection-jwt","content":" Allows using client-side subscriptions, but skip receiving subscription token. This is only available in Centrifugo PRO at this point.  So when generating JWT you can provide additional caps claim which contains channel resource capabilities:  PythonNodeJS import jwt import time claims = { &quot;sub&quot;: &quot;17&quot;, &quot;exp&quot;: int(time.time()) + 30*60, &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;personal:17&quot;], &quot;allow&quot;: [&quot;sub&quot;] } ] } token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   While in case of single channel the benefit of using this approach is not really obvious, it can help when you are using several channels with stric access permissions per connection, where providing capabilities can help to save some traffic and CPU resources since we avoid generating subscription token for each individual channel.  ","version":null,"tagName":"h2"},{"title":"8 – capabilities in connect proxy​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#8--capabilities-in-connect-proxy","content":" This is very similar to the previous approach, but capabilities are passed to Centrifugo in connect proxy result. So if you are using connect proxy for auth then you can still provide capabilities in the same form as in JWT. This is also a Centrifugo PRO feature.  ","version":null,"tagName":"h2"},{"title":"Teardown​","type":1,"pageTitle":"101 ways to subscribe user on a personal channel in Centrifugo","url":"/blog/2022/07/29/101-way-to-subscribe#teardown","content":" Which way to choose? Well, it depends. Since your application will have more than only a personal user channel in many cases you should decide which approach suits you better in each particular case – it's hard to give the universal advice.  Client-side subscriptions are more flexible in general, so I'd suggest using them whenever possible. Though you may use unidirectional transports of Centrifugo where subscribing to channels from the client side is not simple to achieve (though still possible using our server subscribe API). Server-side subscriptions make more sense there.  The good news is that all our official bidirectional client SDKs support all the approaches mentioned in this post. Hope designing the channel configuration on top of Centrifugo will be a pleasant experience for you.  Attributions Internet network vector created by rawpixel.com - www.freepik.comCyber security icons created by Smashicons - Flaticon ","version":null,"tagName":"h2"},{"title":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","type":0,"sectionRef":"#","url":"/blog/2022/12/20/improving-redis-engine-performance","content":"","keywords":"","version":null},{"title":"Broker and PresenceManager​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#broker-and-presencemanager","content":" Before we get started, let's define what Centrifugo's Broker and PresenceManager terms mean.  Broker is an interface responsible for maintaining subscriptions from different Centrifugo nodes (initiated by client connections). That helps to scale client connections over many Centrifugo instances and not worry about the same channel subscribers being connected to different nodes – since all Centrifugo nodes connected with PUB/SUB. Messages published to one node are delivered to a channel subscriber connected to another node.  Another major part of Broker is keeping an expiring publication history for channels (streams). So that Centrifugo may provide a fast cache for messages missed by clients upon going offline for a short period and compensate at most once delivery of Redis PUB/SUB using Publication incremental offsets. Centrifugo uses STREAM and HASH data structures in Redis to store channel history and stream meta information.  In general Centrifugo architecture may be perfectly illustrated by this picture (Gophers are Centrifugo nodes all connected to Broker, and sockets are WebSockets):    PresenceManager is an interface responsible for managing online presence information - list of currently active channel subscribers. While the connection is alive we periodically update presence entries for channels connection subscribed to (for channels where presence is enabled). Presence data should expire if not updated by a client connection for some time. Centrifugo uses two Redis data structures for managing presence in channels - HASH and ZSET.  ","version":null,"tagName":"h2"},{"title":"Redigo​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#redigo","content":" For a long time, the gomodule/redigo package served as the foundation for the Redis Engine implementation in Centrifugo. Huge props go to Mr Gary Burd for creating it.  Redigo offers a connection Pool to Redis. A simple usage of it involves getting the connection from the pool, issuing request to Redis over that connection, and then putting the connection back to the pool after receiving the result from Redis.  Let's write a simple benchmark which demonstrates simple usage of Redigo and measures SET operation performance:  func BenchmarkRedigo(b *testing.B) { pool := redigo.Pool{ MaxIdle: 128, MaxActive: 128, Wait: true, Dial: func() (redigo.Conn, error) { return redigo.Dial(&quot;tcp&quot;, &quot;:6379&quot;) }, } defer pool.Close() b.ResetTimer() b.SetParallelism(128) b.ReportAllocs() b.RunParallel(func(pb *testing.PB) { for pb.Next() { c := pool.Get() _, err := c.Do(&quot;SET&quot;, &quot;redigo&quot;, &quot;test&quot;) if err != nil { b.Fatal(err) } c.Close() } }) }   Let's run it:  BenchmarkRedigo-8 228804 4648 ns/op 62 B/op 2 allocs/op   Seems pretty fast, but we can improve it further.  ","version":null,"tagName":"h2"},{"title":"Redigo with pipelining​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#redigo-with-pipelining","content":" To increase a throughput in Centrifugo, instead of using Redigo's Pool for each operation, we acquired a dedicated connection from the Pool and utilized Redis pipelining to send multiple commands where possible.  Redis pipelining improves performance by executing multiple commands using a single client-server-client round trip. Instead of executing many commands one by one, you can queue the commands in a pipeline and then execute the queued commands as if it is a single command. Redis processes commands in order and sends individual response for each command. Given a single CPU nature of Redis, reducing the number of active connections when using pipelining has a positive impact on throughput – therefore pipelining is beneficial from this angle as well.    You can quickly estimate the benefits of pipelining by running Redis locally and running redis-benchmark which comes with Redis distribution over it:  &gt; redis-benchmark -n 100000 set key value Summary: throughput summary: 84674.01 requests per second   And with pipelining:  &gt; redis-benchmark -n 100000 -P 64 set key value Summary: throughput summary: 666880.00 requests per second   In Centrifugo we are using smart batching technique for collecting pipeline (also described in one of the previous posts in this blog).  To demonstrate benefits from using pipelining let's look at the following benchmark:  const ( maxCommandsInPipeline = 512 numPipelineWorkers = 1 ) type command struct { errCh chan error } type sender struct { cmdCh chan command pool redigo.Pool } func newSender(pool redigo.Pool) *sender { p := &amp;sender{ cmdCh: make(chan command), pool: pool, } go func() { for { for i := 0; i &lt; numPipelineWorkers; i++ { p.runPipelineRoutine() } } }() return p } func (s *sender) send() error { errCh := make(chan error, 1) cmd := command{ errCh: errCh, } // Submit command to be executed by runPipelineRoutine. s.cmdCh &lt;- cmd return &lt;-errCh } func (s *sender) runPipelineRoutine() { conn := p.pool.Get() defer conn.Close() for { select { case cmd := &lt;-s.cmdCh: commands := []command{cmd} conn.Send(&quot;set&quot;, &quot;redigo&quot;, &quot;test&quot;) loop: // Collect batch of commands to send to Redis in one RTT. for i := 0; i &lt; maxCommandsInPipeline; i++ { select { case cmd := &lt;-s.cmdCh: commands = append(commands, cmd) conn.Send(&quot;set&quot;, &quot;redigo&quot;, &quot;test&quot;) default: break loop } } // Flush all collected commands to the network. err := conn.Flush() if err != nil { for i := 0; i &lt; len(commands); i++ { commands[i].errCh &lt;- err } continue } // Read responses to commands, they come in order. for i := 0; i &lt; len(commands); i++ { _, err := conn.Receive() commands[i].errCh &lt;- err } } } } func BenchmarkRedigoPipelininig(b *testing.B) { pool := redigo.Pool{ Wait: true, Dial: func() (redigo.Conn, error) { return redigo.Dial(&quot;tcp&quot;, &quot;:6379&quot;) }, } defer pool.Close() sender := newSender(pool) b.ResetTimer() b.SetParallelism(128) b.ReportAllocs() b.RunParallel(func(pb *testing.PB) { for pb.Next() { err := sender.send() if err != nil { b.Fatal(err) } } }) }   This is a strategy that we employed in Centrifugo for a long time. As you can see code with automatic pipelining gets more complex, and in real life it's even more complicated to support different types of commands, channel send timeouts, and server shutdowns.  What about the performance of this approach?  BenchmarkRedigo-8 228804 4648 ns/op 62 B/op 2 allocs/op BenchmarkRedigoPipelininig-8 1840758 604.7 ns/op 176 B/op 4 allocs/op   Operation latency reduced from 4648 ns/op to 604.7 ns/op – not bad right?  It's worth mentioning that upon increased RTT between application and Redis the approach with pipelining will provide worse throughput. But it still can be better than in pool-based approach. Let's say we have latency 5ms between app and Redis. This means that with pool size of 128 you will be able to issue up to 128 * (1000 / 5) = 25600 requests per second over 128 connections. With the pipelining approach above the theoretical limit is 512 * (1000 / 5) = 102400 requests per second over a single connection (though in case of using code for pipelining shown above we need to have larger parallelism, say 512 instead of 128). And it can scale further if you increase numPipelineWorkers to work over several connections in paralell. Though increasing numPipelineWorkers has negative effect on CPU – we will discuss this later in this post.  Redigo is an awesome battle-tested library that served us great for a long time.  ","version":null,"tagName":"h2"},{"title":"Motivation to migrate​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#motivation-to-migrate","content":" There are three modes in which Centrifugo can work with Redis these days:  Connecting to a standalone single Redis instanceConnecting to Redis in master-replica configuration, where Redis Sentinel controls the failover processConnecting to Redis Cluster  All modes additionally can be used with client-side consistent sharding. So it's possible to scale Redis even without a Redis Cluster setup.  Unfortunately, with pure Redigo library, it's only possible to implement [ 1 ] – i.e. connecting to a single standalone Redis instance.  To support the scheme with Sentinel you whether need to have a proxy between the application and Redis which proxies the connection to Redis master. For example, with Haproxy it's possible in this way:  listen redis server redis-01 127.0.0.1:6380 check port 6380 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 on-marked-down shutdown-sessions on-marked-up shutdown-backup-sessions server redis-02 127.0.0.1:6381 check port 6381 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 backup bind *:6379 mode tcp option tcpka option tcplog option tcp-check tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK balance roundrobin   Or, you need to additionally import FZambia/sentinel library - which provides a communication layer with Redis Sentinel on top of Redigo's connection Pool.  For communicating with Redis Cluster one more library may be used – mna/redisc which is also a layer on top of redigo basic functionality.  Combining redigo + FZambia/sentinel + mna/redisc we managed to implement all three connection modes. This worked, though resulted in rather tricky Redis setup. Also, it was difficult to re-use existing pipelining code we had for a standalone Redis with Redis Cluster. As a result, Centrifugo only used pipelining in a standalone or Sentinel Redis cases. When using Redis Cluster, however, Centrifugo merely used the connection pool to issue requests thus not benefiting from request pipelining. Due to this we had some code duplication to send the same requests in various Redis configurations.  Another thing is that Redigo uses interface{} for command construction. To send command to Redis Redigo has Do method which accepts name of the command and variadic interface{} arguments to construct command arguments:  Do(commandName string, args ...interface{}) (reply interface{}, err error)   While this works well and you can issue any command to Redis, you need to be very accurate when constructing a command. This also adds some allocation overhead. As we know more memory allocations lead to the increased CPU utilization because the allocation process itself requires more processing power and the GC is under more strain.  At some point we felt that eliminating additional dependencies (even though I am the author of one of them) and reducing allocations in Redis communication layer is a nice step forward for Centrifugo. So we started looking around for redigo alternatives.  To summarize, here is what we wanted from Redis library:  Possibility to work with all three Redis setup options we support: standalone, master-replica(s) with Sentinel, Redis Cluster, so we can depend on one library instead of threeLess memory allocations (and more type-safety API is a plus)Support working with RESP2-only Redis servers as we need that for backwards compatibility. And some vendors like Redis Enterprise still support RESP2 protocol onlyThe library should be actively maintained  ","version":null,"tagName":"h2"},{"title":"Go-redis/redis​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#go-redisredis","content":" The most obvious alternative to Redigo is go-redis/redis package. It's popular, regularly gets updates, used by a huge amount of Go projects (Grafana, Thanos, etc.). And maintained byVladimir Mihailenco who created several more awesome Go libraries, like msgpack for example. I personally successfully used go-redis/redis in several other projects I worked on.  To avoid setup boilerplate for various Redis installation variations go-redis/redis has UniversalClient. From docs:  UniversalClient is a wrapper client which, based on the provided options, represents either a ClusterClient, a FailoverClient, or a single-node Client. This can be useful for testing cluster-specific applications locally or having different clients in different environments.  In terms of implementation go-redis/redis also has internal pool of connections to Redis, similar to redigo. It's also possible to use Client.Pipeline method to allocate a Pipeliner interface and use it for pipelining. So UniversalClient reduces setup boilerplate for different Redis installation types and number of dependencies we had, and it provide very similar way to pipeline requests so we could easily re-implement things we had with Redigo.  Go-redis also provides more type-safety when constructing commands compared to Redigo, almost every command in Redis is implemented as a separate method of Client, for example Publish defined as:  func (c Client) Publish(ctx context.Context, channel string, message interface{}) *IntCmd   You can see though that we still have interface{} here for message argument type. I suppose this was implemented in such way for convenience – to pass both string or []byte. But it still produces some extra allocations.  Without pipelining the simplest program with go-redis/redis may look like this:  func BenchmarkGoredis(b *testing.B) { client := redis.NewUniversalClient(&amp;redis.UniversalOptions{ Addrs: []string{&quot;:6379&quot;}, PoolSize: 128, }) defer client.Close() b.ResetTimer() b.SetParallelism(128) b.ReportAllocs() b.RunParallel(func(pb *testing.PB) { for pb.Next() { resp := client.Set(context.Background(), &quot;goredis&quot;, &quot;test&quot;, 0) if resp.Err() != nil { b.Fatal(resp.Err()) } } }) }   Let's run it:  BenchmarkRedigo-8 228804 4648 ns/op 62 B/op 2 allocs/op BenchmarkGoredis-8 268444 4561 ns/op 244 B/op 8 allocs/op   Result is pretty comparable to Redigo, though Go-redis allocates more (btw most of allocations come from the connection liveness check upon getting from the pool which can not be turned off).    It's interesting – if we dive deeper into what is it we can discover that this is the only way in Go to check connection was closed without reading data from it. The approach was originally introduced by go-sql-driver/mysql, it's not cross-platform, and related issue may be found in Go issue tracker.  But as I said in Centrifugo we already used pipelining over the dedicated connection for all operations so we avoid frequently getting connections from the pool. And early experiments proved that go-redis may provide some performance benefits for our use case.  At some point @j178 sent a pull request to Centrifuge library with Broker and PresenceManager implementations based on go-redis/redis. The amount of code to cover all the various Redis setups was reduced, we got only one dependency instead of three 🔥  But what about performance? Here we will show results for several operations which are typical for Centrifugo:  Publish a message to a channel without saving it to the history - this is just a Redis PUBLISH command going through Redis PUB/SUB system (RedisPublish)Publish message to a channel with saving it to history - this involves executing the LUA script on Redis side where we add a publication to STREAM data structure, update meta information HASH, and finally PUBLISH to PUB/SUB (RedisPublish_History)Subscribe to a channel - that's a SUBSCRIBE Redis command, this is important to have it fast as Centrifugo should be able to re-subscribe to all the channels in the system upon mass client reconnect scenario (RedisSubscribe)Recovering missed publication state from channel STREAM, this is again may be called lots of times when all clients reconnect at once (RedisRecover).Updating connection presence information - many connections may periodically update their channel online presence information in Redis (RedisAddPresence)  Here are the benchmark results we got when comparing redigo (v1.8.9) implementation (old) and go-redis/redis (v9.0.0-rc.2) implementation (new) with Redis v6.2.7 on Mac with M1 processor and benchmark paralellism 128:  ❯ benchstat redigo_p128.txt goredis_p128.txt name old time/op new time/op delta RedisPublish-8 1.45µs ±10% 1.88µs ± 4% +29.32% (p=0.000 n=10+10) RedisPublish_History-8 12.5µs ± 6% 9.7µs ± 3% -22.77% (p=0.000 n=10+10) RedisSubscribe-8 1.47µs ±24% 1.47µs ±10% ~ (p=0.469 n=10+10) RedisRecover-8 18.4µs ± 2% 6.3µs ± 0% -65.78% (p=0.000 n=10+8) RedisAddPresence-8 3.72µs ± 1% 3.40µs ± 1% -8.74% (p=0.000 n=10+10) name old alloc/op new alloc/op delta RedisPublish-8 483B ± 0% 499B ± 0% +3.37% (p=0.000 n=9+10) RedisPublish_History-8 1.30kB ± 0% 1.08kB ± 0% -16.67% (p=0.000 n=10+10) RedisSubscribe-8 892B ± 2% 662B ± 6% -25.83% (p=0.000 n=10+10) RedisRecover-8 1.25kB ± 1% 1.00kB ± 0% -19.91% (p=0.000 n=10+10) RedisAddPresence-8 907B ± 0% 827B ± 0% -8.82% (p=0.002 n=7+8) name old allocs/op new allocs/op delta RedisPublish-8 10.0 ± 0% 9.0 ± 0% -10.00% (p=0.000 n=10+10) RedisPublish_History-8 29.0 ± 0% 25.0 ± 0% -13.79% (p=0.000 n=10+10) RedisSubscribe-8 22.0 ± 0% 14.0 ± 0% -36.36% (p=0.000 n=8+7) RedisRecover-8 29.0 ± 0% 23.0 ± 0% -20.69% (p=0.000 n=10+10) RedisAddPresence-8 18.0 ± 0% 17.0 ± 0% -5.56% (p=0.000 n=10+10)   danger Please note that this benchmark is not a pure performance comparison of two Go libraries for Redis – it's a performance comparison of Centrifugo Engine methods upon switching to a new library.  Or visualized in Grafana:    note Centrifugo benchmarks results shown in the post use parallelism 128. If someone interested to check numbers for paralellism 1 or 16 – check out this comment on Github.  We observe a noticeable reduction in allocations in these benchmarks and in most benchmarks (presented here and other not listed in this post) we observed a reduced latency.  Overall, results convinced us that the migration from redigo to go-redis/redis may provide Centrifugo with everything we aimed for – all the goals for a redigo alternative outlined above were successfully fullfilled.  One good thing go-redis/redis allowed us to do is to use Redis pipelining also in a Redis Cluster case. It's possible due to the fact that go-redis/redis re-maps pipeline objects internally based on keys to execute pipeline on the correct node of Redis Cluster. Actually, we could do the same based on redigo + mna/redisc, but here we got it for free.  BTW, there is a page with comparison between redigo and go-redis/redis in go-redis/redis docs which outlines some things I mentioned here and some others.  But we have not migrated to go-redis/redis in the end. And the reason is another library – rueidis.  ","version":null,"tagName":"h2"},{"title":"Rueidis​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#rueidis","content":" While results were good with go-redis/redis we also made an attempt to implement Redis Engine on top of rueian/rueidis library written by @rueian. According to docs, rueidis is:  A fast Golang Redis client that supports Client Side Caching, Auto Pipelining, Generics OM, RedisJSON, RedisBloom, RediSearch, RedisAI, RedisGears, etc.  The readme of rueidis contains benchmark results where it hugely outperforms go-redis/redis in terms of operation latency/throughput in both single Redis and Redis Custer setups:      rueidis works with standalone Redis, Sentinel Redis and Redis Cluster out of the box. Just like UniversalClient of go-redis/redis. So it also allowed us to reduce code boilerplate to work with all these setups.  Again, let's try to write a simple program like we had for Redigo and Go-redis above:  func BenchmarkRueidis(b *testing.B) { client, err := rueidis.NewClient(rueidis.ClientOption{ InitAddress: []string{&quot;:6379&quot;}, }) if err != nil { b.Fatal(err) } b.ResetTimer() b.SetParallelism(128) b.ReportAllocs() b.RunParallel(func(pb *testing.PB) { for pb.Next() { cmd := client.B().Set().Key(&quot;rueidis&quot;).Value(&quot;test&quot;).Build() res := client.Do(context.Background(), cmd) if res.Error() != nil { b.Fatal(res.Error()) } } }) }   And run it:  BenchmarkRedigo-8 228804 4648 ns/op 62 B/op 2 allocs/op BenchmarkGoredis-8 268444 4561 ns/op 244 B/op 8 allocs/op BenchmarkRueidis-8 2908591 418.5 ns/op 4 B/op 1 allocs/op   rueidis library comes with automatic implicit pipelining, so you can send each request in isolated way while rueidis makes sure the request becomes part of the pipeline sent to Redis – thus utilizing the connection between an application and Redis most efficiently with maximized throughput. The idea of implicit pipelining with Redis is not new and Go ecosystem already had joomcode/redispipe library which implemented it (though it comes with some limitations which made it unsuitable for Centrifugo use case).  So applications that use a pool-based approach for communication with Redis may observe dramatic improvements in latency and throughput when switching to the Rueidis library.  For Centrifugo we didn't expect such a huge speed-up as shown in the above graphs since we already used pipelining in Redis Engine. But rueidis implements some ideas which allow it to be efficient. Insights about these ideas are provided by Rueidis author in a &quot;Writing a High-Performance Golang Client Library&quot; series of posts on Medium:  Part 1: Batching on PipelinePart 2: Reading Again From Channels?Part 3: Remove the Bad Busy Loops With the Sync.Cond  I did some prototypes with rueidis which were super-promising in terms of performance. There were some issues found during that early prototyping (mostly with PUB/SUB) – but all of them were quickly resolved by Rueian.  Until v0.0.80 release rueidis did not support RESP2 though, so we could not replace our Redis Engine implementation with it. But as soon as it got RESP2 support we opened a pull request with alternative implementation.  Since auto-pipelining is used in rueidis by default we were able to remove some of our own pipelining management code – so the Engine implementation is more concise now. One more thing to mention is a simpler PUB/SUB code we were able to write with rueidis. One example is that in redigo case we had to periodically PING PUB/SUB connection to maintain it alive, rueidis does this automatically.  Regarding performance, here are the benchmark results we got when comparing redigo (v1.8.9) implementation (old) and rueidis (v0.0.90) implementation (new):  ❯ benchstat redigo_p128.txt rueidis_p128.txt name old time/op new time/op delta RedisPublish-8 1.45µs ±10% 0.56µs ± 1% -61.53% (p=0.000 n=10+9) RedisPublish_History-8 12.5µs ± 6% 9.7µs ± 1% -22.43% (p=0.000 n=10+9) RedisSubscribe-8 1.47µs ±24% 1.45µs ± 1% ~ (p=0.484 n=10+9) RedisRecover-8 18.4µs ± 2% 6.2µs ± 1% -66.08% (p=0.000 n=10+10) RedisAddPresence-8 3.72µs ± 1% 3.60µs ± 1% -3.34% (p=0.000 n=10+10) name old alloc/op new alloc/op delta RedisPublish-8 483B ± 0% 91B ± 0% -81.16% (p=0.000 n=9+10) RedisPublish_History-8 1.30kB ± 0% 0.39kB ± 0% -70.08% (p=0.000 n=10+8) RedisSubscribe-8 892B ± 2% 360B ± 0% -59.66% (p=0.000 n=10+10) RedisRecover-8 1.25kB ± 1% 0.36kB ± 1% -71.52% (p=0.000 n=10+10) RedisAddPresence-8 907B ± 0% 151B ± 1% -83.34% (p=0.000 n=7+9) name old allocs/op new allocs/op delta RedisPublish-8 10.0 ± 0% 2.0 ± 0% -80.00% (p=0.000 n=10+10) RedisPublish_History-8 29.0 ± 0% 10.0 ± 0% -65.52% (p=0.000 n=10+10) RedisSubscribe-8 22.0 ± 0% 6.0 ± 0% -72.73% (p=0.002 n=8+10) RedisRecover-8 29.0 ± 0% 7.0 ± 0% -75.86% (p=0.000 n=10+10) RedisAddPresence-8 18.0 ± 0% 3.0 ± 0% -83.33% (p=0.000 n=10+10)   Or visualized in Grafana:    2.5x times more publication throughput than we had before! Instead of 700k publications/sec, we went towards 1.7 million publications/sec due to drastically decreased publish operation latency (1.45µs -&gt; 0.59µs). This means that our previous Engine implementation under-utilized Redis, and Rueidis just pushes us towards Redis limits. The latency of most other operations is also reduced.  The allocation effectiveness of the implementation based on &quot;rueidis&quot; is best. As you can see rueidis helped us to generate sufficiently fewer memory allocations for all our Redis operations. Allocation improvements directly affect Centrifugo node CPU usage. Though we will talk about CPU more later below.  For Redis Cluster case we also got benchmark results similar to the standalone Redis results above.  I might add that I enjoyed building commands with rueidis. All Redis commands may be constructed using a builder approach. Rueidis comes with builders generated for all Redis commands. As an illustration, this is a process of building a PUBLISH Redis command:    This drastically reduces a chance to make a stupid mistake while constructing a command. Instead of always opening Redis docs to see a command syntax it's now possible to just start typing - and quickly come to the complete command to send.  ","version":null,"tagName":"h2"},{"title":"Switching to Rueidis: reducing CPU usage​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#switching-to-rueidis-reducing-cpu-usage","content":" After making all these benchmarks and implementing Engine in Rueidis I decided to check whether Centrifugo consumes less CPU with it. I expected a notable CPU reduction as Rueidis Engine implementation allocates much less than Redigo-based. Turned out it's not that simple.  I ran Centrifugo with some artificial load and noticed that CPU consumption of the new implementation is actually... worse than we had with Redigo-based engine under equal conditions!😩 But why?  As I mentioned above Redis pipelining is a technique when several commands may be combined into one batch to send over the network. In case of automatic pipelining the size of generated batches start playing a crucial role in application and Redis CPU usage – since smaller command batches result into more read/write system calls to the kernel on both application and Redis server sides. That's why projects like Twemproxy which sit between app and Redis have sich a good effect on Redis CPU usage among other things.  As we have seen above, Rueidis provides a better throughput and latency, but it's more agressive in terms of flushing data to the network. So in its default configuration we get smaller batches under th equal conditions than we had before in our own pipelining implementation based on Redigo (shown in the beginning of this post).  Luckily, there is an option in Rueidis called MaxFlushDelay which allows to slow down write loop a bit to give Rueidis a chance to collect more commands to send in one batch. When this option is used Rueidis will make a pause after each network flush not bigger than selected value of MaxFlushDelay (please note, that this is a delay after flushing collected pipeline commands, not an additional delay for each request). Using some reasonable value it's possible to drastically reduce both application and Redis CPU utilization.  To demonstrate this I created a repo: https://github.com/FZambia/pipelines.  This repo contains three benchmarks where we use automatic pipelining: based on redigo, based on go-redis/redis and rueidis. In these benchmarks we produce concurrent requests, but instead of pushing the system towards the limits we are limiting number of requests sent to Redis, so we put all libraries in equal conditions.  To rate limit requests we are using uber-go/ratelimit library. For example, to allow rate no more than 100k commands per second we can do sth like this:  rl := ratelimit.New(100, ratelimit.Per(time.Millisecond)) for { rl.Take() ... }   We limit requests per second we could actually just write ratelimit.New(100000) – but we aim to get a more smooth distribution of requests over time - so using millisecond resolution.  Let's run all the benchmarks in the default configuration:    Average CPU usage during the test (a bit rough but enough for demonstration):  \tRedigo\tGo-redis/redis\tRueidisApplication CPU, %\t95\t99\t116 Redis CPU, %\t36\t35\t42  OK, Rueidis-based implementation is the worst here despite of allocating less than others. So let's try to change this by setting MaxFlushDelay to sth like 100 microseconds:    Now CPU usage is:  \tRedigo\tGo-redis/redis\tRueidisApplication CPU, %\t95\t99\t59 Redis CPU, %\t36\t35\t12  So we can achieve great CPU usage reduction. CPU went from 116% to 59% for the application side, and from 42% to only 12% for Redis! We are sacrificing latency though. Given the fact the CPU utilization reduction is very notable the trade-off is pretty fair.  caution It's definitely possible to improve CPU usage in Redigo and Go-redis/redis cases too – using similar technique. But the goal here was to improve Rueidis-based engine implementation to make it comparable or better than our Redigo-based implementation in terms of CPU utilization.  As you can see we were able to achieve better CPU results just by using 100 microseconds delay after each network flush. In real life, where we are not running Redis on localhost and have some network latency in between application and Redis, this delay should be insignificant at all. Indeed, adding MaxFlushDelay can even improve (!) the latency you have. You may wonder what happened with benchmarks we showed above after we added MaxFlushDelay option. In Centrifugo we chose default value 100 microseconds, and here are results on localhost (old without delay, new with delay):  &gt; benchstat rueidis_p128.txt rueidis_delay_p128.txt name old time/op new time/op delta RedisPublish-8 559ns ± 1% 468ns ± 0% -16.35% (p=0.000 n=9+8) RedisPublish_History-8 9.72µs ± 1% 9.67µs ± 1% -0.52% (p=0.007 n=9+8) RedisSubscribe-8 1.45µs ± 1% 1.27µs ± 1% -12.49% (p=0.000 n=9+10) RedisRecover-8 6.25µs ± 1% 5.85µs ± 0% -6.32% (p=0.000 n=10+10) RedisAddPresence-8 3.60µs ± 1% 3.33µs ± 1% -7.52% (p=0.000 n=10+10) (rest is not important here...)   It's even better for this set of benchmarks. Though while it's better for these benchmarks the numbers may differ for other under different conditions. For example, in the benchmarks we run we use concurrency 128, if we reduce concurrency we will notice reduced throughput – as batches Rueidis collects become smaller. Smaller batches + some delay to collect = less requests per second to send.  The problem is that the value to pause Rueidis write loop is a very use case specific, it's pretty hard to provide a reasonable default for it. Depending on request rate/size, network latency etc. you may choose a larger or smaller delay. In v4.1.0 we start with hardcoded 100 microsecond MaxFlushDelay which seems sufficient for most use cases and showed good results in the benchmarks - though possibly we will have to make it tunable later.  To check that Centrifugo benchmarks also utilize less CPU I added rate limiter (50k rps per second) to benchmarks and compared version without MaxFlushDelay and with 100 microsecond MaxFlushDelay:  50k req per second\tWithout delay\tWith 100mks delayBenchmarkPublish\tCentrifugo - 75%, Redis - 24%\tCentrifugo - 44%, Redis - 9% BenchmarkPublish_History\tCentrifugo - 80% , Redis - 67%\tCentrifugo - 55%, Redis - 50% BenchmarkSubscribe\tCentrifugo - 80%, Redis - 30%\tCentrifugo - 45% , Redis - 14% BenchmarkRecover\tCentrifugo - 84%, Redis - 51%\tCentrifugo - 51%, Redis - 36% BenchmarkPresence\tCentrifugo - 114%, Redis - 69%\tCentrifugo - 90%, Redis - 60%  note In this test I replaced BenchmarkAddPresence with BenchmarkPresence (get information about all online subscribers in channel) to also make sure we have CPU reduction when using read-intensive method, i.e. when Redis response is reasonably large.  We observe a notable CPU usage improvement here.  Hope you understand now why increasing numPipelineWorkers value in the pipelining code showed before results into increased CPU usage on app and Redis sides – due to smaller batch sizes and more read/write system calls as the consequence.  note BTW, would it be a nice thing if Go benchmarking suite could show a CPU usage of the process in addition to time and alloc stats? 🤔  ","version":null,"tagName":"h2"},{"title":"Adding latency​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#adding-latency","content":" The last thing to check is how new implementation works upon increased RTT between application and Redis. To add artificial latency on localhost on Linux one can use tc tool as shown here by Daniel Stenberg. But I am on MacOS so the simplest way I found was using Shopify/toxiproxy. Sth like running a server:  toxyproxy-server   And then in another terminal I used toxiproxy-cli to create toxic Redis proxy with additional latency on port 26379:  toxiproxy-cli create -l localhost:26379 -u localhost:6379 toxic_redis toxiproxy-cli toxic add -t latency -a latency=5 toxic_redis   The benchmark results are (old is Redigo-based, new is Rueidis-based):  &gt; benchstat redigo_latency_p128.txt rueidis_delay_latency_p128.txt name old time/op new time/op delta RedisPublish-8 31.5µs ± 1% 5.6µs ± 3% -82.26% (p=0.000 n=9+10) RedisPublish_History-8 62.8µs ± 3% 10.6µs ± 4% -83.05% (p=0.000 n=10+10) RedisSubscribe-8 1.52µs ± 5% 6.05µs ± 8% +298.70% (p=0.000 n=8+10) RedisRecover-8 48.3µs ± 3% 7.3µs ± 4% -84.80% (p=0.000 n=10+10) RedisAddPresence-8 52.3µs ± 4% 5.8µs ± 2% -88.94% (p=0.000 n=10+10) (rest is not important here...)   We see that new Engine implementation behaves much better for most cases. But what happened to Subscribe operation? It did not change at all in Redigo case – the same performance as if there is no additional latency involved!  Turned out that when we call Subscribe in Redigo case, Redigo only flushes data to the network without waiting synchronously for subscribe result.  It makes sense in general and we can listen to subscribe notifications asynchronously, but in Centrifugo we relied on the returned error thinking that it includes succesful subscription result from Redis - meaning that we already subscribed to a channel at that point. And this could theoretically lead to some rare bugs in Centrifugo.  Rueidis library waits for subscribe response. So here the behavior of rueidis while differs from redigo in terms of throughput under increased latency just fits Centrifugo better in terms of behavior. So we go with it.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","url":"/blog/2022/12/20/improving-redis-engine-performance#conclusion","content":" Migrating from Redigo to Rueidis library was not just a task of rewriting code, we had to carefully test various aspects of Redis Engine behaviour – latency, throughput, CPU utilization of application, and even CPU utilization of Redis itself under the equal application load conditions.  I think that we will find more projects in Go ecosystem using rueidis library shortly. Not just because of its allocation efficiency and out-of-the-box throughput, but also due to a convenient type-safe command API.  For most Centrifugo users this migration means more efficient CPU usage as new implementation allocates less memory (less work to allocate and less strain on GC) and we tried to find a reasonable batch size to reduce the number of system calls for common operations. While latency and throughput of single Centrifugo node should be better as we make concurrent Redis calls from many goroutines.  Hopefully readers will learn some tips from this post which can help to achieve effective communication with Redis from Go or another programming language.  A few key takeaways:  Redis pipelining may increase throughput and reduce latency, it can also reduce CPU utilization of RedisDon't blindly trust Go benchmark numbers but also think about CPU effect of changes you made (sometimes of the external system also)Reduce the number of system calls to decrease CPU utilizationEverything is a trade-off – latency or resource usage? Your own WebSocket server or Centrifugo?Don't rely on someone's else benchmarks, including those published here. Measure for your own use case. Take into account your load profile, paralellism, network latency, data size, etc.  P.S. One thing worth mentioning and which may be helpful for someone is that during our comparison experiments we discovered that Redis 7 has a major latency increase compared to Redis 6 when executing Lua scripts. So if you have performance sensitive code with Lua scripts take a look at this Redis issue. With the help of Redis developers some things already improved in unstable Redis branch, hopefully that issue will be closed at the time you read this post. ","version":null,"tagName":"h2"},{"title":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","type":0,"sectionRef":"#","url":"/blog/2023/03/31/keycloak-sso-centrifugo","content":"","keywords":"","version":null},{"title":"TLDR​","type":1,"pageTitle":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","url":"/blog/2023/03/31/keycloak-sso-centrifugo#tldr","content":" The integraion is possible since Centrifugo works with standard JWT for authentication and additionally supports JSON Web Key specification.  Here is a final source code.  ","version":null,"tagName":"h2"},{"title":"Keycloak​","type":1,"pageTitle":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","url":"/blog/2023/03/31/keycloak-sso-centrifugo#keycloak","content":" First, run Keycloak using the following Docker command:  docker run --rm -it -p 8080:8080 \\ -e KEYCLOAK_ADMIN=admin \\ -e KEYCLOAK_ADMIN_PASSWORD=admin \\ quay.io/keycloak/keycloak:21.0.1 start-dev   After starting Keycloak, go to http://localhost:8080/admin and login. Then perform the following tasks:  Create a new realm named myrealm.Create a new client named myclient. Set valid redirect URIs to http://localhost:5173/*, and web origins as http://localhost:5173.Create a user named myuser and set a password for it (in Credentials tab).  See this guide for additional details and illustrations of the process.  Make sure your created client is public (this is default) since we will request token directly from the web application.  ","version":null,"tagName":"h2"},{"title":"Centrifugo​","type":1,"pageTitle":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","url":"/blog/2023/03/31/keycloak-sso-centrifugo#centrifugo","content":" Next, run Centrifugo using the following Docker command:  docker run --rm -it -p 8000:8000 \\ -e CENTRIFUGO_ALLOWED_ORIGINS=&quot;http://localhost:5173&quot; \\ -e CENTRIFUGO_TOKEN_JWKS_PUBLIC_ENDPOINT=&quot;http://host.docker.internal:8080/realms/myrealm/protocol/openid-connect/certs&quot; \\ -e CENTRIFUGO_ALLOW_USER_LIMITED_CHANNELS=true \\ -e CENTRIFUGO_ADMIN=true \\ -e CENTRIFUGO_ADMIN_SECRET=secret \\ -e CENTRIFUGO_ADMIN_PASSWORD=admin \\ centrifugo/centrifugo:v4.1.2 centrifugo   Some comments about environment variables used here:  CENTRIFUGO_TOKEN_JWKS_PUBLIC_ENDPOINT allows tell Centrifugo to use JSON Web Key spec when validating tokens, we point to Keycloak's JWKS endpointCENTRIFUGO_ALLOWED_ORIGINS is required since we will build Vite + React based app running on http://localhost:5173CENTRIFUGO_ALLOW_USER_LIMITED_CHANNELS - not required to connect, but you will see in the source code that we additionally subscribe to a user personal channelCENTRIFUGO_ADMIN, CENTRIFUGO_ADMIN_SECRET, CENTRIFUGO_ADMIN_PASSWORD - to enable Centrifugo admin web UI  Also note we are using host.docker.internal to access host port from inside the Docker network.  ","version":null,"tagName":"h2"},{"title":"React app with Vite​","type":1,"pageTitle":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","url":"/blog/2023/03/31/keycloak-sso-centrifugo#react-app-with-vite","content":" Now, let's create a new React app using very popular Vite tool:  npm create vite@latest keycloak_sso_auth -- --template react cd keycloak_sso_auth npm install   Also, install the necessary additional packages for the React app:  npm install --save @react-keycloak/web centrifuge keycloak-js   And start the development server:  npm run dev   Navigate to http://localhost:5173/. You should see default Vite template working, we are going to modify it a bit.  caution Use localhost, not 127.0.0.1 - since we used localhost for Keyloak and Centrifugo configurations above.  Add the following into main.jsx:  import React from 'react' import ReactDOM from 'react-dom/client' import { ReactKeycloakProvider } from '@react-keycloak/web' import App from './App' import './index.css' import Keycloak from &quot;keycloak-js&quot;; const keycloakClient = new Keycloak({ url: &quot;http://localhost:8080&quot;, realm: &quot;myrealm&quot;, clientId: &quot;myclient&quot; }) ReactDOM.createRoot(document.getElementById('root')).render( &lt;ReactKeycloakProvider authClient={keycloakClient}&gt; &lt;React.StrictMode&gt; &lt;App /&gt; &lt;/React.StrictMode&gt; &lt;/ReactKeycloakProvider&gt;, )   Note that we configured Keycloak instance pointing it to our Keycloak server. We also use @react-keycloak/web package to wrap React app into ReactKeycloakProvider component. It simplifies working with Keycloak by providing some useful hooks - we are using this hook below.  Our App component inside App.jsx may look like this:  import React, { useState, useEffect } from 'react'; import logo from './assets/centrifugo.svg' import { Centrifuge } from &quot;centrifuge&quot;; import { useKeycloak } from '@react-keycloak/web' import './App.css' function App() { const { keycloak, initialized } = useKeycloak() if (!initialized) { return null; } return ( &lt;div&gt; &lt;header&gt; &lt;p&gt; SSO with Keycloak and Centrifugo &lt;/p&gt; {keycloak.authenticated ? ( &lt;div&gt; &lt;p&gt;Logged in as {keycloak.tokenParsed?.preferred_username}&lt;/p&gt; &lt;button type=&quot;button&quot; onClick={() =&gt; keycloak.logout()}&gt; Logout &lt;/button&gt; &lt;/div&gt; ) : ( &lt;button type=&quot;button&quot; onClick={() =&gt; keycloak.login()}&gt; Login &lt;/button&gt; )} &lt;/header&gt; &lt;/div &gt; ); } export default App   This is actually enough for SSO flow to start working! You can click on login button and make sure that it's possible to use myuser credentials to log into the application. And log out after that.  The only missing part is Centrifugo. We can initialize connection inside useEffect hook of App component:  useEffect(() =&gt; { if (!initialized || !keycloak.authenticated) { return; } const centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, { token: keycloak.token, getToken: function () { return new Promise((resolve, reject) =&gt; { keycloak.updateToken(5).then(function () { resolve(keycloak.token); }).catch(function (err) { reject(err); keycloak.logout(); }); }) } }); centrifuge.connect(); return () =&gt; { centrifuge.disconnect(); }; }, [keycloak, initialized]);   The important thing here is how we configure tokens: we are using Keycloak client methods to set initial token and refresh the token when required.  I also added some extra elements to the code to make it look a bit nicer. For example, we can listen to Centriffuge client state changes and show connection indicator on the page:  function App() { const [connectionState, setConnectionState] = useState(&quot;disconnected&quot;); const stateToEmoji = { &quot;disconnected&quot;: &quot;🔴&quot;, &quot;connecting&quot;: &quot;🟠&quot;, &quot;connected&quot;: &quot;🟢&quot; } ... useEffect(() =&gt; { ... centrifuge.on('state', function (ctx) { setConnectionState(ctx.newState); }) ... return ( ... &lt;span className={&quot;connectionState &quot; + connectionState}&gt; {stateToEmoji[connectionState]} &lt;/span&gt;   You can find more details about Centrifugo client SDK API and states in client SDK spec.  If you look at source code on Github - you will also find an example of channel subscription to a user personal channel:  function App() { ... const [publishedData, setPublishedData] = useState(&quot;&quot;); ... useEffect(() =&gt; { ... const userChannel = &quot;#&quot; + keycloak.tokenParsed?.sub; const sub = centrifuge.newSubscription(userChannel); sub.on(&quot;publication&quot;, function (ctx) { setPublishedData(JSON.stringify(ctx.data)); }).subscribe(); ...   You can now:  test the SSO setup by logging into applicationmaking sure connection is successfultry publishing a message into a user channel via the Centrifugo Web UI. The published message will appear on application screen in real-time.    That's it! We have successfully set up Keycloak SSO authentication with Centrifugo and a React application. Again, source code is on Github. ","version":null,"tagName":"h2"},{"title":"Centrifugo v5 released","type":0,"sectionRef":"#","url":"/blog/2023/06/29/centrifugo-v5-released","content":"","keywords":"","version":null},{"title":"Dropping old client protocol​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#dropping-old-client-protocol","content":" With the introduction of Centrifugo v4, our previous major release, we rolled out a new version of the client protocol along with a set of client SDKs designed to work in conjunction with it. Nevertheless, we maintained support for the old client protocol in Centrifugo v4 to facilitate a seamless migration of applications.  In Centrifugo v5 we are discontinuing support for the old protocol. If you have been using Centrifugo v4 with the latest SDKs, this change should have no impact on you. From our perspective, removing support for the old protocol allows us to eliminate a considerable amount of non-obvious branching in the source code and cleanup Protobuf schema definitions.  ","version":null,"tagName":"h2"},{"title":"Token behaviour adjustments in SDKs​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#token-behaviour-adjustments-in-sdks","content":" In Centrifugo v5 we are adjusting client SDK specification in the aspect of connection token management. Previously, returning an empty token string from getToken callback resulted in client disconnection with unauthorized reason.  There was some problem with it though. We did not take into account the fact that empty token may be a valid scenario actually. Centrifugo supports options to avoid using token at all for anonymous access. So the lack of possibility to switch between token/no token scenarios did not allow users to easily implement login/logout workflow. The only way was re-initializing SDK.  Now returning an empty string from getToken is a valid scenario which won't result into disconnect on the client side. It's still possible to disconnect client by returning a special error from getToken function.  And we are putting back setToken method to our SDKs – so it's now possible to reset the token to be empty upon user logout.  An abstract example in Javascript which demonstrates how login/logout flow may be now implemented with our SDK:  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket', { // Provide function which returns empty string for anonymous users, // and proper JWT for authenticated users. getToken: getTokenImplementation }); centrifuge.connect(); loginButton.addEventListener('click', function() { centrifuge.disconnect(); // Do actual login here. centrifuge.connect(); }); logoutButton.addEventListener('click', function() { centrifuge.disconnect(); // Reset token - so that getToken will be called on next connect attempt. centrifuge.setToken(&quot;&quot;); // Do actual logout here. centrifuge.connect(); });   We updated all our SDKs to inherit described behaviour - check out v5 migration guide for more details.  ","version":null,"tagName":"h2"},{"title":"history_meta_ttl refactoring​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#history_meta_ttl-refactoring","content":" One of Centrifugo's key advantages for real-time messaging tasks is its ephemeral channels and per-channel history. In version 5, we've improved one aspect of handling history by offering users the ability to tune the history meta TTL option at the channel namespace level.  The history meta TTL is the duration Centrifugo retains meta information about each channel stream, such as the latest offset and current stream epoch. This data allows users to successfully restore connections upon reconnection, particularly useful when subscribed to mostly inactive channels where publications are infrequent. Although the history meta TTL can usually be reasonably large (significantly larger than history TTL), there are certain scenarios where it's beneficial to minimize it as much as possible.  One such use case is illustrated in this example. Using Centrifugo SDK and channels with history, it's possible to reliably stream results of asynchronous tasks to clients.  As another example, consider a ChatGPT use case where clients ask questions, subscribe to a channel with the answer, and then the response is streamed towards the client token by token. This all may be done over a secure, separate channel protected with a token. With the ability to use a relatively small history meta TTL in the channel namespace, implementing such things is now simpler.  Hence, history_meta_ttl is now an option at the channel namespace level (instead of per-engine). However, setting it is optional as we have a global default value for it - see details in the doc.  ","version":null,"tagName":"h2"},{"title":"Node communication protocol update​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#node-communication-protocol-update","content":" When running in cluster Centrifugo nodes can communicate between each other using broker's PUB/SUB. Nodes exchange some information - like statistics, emulation requests, etc.  In Centrifugo v5 we are simplifying and making inter-node communication protocol slightly faster by removing extra encoding layers from it's format. Something similar to what we did for our client protocol in Centrifugo v4.  This change, however, leads to an incompatibility between Centrifugo v4 and v5 nodes in terms of their communication protocols. Consequently, Centrifugo v5 cannot be part of a cluster with Centrifugo v4 nodes.  ","version":null,"tagName":"h2"},{"title":"New HTTP API format​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#new-http-api-format","content":" From the beginning Centrifugo HTTP API exposed one /api endpoint to make requests with all command types.  To work properly HTTP API had to add one additional layer to request JSON payload to be able to distinguish between different API methods:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey API_KEY&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;test&quot;, &quot;data&quot;: {&quot;x&quot;: 1}}}' \\ http://localhost:8000/api   And it worked fine. It additionally supported request batching where users could send many commands to Centrifugo in one request using line-delimited JSON.  However, the fact that we were accommodating various commands via a single API endpoint resulted in nested serialized payloads for each command. The top-level method would determine the structure of the params. We addressed this issue in the client protocol in Centrifugo v4, and now we're addressing a similar issue in the inter-node communication protocol in Centrifugo v5.  At some point we introduced GRPC API in Centrifugo. In GRPC case we don't have a way to send batches of commands without defining a separate method to do so.  These developments highlighted the need for us to align the HTTP API format more closely with the GRPC API. Specifically, we need to separate the command method from the actual method payload, moving towards a structure like this:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: API_KEY&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;test&quot;, &quot;data&quot;: {&quot;x&quot;: 1}}' \\ http://localhost:8000/api/publish   Note:  /api/publish instead of /api in pathpayload does not include method and params keys anymorewe also support X-API-Key header for setting API key to be closer to OpenAPI specification (see more about OpenAPI below)  In v5 we implemented the approach above. Many thanks to @StringNick for the help with the implementation and discussions.  Our HTTP and GRPC API are very similar now. We've also introduced a new batch method to send multiple commands in both HTTP and GRPC APIs, a feature that was previously unavailable in GRPC.  Documentation for v5 was updated to reflect this change. But it worth noting - old API format id still supported. It will be supported for some time while we are migrating our HTTP API libraries to use modern API version. Hopefully users won't be affected by this migration a lot, just switching to a new version of library at some point.  ","version":null,"tagName":"h2"},{"title":"OpenAPI spec and Swagger UI​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#openapi-spec-and-swagger-ui","content":" One additional benefit of moving to the new HTTP format is the possibility to define a clear OpenAPI schema for each API method Centrifugo has. It was previously quite tricky due to the fact we had one endpoint capable to work with all kinds of commands.  This change paves the way for generating HTTP clients based on our OpenAPI specification.  We now have Swagger UI built-in. To access it, launch Centrifugo with the &quot;swagger&quot;: true option and navigate to http://localhost:8000/swagger.  The Swagger UI operates on the internal port, so if you're running Centrifugo using our Kubernetes Helm chart, it won't be exposed to the same ingress as client connection endpoints. This is similar to how our Prometheus, admin, API, and debug endpoints currently work.  ","version":null,"tagName":"h2"},{"title":"OpenTelemetry for server API​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#opentelemetry-for-server-api","content":" Another good addition is an OpenTelemetry tracing support for HTTP and GRPC server API requests. If you are using OpenTelemetry in your services you can now now enable tracing export in Centrifugo and find Centrifugo API request exported traces in your tracing collector UI.  Description and simple example with Jaeger may be found in observability chapter. We only support OTLP HTTP export format and trace format defined in W3C spec: https://www.w3.org/TR/trace-context/.  ","version":null,"tagName":"h2"},{"title":"Separate config for subscription token​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#separate-config-for-subscription-token","content":" With the introduction of JWKS support in Centrifugo v4 (a way to validate JWT tokens using a remote endpoint which manages keys and key rotation - see JWK spec) Centrifugo users can rely on JWKS provider (like Keycloak, AWS Cognito) for making authentication.  But at the same time developers may want to work with channels using subscription tokens managed in a custom way – without using the same JWKS configuration used for connection tokens.  Centrifugo v5 allows doing by introducing the separate_subscription_token_config option.  When separate_subscription_token_config is true Centrifugo does not look at general token options at all when verifying subscription tokens and uses config options starting from subscription_token_ prefix instead.  Here is an example how to use JWKS for connection tokens, but have HMAC-based verification for subscription tokens:  config.json { &quot;token_jwks_public_endpoint&quot;: &quot;https://example.com/openid-connect/certs&quot;, &quot;separate_subscription_token_config&quot;: true, &quot;subscription_token_hmac_secret_key&quot;: &quot;separate_secret_which_must_be_strong&quot; }   ","version":null,"tagName":"h2"},{"title":"Unknown config keys warnings​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#unknown-config-keys-warnings","content":" With every release, Centrifugo offers more and more options. One thing we've noticed is that some options from previous Centrifugo options, which were already removed, still persist in the user's configuration file.  Another issue is that a single typo in the configuration key can cost hours of debugging especially for Centrifugo new users. What is worse, the typo might result in unexpected behavior if the feature isn't properly tested before being run in production.  In Centrifugo v5, we are addressing these problems. Now, Centrifugo logs on WARN level about unknown keys found in the configuration upon server start-up. Not only in the configuration file but also verifying the validity of environment variables (looking at those starting with CENTRIFUGO_ prefix). This should help clean up the configuration to align with the latest Centrifugo release and catch typos at an early stage.  It looks like this:  08:25:33 [WRN] unknown key found in the namespace object key=watch namespace_name=xx 08:25:33 [WRN] unknown key found in the proxy object key=type proxy_name=connect 08:25:33 [WRN] unknown key found in the configuration file key=granulr_proxy_mode 08:25:33 [WRN] unknown key found in the environment key=CENTRIFUGO_ADDRES   These warnings do not prevent server to start so you can gradually clean up the configuration.  ","version":null,"tagName":"h2"},{"title":"Simplifying protocol debug with Postman​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#simplifying-protocol-debug-with-postman","content":" Centrifugo v5 supports a special url parameter for bidirectional websocket which turns on using native WebSocket frame ping-pong mechanism instead of server-to-client application level pings Centrifugo uses by default. This simplifies debugging Centrifugo protocol with tools like Postman, wscat, websocat, etc.  Previously it was inconvenient due to the fact Centrifugo sends periodic ping message to the client ({} in JSON protocol scenario) and expects pong response back within some time period. Otherwise Centrifugo closes connection. This results in problems with mentioned tools because you had to manually send {} pong message upon ping message. So typical session in wscat could look like this:  ❯ wscat --connect ws://localhost:8000/connection/websocket Connected (press CTRL+C to quit) &gt; {&quot;id&quot;: 1, &quot;connect&quot;: {}} &lt; {&quot;id&quot;:1,&quot;connect&quot;:{&quot;client&quot;:&quot;9ac9de4e-5289-4ad6-9aa7-8447f007083e&quot;,&quot;version&quot;:&quot;0.0.0&quot;,&quot;ping&quot;:25,&quot;pong&quot;:true}} &lt; {} Disconnected (code: 3012, reason: &quot;no pong&quot;)   The parameter is called cf_ws_frame_ping_pong, to use it connect to Centrifugo bidirectional WebSocket endpoint like ws://localhost:8000/websocket/connection?cf_ws_frame_ping_pong=true. Here is an example which demonstrates working with Postman WebSocket where we connect to local Centrifugo and subscribe to two channels test1 and test2:    You can then proceed to Centrifugo admin web UI, publish something to these channels and see publications in Postman.  Note, how we sent several JSON commands in one WebSocket frame to Centrifugo from Postman in the example above - this is possible since Centrifugo protocol supports batches of commands in line-delimited format.  We consider this feature to be used only for debugging, in production prefer using our SDKs without using cf_ws_frame_ping_pong parameter – because app-level ping-pong is more efficient and our SDKs detect broken connections due to it.  ","version":null,"tagName":"h2"},{"title":"The future of SockJS​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#the-future-of-sockjs","content":" As you know SockJS is deprecated in Centrifugal ecosystem since Centrifugo v4. In this release we are not removing support for it – but we may do this in the next release.  Unfortunately, SockJS client repo is poorly maintained these days. And some of its iframe-based transports are becoming archaic. If you depend on SockJS and you really need fallback for WebSocket – consider switching to Centrifugo own bidirectional emulation for the browser which works over HTTP-streaming (using modern fetch API with Readable streams) or SSE. It should be more performant and work without sticky sessions requirement (sticky sessions is an optimization in our implementation). More details may be found in Centrifugo v4 release post.  If you think SockJS is still required for your use case - reach us out so we could think about the future steps together.  ","version":null,"tagName":"h2"},{"title":"Introducing Centrifugal Labs LTD​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#introducing-centrifugal-labs-ltd","content":" Finally, some important news we mentioned in the beginning!  Centrifugo is now backed by the company called Centrifugal Labs LTD - a Cyprus-registered technology company. This should help us to finally launch Centrifugo PRO offering – the product we have been working on for a couple of years now and which has some unique and powerful features like real-time analytics or push notification API.  As a Centrifugo user you will start noticing mentions of Centrifugal Labs LTD in our licenses, Github organization, throughout this web site. And that's mostly it - no radical changes at this point. We will still be working on improving Centrifugo trying to find a balance between OSS and PRO versions. Which is difficult TBH – but we will try.  An ideal plan for us – make Centrifugo development sustainable enough to have the possibility for features from the PRO version flow to the OSS version eventually. The reality may be harder than this of course.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Centrifugo v5 released","url":"/blog/2023/06/29/centrifugo-v5-released#conclusion","content":" That's all about most notable things happened in Centrifugo v5. We updated documentation to reflect the changes in v5, also some documentation chapters were rewritten. For example, take a look at the refreshed Design overview. Several more changes and details outlined in the migration guide for Centifugo v5. Please feel free to contact in the community rooms if you have questions about the release. And as usual, let the Centrifugal force be with you! ","version":null,"tagName":"h2"},{"title":"Asynchronous message streaming to Centrifugo with Benthos","type":0,"sectionRef":"#","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos","content":"","keywords":"","version":null},{"title":"Start Centrifugo​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#start-centrifugo","content":" First start Centrifugo (with debug logging to see incoming API requests in logs):  centrifugo genconfig centrifugo -c config.json --log_level debug   Hope this step is already simple for you, if not - check out Quickstart tutorial.  ","version":null,"tagName":"h2"},{"title":"Install and run Benthos​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#install-and-run-benthos","content":" Benthos is an awesome tool which allows consuming data from various inputs, process data, then output it into configured outputs. See more detailed description on Benthos' website.  The number of inputs supported by Benthos is huge: check it out here. Most of the major systems widely used these days are supported. Benthos also supports many outputs – but here we only interested in message transfer to Centrifugo. There is no built-in Centrifugo output in Benthos but it provides a generic http_client output which may be used to send requests to any HTTP server. Benthos may also help with retries, provides tools for additional data processing and transformations.    Just like Centrifugo Benthos written in Go language – so its installation is very straighforward and similar to Centrifugo. See official instructions.  Let's assume you've installed Benthos and have benthos executable available in your system. Let's create Benthos configuration file:  benthos create &gt; config.yaml   Take a look at generated config.yaml - it contains various options for Benthos instance, the most important (for the context of this post) are input and output sections.  And after that you can start Benthos instance with:  benthos -c config.yaml   Now we need to tell Benthos from where to get data and how to send it to Centrifugo.  ","version":null,"tagName":"h2"},{"title":"Configure Benthos input and output​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#configure-benthos-input-and-output","content":" For our example here we will user Redis List as an input, won't add any additional data processing and will output messages consumed from Redis List into Centrifugo publish server HTTP API.  To achieve this add the following as input in Benthos config.yaml:  input: label: &quot;centrifugo_redis_consumer&quot; redis_list: url: &quot;redis://127.0.0.1:6379&quot; key: &quot;centrifugo.publish&quot;   And configure the output like this:  output: label: &quot;centrifugo_http_publisher&quot; http_client: url: &quot;http://localhost:8000/api/publish&quot; verb: POST headers: X-API-Key: &quot;&lt;CENTRIFUGO_API_KEY&gt;&quot; timeout: 5s max_in_flight: 20   The output points to Centrifugo publish HTTP API. Replace &lt;CENTRIFUGO_API_KEY&gt; with your Centrifugo api_key (found in Centrifugo configuration file).  ","version":null,"tagName":"h2"},{"title":"Push messages to Redis queue​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#push-messages-to-redis-queue","content":" Start Benthos instance:  benthos -c config.yaml   You will see errors while Benthos tries to connect to input Redis source. So start Redis server:  docker run --rm -it --name redis redis:7   Now connect to Redis (using redis-cli):  docker exec -it redis redis-cli   And push command to Redis list:  127.0.0.1:6379&gt; rpush centrifugo.publish '{&quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: {&quot;input&quot;: &quot;test&quot;}}' (integer) 1   This message will be consumed from Redis list by Benthos and published to Centrifugo HTTP API. If you have active subscribers to channel chat – you will see messages delivered to them. That's it!  tip When using Redis List input you can scale out Benthos instances to run several of them if needed.  ","version":null,"tagName":"h2"},{"title":"Demo​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#demo","content":" Here is a quick demonstration of the described integration. See how we push messages into Redis List and those are delivered to WebSocket clients:  Sorry, your browser doesn't support embedded video.  ","version":null,"tagName":"h2"},{"title":"Pitfalls of async publishing​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#pitfalls-of-async-publishing","content":" This all seems simple. But publishing messages asynchronously may highlight some pitfalls not visible or not applicable for synchronous publishing to Centrifugo API.  ","version":null,"tagName":"h2"},{"title":"Late delivery​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#late-delivery","content":" Most of the time it will work just fine. But one day you can come across intermediate queue growth and increased delivery lag. This may happen due to temporary Centrifugo or worker process unavailability. As soon as system comes back to normal queued messages will be delivered.  Depending on the real-time feature implemented this may be a concern to think about and decide whether this is desired or not. Your application should be designed accordingly to deal with late delivery.  BTW late delivery may be a case even with synchronous publishing – it just almost never strikes. But theoretically client can reload browser page and load initial app state while message flying from the backend to client over Centrifugo. It's not Centrifugo specific actually - it's just a nature of networks and involved latencies.  In general solution to prevent late delivery UX issues completely is using object versioning. Object version should be updated in the database on every change from which the real-time event is generated. Attach object version information to every real-time message. Also get object version on initial state load. This way you can compare versions and drop non-actual real-time messages on client side.  Possible strategy may be using synchronous API for real-time features where at most once delivery is acceptable and use asynchronous delivery where you need to deliver messages with at least once guarantees. In a latter case you most probably designed proper idempotency behaviour on client side anyway.  ","version":null,"tagName":"h3"},{"title":"Ordering concerns​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#ordering-concerns","content":" Another thing to consider is message ordering. Centrifugo itself may provide message ordering in channels. If you published one message to Centrifugo API, then another one – you can expect that messages will be delivered to a client in the same order. But as soon as you have an intermediary layer like Benthos or any other asynchronous worker process – then you must be careful about ordering. In case of Benthos and example here you can set max_in_flight parameter to 1 instead of 20 and keep only one instance of Benthos running to preserve ordering.  In case of streaming from Kafka you can rely on Kafka message partitioning feature to preserve message ordering.  ","version":null,"tagName":"h3"},{"title":"Throughput when ordering preserved​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#throughput-when-ordering-preserved","content":" If you preserved ordering in your asynchronous workers then the next thing to consider is throughput limitations.  You have a limited number of workers, these workers send requests to Centrifugo one by one. In this case throughput is limited by the number of workers and RTT (round-trip time) between worker process and Centrifugo.  If we talk about using Redis List structure as a queue - you can possibly shard data amongst different Redis List queues by some key to improve throughput. In this case you need to push messages where order should be preserved into a specific queue. In this case your get a setup similar to Kafka partitioning.  In case of using manually partitioned queues or using Kafka you can have parallelism equal to the number of partitions.  Let's say you have 20 workers which can publish messages in parallel and 5ms RTT time between worker and Centrifugo. In this case you can publish 20*(1000/5) = 4000 messages per second max.  To improve throughput futher consider increasing worker number or batching publish requests to Centrifugo (using Centrifugo's batch API).  ","version":null,"tagName":"h3"},{"title":"Error handling​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#error-handling","content":" When publishing asynchronously you should also don't forget about error handling. Benthos will handle network errors automatically for you. But there could be internal errors from Centrifugo returned as part of response. It's not very convenient to handle with Benthos out of the box – so we think about adding transport-level error mode to Centrifugo.  ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Asynchronous message streaming to Centrifugo with Benthos","url":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos#conclusion","content":" Sometimes you want to publish to Centrifugo asynchronously using messaging systems convenient for your company. Usually you can write worker process to re-publish messages to Centrifugo. Sometimes it may be simplified using helpful tools like Benthos.  Here we've shown how Benthos may be used to transfer messages from Redis List queue to Centrifugo API. With some modifications you can achieve the same for other input sources - such as Kafka, RabbitMQ, Nats Jetstream, etc.  But publishing messages asynchronously highlights several pifalls - like late delivery, ordering issues, throughput considerations and error handling – which should be carefully addressed. Different real-time features may require different strategies. ","version":null,"tagName":"h2"},{"title":"Using Centrifugo in RabbitX","type":0,"sectionRef":"#","url":"/blog/2023/08/29/using-centrifugo-in-rabbitx","content":"This post introduces a new format in Centrifugal blog – interview with a Centrifugo user! Let's dive into an exciting chat with the engineering team of RabbitX platform, a global permissionless perpetuals exchange powered on Starknet. We will discover how Centrifugo helped RabbitX to build a broker platform with current trading volume of 25 million USD daily! 🚀🎉 [Q] Hey team - thanks for your desire to share your Centrifugo use case. First of all, could you provide some information about RabbitX - what is it?​ RabbitX is a global permissionless perpetuals exchange built on Starknet. RabbitX is building the most secure and liquid global derivatives network, giving you 24/7 access to global markets anywhere in the world, with 20x leverage. In its core there is an orderbook - where traders match against market makers, which require to support high throughput and low latency tech stack. The technologies that we are using: Tarantool as in-memory database and business logic serverCentrifugo as our major websocket serverDifferent stark tech to support decentralized settlement [Q] Great! What is the goal of Centrifugo in your project? Which real-time features you have?​ Almost all the information users see in our terminal is streamed over Centrifugo. We use it for financial order books, candlestick chart updates, and stat number updates. We can also send real-time personal user notifications via Centrifugo. Instead of all the words, here is a short recording of our terminal trading BTC: [Q] We know that you are using Centrifugo Tarantool engine - could you explain why and how it works in your case?​ Well, that's an interesting thing. We heavily use Tarantool in our system. It grants us immense flexibility, performance, and the power to craft whatever we envision. It ensures the atomicity essential for trading match-making. When we were in search of a WebSocket real-time bus for messages, we were pleasantly surprised to discover that Centrifugo integrates with Tarantool. In our scenario, this allowed us to bypass additional network round-trips, as we can stream data directly from Tarantool to Centrifugo channels. Reducing latency is paramount for financial instruments. Furthermore, I can mention that over our nine months in production, we didn't encounter any issues with Centrifugo – it performed flawlessly! Regarding authentication, we employ Centrifugo's JWT authentication and subscribe proxy. Thus, subscriptions are authorized on our specialized service written in Go. We're also actively using Centrifugo possibility to send initial channel data in the subscribe proxy response. One challenge we overcame was bridging the gap between the subscription's initial request and the continuous message stream in the order book component. To address this, we employed our own sequence numbers in events, coupled with Centrifugo's channel history – this allowed us to deal with missed events when needed. Actually the gaps in event stream are rare in practice and our workaround not needed most of the time, but now we're confident our users never experience this issue. [Q] Looking at RabbitX terminal app we see quite modern UI - could you share more details about it too?​ Our frontend is built on top of React in combination with TradingView Supercharts. And of course we are using centrifuge-js SDK for establishing connections with Centrifugo. [Q] So you are nine months in production at this point. Can you share some real world numbers related to your Centrifugo setup?​ At this point we can have up to a thousand active concurrent traders and send more than 60 messages per second towards one client in peak hours. All the load is served with a single Centrifugo instance (and we have one standby instance). [Q] Anything else you want to share with readers of Centrifugal blog?​ When we designed the system the main goal was to have a homogeneous tech zoo, with a small amount of different technologies, to keep the number of failure points as small as possible. Tarantool is a sort of technology that really allows us to achieve this, we were able to add different decentralized mechanics to our system because of that. It’s not only an in-memory database, but in reality the app server as well. In our case, the fact Centrifugo supports Tarantool broker was a big discovery – the integration went smoothly, and everything has been working great since then.","keywords":"","version":null},{"title":"Discovering Centrifugo PRO: push notifications API","type":0,"sectionRef":"#","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications","content":"","keywords":"","version":null},{"title":"Centrifugo PRO goals​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#centrifugo-pro-goals","content":" When Centrifugo was originally created, its main goal was to help introduce real-time messaging features to existing systems, written in traditional frameworks which work on top of the worker/thread model. Serving many concurrent connections is a non-trivial task in general, and without native efficient concurrency support, it becomes mostly impossible without a shift in the technology stack. Integrating with Centrifugo makes it simple to introduce an efficient real-time layer, while keeping the existing application architecture.  As time went on, Centrifugo got some unique features which now justify its usage even in conjunction with languages/frameworks with good concurrency support. Simply using Centrifugo for at-most-once PUB/SUB may already save a lot of development time. The task, which seems trivial at first glance, has a lot of challenges in practice: client SDKs with reconnect and channel multiplexing, scalability to many nodes, WebSocket fallbacks, etc.  The combination of useful possibilities has made Centrifugo an attractive component for building enterprise-level applications. Let's be honest here - for pet projects, developers often prefer writing WebSocket communications themselves, and Centrifugo may be too heavy and an extra dependency. But in a corporate environment, the decision on which technology to use should take into account a lot of factors, like those we just mentioned above. Using a mature technology is often preferred to building from scratch and making all the mistakes along the way.  With the PRO version, our goal is to provide even more value for established businesses when switching to Centrifugo. We want to solve tricky cases and simplify them for our customers; we want to step into related areas where we see we can provide sufficient value.  One rule we try to follow for PRO features that extend Centrifugo’s scope is this: we are not trying to replicate something that already exists in other systems, but rather, we strive to improve upon it. We focus on solving practical issues that we observe, providing a unique value proposition for our customers. This post describes one such example — we will demonstrate our approach to push notifications, which is one the features of Centrifugo PRO.  ","version":null,"tagName":"h2"},{"title":"Why providing push notifications API​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#why-providing-push-notifications-api","content":"   Why provide a push notifications API at all? Well, actually, real-time messages and push notifications are so close that many developers hardly see the difference before starting to work with both more closely.  I’ve heard several stories where chat functionality on mobile devices was implemented using only native push notifications — without using a separate real-time transport like WebSocket while the app is in the foreground. While this is not a recommended approach due to the delivery properties of push notifications, it proves that real-time messages and push notifications are closely related concepts and sometimes may interchange with each other.  When developers introduce WebSocket communication in an application, they often ask the question—what should I do next to deliver some important messages to a user who is currently not actively using the application? WebSockets are great when the app is in the foreground, but when the app goes to the background, the recommended approach is to close the WebSocket connection. This is important to save battery, and operating systems force the closing of connections after some time anyway.  The delivery of important app data is then possible over push notifications. See a good overview of them on web.dev.  Previously, Centrifugo positioned itself solely as a transport layer for real-time messages. In our FAQ, we emphasized this fact and suggested using separate software products to send push notifications.  Now, with Centrifugo PRO, we provide this functionality to our customers. We have extended our server API with methods to manage and send push notifications. I promised to tell you why we believe our implementation is super cool. Let’s dive into the details.  ","version":null,"tagName":"h2"},{"title":"Push notifications API like no one provides​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#push-notifications-api-like-no-one-provides","content":" Push notifications are super handy, but there’s a bit to do to get them working right. Let's break it down!  On the user's side (frontend)​  Request permission from the user to receive push notifications.Integrate with the platform-specific notification service (e.g., Apple Push Notification Service for iOS, Firebase Cloud Messaging for Android) to obtain the device token.Send the device token to the server for storage and future use.Integrate with the platform-specific notification handler to listen for incoming push notificationsHandle incoming push notifications: display the notification content to the user, either as a banner, alert, or in-app message, depending on the user's preferences and the type of notification. Handle user actions on the notification, such as opening the app, dismissing the notification, or taking a specific action related to the notification content.  On the server (backend)​  Store device tokens in a database when received from the client sideRegularly clean up the database to remove stale or invalid device tokens. and handle scenarios where a device token becomes invalid or is revoked by the user, ensuring that no further notifications are sent to that device.Integrate with platform-specific notification services (e.g., APNS, FCM) to send notifications to devices. Handle errors or failures in sending notifications and implement retry mechanisms if necessary.Track the delivery status of each push notification sent out. Monitor the open rates, click-through rates, and other relevant metrics for the notifications.Use analytics to understand user behavior in response to notifications and refine the notification strategy based on insights gained.  We believe that we were able to achieve a unique combination of design decisions which allows us to provide push notification support like no one else provides. Let’s dive into what makes our approach special!  ","version":null,"tagName":"h2"},{"title":"Frontend decisions​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#frontend-decisions","content":" When providing the push notification feature, other solutions like Pusher or Ably also offer their own SDKs for managing notifications on the client side.  What we've learned, though, during the Centrifugo life cycle, is that creating and maintaining client SDKs for various environments (iOS, Android, Web, Flutter) is one of the hardest parts of the Centrifugo project.  So the decision here was simple and natural: Centrifugo PRO does not introduce any client SDKs for push notifications on the client side.  When integrating with Centrifugo, you can simply use the native SDKs provided by each platform. We bypass the complexities of SDK development and concentrate on server-side improvements. With this decision, we are not introducing any limitations to the client side.  You get:  Wealthy documentation and community support. Platforms like APNs provide comprehensive documentation, tutorials, and best practices, making the integration process smoother.Stability and reliability: native SDKs are rigorously tested and frequently updated by the platform providers. This ensures that they are stable, reliable, and free from critical bugs.Access to the latest features. As platform providers roll out new features or enhancements, native SDKs are usually the first to get updated. This ensures that your application can leverage the latest functionalities without waiting for SDKs to catch up.  This approach was not possible with our real-time SDKs, as WebSocket communication is very low-level, and Centrifugo’s main goal was to provide some high-level features on top of it. However, with push notifications, proceeding without a custom SDK seems like a choice beneficial for everyone.  ","version":null,"tagName":"h2"},{"title":"Server implementation​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#server-implementation","content":" The main work we did was on the server side. Let's go through the entire workflow of push notification delivery and describe what Centrifugo PRO provides for each step.  ","version":null,"tagName":"h2"},{"title":"How we keep tokens​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#how-we-keep-tokens","content":" Let's suppose you got the permission from the user and received the device push token. At this point you must save it to database for sending notifications later using this token. Centrifugo PRO provides API called device_register to do exactly this.  At this point, we use PostgreSQL for storing tokens – which is a very popular SQL database. Probably we will add more storage backend options in the future.  When calling Centrifugo device_register API you can provide user ID, list of topics to subscribe, platform from which the user came from (ios, android, web), also push notifications provider. To deliver push notifications to devices Centrifugo PRO integrates with the following push notification providers:  fcm - Firebase Cloud Messaging (FCM) hms - Huawei Messaging Service (HMS) Push Kit apns - Apple Push Notification service (APNs)     So we basically cover all the most popular platforms out of the box.  After registering the device token, Centrifugo PRO returns a device_id to you. This device ID must be stored on the client device. As long as the frontend has this device_id, it can update the device's push token information from time to time to keep it current (by just calling device_register again, but with device_id attached).  After saving the token, your backend can start sending push notifications to devices.  ","version":null,"tagName":"h3"},{"title":"How we send notifications​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#how-we-send-notifications","content":" To send push notifications we provide another API called send_push_notification. You need to provide some filter in the API request to tell Centrifugo who you want to send notification. You also need to provide push notification payload. For example, using Centrifugo HTTP API:  curl -X POST http://localhost:8000/api/send_push_notification \\ -H &quot;Authorization: apikey &lt;KEY&gt;&quot; \\ -d @- &lt;&lt;'EOF' { &quot;recipient&quot;: { &quot;filter&quot;: { &quot;topics&quot;: [&quot;test&quot;] } }, &quot;notification&quot;: { &quot;fcm&quot;: { &quot;message&quot;: { &quot;notification&quot;: {&quot;title&quot;: &quot;Hello&quot;, &quot;body&quot;: &quot;How are you?&quot;} } } } } EOF   Here is another important decision we made: Centrifugo PRO allows you to specify raw JSON objects for each provider we support. In other words, we do not wrap the push notifications API for FCM, APNS, HMS - we give you a way to construct the entire push notification message.  This means the Centrifugo push API supports all the fields of push notification payloads out-of-the-box, for all push providers. You can simply use the documentation of FCM, APNs, and send the constructed requests to Centrifugo. There is no need for us to update Centrifugo PRO in any way to support new fields added by providers to push APIs.  When you send a push notification with a filter and push payload for each provider you want, it's queued by Centrifugo. We use Redis Streams for queuing and optionally a queue based on PostgreSQL (less efficient, but still robust enough).  The fact that the notification is being queued means a very fast response time – so you can integrate with Centrifugo from within the hot paths of your application backend. You may additionally provide a push expiration time and a unique push identifier. If you have not provided a unique identifier, Centrifugo generates one for you and returns it in the response. The unique identifier may later be used to track push status in Centrifugo PRO's push notification analytics.  We then have efficient workers which process the queue with minimal latency and send push notifications using batch requests for each provider - i.e., we do this in the most effective way possible. We conducted a benchmark of our worker system with FCM – and we can easily send several million pushes per minute.  Another decision we made - Centrifugo PRO supports sending push notifications to a raw list of tokens. This makes it possible for our customers to use their own token storage. For example, such storage could already exist before you started using Centrifugo, or you might need a different storage/schema. In such cases, you can use Centrifugo just as an effective push sender server.  Finally, Centrifugo PRO supports sending delayed push notification - to queue push for a later delivery, so for example you can send notification based on user time zone and let Centrifugo PRO send it when needed. Or you may send slightly delayed push notification together with real-time message and if client provided an ack to real-time message - cancel push notification.  ","version":null,"tagName":"h3"},{"title":"Secure unified topics​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#secure-unified-topics","content":" FCM and HMS have a built-in way of sending notification to large groups of devices over topics mechanism (the same for HMS). One problem with native FCM or HMS topics though is that device can subscribe to any topic from the frontend side without any permission check. In today's world this is usually not desired. So Centrifugo PRO re-implements FCM and HMS topics by introducing an additional API to manage device subscriptions to topics.  Centrifugo PRO device topic subscriptions also add a way to introduce the missing topic semantics for APNs.  Centrifugo PRO additionally provides an API to create persistent bindings of user to notification topics. See user_topic_list and user_topic_update. As soon as user registers a device – it will be automatically subscribed to its own topics pre-created over the API. As soon as user logs out from the app and you update user ID of the device - user topics binded to the device automatically removed/switched.  This design solves one of the issues with push notifications (with FCM in particular) – if two different users use the same device it's becoming problematic to unsubscribe the device from large number of topics upon logout. Also, as soon as user to topic binding added (using user_topic_update API) – it will be synchronized across all user active devices. You can still manage such persistent subscriptions on the application backend side if you prefer and provide the full list inside device_register call - Centrifugo PRO API gives you freedom here.  ","version":null,"tagName":"h3"},{"title":"Push analytics​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#push-analytics","content":" Centrifugo PRO offers the ability to inspect sent push notifications using ClickHouse analytics. Push providers may also offer their own analytics, such as FCM, which provides insight into push notification delivery. Centrifugo PRO also offers a way to analyze push notification delivery and interaction using the update_push_status API. This API allows updating ClickHouse table and add status for each push sent:  deliveredor interacted  It's then possible to make queries to ClickHouse and build various analytical reports. Or use ClickHouse for real-time graphs - for example, from Grafana.  ","version":null,"tagName":"h3"},{"title":"Push notifications UI​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#push-notifications-ui","content":" Finally, Centrifugo PRO provides a simple web UI for inspecting registered devices. It can simplify development, provide a way to look at live data, and send simple push notification alerts to users or topics.    ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Discovering Centrifugo PRO: push notifications API","url":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications#conclusion","content":" We really believe in our push notifications and will be working hard to make them even better. The API we already have serves well to cover common push notification delivery use cases, but we won't stop here. Some areas for improvements are: functionality of built-in push notifications web UI, extending push analytics by providing user friendly UI for the insights about push delivery and engagement. The good thing is that we already have a ground for making this.  Take a look at the documentation of Centrifugo PRO push notification API for more formal details and some things not mentioned here. Probably at the time you are reading this we already added something great to the API.  Even though Centrifugo PRO is pretty new, it already has a lot of helpful features, and we have plans to add even more. You can see what’s coming up next on our Centrifugo PRO planned features board. We're excited to share more blog posts like this one in the future. ","version":null,"tagName":"h2"},{"title":"Framework integrations","type":0,"sectionRef":"#","url":"/docs/3/ecosystem/integrations","content":"Framework integrations tip In general, integrating Centrifugo can be done in several steps even without third-party libraries – see our integration guide. But there are some community-driven projects that provide integration for more native experience or even some additional functionality on top of Centrifugo. laravel-centrifugo integration with Laravel frameworklaravel-centrifugo-broadcaster one more integration with Laravel framework to considerCentrifugoBundle integration with Symfony frameworkDjango-instant integration with Django framework","keywords":"","version":"v3"},{"title":"flow_diagrams","type":0,"sectionRef":"#","url":"/docs/3/flow_diagrams","content":"flow_diagrams For swimlines.io: Client &lt;- App Backend: JWT note: The backend generates JWT for a user and passes it to the client side. Client -&gt; Centrifugo: Client connects to Centrifugo with JWT ...: {fas-spinner} Persistent connection established Client -&gt; Centrifugo: Client issues channel subscribe requests Centrifugo --&gt;&gt; Client: Client receives real-time updates from channels Client -&gt; Centrifugo: Connect request note: Client connects to Centrifugo without JWT. Centrifugo -&gt; App backend: Sends request further (via HTTP or GRPC) note: The application backend validates client connection and tells Centrifugo user credentials in Connect reply. App backend -&gt; Centrifugo: Connect reply Centrifugo -&gt; Client: Connect Reply ...: {fas-spinner} Persistent connection established Client -&gt; App Backend: Publish request note: Client sends data to publish to the application backend. Backend validates it, maybe modifies, optionally saves to the main database, constructs real-time update and publishes it to the Centrifugo server API. App Backend -&gt; Centrifugo: Publish over Centrifugo API Centrifugo --&gt;&gt; Client: {far-bolt fa-lg} Real-time notification note: Centrifugo delivers real-time message to active channel subscribers. Client -&gt; App Backend: Publish request note: Client sends data to publish to the application backend. Backend validates it, maybe modifies, optionally saves to the main database, constructs real-time update and publishes it to the Centrifugo server API. App Backend -&gt; Centrifugo: Publish over Centrifugo API Centrifugo --&gt;&gt; Client: {far-bolt fa-lg} Real-time notification note: Centrifugo delivers real-time message to active channel subscribers. ","keywords":"","version":"v3"},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/3/faq","content":"","keywords":"","version":"v3"},{"title":"How many connections can one Centrifugo instance handle?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#how-many-connections-can-one-centrifugo-instance-handle","content":" This depends on many factors. Real-time transport choice, hardware, message rate, size of messages, Centrifugo features enabled, client distribution over channels, compression on/off, etc. So no certain answer to this question exists. Common sense, performance measurements, and monitoring can help here.  Generally, we suggest not put more than 50-100k clients on one node - but you should measure for your use case.  You can find a description of a test stand with million WebSocket connections in this blog post. Though the point above is still valid – measure and monitor your setup.  ","version":"v3","tagName":"h3"},{"title":"Memory usage per connection?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#memory-usage-per-connection","content":" Depending on transport used and features enabled the amount of RAM required per each connection can vary.  For example, you can expect that each WebSocket connection will cost about 30-50 KB of RAM, thus a server with 1 GB of RAM can handle about 20-30k connections.  For other real-time transports, the memory usage per connection can differ (for example, SockJS connections will cost ~ 2 times more RAM than pure WebSocket connections). So the best way is again – measure for your custom case since depending on Centrifugo transport/features memory usage can vary.  ","version":"v3","tagName":"h3"},{"title":"Can Centrifugo scale horizontally?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#can-centrifugo-scale-horizontally","content":" Yes, it can do this using built-in engines: Redis, KeyDB, Tarantool, or Nats broker.  See engines and scalability considerations.  ","version":"v3","tagName":"h3"},{"title":"Message delivery model​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#message-delivery-model","content":" See design overview  ","version":"v3","tagName":"h3"},{"title":"Message order guarantees​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#message-order-guarantees","content":" See design overview.  ","version":"v3","tagName":"h3"},{"title":"Should I create channels explicitly?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#should-i-create-channels-explicitly","content":" No. By default, channels are created automatically as soon as the first client subscribed to it. And destroyed automatically when the last client unsubscribes from a channel.  When history inside the channel is on then a window of last messages is kept automatically during the retention period. So a client that comes later and subscribes to a channel can retrieve those messages using the call to the history API (or maybe by using the automatic recovery feature which also uses a history internally).  ","version":"v3","tagName":"h3"},{"title":"What about best practices with the number of channels?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#what-about-best-practices-with-the-number-of-channels","content":" Channel is a very lightweight ephemeral entity - Centrifugo can deal with lots of channels, don't be afraid to have many channels in an application.  But keep in mind that one client should be subscribed to a reasonable number of channels at one moment. Client-side subscription to a channel requires a separate frame from client to server – more frames mean more heavy initial connection, more heavy reconnect, etc.  One example which may lead to channel misusing is a messenger app where user can be part of many groups. In this case, using a separate channel for each group/chat in a messenger may be a bad approach. The problem is that messenger app may have chat list screen – a view that displays all user groups (probably with pagination). If you are using separate channel for each group then this may lead to lots of subscriptions. Also, with pagination, to receive updates from older chats (not visible on a screen due to pagination) – user may need to subscribe on their channels too. In this case, using a single personal channel for each user is a preferred approach. As soon as you need to deliver a message to a group you can use Centrifugo broadcast API to send it to many users. If your chat groups are huge in size then you may also need additional queuing system between your application backend and Centrifugo to broadcast a message to many personal channels.  ","version":"v3","tagName":"h3"},{"title":"Any way to exclude message publisher from receiving a message from a channel?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#any-way-to-exclude-message-publisher-from-receiving-a-message-from-a-channel","content":" Currently, no.  We know that services like Pusher provide a way to exclude current client by providing a client ID (socket ID) in publish request. A couple of problems with this:  Client can reconnect while message travels over wire/Backend/Centrifugo – in this case client has a chance to receive a message unexpectedly since it will have another client ID (socket ID)Client can call a history manually or message recovery process can run upon reconnect – in this case a message will present in a history  Both cases may result in duplicate messages. These reasons prevent us adding such functionality into Centrifugo, the correct application architecture requires having some sort of idempotent identifier which allow dealing with message duplicates.  Once added nobody will think about idempotency and this can lead to hard to catch/fix problems in an application. This can also make enabling channel history harder at some point.  Centrifugo behaves similar to Kafka here – i.e. channel should be considered as immutable stream of events where each channel subscriber simply receives all messages published to a channel.  In the future releases Centrifugo may have some sort of server-side message filtering, but we are searching for a proper and safe way of adding it.  ","version":"v3","tagName":"h3"},{"title":"Can I have both binary and JSON clients in one channel?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#can-i-have-both-binary-and-json-clients-in-one-channel","content":" No. It's not possible to transparently encode binary data into JSON protocol (without converting binary to base64 for example which we don't want to do due to increased complexity and performance penalties). So if you have clients in a channel which work with JSON – you need to use JSON payloads everywhere.  Most Centrifugo bidirectional connectors are using binary Protobuf protocol between a client and Centrifugo. But you can send JSON over Protobuf protocol just fine (since JSON is a UTF-8 encoded sequence of bytes in the end).  To summarize:  if you are using binary Protobuf clients and binary payloads everywhere – you are fine.if you are using binary or JSON clients and valid JSON payloads everywhere – you are fine.if you try to send binary data to JSON protocol based clients – you will get errors from Centrifugo.  ","version":"v3","tagName":"h3"},{"title":"Online presence for chat apps - online status of your contacts​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#online-presence-for-chat-apps---online-status-of-your-contacts","content":" While online presence is a good feature it does not fit well for some apps. For example, if you make a chat app - you may probably use a single personal channel for each user. In this case, you cannot find who is online at moment using the built-in Centrifugo presence feature as users do not share a common channel.  You can solve this using a separate service that tracks the online status of your users (for example in Redis) and has a bulk API that returns online status approximation for a list of users. This way you will have an efficient scalable way to deal with online statuses. This is also available as Centrifugo PRO feature.  ","version":"v3","tagName":"h3"},{"title":"Centrifugo stops accepting new connections, why?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#centrifugo-stops-accepting-new-connections-why","content":" The most popular reason behind this is reaching the open file limit. You can make it higher, we described how to do this nearby in this doc. Also, check out an article in our blog which mentions possible problems when dealing with many persistent connections like WebSocket.  ","version":"v3","tagName":"h3"},{"title":"Can I use Centrifugo without reverse-proxy like Nginx before it?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#can-i-use-centrifugo-without-reverse-proxy-like-nginx-before-it","content":" Yes, you can - Go standard library designed to allow this. Though proxy before Centrifugo can be very useful for load balancing clients.  ","version":"v3","tagName":"h3"},{"title":"Does Centrifugo work with HTTP/2?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#does-centrifugo-work-with-http2","content":" Yes, Centrifugo works with HTTP/2.  You can disable HTTP/2 running Centrifugo server with GODEBUG environment variable:  GODEBUG=&quot;http2server=0&quot; centrifugo -c config.json   Keep in mind that when using WebSocket you are working only over HTTP/1.1, so HTTP/2 support mostly makes sense for SockJS HTTP transports and unidirectional transports: like EventSource (SSE) and HTTP-streaming.  ","version":"v3","tagName":"h3"},{"title":"Is there a way to use a single connection to Centrifugo from different browser tabs?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#is-there-a-way-to-use-a-single-connection-to-centrifugo-from-different-browser-tabs","content":" If the underlying transport is HTTP-based, and you use HTTP/2 then this will work automatically. For WebSocket, each browser tab creates a new connection.  ","version":"v3","tagName":"h3"},{"title":"What if I need to send push notifications to mobile or web applications?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#what-if-i-need-to-send-push-notifications-to-mobile-or-web-applications","content":" Sometimes it's confusing to see a difference between real-time messages and push notifications. Centrifugo is a real-time messaging server. It can not send push notifications to devices - to Apple iOS devices via APNS, Android devices via GCM, or browsers over Web Push API. This is a goal for another software.  But the reasonable question here is how can you know when you need to send a real-time message to an online client or push notification to its device for an offline client. The solution is pretty simple. You can keep critical notifications for a client in the database. And when a client reads a message you should send an ack to your backend marking that notification as read by the client. Periodically you can check which notifications were sent to clients but they have not read it (no read ack received). For such notifications, you can send push notifications to its device using your own or another open-source solution. Look at Firebase for example.  ","version":"v3","tagName":"h3"},{"title":"How can I know a message is delivered to a client?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#how-can-i-know-a-message-is-delivered-to-a-client","content":" You can, but Centrifugo does not have such an API. What you have to do to ensure your client has received a message is sending confirmation ack from your client to your application backend as soon as the client processed the message coming from a Centrifugo channel.  ","version":"v3","tagName":"h3"},{"title":"Can I publish new messages over a WebSocket connection from a client?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#can-i-publish-new-messages-over-a-websocket-connection-from-a-client","content":" It's possible to publish messages into channels directly from a client (when publish channel option is enabled). But we strongly discourage this in production usage as those messages just go through Centrifugo without any additional control and validation from the application backend.  We suggest using one of the available approaches:  When a user generates an event it must be first delivered to your app backend using a convenient way (for example AJAX POST request for a web application), processed on the backend (validated, saved into the main application database), and then published to Centrifugo using Centrifugo HTTP or GRPC API.Utilize the RPC proxy feature – in this case, you can call RPC over Centrifugo WebSocket which will be translated to an HTTP request to your backend. After receiving this request on the backend you can publish a message to Centrifugo server API. This way you can utilize WebSocket transport between the client and your server in a bidirectional way. HTTP traffic will be concentrated inside your private network.Utilize the publish proxy feature – in this case client can call publish on the frontend, this publication request will be transformed into HTTP or GRPC call to the application backend. If your backend allows publishing - Centrifugo will pass the payload to the channel (i.e. will publish message to the channel itself).  Sometimes publishing from a client directly into a channel (without any backend involved) can be useful though - for personal projects, for demonstrations (like we do in our examples) or if you trust your users and want to build an application without backend. In all cases when you don't need any message control on your backend.  ","version":"v3","tagName":"h3"},{"title":"How to create a secure channel for two users only (private chat case)?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#how-to-create-a-secure-channel-for-two-users-only-private-chat-case","content":" There are several ways to achieve it:  use a private channel (starting with $) - every time a user subscribes to it your backend should provide a sign to confirm that subscription request. Read more in channels chapternext is user limited channels (with #) - you can create a channel with a name like dialog#42,567 to limit subscribers only to the user with id 42 and user with ID 567, this does not fit well for channels with many or dynamic possible subscribersyou can use subscribe proxy feature to validate subscriptions, see chapter about proxyfinally, you can create a hard-to-guess channel name (based on some secret key and user IDs or just generate and save this long unique name into your main app database) so other users won't know this channel to subscribe on it. This is the simplest but not the safest way - but can be reasonable to consider in many situations  ","version":"v3","tagName":"h3"},{"title":"What's the best way to organize channel configuration?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#whats-the-best-way-to-organize-channel-configuration","content":" In most situations, your application needs several different real-time features. We suggest using namespaces for every real-time feature if it requires some option enabled.  For example, if you need join/leave messages for a chat app - create a special channel namespace with this join_leave option enabled. Otherwise, your other channels will receive join/leave messages too - increasing load and traffic in the system but not used by clients.  The same relates to other channel options.  ","version":"v3","tagName":"h3"},{"title":"Does Centrifugo support webhooks?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#does-centrifugo-support-webhooks","content":" Proxy feature allows integrating Centrifugo with your session mechanism (via connect proxy) and provides a way to react to connection events (rpc, subscribe, publish). Also, it opens a road for bidirectional communication with RPC calls. And periodic connection refresh hooks are also there.  Centrifugo does not support unsubscribe/disconnect hooks – see the reasoning below.  ","version":"v3","tagName":"h3"},{"title":"Why Centrifugo does not have disconnect hooks?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#why-centrifugo-does-not-have-disconnect-hooks","content":" Centrifugo does not support disconnect hooks at this point.  First of all, there is no guarantee that the disconnect process will have a time to execute on the client-side (as the client can just switch off its device or simply lose internet connection). This means that a server may notice a connection loss with some delay (thanks to PING/PONG).  Also, Centrifugo node can be unexpectedly killed. So there is a chance that disconnect event won't have a chance to be emitted to the backend.  One more reason is that Centrifugo designed to scale to many concurrent connections. Think millions of them. As we mentioned in our blog there are cases when all connections start reconnecting at the same time. In this case Centrifugo could potentially generate lots of disconnect events. To reduce the load during connect process Centrifugo has JWT authentication. Even if disconnect events were queued/rate-limited there could be situations when your app processes disconnect hook while user already reconnected and connect event processed. This is a racy situation which you will need to handle somehow (possibly based on unique client ID attached to each connection).  If you need to know that client disconnected and program your business logic around this fact then the reasonable approach could be periodically call your backend from the client-side and update user status somewhere on the backend (use Redis maybe). This is a pretty robust solution where you can't occasionally miss disconnect events. You can also utilize Centrifugo refresh proxy for the task of periodic backend pinging. In this case you will notice that user (or particular client) left app with some delay – this may be a acceptable trade-off in many cases.  Having said that, processing disconnect events may be reasonable – as a best-effort solution while taking into account everything said above. Centrifuge library for Go language (which is the core of Centrifugo) supports client disconnect callbacks on a server-side – so technically the possibility exists. If someone comes with a use case which definitely wins from having disconnect hooks in Centrifugo we are ready to discuss this and try to design a proper solution together.  ","version":"v3","tagName":"h3"},{"title":"Is it possible to listen to join/leave events on the app backend side?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#is-it-possible-to-listen-to-joinleave-events-on-the-app-backend-side","content":" No, join/leave events are only available in the client protocol. In most cases join event can be handled by using subscribe proxy. Leave events are harder – there is no unsubscribe hook available (mostly the same reasons as for disconnect hook described above). So the workaround here can be similar to one for disconnect – ping an app backend periodically while client is subscribed and thus know that client is currently in a channel with some approximation in time.  ","version":"v3","tagName":"h3"},{"title":"How scalable is the online presence and join/leave features?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#how-scalable-is-the-online-presence-and-joinleave-features","content":" Online presence is good for channels with a reasonably small number of active subscribers. As soon as there are tons of active subscribers, presence information becomes very expensive in terms of bandwidth (as it contains full information about all clients in a channel).  There is presence_stats API method that can be helpful if you only need to know the number of clients (or unique users) in a channel. But in the case of the Redis engine even presence_stats call is not optimized for channels with more than several thousand active subscribers.  You may consider using a separate service to deal with presence status information that provides information in near real-time maybe with some reasonable approximation. Centrifugo PRO provides a user status feature which may fit your needs.  The same is true for join/leave messages - as soon as you turn on join/leave events for a channel with many active subscribers each subscriber starts generating indiviaual join/leave events. This may result in many messages sent to each subscriber in a channel, drastically multiplying amount of messages traveling through the system. Especially when all clients reconnect simulteniously. So be careful and estimate the possible load. There is no magic, unfortunately.  ","version":"v3","tagName":"h3"},{"title":"I have not found an answer to my question here:​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/3/faq#i-have-not-found-an-answer-to-my-question-here","content":" Ask in our community rooms:    ","version":"v3","tagName":"h3"},{"title":"Attributions","type":0,"sectionRef":"#","url":"/docs/3/attributions","content":"","keywords":"","version":"v3"},{"title":"Landing Page Images​","type":1,"pageTitle":"Attributions","url":"/docs/3/attributions#landing-page-images","content":" The following images have been used in the landing page.  Icons made by Freepik  https://www.flaticon.com/packs/web-development-19 ","version":"v3","tagName":"h2"},{"title":"Centrifuge library","type":0,"sectionRef":"#","url":"/docs/3/ecosystem/centrifuge","content":"Centrifuge library Centrifugo is a server built on top of Centrifuge library for Go language. Due to its standalone language-agnostic nature Centrifugo dictates some rules developers should follow when integrating. If you need more freedom and a more tight integration of real-time server with application business logic you may consider using Centrifuge library to build something similar to Centrifugo but with customized behavior. Library README has detailed description, link to examples and introduction post. Many Centrifugo features should be re-implemented when using Centrifuge - like API layer, admin web UI, proxy etc (if you need those of course). And you need to write backend in Go language. But core functionality like a client-server protocol (all Centrifugo client connectors work with Centrifuge library based server) and Redis engine to scale come out of the box. tip Many things said in Centrifugo doc can be considered as extra documentation for Centrifuge library (for example part about infrastructure tuning or transport description). But not all of them.","keywords":"","version":"v3"},{"title":"Design overview","type":0,"sectionRef":"#","url":"/docs/3/getting-started/design","content":"","keywords":"","version":"v3"},{"title":"Idiomatic usage​","type":1,"pageTitle":"Design overview","url":"/docs/3/getting-started/design#idiomatic-usage","content":" Originally Centrifugo was built with the unidirectional flow as the main approach. Though Centrifugo itself used a bidirectional protocol between a client and a server to allow client dynamically create subscriptions, Centrifugo did not allow using it for sending data from client to server.  With this approach publications travel only from server to a client. All requests that generate new data first go to the application backend (for example over AJAX call of backend API). The backend can validate the message, process it, save it into a database for long-term persistence – and then publish an event from a backend side to Centrifugo API.  This is a pretty natural workflow for applications since this is how applications traditionally work (without real-time features) and Centrifugo is decoupled from the application in this case.    During Centrifugo v2 life cycle this paradigm evolved a bit. It's now possible to send RPC requests from client to Centrifugo and the request will be then proxied to the application backend. Also, connection attempts and publications to channels can now be proxied. So bidirectional connection between client and Centrifugo is now available for utilizing by developers in both directions. For example, here is how publish diagram could look like when using publish request proxy feature:    So at the moment, the number of possible integration ways increased.  ","version":"v3","tagName":"h2"},{"title":"Message history considerations​","type":1,"pageTitle":"Design overview","url":"/docs/3/getting-started/design#message-history-considerations","content":" Idiomatic Centrifugo usage requires having the main application database from which initial and actual state can be loaded at any point in time.  While Centrifugo has channel history, it has been mostly designed to reduce the load on the main application database when all users reconnect at once (in case of load balancer configuration reload, Centrifugo restart, temporary network problems, etc). This allows to radically reduce the load on the application main database during reconnect storm. Since such disconnects are usually pretty short in time having a reasonably small number of messages cached in history is sufficient.  The addition of history iteration API shifts possible use cases a bit. Calling history chunk by chunk allows keeping larger number of publications per channel. But depending on Engine used and configuration of the underlying storage history stream persistence characteristics can vary. For example, with Memory Engine history will be lost upon Centrifugo restart. With Redis or Tarantool engines history will survive Centrifugo restarts but depending on a storage configuration it can be lost upon storage restart – so you should take into account storage configuration and persistence properties as well. For example, consider enabling Redis RDB and AOF, configure replication for storage high-availability, use Redis Cluster or maybe synchronous replication with Tarantool.  Centrifugo provides ways to distinguish whether the missed messages can't be restored from Centrifugo history upon recovery so a client should restore state from the main application database. So Centrifugo message history can be used as a complementary way to restore messages and thus reduce a load on the main application database most of the time.  ","version":"v3","tagName":"h2"},{"title":"Message delivery model​","type":1,"pageTitle":"Design overview","url":"/docs/3/getting-started/design#message-delivery-model","content":" By default, the message delivery model of Centrifugo is at most once. With history and the position/recovery features enabled it's possible to achieve at least once guarantee within history retention time and size. After abnormal disconnect clients have an option to recover missed messages from the publication stream cache that Centrifugo maintains.  Without the positioning or recovery features enabled a message sent to Centrifugo can be theoretically lost while moving towards clients. Centrifugo tries to do its best to prevent message loss on a way to online clients, but the application should tolerate a loss.  As noted Centrifugo has a feature called message recovery to automatically recover messages missed due to short network disconnections. Also, it compensates at most once delivery of broker (Redis, Tarantool) PUB/SUB by using additional publication offset checks and periodic offset synchronization.  At this moment Centrifugo message recovery is designed for a short-term disconnect period (think no more than one hour for a typical chat application, but this can vary). After this period (which can be configured per channel basis) Centrifugo removes messages from the channel history cache. In this case, Centrifugo may tell the client that some messages can not be recovered, so your application state should be loaded from the main database.  ","version":"v3","tagName":"h2"},{"title":"Message order guarantees​","type":1,"pageTitle":"Design overview","url":"/docs/3/getting-started/design#message-order-guarantees","content":" Message order in channels is guaranteed to be the same while you publish messages into channel one after another or publish them in one request. If you do parallel publications into the same channel then Centrifugo can't guarantee message order since those may be processed concurrently by Centrifugo.  ","version":"v3","tagName":"h2"},{"title":"Graceful degradation​","type":1,"pageTitle":"Design overview","url":"/docs/3/getting-started/design#graceful-degradation","content":" It is recommended to design an application in a way that users don't even notice when Centrifugo does not work. Use graceful degradation. For example, if a user posts a new comment over AJAX to your application backend - you should not rely only on Centrifugo to receive a new comment from a channel and display it. You should return new comment data in AJAX call response and render it. This way user that posts a comment will think that everything works just fine. Be careful to not draw comments twice in this case - think about idempotent identifiers for your entities.  ","version":"v3","tagName":"h2"},{"title":"Online presence considerations​","type":1,"pageTitle":"Design overview","url":"/docs/3/getting-started/design#online-presence-considerations","content":" Online presence in a channel is designed to be eventually consistent. It will return the correct state most of the time. But when using Redis or Tarantool engines, due to the network failures and unexpected shut down of Centrifugo node, there are chances that clients can be presented in a presence up to one minute more (until presence entry expiration).  Also, channel presence does not scale well for channels with lots of active subscribers. This is due to the fact that presence returns the entire snapshot of all clients in a channel – as soon as the number of active subscribers grows the response size becomes larger. In some cases, presence_stats API call can be sufficient to avoid receiving the entire presence state.  ","version":"v3","tagName":"h2"},{"title":"Scalability considerations​","type":1,"pageTitle":"Design overview","url":"/docs/3/getting-started/design#scalability-considerations","content":" Centrifugo can scale horizontally with built-in engines (Redis, Tarantool, KeyDB) or with Nats broker. See engines.  All supported brokers are fast – they can handle hundreds of thousands of requests per second. This should be enough for most applications.  But, if you approach broker resource limits (CPU or memory) then it's possible:  Use Centrifugo consistent sharding support to balance queries between different broker instances (supported for Redis, KeyDB, Tarantool)Use Redis Cluster (it's also possible to consistently shard data between different Redis Clusters)Nats broker should scale well itself in cluster setup  All brokers can be set up in highly available way so there won't be a single point of failure.  All Centrifugo data (history, online presence) is designed to be ephemeral and have an expiration time. Due to this fact and the fact that Centrifugo provides hooks for the application to understand history loss makes the process of resharding mostly automatic. As soon as you need to add additional broker shard (when using client-side sharding) you can just add it to the configuration and restart Centrifugo. Since data is sharded consistently part of the data will stay on the same broker nodes. Applications should handle cases that channel data moved to another shard and restore a state from the main application database when needed. ","version":"v3","tagName":"h2"},{"title":"Main highlights","type":0,"sectionRef":"#","url":"/docs/3/getting-started/highlights","content":"","keywords":"","version":"v3"},{"title":"Simple integration​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#simple-integration","content":" Centrifugo was originally designed to be used in conjunction with frameworks without built-in concurrency support (like Django, Laravel, etc.). It works as a standalone service with well-defined communication contracts. It fits very well in both monolithic and microservice architecture. Application developers should not change backend philosophy at all – just integrate with Centrifugo HTTP or GRPC API and let users enjoy real-time updates.  ","version":"v3","tagName":"h3"},{"title":"Great performance​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#great-performance","content":" Centrifugo is pretty fast. It's written in Go language, uses fast and battle-tested open-source libraries internally, has some internal optimizations like message queuing on broadcasts, smart batching to reduce the number of RTT with broker, connection hub sharding to avoid contention, JSON and Protobuf encoding speedups over code generation and other.  See a Million WebSocket with Centrifugo post in our blog to see some real-world numbers.  ","version":"v3","tagName":"h3"},{"title":"Built-in scalability​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#built-in-scalability","content":" Centrifugo scales to many machines with a help of PUB/SUB brokers. The main PUB/SUB engine Centrifugo integrates with is Redis. It supports client-side consistent sharding and Redis Cluster – so a single Redis instance won't be a bottleneck also. There are other options to scale: KeyDB, Nats, Tarantool.  ","version":"v3","tagName":"h3"},{"title":"Strict client protocol​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#strict-client-protocol","content":" Centrifugo supports JSON and binary Protobuf protocol for client-server communication. The bidirectional protocol is defined by strict schema and several ready-to-use connectors wrap this protocol, handle asynchronous message passing, timeouts, reconnect, and various Centrifugo client API features.  ","version":"v3","tagName":"h3"},{"title":"Variety of real-time transports​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#variety-of-real-time-transports","content":" The main transport in Centrifugo is WebSocket. It's a bidirectional transport on top of TCP with low overhead. For browsers that do not support WebSocket Centrifugo provides SockJS support.  Centrifugo v3 also introduced support for unidirectional transports for real-time updates: like SSE (EventSource), HTTP streaming, GRPC unidirectional stream. Using unidirectional transport is sufficient for many real-time applications and does not require using custom client connectors – just native APIs or GRPC-generated code.  ","version":"v3","tagName":"h3"},{"title":"Flexible authentication​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#flexible-authentication","content":" Centrifugo can authenticate connections using JWT (JSON Web Token) or by issuing an HTTP/GRPC request to your application backend upon connection attempt. It's possible to proxy original request headers or request metadata (in the case of GRPC connection). It supports the JWK specification.  ","version":"v3","tagName":"h3"},{"title":"Connection management​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#connection-management","content":" Connections can expire, developers can choose a way to handle connection refresh – using client-side refresh workflow, or server-side call from Centrifugo to the application backend.  ","version":"v3","tagName":"h3"},{"title":"Channel (room) concept​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#channel-room-concept","content":" Centrifugo is a PUB/SUB server – users subscribe to channels to receive real-time updates. Message sent to a channel will be delivered to all active subscribers.  There are several different types of channels to deal with permissions.  ","version":"v3","tagName":"h3"},{"title":"Different types of subscriptions​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#different-types-of-subscriptions","content":" Centrifugo is unique in terms of the fact that it supports both client-side and server-side channel subscriptions.  ","version":"v3","tagName":"h3"},{"title":"RPC over bidirectional connection​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#rpc-over-bidirectional-connection","content":" You can fully utilize bidirectional persistent connections by sending RPC calls from the client-side to a configured endpoint on your backend. Calling RPC over WebSocket avoids sending headers on each request – thus reducing external traffic and, in most cases, provides better latency characteristics.  ","version":"v3","tagName":"h3"},{"title":"Online presence information​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#online-presence-information","content":" It's possible to turn on an online presence feature for channels so you will have information about active channel subscribers. Channel join and leave events (when a user subscribes/unsubscribes) can also be sent.  ","version":"v3","tagName":"h3"},{"title":"Message history in channels​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#message-history-in-channels","content":" Optionally Centrifugo allows turning on history for publications in channels. This publication history has a limited size and retention period (TTL). With a channel history, Centrifugo can help to survive the mass reconnect scenario. Clients can automatically recover missed messages from a cache – thus reducing the load on your primary database. It's also possible to manually iterate over a stream from a client or a server-side.  ","version":"v3","tagName":"h3"},{"title":"Embedded admin web UI​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#embedded-admin-web-ui","content":" Built-in administrative web UI allows publishing messages to channels, looking at Centrifugo cluster state, monitoring stats, etc.  ","version":"v3","tagName":"h3"},{"title":"Cross-platform​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#cross-platform","content":" Centrifugo works on Linux, macOS, and Windows.  ","version":"v3","tagName":"h3"},{"title":"Ready to deploy​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#ready-to-deploy","content":" Centrifugo supports various deploy ways: in Docker, using prepared RPM or DEB packages, via Kubernetes Helm chart. It supports automatic TLS with Let's Encrypt TLS, outputs Prometheus/Graphite metrics, has an official Grafana dashboard for Prometheus data source.  ","version":"v3","tagName":"h3"},{"title":"Open-source​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#open-source","content":" Centrifugo stands on top of open-source library Centrifuge (MIT license). The OSS version of Centrifugo is based on the permissive open-source license (Apache 2.0). All client connectors are also MIT-licensed.  ","version":"v3","tagName":"h3"},{"title":"Pro features​","type":1,"pageTitle":"Main highlights","url":"/docs/3/getting-started/highlights#pro-features","content":" Centrifugo PRO extends Centrifugo with several unique features which can give interesting advantages for business adopters.  With Centrifugo PRO it's possible to trace specific user or specific channel events in real-time. Centrifugo PRO integrates with ClickHouse for real-time connection analytics. This all may help with understanding client behavior, inspect and analyze an application on a very granular level.  Centrifugo PRO offers even more extensions that tend to be useful in practice. This includes user active status and throttling features. Active status is useful to build messenger-like applications where you want to show online indicators of users based on last activity time, throttling can help you limit the number of operations each user may execute on a Centrifugo cluster.  For additional details, refer to the Centrifugo PRO documentation. ","version":"v3","tagName":"h3"},{"title":"Install Centrifugo","type":0,"sectionRef":"#","url":"/docs/3/getting-started/installation","content":"","keywords":"","version":"v3"},{"title":"Install from the binary release​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/3/getting-started/installation#install-from-the-binary-release","content":" For a local development the simplest way to get Centrifugo is from binary release (i.e. single all-contained executable file).  Binary releases available on Github. Download latest release for your operating system, unpack it and you are done. Centrifugo is pre-built for:  Linux 64-bit (linux_amd64)Linux 32-bit (linux_386)Linux ARM 64-bit (linux_arm64)MacOS (darwin_amd64)MacOS on Apple Silicon (darwin_arm64)Windows (windows_amd64)FreeBSD (freebsd_amd64)ARM v6 (linux_armv6)  Archives contain a single statically compiled binary centrifugo file that is ready to run:  ./centrifugo -h   See the version of Centrifugo:  ./centrifugo version   Centrifugo requires a configuration file with several secret keys. If you are new to Centrifugo then there is genconfig command which generates a minimal configuration file to get started:  ./centrifugo genconfig   It creates a configuration file config.json with some auto-generated option values in a current directory (by default).  tip It's possible to generate file in YAML or TOML format, i.e. ./centrifugo genconfig -c config.toml  Having a configuration file you can finally run Centrifugo instance:  ./centrifugo --config=config.json   We will talk about a configuration in detail in the next sections.  You can also put or symlink centrifugo into your bin OS directory and run it from anywhere:  centrifugo --config=config.json   ","version":"v3","tagName":"h2"},{"title":"Docker image​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/3/getting-started/installation#docker-image","content":" Centrifugo server has a docker image available on Docker Hub.  docker pull centrifugo/centrifugo   Run:  docker run --ulimit nofile=65536:65536 -v /host/dir/with/config/file:/centrifugo -p 8000:8000 centrifugo/centrifugo centrifugo -c config.json   Note that docker allows setting nofile limits in command-line arguments which is pretty important to handle lots of simultaneous persistent connections and not run out of open file limit (each connection requires one file descriptor). See also infrastructure tuning chapter.  caution Pin to the exact Docker Image tag in production, for example: centrifugo/centrifugo:v3.0.0, this will help to avoid unexpected problems during re-deploy process.  ","version":"v3","tagName":"h2"},{"title":"Docker-compose example​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/3/getting-started/installation#docker-compose-example","content":" Create configuration file config.json:  { &quot;token_hmac_secret_key&quot;: &quot;my_secret&quot;, &quot;api_key&quot;: &quot;my_api_key&quot;, &quot;admin_password&quot;: &quot;password&quot;, &quot;admin_secret&quot;: &quot;secret&quot;, &quot;admin&quot;: true }   Create docker-compose.yml:  centrifugo: container_name: centrifugo image: centrifugo/centrifugo:v3 volumes: - ./config.json:/centrifugo/config.json command: centrifugo -c config.json ports: - 8000:8000 ulimits: nofile: soft: 65535 hard: 65535   Run with:  docker-compose up   ","version":"v3","tagName":"h2"},{"title":"Kubernetes Helm chart​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/3/getting-started/installation#kubernetes-helm-chart","content":" See our official Kubernetes Helm chart. Follow instructions in a Centrifugo chart README to bootstrap Centrifugo inside your Kubernetes cluster.  ","version":"v3","tagName":"h2"},{"title":"RPM and DEB packages for Linux​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/3/getting-started/installation#rpm-and-deb-packages-for-linux","content":" Every time we make a new Centrifugo release we upload rpm and deb packages for popular Linux distributions on packagecloud.io.  At moment, we support versions of the following distributions:  64-bit Debian 9 Stretch64-bit Debian 10 Buster64-bit Debian 11 Bullseye64-bit Ubuntu 16.04 Xenial64-bit Ubuntu 18.04 Bionic64-bit Ubuntu 20.04 Focal Fossa64-bit Centos 764-bit Centos 8  See full list of available packages and installation instructions.  Centrifugo also works on 32-bit architecture, but we don't support packaging for it since 64-bit is more convenient for servers today.  ","version":"v3","tagName":"h2"},{"title":"With brew on macOS​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/3/getting-started/installation#with-brew-on-macos","content":" If you are developing on macOS then you can install Centrifugo over brew:  brew tap centrifugal/centrifugo brew install centrifugo   ","version":"v3","tagName":"h2"},{"title":"Build from source​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/3/getting-started/installation#build-from-source","content":" You need Go language installed:  git clone https://github.com/centrifugal/centrifugo.git cd centrifugo go build ./centrifugo  ","version":"v3","tagName":"h2"},{"title":"Integration guide","type":0,"sectionRef":"#","url":"/docs/3/getting-started/integration","content":"","keywords":"","version":"v3"},{"title":"0. Install​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#0-install","content":" First, you need to do is download/install Centrifugo server. See install chapter for details.  ","version":"v3","tagName":"h2"},{"title":"1. Configure Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#1-configure-centrifugo","content":" Create basic configuration file with token_hmac_secret_key (or token_rsa_public_key) and api_key set and then run Centrifugo. See this chapter for details about token_hmac_secret_key/token_rsa_public_key and chapter about server API for API description. The simplest way to do this automatically is by using genconfig command:  ./centrifugo genconfig   – which will generate config.json file for you with all required fields.  Properly configure allowed_origins option.  ","version":"v3","tagName":"h2"},{"title":"2. Configure your backend​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#2-configure-your-backend","content":" In the configuration file of your application backend register several variables: Centrifugo secret and Centrifugo API key you set on a previous step and Centrifugo API address. By default, the API address is http://localhost:8000/api. You must never reveal token secret and API key to your users.  ","version":"v3","tagName":"h2"},{"title":"3. Connect to Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#3-connect-to-centrifugo","content":" Now your users can start connecting to Centrifugo. You should get a client library (see list of available client SDK) for your application frontend. Every library has a method to connect to Centrifugo. See information about Centrifugo connection endpoints here. Every client should provide a connection token (JWT) on connect. You must generate this token on your backend side using Centrifugo secret key you set to backend configuration (note that in the case of RSA tokens you are generating JWT with a private key). See how to generate this JWT in special chapter. You pass this token from the backend to your frontend app (pass it in template context or use separate request from client-side to get user-specific JWT from backend side). And use this token when connecting to Centrifugo (for example browser client has a special method setToken).  There is also a way to authenticate connections without using JWT - see chapter about proxying to backend.  You are connecting to Centrifugo using one of the available transports. At this moment you can choose from:  WebSocket, with JSON or binary protobuf protocol. See more info in a chapter about WebSocket transportSockJS (only supports JSON protocol). See more info about SockJS transport  ","version":"v3","tagName":"h2"},{"title":"4. Subscribe to channels​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#4-subscribe-to-channels","content":" After connecting to Centrifugo subscribe clients to channels they are interested in. See more about channels in special chapter. All client libraries provide a way to handle messages coming to a client from a channel after subscribing to it.  There is also a way to subscribe connection to a list of channels on the server side at the moment of connection establishment. See chapter about server-side subscriptions.  ","version":"v3","tagName":"h2"},{"title":"5. Publish to channel​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#5-publish-to-channel","content":" So everything should work now – as soon as a user opens some page of your application it must successfully connect to Centrifugo and subscribe to a channel (or channels). Now let's imagine you want to send a real-time message to users subscribed on a specific channel. This message can be a reaction to some event that happened in your app: someone posted a new comment, the administrator just created a new post, the user pressed the like button, etc. Anyway, this is an event your backend just got, and you want to immediately share it with interested users. You can do this using Centrifugo HTTP API. To simplify your life we have several API libraries for different languages. You can publish messages into a channel using one of those libraries or you can simply follow API description to construct API requests yourself - this is very simple. Also Centrifugo supports GRPC API. As soon as you published a message to the channel it must be delivered to your client.  ","version":"v3","tagName":"h2"},{"title":"6. Deploy to production​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#6-deploy-to-production","content":" To put this all into production you need to deploy Centrifugo on your production server. To help you with this we have many things like Docker image, rpm and deb packages, Nginx configuration. See Infrastructure tuning chapter for some actions you have to do to prepare your server infrastructure for handling many persistent connections.  ","version":"v3","tagName":"h2"},{"title":"7. Monitor Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#7-monitor-centrifugo","content":" Don't forget to monitor your production Centrifugo setup.  ","version":"v3","tagName":"h2"},{"title":"8. Scale Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#8-scale-centrifugo","content":" As soon as you are close to machine resource limits you may want to scale Centrifugo – you can run many Centrifugo instances and load-balance clients between them using Redis engine.  ","version":"v3","tagName":"h2"},{"title":"9. Read FAQ​","type":1,"pageTitle":"Integration guide","url":"/docs/3/getting-started/integration#9-read-faq","content":" That's all for basics. The documentation actually covers lots of other concepts Centrifugo server has: scalability, private channels, admin web interface, SockJS fallback, Protobuf support, and more. And don't forget to read our FAQ. ","version":"v3","tagName":"h2"},{"title":"Centrifugo introduction","type":0,"sectionRef":"#","url":"/docs/3/getting-started/introduction","content":"","keywords":"","version":"v3"},{"title":"Motivation​","type":1,"pageTitle":"Centrifugo introduction","url":"/docs/3/getting-started/introduction#motivation","content":" Centrifugo was born to help applications with a server-side written in a language or a framework without built-in concurrency support. In this case, dealing with persistent connections is a real headache that usually can only be resolved by introducing a shift in the technology stack and spending enough time to create a production-ready solution.  For example, frameworks like Django, Flask, Yii, Laravel, Ruby on Rails, and others have poor and not performant support of working with many persistent connections for the real-time messaging task.  In this case, Centrifugo is a very straightforward and non-obtrusive way to introduce real-time updates and handle lots of persistent connections without radical changes in application backend architecture. Developers could proceed writing a backend with a favorite language or favorite framework, keep existing architecture – and just let Centrifugo deal with persistent connections.  At the moment, Centrifugo provides some advanced and unique features that can simplify a developer's life and save months of development, even if the application backend is built with the asynchronous concurrent language. One example is that Centrifugo can scale out-of-the-box to many machines with several supported brokers. And there are more things to mention – see detailed highlights further in the docs.  ","version":"v3","tagName":"h2"},{"title":"Concepts​","type":1,"pageTitle":"Centrifugo introduction","url":"/docs/3/getting-started/introduction#concepts","content":" As mentioned above, Centrifugo runs as a standalone service that cares about handling persistent connections from application users. Application backend and frontend can be written in any programming language. Clients connect to Centrifugo and subscribe to channels.  As soon as some event happens application backend can publish a message with event payload into a channel using Centrifugo API. The message will be delivered to all clients currently connected and subscribed to a channel.  So Centrifugo is a user-facing PUB/SUB server in a nutshell. Here is a simplified scheme:    ","version":"v3","tagName":"h2"},{"title":"Join community​","type":1,"pageTitle":"Centrifugo introduction","url":"/docs/3/getting-started/introduction#join-community","content":" We have rooms in Telegram and Discord:     See you there! ","version":"v3","tagName":"h2"},{"title":"Client API showcase","type":0,"sectionRef":"#","url":"/docs/3/getting-started/client_api","content":"","keywords":"","version":"v3"},{"title":"Connecting to a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#connecting-to-a-server","content":" Each Centrifugo client allows connecting to a server.  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket'); centrifuge.connect();   In most cases you will need to pass JWT (JSON Web Token) for authentication, so the example above transforms to:  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket'); centrifuge.setToken('&lt;USER-JWT&gt;') centrifuge.connect();   See authentication chapter for more information on how to generate connection JWT.  If you are using connect proxy then you may go without setting JWT.  ","version":"v3","tagName":"h2"},{"title":"Disconnecting from a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#disconnecting-from-a-server","content":" After connecting you can disconnect from a server at any moment.  centrifuge.disconnect();   ","version":"v3","tagName":"h2"},{"title":"Reconnecting to a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#reconnecting-to-a-server","content":" Centrifugo clients automatically reconnect to a server in case of temporary connection loss, also clients periodically ping the server to detect broken connections.  ","version":"v3","tagName":"h2"},{"title":"Connection lifecycle events​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#connection-lifecycle-events","content":" All client implementations allow setting handlers on connect and disconnect events.  For example:  centrifuge.on('connect', function(connectCtx){ console.log('connected', connectCtx) }); centrifuge.on('disconnect', function(disconnectCtx){ console.log('disconnected', disconnectCtx) });   ","version":"v3","tagName":"h2"},{"title":"Subscribe to a channel​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#subscribe-to-a-channel","content":" Another core functionality of client API is the possibility to subscribe to a channel to receive all messages published to that channel.  centrifuge.subscribe('channel', function(messageCtx) { console.log(messageCtx); })   Client can subscribe to different channels. Subscribe method returns the Subscription object. It's also possible to react to different Subscription events: join and leave events, subscribe success and subscribe error events, unsubscribe events.  In idiomatic case messages published to channels from application backend over Centrifugo server API. Though it's not always true.  Centrifugo also provides a message recovery feature to restore missed publications in channels. Publications can be missed due to temporary disconnects (bad network) or server reloads. Recovery happens automatically on reconnect (due to bad network or server reloads) as soon as recovery in the channel properly configured. Client keeps last seen Publication offset and restores missed publications since known offset upon reconnecting. If recovery failed then client implementation provides a flag inside subscribe event to let the application know that some publications were missed – so you may need to load state from scratch from the application backend. Not all Centrifugo clients implement a recovery feature – refer to specific client implementation docs. More details about recovery in a dedicated chapter.  ","version":"v3","tagName":"h2"},{"title":"Server-side subscriptions​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#server-side-subscriptions","content":" To handle publications coming from server-side subscriptions client API allows listening publications simply on Centrifuge client instance:  centrifuge.on('publish', function(messageCtx) { console.log(messageCtx); });   It's also possible to react on different server-side Subscription events: join and leave events, subscribe success, unsubscribe event. There is no subscribe error event here since the subscription was initiated on the server-side.  ","version":"v3","tagName":"h2"},{"title":"Send RPC​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#send-rpc","content":" A client can send RPC to a server. RPC is a call that is not related to channels at all. It's just a way to call the server method from the client-side over the WebSocket or SockJS connection. RPC is only available when RPC proxy configured.  const rpcRequest = {'key': 'value'}; const data = await centrifuge.namedRPC('example_method', rpcRequest);   ","version":"v3","tagName":"h2"},{"title":"Call channel history​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#call-channel-history","content":" Once subscribed client can call publication history inside a channel (only for channels where history configured) to get last publications in channel:  Get stream current top position:  const resp = await subscription.history(); console.log(resp.offset); console.log(resp.epoch);   Get up to 10 publications from history since known stream position:  const resp = await subscription.history({limit: 10, since: {offset: 0, epoch: '...'}}); console.log(resp.publications);   Get up to 10 publications from history since current stream beginning:  const resp = await subscription.history({limit: 10}); console.log(resp.publications);   Get up to 10 publications from history since current stream end in reversed order (last to first):  const resp = await subscription.history({limit: 10, reverse: true}); console.log(resp.publications);   ","version":"v3","tagName":"h2"},{"title":"Presence and presence stats​","type":1,"pageTitle":"Client API showcase","url":"/docs/3/getting-started/client_api#presence-and-presence-stats","content":" Once subscribed client can call presence and presence stats information inside channel (only for channels where presence configured):  For presence (full information about active subscribers in channel):  const resp = await subscription.presence(); // resp contains presence information - a map client IDs as keys // and client information as values.   For presence stats (just a number of clients and unique users in a channel):  const resp = await subscription.presenceStats(); // resp contains a number of clients and a number of unique users.  ","version":"v3","tagName":"h2"},{"title":"Migrating to v3","type":0,"sectionRef":"#","url":"/docs/3/getting-started/migration_v3","content":"","keywords":"","version":"v3"},{"title":"Client-side changes​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#client-side-changes","content":" Client protocol has some backward incompatible changes regarding working with history API and removing deprecated fields.  ","version":"v3","tagName":"h2"},{"title":"No unlimited history by default​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#no-unlimited-history-by-default","content":" Call to history API from client-side now does not return all publications from history cache. It returns only information about a stream with zero publications. Clients should explicitly provide a limit when calling history API. Also, the maximum allowed limit can be set by client_history_max_publication_limit option (by default 300).  We provide a boolean flag use_unlimited_history_by_default on configuration file top level to enable previous behavior while you migrate client applications to use explicit limit.  ","version":"v3","tagName":"h3"},{"title":"Publication limit for recovery​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#publication-limit-for-recovery","content":" The maximum number of messages that can be recovered is now limited by client_recovery_max_publication_limit option which is by default 300.  ","version":"v3","tagName":"h3"},{"title":"Seq/Gen fields removed​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#seqgen-fields-removed","content":" Deprecated seq/gen now removed and Centrifugo uses offset field for a position in a stream. This means that there is no need for v3_use_offset option anymore – it's not used in Centrifugo v3.  ","version":"v3","tagName":"h3"},{"title":"Server-side changes​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#server-side-changes","content":" ","version":"v3","tagName":"h2"},{"title":"Time interval options are duration​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#time-interval-options-are-duration","content":" In Centrifugo v3 all time intervals should be configured using duration.  For example &quot;proxy_connect_timeout&quot;: 1 should be changed to &quot;proxy_connect_timeout&quot;: &quot;1s&quot;.  We provide a configuration converter which takes this change into account.  ","version":"v3","tagName":"h3"},{"title":"Channel options changes​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#channel-options-changes","content":" In Centrifugo v3 history_recover option becomes recover.  Option history_lifetime renamed to history_ttl and it's now a duration.  Option server_side removed, see protected option as a replacement.  We provide a configuration converter which takes these changes into account.  ","version":"v3","tagName":"h3"},{"title":"Some command-line flags removed​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#some-command-line-flags-removed","content":" Configuring over command-line flags is not very convenient for production deployments, Centrifugo v3 reduced the number of command-line flags available – it mostly has flags frequently useful for development now.  ","version":"v3","tagName":"h3"},{"title":"Enforced request Origin check​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#enforced-request-origin-check","content":" In Centrifugo v3 you should explicitly set a list of allowed origins which are allowed to connect to client transport endpoints.  config.json { ... &quot;allowed_origins&quot;: [&quot;https://mysite.com&quot;] }   There is a way to disable origin check, but it's discouraged and insecure in case you are using connect proxy feature.  config.json { ... &quot;allowed_origins&quot;: [&quot;*&quot;] }   ","version":"v3","tagName":"h3"},{"title":"Updated GRPC API Protobuf package​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#updated-grpc-api-protobuf-package","content":" In Centrifugo v3 we addressed an issue where package name in Protobuf definitions resulted in some inconvenience and attempts to rename it. But it's not possible to rename it since GRPC uses it as part of RPC methods internally. Now GRPC API package looks like this:  package centrifugal.centrifugo.api;   This means you need to regenerate your GRPC code which communicates with Centrifugo using the latest Protobuf definitions. Refer to the GRPC API doc.  ","version":"v3","tagName":"h3"},{"title":"Channels API method changed​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#channels-api-method-changed","content":" The response format of channels API call changed in v3. See description in API doc.  The channels method has new additional possibilities like showing the number of connections in a channel and filter channels by pattern.  info Channels API call still has the same concern as before: this method does not scale well for many active channels in a system and is mostly recommended for administrative/debug purposes.  ","version":"v3","tagName":"h3"},{"title":"HTTP proxy changes​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#http-proxy-changes","content":" When using HTTP proxy you should now set an explicit list of headers you want to proxy. To mimic the behavior of Centrifugo v2 add to your configuration:  { &quot;proxy_http_headers&quot;: [ &quot;Origin&quot;, &quot;User-Agent&quot;, &quot;Cookie&quot;, &quot;Authorization&quot;, &quot;X-Real-Ip&quot;, &quot;X-Forwarded-For&quot;, &quot;X-Request-Id&quot; ] }   If you had a list of extra HTTP headers using proxy_extra_http_headers then additionally extend list above with values from proxy_extra_http_headers. Then you can remove proxy_extra_http_headers - it's not used anymore.  Another important change is how Centrifugo proxies binary data over HTTP JSON proxy. Previously proxy mode (whether to use base64 fields or not) could be configured using encoding=binary URL param of connection. With Centrifugo v3 it's only possible to use binary mode by enabling &quot;proxy_binary_encoding&quot;: true option. BTW according to our community poll only 2% of Centrifugo users used binary mode in HTTP proxy. If you have problems with new behavior – write about your situation to our community chats – and we will see what's possible.  ","version":"v3","tagName":"h3"},{"title":"JWT changes​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#jwt-changes","content":" eto claim of subscription JWT removed. But since Centrifugo v3 introduced an additional expire_at claim it's still possible to implement one-time subscription tokens without enabling subscription expiration workflow by setting &quot;expire_at: 0&quot; in subscription JWT claims.  ","version":"v3","tagName":"h3"},{"title":"Redis configuration changes​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#redis-configuration-changes","content":" Redis configuration was a bit messy - especially in the Redis sharding case, in v3 we decided to clean up it a bit. Make it more explicit and reduce the number of possible ways to configure.  Refer to the Redis Engine docs for the new configuration details. The important thing is that there is no separate redis_host and redis_port option anymore – those are replaced with single redis_address option.  ","version":"v3","tagName":"h3"},{"title":"Redis streams used by default​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#redis-streams-used-by-default","content":" Centrifugo v3 will use Redis Stream data structure to keep history instead of lists.  danger This requires Redis &gt;= 5.0.1 to work. If you still need List data structure or have an old Redis version you can use &quot;redis_use_lists&quot;: true to mimic the default behavior of Centrifugo v2.  ","version":"v3","tagName":"h3"},{"title":"SockJS disabled by default​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#sockjs-disabled-by-default","content":" Our poll showed that most Centrifugo users do not use SockJS transport. In v3 it's disabled by default. You can enable it by setting &quot;sockjs&quot;: true in configuration.  ","version":"v3","tagName":"h3"},{"title":"Other configuration changes​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#other-configuration-changes","content":" Here is a full list of configuration option changes. We provide a best-effort configuration converter.  allowed_origins is now required to be set to authorize requests with Origin header  v3_use_offset removed  redis_streams removed  tls_autocert_force_rsa removed  redis_pubsub_num_workers removed  sockjs_disable removed  secret renamed to token_hmac_secret_key  history_lifetime renamed to history_ttl  history_recover renamed to recover  client_presence_ping_interval renamed to client_presence_update_interval  client_ping_interval renamed to websocket_ping_interval  client_message_write_timeout renamed to websocket_write_timeout  client_request_max_size renamed to websocket_message_size_limit  client_presence_expire_interval renamed to presence_ttl  memory_history_meta_ttl renamed to history_meta_ttl  redis_history_meta_ttl renamed to history_meta_ttl  redis_sequence_ttl renamed to history_meta_ttl  redis_presence_ttl renamed to presence_ttl  presence_ttl should be converted to duration  websocket_write_timeout should be converted to duration  websocket_ping_interval should be converted to duration  client_presence_update_interval should be converted to duration  history_ttl should be converted to duration  history_meta_ttl should be converted to duration  nats_dial_timeout should be converted to duration  nats_write_timeout should be converted to duration  graphite_interval should be converted to duration  shutdown_timeout should be converted to duration  shutdown_termination_delay should be converted to duration  proxy_connect_timeout should be converted to duration  proxy_refresh_timeout should be converted to duration  proxy_rpc_timeout should be converted to duration  proxy_subscribe_timeout should be converted to duration  proxy_publish_timeout should be converted to duration  client_expired_close_delay should be converted to duration  client_expired_sub_close_delay should be converted to duration  client_stale_close_delay should be converted to duration  client_channel_position_check_delay should be converted to duration  node_info_metrics_aggregate_interval should be converted to duration  websocket_ping_interval should be converted to duration  websocket_write_timeout should be converted to duration  sockjs_heartbeat_delay should be converted to duration  redis_idle_timeout should be converted to duration  redis_connect_timeout should be converted to duration  redis_read_timeout should be converted to duration  redis_write_timeout should be converted to duration  redis_cluster_addrs renamed to redis_cluster_address  redis_sentinels renamed to redis_sentinel_address  redis_master_name renamed to redis_sentinel_master_name  ","version":"v3","tagName":"h3"},{"title":"v2 to v3 config converter​","type":1,"pageTitle":"Migrating to v3","url":"/docs/3/getting-started/migration_v3#v2-to-v3-config-converter","content":" Here is a converter between Centrifugo v2 and v3 JSON configuration. It can help to translate most of the things automatically for you.  If you are using Centrifugo with TOML format then you can use online converter as initial step. Or yaml-to-json and json-to-yaml for YAML.  tip It's fully client-side: your data won't be sent anywhere.  danger Unfortunately, we can't migrate environment variables and command-line flags automatically - so if you are using env vars or command-line flags to configure Centrifugo you still need to migrate manually. Also, be aware: this converter tool is the best effort only – we can not guarantee it solves all corner cases, especially in Redis configuration. You may still need to fix some things manually, for example - properly fill allowed_origins.      Convert Here will be configuration for v3 Here will be log of changes made in your config ","version":"v3","tagName":"h3"},{"title":"Quickstart tutorial ⏱️","type":0,"sectionRef":"#","url":"/docs/3/getting-started/quickstart","content":"","keywords":"","version":"v3"},{"title":"More examples​","type":1,"pageTitle":"Quickstart tutorial ⏱️","url":"/docs/3/getting-started/quickstart#more-examples","content":" Several more examples are located on Github – check out this repo.  Also, check out our blog with several tutorials. ","version":"v3","tagName":"h3"},{"title":"Database-driven namespace configuration","type":0,"sectionRef":"#","url":"/docs/3/pro/db_namespaces","content":"","keywords":"","version":"v3"},{"title":"How it works​","type":1,"pageTitle":"Database-driven namespace configuration","url":"/docs/3/pro/db_namespaces#how-it-works","content":" As soon as you point Centrifugo PRO to an admin storage and enable storage namespace management, Centrifugo will load namespaces from database table on start. Changes made in web UI will then propagate to all running Centrifugo nodes in up to 30 seconds.  info Centrifugo nodes cache namespace configuration in memory so if Centrifugo temporarily lost connection to a database it will continue working with previous namespace configuration until connection problems will be resolved.  ","version":"v3","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Database-driven namespace configuration","url":"/docs/3/pro/db_namespaces#configuration","content":" By default namespace database management is off – i.e. namespaces loaded on Centrifugo start from a configuration file (or environment variable).  To enable namespace management through database add the following into configuration file:  config.json { ... &quot;admin_storage&quot;: { &quot;enabled&quot;: true, &quot;storage_type&quot;: &quot;sqlite&quot;, &quot;storage_dsn&quot;: &quot;/path/to/centrifugo.db&quot;, &quot;manage_namespaces&quot;: true } }   Centrifugo PRO supports several SQL database backends to keep namespace information:  SQLite (storage_type: sqlite)PostgreSQL (storage_type: postgresql)MySQL (storage_type: mysql)  Each storage type has its own storage_dsn format. For SQLite it's just a path to a db file.  PostgreSQL dsn format described here. Example:  config.json { ... &quot;admin_storage&quot;: { &quot;enabled&quot;: true, &quot;storage_type&quot;: &quot;postgresql&quot;, &quot;storage_dsn&quot;: &quot;host=localhost user=postgres password=mysecretpassword dbname=centrifugo port=5432 sslmode=disable&quot;, &quot;manage_namespaces&quot;: true } }   MySQL dsn format described here. Example:  config.json { ... &quot;admin_storage&quot;: { &quot;enabled&quot;: true, &quot;storage_type&quot;: &quot;mysql&quot;, &quot;storage_dsn&quot;: &quot;user:pass@tcp(127.0.0.1:3306)/dbname?charset=utf8mb4&amp;parseTime=True&amp;loc=Local&quot;, &quot;manage_namespaces&quot;: true } }  ","version":"v3","tagName":"h2"},{"title":"Install and run PRO version","type":0,"sectionRef":"#","url":"/docs/3/pro/install_and_run","content":"","keywords":"","version":"v3"},{"title":"Binary release​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/3/pro/install_and_run#binary-release","content":" Centrifugo PRO binary releases available on Github. Note that we use a separate repo for PRO releases. Download latest release for your operating system, unpack it and run (see how to set license key below).  ","version":"v3","tagName":"h3"},{"title":"Docker image​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/3/pro/install_and_run#docker-image","content":" Centrifugo PRO uses a different image from OSS version – centrifugo/centrifugo-pro:  docker run --ulimit nofile=65536:65536 -v /host/dir/with/config/file:/centrifugo -p 8000:8000 centrifugo/centrifugo-pro:v3.2.2 centrifugo -c config.json   ","version":"v3","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/3/pro/install_and_run#kubernetes","content":" You can use our official Helm chart but make sure you changed Docker image to use PRO version and point to the correct image tag:  values.yaml ... image: registry: docker.io repository: centrifugo/centrifugo-pro tag: v3.2.2   ","version":"v3","tagName":"h3"},{"title":"Debian and Ubuntu​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/3/pro/install_and_run#debian-and-ubuntu","content":" DEB package available in release assets.  wget https://github.com/centrifugal/centrifugo-pro/releases/download/v3.2.2/centrifugo-pro_3.2.2-0_amd64.deb sudo dpkg -i centrifugo-pro_3.2.2-0_amd64.deb   ","version":"v3","tagName":"h3"},{"title":"Centos​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/3/pro/install_and_run#centos","content":" RPM package available in release assets.  wget https://github.com/centrifugal/centrifugo-pro/releases/download/v3.2.2/centrifugo-pro-3.2.2-0.x86_64.rpm sudo yum install centrifugo-pro-3.2.2-0.x86_64.rpm   ","version":"v3","tagName":"h3"},{"title":"Setting PRO license key​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/3/pro/install_and_run#setting-pro-license-key","content":" Centrifugo PRO inherits all features and configuration options from open-source version. The only difference is that it expects a valid license key on start to avoid sandbox mode limits.  Once you have installed a PRO version and have a license key you can set it in configuration over license field, or pass over environment variables as CENTRIFUGO_LICENSE. Like this:  config.json { ... &quot;license&quot;: &quot;&lt;YOUR_LICENSE_KEY&gt;&quot; }   tip If license properly set then on Centrifugo PRO start you should see license information in logs: owner, license type and expiration date. All PRO features should be unlocked at this point. Warning about sandbox mode in logs on server start must disappear. ","version":"v3","tagName":"h2"},{"title":"Centrifugo PRO overview","type":0,"sectionRef":"#","url":"/docs/3/pro/overview","content":"","keywords":"","version":"v3"},{"title":"Features​","type":1,"pageTitle":"Centrifugo PRO overview","url":"/docs/3/pro/overview#features","content":" Centrifugo PRO includes the following features:  Everything from Centrifugo OSSChannel and user tracing provides a way to look at all client protocol frames in the specified channel or per user ID.Real-time analytics with ClickHouse for a great system observability, reporting and trending.User status feature to understand activity state for a list of users.Operation throttling to protect client API from misusing and frontend bugs.User connections API to query for all active user sessions with additional information.User blocking API to block/unblock abusive users by ID.JWT revoking and invalidation to revoking tokens by token ID (JTI) and invalidating user's tokens on issue time basis.Faster performance to reduce resource usage on server side.Singleflight for online presence and history to reduce load on the broker.Near real-time CPU and RSS memory usage stats.  info PRO features can change with time. We reserve a right to move features from PRO to OSS version if there is a clear signal that this is required to do for the Centrifugo ecosystem.  ","version":"v3","tagName":"h2"},{"title":"Sandbox mode​","type":1,"pageTitle":"Centrifugo PRO overview","url":"/docs/3/pro/overview#sandbox-mode","content":" You can try out Centrifugo PRO for free. When you start Centrifugo PRO without license key then it's running in a sandbox mode. Sandbox mode limits the usage of Centrifigo PRO in several ways. For example:  Centrifugo handles up to 50 concurrent connectionsup to 2 server nodes supportedup to 20 API requests per second allowed  This mode should be enough for development and trying out PRO features, but must not be used in production environment as we can introduce additional limitations in the future.  caution Centrifugo PRO is distributed under commercial license which is different from OSS version. By downloading Centrifugo PRO you automatically accept license terms.  ","version":"v3","tagName":"h2"},{"title":"Pricing​","type":1,"pageTitle":"Centrifugo PRO overview","url":"/docs/3/pro/overview#pricing","content":" To run without limits Centrifugo PRO requires a license key. At this point we are not issuing license keys for Centrifugo PRO as we are in the process of defining the pricing strategy for it. Please contact us over sales@centrifugal.dev, we can add you to the list of interested customers and will appreciate if you share which features you are mostly interested in. ","version":"v3","tagName":"h2"},{"title":"Faster performance","type":0,"sectionRef":"#","url":"/docs/3/pro/performance","content":"","keywords":"","version":"v3"},{"title":"Faster HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#faster-http-api","content":" Centrifugo PRO has an optimized JSON serialization/deserialization for HTTP API.  The effect can be noticeable under load. The exact numbers heavily depend on usage scenario. According to our benchmarks you can expect 10-15% more requests/sec for small message publications over HTTP API, and up to several times throughput boost when you are frequently get lots of messages from a history, see a couple of examples below.  ","version":"v3","tagName":"h2"},{"title":"Faster GRPC API​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#faster-grpc-api","content":" Centrifugo PRO has an optimized Protobuf serialization/deserialization for GRPC API. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v3","tagName":"h2"},{"title":"Faster HTTP proxy​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#faster-http-proxy","content":" Centrifugo PRO has an optimized JSON serialization/deserialization for HTTP proxy. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v3","tagName":"h2"},{"title":"Faster GRPC proxy​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#faster-grpc-proxy","content":" Centrifugo PRO has an optimized Protobuf serialization/deserialization for GRPC API. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v3","tagName":"h2"},{"title":"Faster JWT decoding​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#faster-jwt-decoding","content":" Centrifugo PRO has an optimized decoding of JWT claims.  ","version":"v3","tagName":"h2"},{"title":"Faster GRPC unidirectional stream​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#faster-grpc-unidirectional-stream","content":" Centrifugo PRO has an optimized Protobuf deserialization for GRPC unidirectional stream. This only affects deserialization of initial connect command.  ","version":"v3","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#examples","content":" Let's look at quick live comparisons of Centrifugo OSS and Centrifugo PRO regarding HTTP API performance.  ","version":"v3","tagName":"h2"},{"title":"Publish HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#publish-http-api","content":" Sorry, your browser doesn't support embedded video.  In this video you can see a 13% speed up for publish operation. But for more complex API calls with larger payloads the difference can be much bigger. See next example that demonstrates this.  ","version":"v3","tagName":"h3"},{"title":"History HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/3/pro/performance#history-http-api","content":" Sorry, your browser doesn't support embedded video.  In this video you can see an almost 2x overall speed up while asking 100 messages from Centrifugo history API. ","version":"v3","tagName":"h3"},{"title":"CPU and RSS stats","type":0,"sectionRef":"#","url":"/docs/3/pro/process_stats","content":"CPU and RSS stats A useful addition of Centrifugo PRO is an ability to show CPU and RSS memory usage of each node in admin web UI. Here is how this looks like: The information updated in near real-time (with several seconds delay). It's also available as part of info API.","keywords":"","version":"v3"},{"title":"Singleflight","type":0,"sectionRef":"#","url":"/docs/3/pro/singleflight","content":"Singleflight Centrifugo PRO provides an additional boolean option use_singleflight (default false). When this option enabled Centrifugo will automatically try to merge identical requests to history, online presence or presence stats issued at the same time into one real network request. This option can radically reduce a load on a broker in the following situations: Many clients subscribed to the same channel and in case of massive reconnect scenario try to access history simultaneously to restore a state (whether manually using history API or over automatic recovery feature)Many clients subscribed to the same channel and positioning feature is on so Centrifugo tracks client positionMany clients subscribed to the same channel and in case of massive reconnect scenario try to call presence or presence stats simultaneously Using this option only makes sense with remote engine (Redis, KeyDB, Tarantool), it won't provide a benefit in case of using a Memory engine. To enable: config.json { ... &quot;use_singleflight&quot;: true } Or via CENTRIFUGO_USE_SINGLEFLIGHT environment variable.","keywords":"","version":"v3"},{"title":"Operation throttling","type":0,"sectionRef":"#","url":"/docs/3/pro/throttling","content":"","keywords":"","version":"v3"},{"title":"Redis throttling​","type":1,"pageTitle":"Operation throttling","url":"/docs/3/pro/throttling#redis-throttling","content":" At this moment Centrifugo PRO provides throttling over Redis. It's only possible to throttle by the user ID. Requests from anonymous users can't be throttled. Throttling with Redis uses token bucket algorithm internally.  Here is a list of operations that can be throttled:  connectsubscribepublishhistorypresencepresence_statsrefreshsub_refreshrpc (with method resolution)  An example configuration:  config.json { ... &quot;redis_throttling&quot;: { &quot;enabled&quot;: false, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;buckets&quot;: { &quot;publish&quot;: { &quot;enabled&quot;: true, &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1, &quot;capacity&quot;: 1 }, &quot;rpc&quot;: { &quot;enabled&quot;: true, &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 10, &quot;capacity&quot;: 1, &quot;method_override&quot;: [ { &quot;method&quot;: &quot;updateActiveStatus&quot;, &quot;interval&quot;: &quot;20s&quot;, &quot;rate&quot;: 1, &quot;capacity&quot;: 1 } ] } } } }   This configuration enables throttling and throttles publish attempts in a way that only 1 publication is possible in 1 second from the same user.  Redis configuration for throttling feature matches Centrifugo Redis engine configuration. So Centrifugo supports client-side consistent sharding to scale Redis, Redis Sentinel, Redis Cluster for throttling feature too.  It's also possible to reuse Centrifugo Redis engine by setting use_redis_from_engine option instead of custom throttling Redis address declaration, like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;redis_throttling&quot;: { &quot;enabled&quot;: false, &quot;use_redis_from_engine&quot;: true, &quot;buckets&quot;: { &quot;publish&quot;: { &quot;enabled&quot;: true, &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1, &quot;capacity&quot;: 1 } } } }   In this case throttling will simply connect to Redis instances configured for an Engine. ","version":"v3","tagName":"h2"},{"title":"Analytics with ClickHouse","type":0,"sectionRef":"#","url":"/docs/3/pro/analytics","content":"","keywords":"","version":"v3"},{"title":"Configuration​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/3/pro/analytics#configuration","content":" To enable integration with ClickHouse add the following section to a configuration file:  config.json { ... &quot;clickhouse_analytics&quot;: { &quot;enabled&quot;: true, &quot;clickhouse_dsn&quot;: [ &quot;tcp://127.0.0.1:9000&quot;, &quot;tcp://127.0.0.1:9001&quot;, &quot;tcp://127.0.0.1:9002&quot;, &quot;tcp://127.0.0.1:9003&quot; ], &quot;clickhouse_database&quot;: &quot;centrifugo&quot;, &quot;clickhouse_cluster&quot;: &quot;centrifugo_cluster&quot;, &quot;export_connections&quot;: true, &quot;export_operations&quot;: true, &quot;export_http_headers&quot;: [ &quot;User-Agent&quot;, &quot;Origin&quot;, &quot;X-Real-Ip&quot;, ] } }   All ClickHouse analytics options scoped to clickhouse_analytics section of configuration.  Toggle this feature using enabled boolean option.  tip While we have a nested configuration here it's still possible to use environment variables to set options. For example, use CENTRIFUGO_CLICKHOUSE_ANALYTICS_ENABLED env var name for configure enabled option mentioned above. I.e. nesting expressed as _ in Centrifugo.  Centrifugo can export data to different ClickHouse instances, addresses of ClickHouse can be set over clickhouse_dsn option.  You also need to set a ClickHouse cluster name (clickhouse_cluster) and database name clickhouse_database.  export_connections tells Centrifugo to export connection information snapshots. Information about connection will be exported once a connection established and then periodically while connection alive. See below on table structure to see which fields are available.  export_operations tells Centrifugo to export individual client operation information. See below on table structure to see which fields are available.  export_http_headers is a list of HTTP headers to export for connection information.  export_grpc_metadata is a list of metadata keys to export for connection information for GRPC unidirectional transport.  skip_schema_initialization (new in Centrifugo PRO v3.1.1) - boolean, default false. By default Centrifugo tries to initialize table schema on start (if not exists). This flag allows skipping initialization process.  skip_ping_on_start (new in Centrifugo PRO v3.1.1) - boolean, default false. Centrifugo pings Clickhouse servers by default on start, if any of servers is unavailable – Centrifugo fails to start. This option allow skipping this check thus Centrifugo is able to start even if Clickhouse cluster not working correctly.  ","version":"v3","tagName":"h2"},{"title":"Connections table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/3/pro/analytics#connections-table","content":" SHOW CREATE TABLE centrifugo.connections; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.connections ( `client` FixedString(36), `user` String, `name` String, `version` String, `transport` String, `channels` Array(String), `headers.key` Array(String), `headers.value` Array(String), `metadata.key` Array(String), `metadata.value` Array(String), `time` DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/connections', '{replica}') PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.connections_distributed; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.connections_distributed ( `client` FixedString(36), `user` String, `name` String, `version` String, `transport` String, `channels` Array(String), `headers.key` Array(String), `headers.value` Array(String), `metadata.key` Array(String), `metadata.value` Array(String), `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'connections', murmurHash3_64(client)) │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v3","tagName":"h2"},{"title":"Operations table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/3/pro/analytics#operations-table","content":" SHOW CREATE TABLE centrifugo.operations; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations ( `client` FixedString(36), `user` String, `op` String, `channel` String, `method` String, `error` UInt32, `disconnect` UInt32, `duration` UInt64, `time` DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/operations', '{replica}') PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.operations_distributed; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations_distributed ( `client` FixedString(36), `user` String, `op` String, `channel` String, `method` String, `error` UInt32, `disconnect` UInt32, `duration` UInt64, `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'operations', murmurHash3_64(client)) │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v3","tagName":"h2"},{"title":"Query examples​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/3/pro/analytics#query-examples","content":" Show unique users which were connected:  SELECT DISTINCT user FROM centrifugo.connections_distributed; ┌─user─────┐ │ user_1 │ │ user_2 │ │ user_3 │ │ user_4 │ │ user_5 │ └──────────┘   Show total number of publication attempts which were throttled by Centrifugo (received Too many requests error with code 111):  SELECT COUNT(*) FROM centrifugo.operations_distributed WHERE (error = 111) AND (op = 'publish'); ┌─count()─┐ │ 4502 │ └─────────┘   The same for a specific user:  SELECT COUNT(*) FROM centrifugo.operations_distributed WHERE (error = 111) AND (op = 'publish') AND (user = 'user_200'); ┌─count()─┐ │ 1214 │ └─────────┘   Show number of unique users subscribed to a specific channel in last 5 minutes (this is approximate since connections table contain periodic snapshot entries, clients could subscribe/unsubscribe in between snapshots – this is reflected in operations table):  SELECT COUNT(Distinct(user)) FROM centrifugo.connections_distributed WHERE arrayExists(x -&gt; (x = 'chat:index'), channels) AND (time &gt;= (now() - toIntervalMinute(5))); ┌─uniqExact(user)─┐ │ 101 │ └─────────────────┘   Show top 10 users which called publish operation during last one minute:  SELECT COUNT(op) AS num_ops, user FROM centrifugo.operations_distributed WHERE (op = 'publish') AND (time &gt;= (now() - toIntervalMinute(1))) GROUP BY user ORDER BY num_ops DESC LIMIT 10; ┌─num_ops─┬─user─────┐ │ 56 │ user_200 │ │ 11 │ user_75 │ │ 6 │ user_87 │ │ 6 │ user_65 │ │ 6 │ user_39 │ │ 5 │ user_28 │ │ 5 │ user_63 │ │ 5 │ user_89 │ │ 3 │ user_32 │ │ 3 │ user_52 │ └─────────┴──────────┘   ","version":"v3","tagName":"h2"},{"title":"Development​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/3/pro/analytics#development","content":" The recommended way to run ClickHouse in production is with cluster. But during development you may want to run Centrifugo with single instance ClickHouse.  To do this set only one ClickHouse dsn and do not set cluster name:  config.json { ... &quot;clickhouse_analytics&quot;: { &quot;enabled&quot;: true, &quot;clickhouse_dsn&quot;: [ &quot;tcp://127.0.0.1:9000&quot; ], &quot;clickhouse_database&quot;: &quot;centrifugo&quot;, &quot;clickhouse_cluster&quot;: &quot;&quot;, &quot;export_connections&quot;: true, &quot;export_operations&quot;: true, &quot;export_http_headers&quot;: [ &quot;Origin&quot;, &quot;User-Agent&quot; ] } }   Run ClickHouse locally:  docker run -it --rm -v /tmp/clickhouse:/var/lib/clickhouse -p 9000:9000 --name click yandex/clickhouse-server   Run ClickHouse client:  docker run -it --rm --link click:clickhouse-server yandex/clickhouse-client --host clickhouse-server   Issue queries:  :) SELECT * FROM centrifugo.operations ┌─client───────────────────────────────┬─user─┬─op──────────┬─channel─────┬─method─┬─error─┬─disconnect─┬─duration─┬────────────────time─┐ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ connecting │ │ │ 0 │ 0 │ 217894 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ connect │ │ │ 0 │ 0 │ 0 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ $chat:index │ │ 0 │ 0 │ 92714 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ presence │ $chat:index │ │ 0 │ 0 │ 3539 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test1 │ │ 0 │ 0 │ 2402 │ 2021-07-31 08:15:12 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test2 │ │ 0 │ 0 │ 634 │ 2021-07-31 08:15:12 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test3 │ │ 0 │ 0 │ 412 │ 2021-07-31 08:15:12 │ └──────────────────────────────────────┴──────┴─────────────┴─────────────┴────────┴───────┴────────────┴──────────┴─────────────────────┘   ","version":"v3","tagName":"h2"},{"title":"How export works​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/3/pro/analytics#how-export-works","content":" When ClickHouse analytics enabled Centrifugo nodes start exporting events to ClickHouse. Each node issues insert with events once in 5 seconds (flushing collected events in batches thus making insertion in ClickHouse efficient). Maximum batch size is 100k for each table at the momemt. If insert to ClickHouse failed Centrifugo retries it once and then buffers events in memory (up to 1 million entries). If ClickHouse still unavailable after collecting 1 million events then new events will be dropped until buffer has space. These limits are not configurable at the moment but just reach us out if you need to tune these values.  Several metrics are exposed to monitor export process health:  centrifugo_clickhouse_analytics_flush_duration_seconds summarycentrifugo_clickhouse_analytics_batch_size summarycentrifugo_clickhouse_analytics_drop_count counter ","version":"v3","tagName":"h2"},{"title":"Console commands","type":0,"sectionRef":"#","url":"/docs/3/server/console_commands","content":"","keywords":"","version":"v3"},{"title":"version command​","type":1,"pageTitle":"Console commands","url":"/docs/3/server/console_commands#version-command","content":" To show Centrifugo version and exit run:  centrifugo version   ","version":"v3","tagName":"h2"},{"title":"checkconfig command​","type":1,"pageTitle":"Console commands","url":"/docs/3/server/console_commands#checkconfig-command","content":" Centrifugo has special command to check configuration file checkconfig:  centrifugo checkconfig --config=config.json   If any errors found during validation – program will exit with error message and exit code 1.  ","version":"v3","tagName":"h2"},{"title":"genconfig command​","type":1,"pageTitle":"Console commands","url":"/docs/3/server/console_commands#genconfig-command","content":" Another command is genconfig:  centrifugo genconfig -c config.json   It will automatically generate the minimal required configuration file.  If any errors happen – program will exit with error message and exit code 1.  genconfig also supports generation of YAML and TOML configuration file formats - just provide an extension to a file:  centrifugo genconfig -c config.toml   ","version":"v3","tagName":"h2"},{"title":"gentoken command​","type":1,"pageTitle":"Console commands","url":"/docs/3/server/console_commands#gentoken-command","content":" Another command is gentoken:  centrifugo gentoken -c config.json -u 28282   It will automatically generate HMAC SHA-256 based token for user with ID 28282 (which expires in 1 week).  You can change token TTL with -t flag (number of seconds):  centrifugo gentoken -c config.json -u 28282 -t 3600   This way generated token will be valid for 1 hour.  If any errors happen – program will exit with error message and exit code 1.  ","version":"v3","tagName":"h2"},{"title":"checktoken command​","type":1,"pageTitle":"Console commands","url":"/docs/3/server/console_commands#checktoken-command","content":" One more command is checktoken:  centrifugo checktoken -c config.json &lt;TOKEN&gt;   It will validate your connection JWT, so you can test it before using while developing application.  If any errors happen or validation failed – program will exit with error message and exit code 1. ","version":"v3","tagName":"h2"},{"title":"Token revocation API","type":0,"sectionRef":"#","url":"/docs/3/pro/token_revocation","content":"","keywords":"","version":"v3"},{"title":"How it works​","type":1,"pageTitle":"Token revocation API","url":"/docs/3/pro/token_revocation#how-it-works","content":" By default, information about token revocations shared throughout Centrifugo cluster and kept in a process memory. So token revocation information will be lost upon Centrifugo restart.  But it's possible to enable revocation information persistence by configuring a persistence storage – in this case token revocation information will survive Centrifugo restarts.  Centrifugo also automatically expires entries in the storage to keep working set reasonably small. Keeping pool of revoked tokens small allows avoiding expensive database lookups on every check – information is loaded periodically from the database and all checks performed over in-memory data structure – thus token revocation checks are cheap and have a small impact on the overall system performance.  ","version":"v3","tagName":"h2"},{"title":"Configure​","type":1,"pageTitle":"Token revocation API","url":"/docs/3/pro/token_revocation#configure","content":" Token revocation features (both revocation by token ID and user token invalidation by issue time) are enabled by default in Centrifugo PRO (as soon as your JWTs has jti and iat claims you will be able to use revocation APIs). By default revocation information kept in a process memory.  There are two types of persistent engines supported at the moment:  redisdatabase  ","version":"v3","tagName":"h2"},{"title":"Redis persistence engine​","type":1,"pageTitle":"Token revocation API","url":"/docs/3/pro/token_revocation#redis-persistence-engine","content":" Revocation data can be kept in Redis. To enable this configuration should be:  { ... &quot;token_revoke&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; }, &quot;user_tokens_invalidate&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; } }   danger Unlike many other Redis features in Centrifugo consistent sharding is not supported for revocation data. The reason is that we don't want to loose revocation information when additional Redis node added. So only one Redis shard can be provided for token_revoke and user_tokens_invalidate features. This should be fine given that working set of revoked entities should be reasonably small and old entries expire. If you try to set several Redis shards here Centrifugo will exit with an error on start.  caution One more thing you may notice is that Redis configuration here does not have use_redis_from_engine option. The reason is that since Redis is not shardable here reusing Redis configuration here could cause problems at the moment of main Redis scaling – which we want to avoid thus require explicit configuration here.  ","version":"v3","tagName":"h3"},{"title":"Database persistence engine​","type":1,"pageTitle":"Token revocation API","url":"/docs/3/pro/token_revocation#database-persistence-engine","content":" Revocation data can be kept in the relational database. Only PostgreSQL is supported.  To enable this configuration should be like:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;token_revoke&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; }, &quot;user_tokens_invalidate&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; } }   ","version":"v3","tagName":"h3"},{"title":"Revoke token API​","type":1,"pageTitle":"Token revocation API","url":"/docs/3/pro/token_revocation#revoke-token-api","content":" Allows revoking individual tokens. For example, this may be useful when token leakage has been detected and you want to revoke access for a particular tokens. BTW Centrifugo PRO provides user_connections API which has an information about tokens for active users connections (if set in JWT).  caution This API assumes that JWTs you are using contain &quot;jti&quot; claim which is a unique token ID (according to RFC).  Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;revoke_token&quot;, &quot;params&quot;: {&quot;uid&quot;: &quot;xxx-xxx-xxx&quot;, &quot;expire_at&quot;: 1635845122}}' \\ http://localhost:8000/api   Revoke token params​  Parameter name\tParameter type\tRequired\tDescriptionuid\tstring\tyes\tToken unique ID (JTI claim in case of JWT) expire_at\tint\tno\tUnix time in the future when revocation information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time (matching the expiration time of your JWTs) to keep working set of revocations small (since Centrifugo nodes periodically load all entries from the database table to construct in-memory cache).  Revoke token result​  Empty object at the moment.  ","version":"v3","tagName":"h2"},{"title":"Invalidate user tokens API​","type":1,"pageTitle":"Token revocation API","url":"/docs/3/pro/token_revocation#invalidate-user-tokens-api","content":" Allows revoking all tokens for a user which were issued before a certain time. For example, this may be useful after user changed a password in an application.  caution This API assumes that JWTs you are using contain &quot;iat&quot; claim which is a time token was issued at (according to RFC).  Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;invalidate_user_tokens&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;test&quot;, &quot;issued_before&quot;: 1635845022, &quot;expire_at&quot;: 1635845122}}' \\ http://localhost:8000/api   Invalidate user tokens params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to invalidate tokens for issued_before\tint\tyes\tAll tokens issued at before this time will be considered revoked (in case of JWT this requires iat to be properly set in JWT) expire_at\tint\tno\tUnix time in the future when revocation information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time (matching the expiration time of your JWTs) to keep working set of revocations small (since Centrifugo nodes periodically load all entries from the database table to construct in-memory cache).  Invalidate user tokens result​  Empty object at the moment. ","version":"v3","tagName":"h2"},{"title":"User and channel tracing","type":0,"sectionRef":"#","url":"/docs/3/pro/tracing","content":"","keywords":"","version":"v3"},{"title":"Save to a file​","type":1,"pageTitle":"User and channel tracing","url":"/docs/3/pro/tracing#save-to-a-file","content":" It's possible to connect to the admin tracing endpoint with CURL using the admin session token. And then save tracing output to a file for later processing.  curl -X POST http://localhost:8000/admin/trace -H &quot;Authorization: token &lt;ADMIN_AUTH_TOKEN&gt;&quot; -d '{&quot;type&quot;: &quot;user&quot;, &quot;entity&quot;: &quot;56&quot;}' -o trace.txt   Currently, you should copy the admin auth token from browser developer tools, this may be improved in the future. ","version":"v3","tagName":"h3"},{"title":"User blocking API","type":0,"sectionRef":"#","url":"/docs/3/pro/user_block","content":"","keywords":"","version":"v3"},{"title":"How it works​","type":1,"pageTitle":"User blocking API","url":"/docs/3/pro/user_block#how-it-works","content":" By default, information about user block/unblock requests shared throughout Centrifugo cluster and kept in memory. So user will be blocked until Centrifugo restart.  But it's possible to enable blocking information persistence by configuring a persistence storage – in this case information will survive Centrifugo restarts.  Centrifugo also automatically expires entries in the storage to keep working set of blocked users reasonably small. Keeping pool of blocked users small allows avoiding expensive database lookups on every check – information is loaded periodically from the storage and all checks performed over in-memory data structure – thus user blocking checks are cheap and have a small impact on the overall system performance.  ","version":"v3","tagName":"h2"},{"title":"Configure​","type":1,"pageTitle":"User blocking API","url":"/docs/3/pro/user_block#configure","content":" User block feature is enabled by default in Centrifugo PRO (blocking information will be stored in process memory). To keep blocking information persistently you need to configure persistence engine.  There are two types of persistent engines supported at the moment:  redisdatabase  ","version":"v3","tagName":"h2"},{"title":"Redis persistence engine​","type":1,"pageTitle":"User blocking API","url":"/docs/3/pro/user_block#redis-persistence-engine","content":" Blocking data can be kept in Redis. To enable this configuration should be:  { ... &quot;user_block&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; } }   danger Unlike many other Redis features in Centrifugo consistent sharding is not supported for blocking data. The reason is that we don't want to loose blocking information when additional Redis node added. So only one Redis shard can be provided for user_block feature. This should be fine given that working set of blocked users should be reasonably small and old entries expire. If you try to set several Redis shards here Centrifugo will exit with an error on start.  caution One more thing you may notice is that Redis configuration here does not have use_redis_from_engine option. The reason is that since Redis is not shardable here reusing Redis configuration here could cause problems at the moment of Redis scaling – which we want to avoid thus require explicit configuration here.  ","version":"v3","tagName":"h3"},{"title":"Database persistence engine​","type":1,"pageTitle":"User blocking API","url":"/docs/3/pro/user_block#database-persistence-engine","content":" Blocking data can be kept in the relational database. Only PostgreSQL is supported.  To enable this configuration should be like:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;user_block&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; } }   ","version":"v3","tagName":"h3"},{"title":"Block user API​","type":1,"pageTitle":"User blocking API","url":"/docs/3/pro/user_block#block-user-api","content":" Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;block_user&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;2695&quot;, &quot;expire_at&quot;: 1635845122}}' \\ http://localhost:8000/api   Block user params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to block expire_at\tint\tno\tUnix time in the future when user blocking information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time to keep working set of blocked users small (since Centrifugo nodes periodically load all entries from the storage to construct in-memory cache).  Block user result​  Empty object at the moment.  ","version":"v3","tagName":"h2"},{"title":"Unblock user API​","type":1,"pageTitle":"User blocking API","url":"/docs/3/pro/user_block#unblock-user-api","content":" Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;unblock_user&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;2695&quot;}}' \\ http://localhost:8000/api   Unblock user params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to unblock  Unblock user result​  Empty object at the moment. ","version":"v3","tagName":"h2"},{"title":"User connections API","type":0,"sectionRef":"#","url":"/docs/3/pro/user_connections","content":"User connections API Centrifugo PRO provides an additional API call user_connections. It allows getting all active sessions of the user (by user ID) without turning on presence feature for channels at all. It's also possible to attach any JSON payload to a connection which will be then visible in the result of user_connections call. The important thing is that this additional meta information won't be exposed to a client-side (unlike connection info for example). This feature can be useful to manage active user sessions – for example in a messenger application. Users can look at a list of own current sessions and close some of them (possible with Centrifugo disconnect server API). Below is a feature showcase using admin web UI, but this call is available over HTTP or GRPC server API. Sorry, your browser doesn't support embedded video. Let's look at example. Generate a JWT for user 42: centrifugo genconfig centrifugo gentoken -u 42 HMAC SHA-256 JWT for user 42 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y Run Centrifugo with uni_http_stream transport enabled (it will allow us to connect from console): CENTRIFUGO_UNI_HTTP_STREAM=1 centrifugo -c config.json Create new terminal window and run: curl -X POST http://localhost:8000/connection/uni_http_stream --data '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y&quot;, &quot;name&quot;: &quot;terminal&quot;}' In another terminal create one more connection: curl -X POST http://localhost:8000/connection/uni_http_stream --data '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y&quot;, &quot;name&quot;: &quot;terminal&quot;}' Now let's call user_connections over HTTP API: curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;user_connections&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;42&quot;}}' \\ http://localhost:8000/api The result: { &quot;result&quot;: { &quot;connections&quot;: { &quot;db8bc772-2654-4283-851a-f29b888ace74&quot;: { &quot;app_name&quot;: &quot;terminal&quot;, &quot;transport&quot;: &quot;uni_http_stream&quot;, &quot;protocol&quot;: &quot;json&quot; }, &quot;4bc3ca70-ecc5-439d-af14-a78ae18e31c7&quot;: { &quot;app_name&quot;: &quot;terminal&quot;, &quot;transport&quot;: &quot;uni_http_stream&quot;, &quot;protocol&quot;: &quot;json&quot; } } } } Here we can see that user has 2 connections from terminal app. Each connection can be annotated with meta JSON information which is set during connection establishment (over meta claim of JWT or by returning meta in connect proxy result). User connections params​ Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID User connections result​ Field name\tField type\tOptional\tDescriptionconnections\tmap of string to UserConnectionInfo\tno\tactive user connections map where key is client ID and value is UserConnectionInfo UserConnectionInfo​ Field name\tField type\tOptional\tDescriptionapp_name\tstring\tyes\tclient app name (if provided by client) app_version\tstring\tyes\tclient app version (if provided by client) transport\tstring\tno\tclient connection transport protocol\tstring\tno\tclient connection protocol (json or protobuf) token_uid\tstring\tyes\tclient JWT unique ID (if set) token_issued_at\tint\tyes\tclient JWT issued at time as Unix seconds (if set) meta\tJSON\tyes\tmeta information attached to a connection","keywords":"","version":"v3"},{"title":"User status","type":0,"sectionRef":"#","url":"/docs/3/pro/user_status","content":"","keywords":"","version":"v3"},{"title":"Client-side status update RPC​","type":1,"pageTitle":"User status","url":"/docs/3/pro/user_status#client-side-status-update-rpc","content":" Centrifugo PRO provides a built-in RPC method of client API called update_user_status. Call it with empty parameters from a client side whenever user performs a useful action that proves it's active status in your app. For example, in Javascript:  await centrifuge.namedRPC('update_user_status', {});   note Don't forget to debounce this method calls on a client side to avoid exposing RPC on every mouse move event for example.  This RPC call sets user's last active time value in Redis (with sharding and Cluster support). Information about active status will be kept in Redis for a configured time interval, then expire.  ","version":"v3","tagName":"h3"},{"title":"update_user_status server API​","type":1,"pageTitle":"User status","url":"/docs/3/pro/user_status#update_user_status-server-api","content":" It's also possible to call update_user_status using Centrifugo server API (for example if you want to force status during application development or you want to proxy status updates over your app backend when using unidirectional transports):  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;update_user_status&quot;, &quot;params&quot;: {&quot;users&quot;: [&quot;42&quot;]}}' \\ http://localhost:8000/api   Update user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to update status for  Update user status result​  Empty object at the moment.  ","version":"v3","tagName":"h3"},{"title":"get_user_status server API​","type":1,"pageTitle":"User status","url":"/docs/3/pro/user_status#get_user_status-server-api","content":" Now on a backend side you have access to a bulk API to effectively get status of particular users.  Call RPC method of server API (over HTTP or GRPC):  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;get_user_status&quot;, &quot;params&quot;: {&quot;users&quot;: [&quot;42&quot;]}}' \\ http://localhost:8000/api   You should get a response like this:  { &quot;result&quot;:{ &quot;statuses&quot;:[ { &quot;user&quot;:&quot;42&quot;, &quot;active&quot;:1627107289, &quot;online&quot;:1627107289 } ] } }   In case information about last status update time not available the response will be like this:  { &quot;result&quot;:{ &quot;statuses&quot;:[ { &quot;user&quot;:&quot;42&quot; } ] } }   I.e. status object will present in a response but active field won't be set for status object.  Note that Centrifugo also maintains online field inside user status object. This field updated periodically by Centrifugo itself while user has active connection with a server. So you can draw away statuses in your application: i.e. when user connected (online time) but not using application for a long time (active time).  Get user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to get status for  Get user status result​  Field name\tField type\tOptional\tDescriptionstatuses\tarray of UserStatus\tno\tStatuses for each user in params (same order)  UserStatus​  Field name\tField type\tOptional\tDescriptionuser\tstring\tno\tUser ID active\tinteger\tyes\tLast active time (Unix seconds) online\tinteger\tyes\tLast online time (Unix seconds)  ","version":"v3","tagName":"h3"},{"title":"delete_user_status server API​","type":1,"pageTitle":"User status","url":"/docs/3/pro/user_status#delete_user_status-server-api","content":" If you need to clear user status information for some reason there is a delete_user_status server API call:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;delete_user_status&quot;, &quot;params&quot;: {&quot;users&quot;: [&quot;42&quot;]}}' \\ http://localhost:8000/api   Delete user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to delete status for  Delete user status result​  Empty object at the moment.  ","version":"v3","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"User status","url":"/docs/3/pro/user_status#configuration","content":" To enable Redis active status feature:  config.json { ... &quot;redis_active_status&quot;: { &quot;enabled&quot;: true, &quot;redis_address&quot;: &quot;127.0.0.1:6379&quot; } }   Redis configuration for user status feature matches Centrifugo Redis engine configuration. So Centrifugo supports client-side consistent sharding to scale Redis, Redis Sentinel, Redis Cluster for user status feature too.  It's also possible to reuse Centrifugo Redis engine by setting use_redis_from_engine option instead of custom throttling Redis address declaration, like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;redis_active_status&quot;: { &quot;enabled&quot;: true, &quot;use_redis_from_engine&quot;: true, } }   In this case Redis active status will simply connect to Redis instances configured for Centrifugo Redis engine.  expire_interval is a duration for how long Redis keys will be kept for each user. Expiration time extended on every update. By default expiration time is 31 day. To set it to 1 day:  config.json { ... &quot;redis_active_status&quot;: { ... &quot;expire_interval&quot;: &quot;24h&quot; } }  ","version":"v3","tagName":"h3"},{"title":"Admin web UI","type":0,"sectionRef":"#","url":"/docs/3/server/admin_web","content":"","keywords":"","version":"v3"},{"title":"Options​","type":1,"pageTitle":"Admin web UI","url":"/docs/3/server/admin_web#options","content":" admin (boolean, default: false) – enables/disables admin web UIadmin_password (string, default: &quot;&quot;) – this is a password to log into admin web interfaceadmin_secret (string, default: &quot;&quot;) - this is a secret key for authentication token set on successful login.  Make both admin_password and admin_secret strong and keep them in secret.  After configuring, restart Centrifugo and go to http://localhost:8000 (by default) - you should see web interface.  tip Although there is a password based authentication a good advice is to protect web interface by firewall rules in production.    ","version":"v3","tagName":"h2"},{"title":"Using custom web interface​","type":1,"pageTitle":"Admin web UI","url":"/docs/3/server/admin_web#using-custom-web-interface","content":" If you want to use custom web interface you can specify path to web interface directory dist:  config.json { ..., &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;&lt;PASSWORD&gt;&quot;, &quot;admin_secret&quot;: &quot;&lt;SECRET&gt;&quot;, &quot;admin_web_path&quot;: &quot;&lt;PATH_TO_WEB_DIST&gt;&quot; }   This can be useful if you want to modify official web interface code in some way and test it with Centrifugo.  ","version":"v3","tagName":"h2"},{"title":"Admin insecure mode​","type":1,"pageTitle":"Admin web UI","url":"/docs/3/server/admin_web#admin-insecure-mode","content":" There is also an option to run Centrifugo in insecure admin mode - in this case you don't need to set admin_password and admin_secret in config – in web interface you will be logged in automatically without any password. Note that this is only an option for production if you protected admin web interface with firewall rules. Otherwise anyone in internet will have full access to admin functionality described above. To enable insecure admin mode:  config.json { ..., &quot;admin&quot;: true, &quot;admin_insecure&quot;: true, &quot;admin_password&quot;: &quot;&lt;PASSWORD&gt;&quot;, &quot;admin_secret&quot;: &quot;&lt;SECRET&gt;&quot; }  ","version":"v3","tagName":"h2"},{"title":"Error and disconnect codes","type":0,"sectionRef":"#","url":"/docs/3/server/codes","content":"","keywords":"","version":"v3"},{"title":"Client error codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#client-error-codes","content":" Client errors are errors that can be returned to a client in replies to commands. This is specific for bidirectional client protocol only. For example, an error can be returned inside a reply to a subscribe command issued by a client.  Here is the list of Centrifugo built-in client error codes (with proxy feature you have a way to use custom error codes in replies or reuse existing).  ","version":"v3","tagName":"h2"},{"title":"Internal​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#internal","content":" Code: 100, Message: &quot;internal server error&quot;.  Error Internal means server error, if returned this is a signal that something went wrong with a server itself and client most probably not guilty.  ","version":"v3","tagName":"h3"},{"title":"Unauthorized​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#unauthorized","content":" Code: 101, Message: &quot;unauthorized&quot;.  Error Unauthorized says that request is unauthorized.  ","version":"v3","tagName":"h3"},{"title":"Unknown Channel​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#unknown-channel","content":" Code: 102, Message: &quot;unknown channel&quot;.  Error Unknown Channel means that channel name does not exist.  Usually this is returned when client uses channel with a namespace which is not defined in Centrifugo configuration.  ","version":"v3","tagName":"h3"},{"title":"Permission Denied​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#permission-denied","content":" Code: 103, Message: &quot;permission denied&quot;.  Error Permission Denied means that access to resource not allowed.  ","version":"v3","tagName":"h3"},{"title":"Method Not Found​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#method-not-found","content":" Code: 104, Message: &quot;method not found&quot;.  Error Method Not Found means that method sent in command does not exist.  ","version":"v3","tagName":"h3"},{"title":"Already Subscribed​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#already-subscribed","content":" Code: 105, Message: &quot;already subscribed&quot;.  Error Already Subscribed returned when client wants to subscribe on channel it already subscribed to.  ","version":"v3","tagName":"h3"},{"title":"Limit Exceeded​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#limit-exceeded","content":" Code: 106, Message: &quot;limit exceeded&quot;.  Error Limit Exceeded says that some sort of limit exceeded, server logs should give more detailed information. See also ErrorTooManyRequests which is more specific for rate limiting purposes.  ","version":"v3","tagName":"h3"},{"title":"Bad Request​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#bad-request","content":" Code: 107, Message: &quot;bad request&quot;.  Error Bad Request says that server can not process received data because it is malformed. Retrying request does not make sense.  ","version":"v3","tagName":"h3"},{"title":"Not Available​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#not-available","content":" Code: 108, Message: &quot;not available&quot;.  Error Not Available means that resource is not enabled.  For example, this can be returned when trying to access history or presence in a channel that is not configured for having history or presence features.  ","version":"v3","tagName":"h3"},{"title":"Token Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#token-expired","content":" Code: 109, Message: &quot;token expired&quot;.  Error Token Expired indicates that connection token expired.  ","version":"v3","tagName":"h3"},{"title":"Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#expired","content":" Code: 110, Message: &quot;expired&quot;.  Error Expired indicates that connection expired (no token involved).  ","version":"v3","tagName":"h3"},{"title":"Too Many Requests​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#too-many-requests","content":" Code: 111, Message: &quot;too many requests&quot;.  Error Too Many Requests means that server rejected request due to rate limiting strategies.  ","version":"v3","tagName":"h3"},{"title":"Unrecoverable Position​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#unrecoverable-position","content":" Code: 112, Message: &quot;unrecoverable position&quot;.  Error Unrecoverable Position means that stream does not contain required range of publications to fulfill a history query.  This can happen due to wrong epoch passed.  ","version":"v3","tagName":"h3"},{"title":"Client disconnect codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#client-disconnect-codes","content":" Client can be disconnected by a Centrifugo server with custom code and string reason. Here is the list of Centrifugo built-in disconnect codes (with proxy feature you have a way to use custom disconnect codes).  note We expect that in most situations developers don't need to programmatically deal with handling various disconnect codes, but since Centrifugo sends them and codes shown in server metrics – they are documented. Actually most client connectors don't provide access to reading a disconnect code these days (only a reason). This is what we are planning to improve.  ","version":"v3","tagName":"h2"},{"title":"Normal​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#normal","content":" Code: 3000.  DisconnectNormal is clean disconnect when client cleanly closed connection. This is mostly useful for server metrics, since client never receives this disconnect code (since already gone).  ","version":"v3","tagName":"h3"},{"title":"Shutdown​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#shutdown","content":" Code: 3001, Reason: &quot;shutdown&quot;, Reconnect: true.  Disconnect Shutdown sent when node is going to shut down.  ","version":"v3","tagName":"h3"},{"title":"Invalid Token​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#invalid-token","content":" Code: 3002, Reason: &quot;invalid token&quot;, Reconnect: false.  Disconnect Invalid Token sent when client came with invalid token.  ","version":"v3","tagName":"h3"},{"title":"Bad Request​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#bad-request-1","content":" Code: 3003, Reason: &quot;bad request&quot;, Reconnect: false.  Disconnect Bad Request sent when client uses malformed protocol  ","version":"v3","tagName":"h3"},{"title":"Server Error​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#server-error","content":" Code: 3004, Reason: &quot;internal server error&quot;, Reconnect: true.  Disconnect Server Error sent when internal error occurred on server.  ","version":"v3","tagName":"h3"},{"title":"Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#expired-1","content":" Code: 3005, Reason: &quot;expired&quot;, Reconnect: true.  Disconnect Expired sent when client connection expired.  ","version":"v3","tagName":"h3"},{"title":"Subscription Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#subscription-expired","content":" Code: 3006, Reason: &quot;subscription expired&quot;, Reconnect: true.  Disconnect Subscription Expired sent when client subscription expired.  ","version":"v3","tagName":"h3"},{"title":"Stale​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#stale","content":" Code: 3007, Reason: &quot;stale&quot;, Reconnect: false.  Disconnect Stale sent to close connection that did not become authenticated in configured interval after dialing. Usually this means a broken client implementation.  ","version":"v3","tagName":"h3"},{"title":"Slow​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#slow","content":" Code: 3008, Reason: &quot;slow&quot;, Reconnect: true.  Disconnect Slow sent when a client can't read messages fast enough.  ","version":"v3","tagName":"h3"},{"title":"Write Error​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#write-error","content":" Code: 3009, Reason: &quot;write error&quot;, Reconnect: true.  Disconnect Write Error sent when an error occurred while writing to client connection.  ","version":"v3","tagName":"h3"},{"title":"Insufficient State​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#insufficient-state","content":" Code: 3010, Reason: &quot;insufficient state&quot;, Reconnect: true.  Disconnect Insufficient State sent when server detects wrong client position in channel Publication stream. Disconnect allows client to restore missed publications on reconnect.  ","version":"v3","tagName":"h3"},{"title":"Force Reconnect​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#force-reconnect","content":" Code: 3011, Reason: &quot;force reconnect&quot;, Reconnect: true.  Disconnect Force Reconnect sent when server disconnects connection but want it to return back shortly.  ","version":"v3","tagName":"h3"},{"title":"Force No Reconnect​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#force-no-reconnect","content":" Code: 3012, Reason: &quot;force disconnect&quot;, Reconnect: false.  Disconnect Force No Reconnect sent when server disconnects connection and asks it to not reconnect again.  ","version":"v3","tagName":"h3"},{"title":"Connection Limit​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#connection-limit","content":" Code: 3013, Reason: &quot;connection limit&quot;, Reconnect: false.  Disconnect Connection Limit can be sent when client connection exceeds a configured connection limit (per user ID or due to other rule).  ","version":"v3","tagName":"h3"},{"title":"Server API error codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#server-api-error-codes","content":" Server API errors are errors that can be returned to a API caller in replies to commands (in both HTTP and GRPC server APIs).  ","version":"v3","tagName":"h2"},{"title":"Internal​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#internal-1","content":" Code: 100, Message: &quot;internal server error&quot;.  ErrorInternal means server error, if returned this is a signal that something went wrong with Centrifugo itself.  ","version":"v3","tagName":"h3"},{"title":"Unknown channel​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#unknown-channel-1","content":" Code: 102, Message: &quot;unknown channel&quot;.  Error Unknown Channel means that namespace in channel name does not exist.  ","version":"v3","tagName":"h3"},{"title":"Method Not Found​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#method-not-found-1","content":" Code: 104, Message: &quot;method not found&quot;.  Error Method Not Found means that method sent in command does not exist in Centrifugo.  ","version":"v3","tagName":"h3"},{"title":"Bad Request​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#bad-request-2","content":" Code: 107, Message: &quot;bad request&quot;.  Error Bad Request says that Centrifugo can not parse received data because it is malformed or there are required fields missing.  ","version":"v3","tagName":"h3"},{"title":"Not Available​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#not-available-1","content":" Code: 108, Message: &quot;not available&quot;.  Error Not Available means that resource is not enabled.  ","version":"v3","tagName":"h3"},{"title":"Unrecoverable Position​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/3/server/codes#unrecoverable-position-1","content":" Code: 112, Message: &quot;unrecoverable position&quot;.  ErrorUnrecoverablePosition means that stream does not contain required range of publications to fulfill a history query.  This can happen due to wrong epoch. ","version":"v3","tagName":"h3"},{"title":"Infrastructure tuning","type":0,"sectionRef":"#","url":"/docs/3/server/infra_tuning","content":"","keywords":"","version":"v3"},{"title":"Open files limit​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/3/server/infra_tuning#open-files-limit","content":" You should increase a max number of open files Centrifugo process can open if you want to handle more connections.  To get the current open files limit run:  ulimit -n   On Linux you can check limits for a running process using:  cat /proc/&lt;PID&gt;/limits   The open file limit shows approximately how many clients your server can handle. Each connection consumes one file descriptor. On most operating systems this limit is 128-256 by default.  See this document to get more info on how to increase this number.  If you install Centrifugo using RPM from repo then it automatically sets max open files limit to 65536.  You may also need to increase max open files for Nginx (or any other proxy before Centrifugo).  ","version":"v3","tagName":"h3"},{"title":"Ephemeral port exhaustion​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/3/server/infra_tuning#ephemeral-port-exhaustion","content":" Ephemeral ports exhaustion problem can happen between your load balancer and Centrifugo server. If your clients connect directly to Centrifugo without any load balancer or reverse proxy software between then you are most likely won't have this problem. But load balancing is a very common thing.  The problem arises due to the fact that each TCP connection uniquely identified in the OS by the 4-part-tuple:  source ip | source port | destination ip | destination port   On load balancer/server boundary you are limited in 65536 possible variants by default. Actually due to some OS limits not all 65536 ports are available for usage (usually about 15k ports available by default).  In order to eliminate a problem you can:  Increase the ephemeral port range by tuning ip_local_port_range optionDeploy more Centrifugo server instances to load balance acrossDeploy more load balancer instancesUse virtual network interfaces  See a post in Pusher blog about this problem and more detailed solution steps.  ","version":"v3","tagName":"h3"},{"title":"Sockets in TIME_WAIT state​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/3/server/infra_tuning#sockets-in-time_wait-state","content":" On load balancer/server boundary one more problem can arise: sockets in TIME_WAIT state.  Under load when lots of connections and disconnections happen socket descriptors can stay in TIME_WAIT state. Those descriptors can not be reused for a while. So you can get various errors when using Centrifugo. For example something like (99: Cannot assign requested address) while connecting to upstream in Nginx error log and 502 on client side.  Look how many socket descriptors in TIME_WAIT state.  netstat -an |grep TIME_WAIT | grep &lt;CENTRIFUGO_PID&gt; | wc -l   Nice article about TIME_WAIT sockets: http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html  The advices here are similar to ephemeral port exhaustion problem:  Increase the ephemeral port range by tuning ip_local_port_range optionDeploy more Centrifugo server instances to load balance acrossDeploy more load balancer instancesUse virtual network interfaces  ","version":"v3","tagName":"h3"},{"title":"Proxy max connections​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/3/server/infra_tuning#proxy-max-connections","content":" Proxies like Nginx and Envoy have default limits on maximum number of connections which can be established.  Make sure you have a reasonable limit for max number of incoming and outgoing connections in your proxy configuration.  ","version":"v3","tagName":"h3"},{"title":"Conntrack table​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/3/server/infra_tuning#conntrack-table","content":" More rare (since default limit is usually sufficient) your possible number of connections can be limited by conntrack table. Netfilter framework which is part of iptables keeps information about all connections and has limited size for this information. See how to see its limits and instructions to increase in this article. ","version":"v3","tagName":"h3"},{"title":"Load balancing","type":0,"sectionRef":"#","url":"/docs/3/server/load_balancing","content":"","keywords":"","version":"v3"},{"title":"Nginx configuration​","type":1,"pageTitle":"Load balancing","url":"/docs/3/server/load_balancing#nginx-configuration","content":" Although it's possible to use Centrifugo without any reverse proxy before it, it's still a good idea to keep Centrifugo behind mature reverse proxy to deal with edge cases when handling HTTP/Websocket connections from the wild. Also you probably want some sort of load balancing eventually between Centrifugo nodes so that proxy can be such a balancer too.  In this section we will look at Nginx configuration to deploy Centrifugo.  Minimal Nginx version – 1.3.13 because it was the first version that can proxy Websocket connections.  There are 2 ways: running Centrifugo server as separate service on its own domain or embed it to a location of your web site (for example to /centrifugo).  ","version":"v3","tagName":"h2"},{"title":"Separate domain for Centrifugo​","type":1,"pageTitle":"Load balancing","url":"/docs/3/server/load_balancing#separate-domain-for-centrifugo","content":" upstream centrifugo { # uncomment ip_hash if using SockJS transport with many upstream servers. #ip_hash; server 127.0.0.1:8000; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } #server { #\tlisten 80; #\tserver_name centrifugo.example.com; #\trewrite ^(.*) https://$server_name$1 permanent; #} server { server_name centrifugo.example.com; listen 80; #listen 443 ssl; #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #ssl_certificate /etc/nginx/ssl/server.crt; #ssl_certificate_key /etc/nginx/ssl/server.key; include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; tcp_nodelay on; gzip on; gzip_min_length 1000; gzip_proxied any; # Only retry if there was a communication error, not a timeout # on the Centrifugo server (to avoid propagating &quot;queries of death&quot; # to all frontends) proxy_next_upstream error; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; location /connection { proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_http_version 1.1; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } location / { proxy_pass http://centrifugo; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } }   ","version":"v3","tagName":"h3"},{"title":"Embed to a location of web site​","type":1,"pageTitle":"Load balancing","url":"/docs/3/server/load_balancing#embed-to-a-location-of-web-site","content":" upstream centrifugo { # uncomment ip_hash if using SockJS transport with many upstream servers. #ip_hash; server 127.0.0.1:8000; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { # ... your web site Nginx config location /centrifugo/ { rewrite ^/centrifugo/(.*) /$1 break; proxy_pass http://centrifugo; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; } location /centrifugo/connection { rewrite ^/centrifugo(.*) $1 break; proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } }   ","version":"v3","tagName":"h3"},{"title":"worker_connections​","type":1,"pageTitle":"Load balancing","url":"/docs/3/server/load_balancing#worker_connections","content":" You may also need to update worker_connections option of Nginx:  events { worker_connections 65535; }  ","version":"v3","tagName":"h3"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/docs/3/server/monitoring","content":"","keywords":"","version":"v3"},{"title":"Prometheus​","type":1,"pageTitle":"Monitoring","url":"/docs/3/server/monitoring#prometheus","content":" To enable Prometheus endpoint start Centrifugo with prometheus option on:  config.json { ... &quot;prometheus&quot;: true }   This will enable /metrics endpoint so the Centrifugo instance can be monitored by your Prometheus server.  ","version":"v3","tagName":"h3"},{"title":"Graphite​","type":1,"pageTitle":"Monitoring","url":"/docs/3/server/monitoring#graphite","content":" To enable automatic export to Graphite (via TCP):  config.json { ... &quot;graphite&quot;: true, &quot;graphite_host&quot;: &quot;localhost&quot;, &quot;graphite_port&quot;: 2003 }   By default, stats will be aggregated over 10 seconds intervals inside Centrifugo and then pushed to Graphite over TCP connection.  If you need to change this aggregation interval use the graphite_interval option (in seconds, default 10).  ","version":"v3","tagName":"h3"},{"title":"Grafana dashboard​","type":1,"pageTitle":"Monitoring","url":"/docs/3/server/monitoring#grafana-dashboard","content":" Check out Centrifugo official Grafana dashboard for Prometheus storage. You can import that dashboard to your Grafana, point to Prometheus storage – and enjoy visualized metrics.   ","version":"v3","tagName":"h3"},{"title":"History and recovery","type":0,"sectionRef":"#","url":"/docs/3/server/history_and_recovery","content":"","keywords":"","version":"v3"},{"title":"History design​","type":1,"pageTitle":"History and recovery","url":"/docs/3/server/history_and_recovery#history-design","content":" History properties configured on a namespace level, to enable history both history_size and history_ttl should be set to a value greater than zero.  Centrifugo is not designed to keep publications streams forever. Streams are ephemeral and can expire or can be lost at any moment. But Centrifugo provides a way for an application or a client to understand that stream history lost. In this case, the main application database should be the source of truth and state recovery.  When history is on every publication is published into a channel saved into history. Depending on the engine used history stream implementation can differ. For example, in the case of the Memory engine, all history is stored in process memory. So as soon as Centrifugo restarted all history is cleared. When using Redis engine history is kept in Redis Stream data structure - persistence properties are then inherited from Redis persistence configuration (the same for KeyDB engine). For Tarantool history is kept inside spaces.  Each publication when added to history has an offset field. This is an incremental uint64 field. Each stream is identified by the epoch field - which is an arbitrary string. As soon as the underlying engine loses data epoch field will change for a stream thus letting consumers know that stream can't be used as the source of truth anymore.  tip History in channels is not enabled by default. See how to enable it over channel options. History is available in both server and client API.  ","version":"v3","tagName":"h2"},{"title":"History iteration API​","type":1,"pageTitle":"History and recovery","url":"/docs/3/server/history_and_recovery#history-iteration-api","content":" History iteration based on three fields:  limitsincereverse  Combining these fields you can iterate over a stream in both directions.  Get current stream top offset and epoch:  history(limit: 0, since: null, reverse: false)   Get full history from the current beginning (but up to client_history_max_publication_limit, which is 300 by default):  history(limit: -1, since: null, reverse: false)   Get full history from the current end (but up to client_history_max_publication_limit, which is 300 by default):  history(limit: -1, since: null, reverse: true)   Get history from the current beginning (up to 10):  history(limit: 10, since: null, reverse: false)   Get history from the current end in reversed direction (up to 10):  history(limit: 10, since: null, reverse: true)   Get up to 10 publications since known stream position (described by offset and epoch values):  history(limit: 10, since: {offset: 0, epoch: &quot;epoch&quot;}, reverse: false)   Get up to 10 publications since known stream position (described by offset and epoch values) but in reversed direction (from last to earliest):  history(limit: 10, since: {offset: 11, epoch: &quot;epoch&quot;}, reverse: true)   Here is an example program in Go which endlessly iterates over stream both ends (using gocent API library), upon reaching the end of stream the iteration goes in reversed direction (not really useful in real world but fun):  // Iterate by 10. limit := 10 // Paginate in reversed order first, then invert it. reverse := true // Start with nil StreamPosition, then fill it with value while paginating. var sp *gocent.StreamPosition for { historyResult, err = c.History( ctx, channel, gocent.WithLimit(limit), gocent.WithReverse(reverse), gocent.WithSince(sp), ) if err != nil { log.Fatalf(&quot;Error calling history: %v&quot;, err) } for _, pub := range historyResult.Publications { log.Println(pub.Offset, &quot;=&gt;&quot;, string(pub.Data)) sp = &amp;gocent.StreamPosition{ Offset: pub.Offset, Epoch: historyResult.Epoch, } } if len(historyResult.Publications) &lt; limit { // Got all pubs, invert pagination direction. reverse = !reverse log.Println(&quot;end of stream reached, change iteration direction&quot;) } }   ","version":"v3","tagName":"h2"},{"title":"Automatic message recovery​","type":1,"pageTitle":"History and recovery","url":"/docs/3/server/history_and_recovery#automatic-message-recovery","content":" One of the most interesting features of Centrifugo is automatic message recovery after short network disconnects. This mechanism allows a client to automatically restore missed publications on successful resubscribe to a channel after being disconnected for a while.  In general, you could query your application backend for the actual state on every client reconnect - but the message recovery feature allows Centrifugo to deal with this and restore missed publications from the history cache thus radically reducing the load on your application backend and your main application database in some scenarios (when many clients reconnecting at the same time).  danger Message recovery protocol feature designed to be used together with reasonably small Publication stream size as all missed publications sent towards the client in one protocol frame on resubscribing to a channel. Thus, it is mostly suitable for short-time disconnects. It helps a lot to survive a reconnect storm when many clients reconnect at one moment (balancer reload, network glitch) - but it's not a good idea to recover a long list of missed messages after clients being offline for a long time.  To enable recovery mechanism for channels set the recover boolean configuration option to true on the configuration file top-level or for a channel namespace. Make sure to enable this option in namespaces where history is on.  When re-subscribing on channels Centrifugo will return missed publications to a client in a subscribe Reply, also it will return a special recovered boolean flag to indicate whether all missed publications successfully recovered after a disconnect or not.  The number of publications that is possible to automatically recover is controlled by the client_recovery_max_publication_limit option which is 300 by default.  Centrifugo recovery model based on two fields in the protocol: offset and epoch. All fields are managed automatically by Centrifugo client libraries (for bidirectional transport), but it's good to know how recovery works under the hood.  The recovery feature heavily relies on offset and epoch values described above.  epoch handles cases when history storage has been restarted while the client was in a disconnected state so publication numeration in a channel started from scratch. For example at the moment Memory engine does not persist publication sequences on disk so every restart will start numeration from scratch. After each restart a new epoch field is generated, and we can understand in the recovery process that the client could miss messages thus returning the correct recovered flag in a subscribe Reply. This also applies to the Redis engine – if you do not use AOF with fsync then sequences can be lost after Redis restart. When using the Redis engine you need to use a fully in-memory model strategy or AOF with fsync to guarantee the reliability of the recovered flag sent by Centrifugo.  When a server receives subscribe command with the boolean flag recover set to true and offset, epoch set to values last seen by a client (see SubscribeRequest type in protocol definitions) it will try to find all missed publications from history cache. Recovered publications will be passed to the client in a subscribe Reply in the correct order, so your publication handler will be automatically called to process each missed message.  You can also manually implement your recovery algorithm on top of the basic PUB/SUB possibilities that Centrifugo provides. As we said above you can simply ask your backend for an actual state after every client reconnects completely bypassing the recovery mechanism described here. Also, it's possible to manually iterate over the Centrifugo stream using the history iteration API described above. ","version":"v3","tagName":"h2"},{"title":"Channels","type":0,"sectionRef":"#","url":"/docs/3/server/channels","content":"","keywords":"","version":"v3"},{"title":"Channel name rules​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#channel-name-rules","content":" Only ASCII symbols must be used in channel string.  Channel name length limited by 255 characters by default (can be changed via configuration file option channel_max_length).  Several symbols in channel names reserved for Centrifugo internal needs:  : – for namespace channel boundary (see below)$ – for private channel prefix (see below)# – for user channel boundary (see below)* – for the future Centrifugo needs&amp; – for the future Centrifugo needs/ – for the future Centrifugo needs  ","version":"v3","tagName":"h2"},{"title":"namespace boundary (:)​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#namespace-boundary-","content":" : – is a channel namespace boundary. Namespaces are used to set custom options to a group of channels. Each channel belonging to the same namespace will have the same channel options. Read more about available channel options and more about namespaces below.  If the channel is public:chat - then Centrifugo will apply options to this channel from the channel namespace with the name public.  info A namespace is part of the channel name. If a user subscribed to a channel with namespace, like public:chat – then you need to publish messages into public:chat channel to be delivered to the user. We often see some confusion from developers trying to publish messages into chat and thinking that namespace is somehow stripped upon subscription. It's not true.  ","version":"v3","tagName":"h3"},{"title":"private channel prefix ($)​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#private-channel-prefix-","content":" If the channel starts with $ then it is considered private. The subscription on a private channel must be properly signed by your backend.  Use private channels if you pass sensitive data inside the channel and want to control access permissions on your backend.  For example $secrets is a private channel, $public:chat - is a private channel that belongs to namespace public.  Subscription request to private channels requires additional JWT from your application backend. Read detailed chapter about private channels.  If you need a personal channel for a single user (or maybe a channel for a short and stable set of users) then consider using a user-limited channel (see below) as a simpler alternative that does not require an additional subscription token from your backend.  Also, consider using server-side subscriptions or subscribe proxy feature of Centrifugo to model channels with restrictive access.  ","version":"v3","tagName":"h3"},{"title":"user channel boundary (#)​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#user-channel-boundary-","content":" # – is a user channel boundary. This is a separator to create personal channels for users (we call this user-limited channels) without the need to provide a subscription token.  For example, if the channel is news#42 then the only user with ID 42 can subscribe to this channel (Centrifugo knows user ID because clients provide it in connection credentials with connection JWT).  If you want to create a user-limited channel in namespace personal then you can use a name like personal:user#42 for example.  tip Channel like $personal:user#42 - i.e. channel with both private prefix $ and user channel boundary # does not have a lot of sense, most probably you can just use personal:user#42 as the ID of the user protected by authentication JWT or set by application backend when the connect proxy feature is used.  Moreover, you can provide several user IDs in channel name separated by a comma: dialog#42,43 – in this case only the user with ID 42 and user with ID 43 will be able to subscribe on this channel.  This is useful for channels with a static list of allowed users, for example for single user personal messages channel, for dialog channel between certainly defined users. As soon as you need to manage access to a channel dynamically for many users this channel type does not suit well.  ","version":"v3","tagName":"h3"},{"title":"Channel options​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#channel-options","content":" Channel behavior can be modified by using channel options. Channel options can be defined on configuration top-level and for every namespace.  ","version":"v3","tagName":"h2"},{"title":"publish​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#publish","content":" publish (boolean, default false) – when on allows clients to publish messages into channels directly (from a client-side).  Keep in mind that your application will never receive such messages. In an idiomatic use case, all messages must be published to Centrifugo by an application backend using Centrifugo API (HTTP or GRPC). Or using publish proxy. Since in those cases your application has a chance to validate a message, save it into a database, and only after that broadcast to all subscribers.  But the publish option still can be useful to send something without backend-side validation and saving it into a database. This option can also be handy for demos and quick prototyping real-time app ideas.  ","version":"v3","tagName":"h3"},{"title":"subscribe_to_publish​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#subscribe_to_publish","content":" subscribe_to_publish (boolean, default false) - when the publish option is enabled client can publish into a channel without being subscribed to it. This option enables an automatic check that the client subscribed to a channel before allowing a client to publish.  ","version":"v3","tagName":"h3"},{"title":"anonymous​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#anonymous","content":" anonymous (boolean, default false) – this option enables anonymous user access (i.e. for a user with an empty user ID). In most situations, your application works with authenticated users so every user has its unique user ID (set inside JWT sub claim or provided by backend when using connect proxy). But if you provide real-time features for public access you may need unauthenticated access to some channels. Turn on this option and use an empty string as a user ID. See also related global option client_anonymous which allows anonymous users to connect without JWT.  ","version":"v3","tagName":"h3"},{"title":"presence​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#presence","content":" presence (boolean, default false) – enable/disable online presence information for channels. Online presence is information about clients currently subscribed to the channel. It contains each subscriber's client ID, user ID, connection info, and channel info. By default, this option is off so no presence information will be available for channels.  Enabling channel online presence adds some overhead since Centrifugo needs to maintain an additional data structure (in a process memory or a broker memory/disk).  ","version":"v3","tagName":"h3"},{"title":"presence_disable_for_client​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#presence_disable_for_client","content":" presence_disable_for_client (boolean, default false) – allows making presence calls available only for a server-side API. By default, presence information is available for both client and server-side APIs.  ","version":"v3","tagName":"h3"},{"title":"join_leave​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#join_leave","content":" join_leave (boolean, default false) – enable/disable sending join(leave) messages when the client subscribes to a channel (unsubscribes from a channel). Join/leave event includes information about the connection that triggered an event – client ID, user ID, connection info, and channel info.  caution Keep in mind that join/leave messages can generate a big number of messages in a system if turned on for channels with a large number of active subscribers. If you have channels with a large number of subscribers consider avoiding using this feature or to scale Centrifugo.  ","version":"v3","tagName":"h3"},{"title":"history_size​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#history_size","content":" history_size (integer, default 0) – history size (amount of messages) for channels. As Centrifugo keeps all history messages in process memory (or in a broker memory) it's very important to limit the maximum amount of messages in channel history with a reasonable value. history_size defines the maximum amount of messages that Centrifugo will keep for each channel in the namespace. As soon as history has more messages than defined by history size – old messages will be evicted.  Setting only history_size is not enough to enable history in channels – you also need to wisely configure history_ttl option (see below).  caution Enabling channel history adds some overhead (both memory and CPU) since Centrifugo needs to maintain an additional data structure (in a process memory or a broker memory/disk).  ","version":"v3","tagName":"h3"},{"title":"history_ttl​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#history_ttl","content":" history_ttl (duration, default 0s) – interval how long to keep channel history messages (with seconds precision).  As all history is storing in process memory (or in a broker memory) it is also very important to get rid of old history data for unused (inactive for a long time) channels.  By default history TTL duration is zero – this means that channel history is disabled.  Again – to turn on history you should wisely configure both history_size and history_ttl options.  For example for top-level channels (which do not belong to a namespace):  config.json { ... &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;60s&quot; }   ","version":"v3","tagName":"h3"},{"title":"position​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#position","content":" position (boolean, default false) – when the position feature is on Centrifugo tries to compensate at most once delivery of PUB/SUB messages checking client position inside a stream.  If Centrifugo detects a bad position of the client (i.e. potential message loss) it disconnects a client with the Insufficient state disconnect code. Also, when the position option is enabled Centrifugo exposes the current stream top offset and current epoch in subscribe reply making it possible for a client to manually recover its state upon disconnect using history API.  position option must be used in conjunction with reasonably configured message history for a channel i.e. history_size and history_ttl must be set (because Centrifugo uses channel history to check client position in a stream).  ","version":"v3","tagName":"h3"},{"title":"recover​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#recover","content":" recover (boolean, default false) – when enabled Centrifugo will try to recover missed publications after a client reconnects for some reason (bad internet connection for example). Also when the recovery feature is on Centrifugo automatically enables properties of the position option described above.  recover option must be used in conjunction with reasonably configured message history for channel i.e. history_size and history_ttl must be set (because Centrifugo uses channel history to recover messages).  Also, note that not all real-time events require this feature turned on so think wisely when you need this. When this option is turned on your application should be designed in a way to tolerate duplicate messages coming from a channel (currently Centrifugo returns recovered publications in order and without duplicates but this is an implementation detail that can be theoretically changed in the future). See more details about how recovery works in special chapter.  ","version":"v3","tagName":"h3"},{"title":"history_disable_for_client​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#history_disable_for_client","content":" history_disable_for_client (boolean, default false) – allows making history available only for a server-side API. By default false – i.e. history calls are available for both client and server-side APIs.  note History recovery mechanism if enabled will continue to work for clients anyway even if history_disable_for_client is on.  ","version":"v3","tagName":"h3"},{"title":"protected​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#protected","content":" protected (boolean, default false) – when on will prevent a client to subscribe to arbitrary channels in a namespace. In this case, Centrifugo will only allow a client to subscribe on user-limited channels, on channels returned by the proxy response, or channels listed inside JWT. Client-side subscriptions to arbitrary channels will be rejected with PermissionDenied error. Server-side channels belonging to the protected namespace passed by the client itself during connect will be ignored.  ","version":"v3","tagName":"h3"},{"title":"proxy_subscribe​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#proxy_subscribe","content":" proxy_subscribe (boolean, default false) – turns on subscribe proxy, more info in proxy chapter  ","version":"v3","tagName":"h3"},{"title":"proxy_publish​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#proxy_publish","content":" proxy_publish (boolean, default false) – turns on publish proxy, more info in proxy chapter  ","version":"v3","tagName":"h3"},{"title":"subscribe_proxy_name​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#subscribe_proxy_name","content":" subscribe_proxy_name (string, default &quot;&quot;) – turns on subscribe proxy when granular proxy mode is used. Note that proxy_subscribe option defined above is ignored in granular proxy mode.  ","version":"v3","tagName":"h3"},{"title":"publish_proxy_name​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#publish_proxy_name","content":" publish_proxy_name (string, default &quot;&quot;) – turns on publish proxy when granular proxy mode is used. Note that proxy_publish option defined above is ignored in granular proxy mode.  ","version":"v3","tagName":"h3"},{"title":"Channel options config example​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#channel-options-config-example","content":" Let's look at how to set some of these options in a config:  config.json { &quot;token_hmac_secret_key&quot;: &quot;my-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;anonymous&quot;: true, &quot;publish&quot;: true, &quot;subscribe_to_publish&quot;: true, &quot;presence&quot;: true, &quot;join_leave&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot;, &quot;recover&quot;: true }   Here we set channel options on config top-level – these options will affect channels without namespace. Below we describe namespaces and how to define channel options for a namespace.  ","version":"v3","tagName":"h2"},{"title":"Channel namespaces​","type":1,"pageTitle":"Channels","url":"/docs/3/server/channels#channel-namespaces","content":" It's possible to configure a list of channel namespaces. Namespaces are optional but very useful.  A namespace allows setting custom options for channels starting with the namespace name. This provides great control over channel behavior so you have a flexible way to define different channel options for different real-time features in the application.  Namespace has a name, and the same channel options (with the same defaults) as described above.  name - unique namespace name (name must consist of letters, numbers, underscores, or hyphens and be more than 2 symbols length i.e. satisfy regexp ^[-a-zA-Z0-9_]{2,}$).  If you want to use namespace options for a channel - you must include namespace name into channel name with : as a separator:  public:messages  gossips:messages  Where public and gossips are namespace names. Centrifugo will look for : symbol in the channel name, will extract the namespace name, and will apply namespace options whenever required.  All things together here is an example of config.json which includes some top-level channel options set and has 2 additional channel namespaces configured:  config.json { &quot;token_hmac_secret_key&quot;: &quot;very-long-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;anonymous&quot;: true, &quot;publish&quot;: true, &quot;presence&quot;: true, &quot;join_leave&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;30s&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;public&quot;, &quot;publish&quot;: true, &quot;anonymous&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot;, &quot;recover&quot;: true }, { &quot;name&quot;: &quot;gossips&quot;, &quot;presence&quot;: true, &quot;join_leave&quot;: true } ] }   Channel news will use globally defined channel options.Channel public:news will use public namespace options.Channel gossips:news will use gossips namespace options.Channel xxx:hello will result into subscription error since there is no xxx namespace defined in configuration above.  Channel namespaces also work with private channels and user-limited channels. For example, if you have a namespace called dialogs then the private channel can be constructed as $dialogs:gossips, user-limited channel can be constructed as dialogs:dialog#1,2.  note There is no inheritance in channel options and namespaces – for example, you defined presence: true on a top level of configuration and then defined a namespace – that namespace won't have online presence enabled - you must enable it for a namespace explicitly. ","version":"v3","tagName":"h2"},{"title":"Server-side subscriptions","type":0,"sectionRef":"#","url":"/docs/3/server/server_subs","content":"","keywords":"","version":"v3"},{"title":"Dynamic server-side subscriptions​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/3/server/server_subs#dynamic-server-side-subscriptions","content":" See subscribe and unsubscribe server API  ","version":"v3","tagName":"h3"},{"title":"Automatic personal channel subscription​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/3/server/server_subs#automatic-personal-channel-subscription","content":" It's possible to automatically subscribe a user to a personal server-side channel.  To enable this you need to enable the user_subscribe_to_personal boolean option (by default false). As soon as you do this every connection with a non-empty user ID will be automatically subscribed to a personal user-limited channel. Anonymous users with empty user IDs won't be subscribed to any channel.  For example, if you set this option and the user with ID 87334 connects to Centrifugo it will be automatically subscribed to channel #87334 and you can process personal publications on the client-side in the same way as shown above.  As you can see by default generated personal channel name belongs to the default namespace (i.e. no explicit namespace used). To set custom namespace name use user_personal_channel_namespace option (string, default &quot;&quot;) – i.e. the name of namespace from configured configuration namespaces array. In this case, if you set user_personal_channel_namespace to personal for example – then the automatically generated personal channel will be personal:#87334 – user will be automatically subscribed to it on connect and you can use this channel name to publish personal notifications to the online user.  ","version":"v3","tagName":"h3"},{"title":"Maintain single user connection​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/3/server/server_subs#maintain-single-user-connection","content":" Usage of personal channel subscription also opens a road to enable one more feature: maintaining only a single connection for each user globally around all Centrifugo nodes.  user_personal_single_connection boolean option (default false) turns on a mode in which Centrifugo will try to maintain only a single connection for each user at the same moment. As soon as the user establishes a connection other connections from the same user will be closed with connection limit reason (client won't try to automatically reconnect).  This feature works with a help of presence information inside a personal channel. So presence should be turned on in a personal channel.  Example config:  { &quot;user_subscribe_to_personal&quot;: true, &quot;user_personal_single_connection&quot;: true, &quot;user_personal_channel_namespace&quot;: &quot;personal&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot;, &quot;presence&quot;: true } ] }   note Centrifugo can't guarantee that other user connections will be closed – since Disconnect messages are distributed around Centrifugo nodes with at most once guarantee. So don't add critical business logic based on this feature to your application. Though this should work just fine most of the time if the connection between the Centrifugo node and PUB/SUB broker is OK. ","version":"v3","tagName":"h3"},{"title":"Configure Centrifugo","type":0,"sectionRef":"#","url":"/docs/3/server/configuration","content":"","keywords":"","version":"v3"},{"title":"Configuration sources​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#configuration-sources","content":" Centrifugo can be configured in several ways.  ","version":"v3","tagName":"h2"},{"title":"Command-line flags​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#command-line-flags","content":" Centrifugo supports several command-line flags. See centrifugo -h for available flags. Command-line flags limited to most frequently used. In general, we suggest to avoid using flags for configuring Centrifugo in a production environment – prefer environment or configuration file sources.  Command-line options have the highest priority when set than other ways to configure Centrifugo.  ","version":"v3","tagName":"h3"},{"title":"OS environment variables​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#os-environment-variables","content":" All Centrifugo options can be set over env in the format CENTRIFUGO_&lt;OPTION_NAME&gt; (i.e. option name with CENTRIFUGO_ prefix, all in uppercase).  Setting options over env is mostly straightforward except namespaces – see how to set namespaces via env. Environment variables have the second priority after flags.  Boolean options can be set using strings according to Go language ParseBool function. I.e. to set true you can just use &quot;true&quot; value for an environment variable (or simply &quot;1&quot;). To set false use &quot;false&quot; or &quot;0&quot;. Example:  export CENTRIFUGO_PROMETHEUS=&quot;1&quot;   Also, array options, like allowed_origins can be set over environment variables as a single string where values separated by a space. For example:  export CENTRIFUGO_ALLOWED_ORIGINS=&quot;https://mysite1.example.com https://mysite2.example.com&quot;   For a nested object configuration (which we have, for example, in Centrifugo PRO ClickHouse analytics) it's still possible to use environment variables to set options. In this case replace nesting with _ when constructing environment variable name.  Empty environment variables are considered unset (!) and will fall back to the next configuration source.  ","version":"v3","tagName":"h3"},{"title":"Configuration file​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#configuration-file","content":" Configuration file supports all options mentioned in Centrifugo documentation and can be in one of three supported formats: JSON, YAML, or TOML. Config file options have the lowest priority among configuration sources (i.e. option set over environment variable prevails over the same option in config file).  A simple way to start with Centrifugo is to run:  centrifugo genconfig   This command generates config.json configuration file in a current directory. This file already has the minimal number of options set. So it's then possible to start Centrifugo:  centrifugo -c config.json   ","version":"v3","tagName":"h3"},{"title":"Config file formats​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#config-file-formats","content":" Centrifugo supports three configuration file formats: JSON, YAML, or TOML.  ","version":"v3","tagName":"h2"},{"title":"JSON config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#json-config-format","content":" Here is an example of Centrifugo JSON configuration file:  config.json { &quot;allowed_origins&quot;: [&quot;http://localhost:3000&quot;], &quot;token_hmac_secret_key&quot;: &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot;, &quot;api_key&quot;: &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; }   token_hmac_secret_key used to check JWT signature (more info about JWT in authentication chapter). If you are using connect proxy then you may use Centrifugo without JWT.  api_key used for Centrifugo API endpoint authorization, see more in chapter about server HTTP API. Keep both values secret and never reveal them to clients.  allowed_origins option described below.  ","version":"v3","tagName":"h3"},{"title":"TOML config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#toml-config-format","content":" Centrifugo also supports TOML format for configuration file:  centrifugo --config=config.toml   Where config.toml contains:  config.toml allowed_origins: [ &quot;http://localhost:3000&quot; ] token_hmac_secret_key = &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot; api_key = &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; log_level = &quot;debug&quot;   In the example above we also defined logging level to be debug which is useful to have while developing an application. In the production environment debug logging can be too chatty.  ","version":"v3","tagName":"h3"},{"title":"YAML config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#yaml-config-format","content":" YAML format is also supported:  config.yaml allowed_origins: - &quot;http://localhost:3000&quot; token_hmac_secret_key: &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot; api_key: &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; log_level: debug   With YAML remember to use spaces, not tabs when writing a configuration file.  ","version":"v3","tagName":"h3"},{"title":"Important options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#important-options","content":" Let's describe some important options you can configure when running Centrifugo.  ","version":"v3","tagName":"h2"},{"title":"allowed_origins​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#allowed_origins","content":" This option allows setting an array of allowed origin patterns (array of strings) for WebSocket and SockJS endpoints to prevent CSRF or WebSocket hijacking attacks. Also, it's used for HTTP-based unidirectional transports to enable CORS for configured origins.  As soon as allowed_origins is defined every connection request with Origin set will be checked against each pattern in an array.  Connection requests without Origin header set are passing through without any checks (i.e. always allowed).  For example, a client connects to Centrifugo from a web browser application on http://localhost:3000. In this case, allowed_origins should be configured in this way:  &quot;allowed_origins&quot;: [ &quot;http://localhost:3000&quot; ]   When connecting from https://example.com:  &quot;allowed_origins&quot;: [ &quot;https://example.com&quot; ]   Origin pattern can contain wildcard symbol * to match subdomains:  &quot;allowed_origins&quot;: [ &quot;https://*.example.com&quot; ]   – in this case requests with Origin header like https://foo.example.com or https://bar.example.com will pass the check.  It's also possible to allow all origins in the following way (but this is discouraged and insecure when using connect proxy feature):  &quot;allowed_origins&quot;: [ &quot;*&quot; ]   ","version":"v3","tagName":"h3"},{"title":"address​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#address","content":" Bind your Centrifugo to a specific interface address (string, by default &quot;&quot; - listen on all available interfaces).  ","version":"v3","tagName":"h3"},{"title":"port​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#port","content":" Port to bind Centrifugo to (string, by default &quot;8000&quot;).  ","version":"v3","tagName":"h3"},{"title":"engine​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#engine","content":" Engine to use - memory, redis or tarantool. It's a string option, by default memory. Read more about engines in special chapter.  ","version":"v3","tagName":"h3"},{"title":"Advanced options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#advanced-options","content":" These options allow tweaking server behavior, in most cases default values are good to start with.  ","version":"v3","tagName":"h2"},{"title":"client_channel_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#client_channel_limit","content":" Default: 128  Sets the maximum number of different channel subscriptions a single client can have.  tip When designing an application avoid subscribing to an unlimited number of channels per one client. Keep number of subscriptions for each client reasonably small – this will help keeping handshake process lightweight and fast.  ","version":"v3","tagName":"h3"},{"title":"channel_max_length​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#channel_max_length","content":" Default: 255  Sets the maximum length of the channel name.  ","version":"v3","tagName":"h3"},{"title":"client_user_connection_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#client_user_connection_limit","content":" Default: 0  The maximum number of connections from a user (with known user ID) to Centrifugo node. By default, unlimited.  The important thing to emphasize is that client_user_connection_limit works only per one Centrifugo node and exists mostly to protect Centrifugo from many connections from a single user – but not for business logic limitations. This means that if you set this to 1 and scale nodes – say run 10 Centrifugo nodes – then a user will be able to create 10 connections (one to each node).  ","version":"v3","tagName":"h3"},{"title":"client_queue_max_size​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#client_queue_max_size","content":" Default: 1048576  Maximum client message queue size in bytes to close slow reader connections. By default - 1mb.  ","version":"v3","tagName":"h3"},{"title":"client_anonymous​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#client_anonymous","content":" Default: false  Enable a mode when all clients can connect to Centrifugo without JWT. In this case, all connections without a token will be treated as anonymous (i.e. with empty user ID) and only can subscribe to channels with anonymous option enabled.  ","version":"v3","tagName":"h3"},{"title":"client_concurrency​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#client_concurrency","content":" Default: 0  client_concurrency when set tells Centrifugo that commands from a client must be processed concurrently.  By default, concurrency disabled – Centrifugo processes commands received from a client one by one. This means that if a client issues two RPC requests to a server then Centrifugo will process the first one, then the second one. If the first RPC call is slow then the client will wait for the second RPC response much longer than it could (even if the second RPC is very fast). If you set client_concurrency to some value greater than 1 then commands will be processed concurrently (in parallel) in separate goroutines (with maximum concurrency level capped by client_concurrency value). Thus, this option can effectively reduce the latency of individual requests. Since separate goroutines are involved in processing this mode adds some performance and memory overhead – though it should be pretty negligible in most cases. This option applies to all commands from a client (including subscribe, publish, presence, etc).  ","version":"v3","tagName":"h3"},{"title":"gomaxprocs​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#gomaxprocs","content":" Default: 0  By default, Centrifugo runs on all available CPU cores. To limit the number of cores Centrifugo can utilize in one moment use this option.  ","version":"v3","tagName":"h3"},{"title":"Endpoint configuration.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#endpoint-configuration","content":" After Centrifugo started there are several endpoints available.  ","version":"v3","tagName":"h2"},{"title":"Default endpoints.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#default-endpoints","content":" Bidirectional WebSocket default endpoint:  ws://localhost:8000/connection/websocket   Bidirectional SockJS default endpoint (disabled by default):  http://localhost:8000/connection/sockjs   Unidirectional EventSource endpoint (disabled by default):  http://localhost:8000/connection/uni_sse   Unidirectional HTTP streaming endpoint (disabled by default):  http://localhost:8000/connection/uni_http_stream   Unidirectional WebSocket endpoint (disabled by default):  http://localhost:8000/connection/uni_websocket   Unidirectional EventSource (SSE) endpoint (disabled by default):  http://localhost:8000/connection/uni_sse   Server HTTP API endpoint:  http://localhost:8000/api   By default, all endpoints work on port 8000. This can be changed with port option:  { &quot;port&quot;: 9000 }   In production setup, you may have a proper domain name in endpoint addresses above instead of localhost. While domain name and port parts can differ depending on setup – URL paths stay the same: /connection/sockjs, /connection/websocket, /api etc.  ","version":"v3","tagName":"h3"},{"title":"Admin endpoints.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#admin-endpoints","content":" Admin web UI endpoint works on root path by default, i.e. http://localhost:8000.  For more details about admin web UI, refer to the Admin web UI documentation.  ","version":"v3","tagName":"h3"},{"title":"Debug endpoints.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#debug-endpoints","content":" Next, when Centrifugo started in debug mode some extra debug endpoints become available. To start in debug mode add debug option to config:  { ... &quot;debug&quot;: true }   And endpoint:  http://localhost:8000/debug/pprof/   – will show useful information about the internal state of Centrifugo instance. This info is especially helpful when troubleshooting. See wiki page for more info.  ","version":"v3","tagName":"h3"},{"title":"Health check endpoint​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#health-check-endpoint","content":" Use health boolean option (by default false) to enable the health check endpoint which will be available on path /health. Also available over command-line flag:  centrifugo -c config.json --health   ","version":"v3","tagName":"h3"},{"title":"Custom internal ports​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#custom-internal-ports","content":" We strongly recommend not expose API, admin, debug, health, and Prometheus endpoints to the Internet. The following Centrifugo endpoints are considered internal:  API endpoint (/api) - for HTTP API requestsAdmin web interface endpoints (/, /admin/auth, /admin/api) - used by web interfacePrometheus endpoint (/metrics) - used for exposing server metrics in Prometheus formatHealth check endpoint (/health) - used to do health checksDebug endpoints (/debug/pprof) - used to inspect internal server state  It's a good practice to protect all these endpoints with a firewall. For example, it's possible to configure in location section of the Nginx configuration.  Though sometimes you don't have access to a per-location configuration in your proxy/load balancer software. For example when using Amazon ELB. In this case, you can change ports on which your internal endpoints work.  To run internal endpoints on custom port use internal_port option:  { ... &quot;internal_port&quot;: 9000 }   So admin web interface will work on address:  http://localhost:9000   Also, debug page will be available on a new custom port too:  http://localhost:9000/debug/pprof/   The same for API and Prometheus endpoints.  ","version":"v3","tagName":"h3"},{"title":"Disable default endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#disable-default-endpoints","content":" To disable websocket endpoint set websocket_disable boolean option to true.  To disable API endpoint set api_disable boolean option to true.  ","version":"v3","tagName":"h3"},{"title":"Customize handler endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#customize-handler-endpoints","content":" It's possible to customize server HTTP handler endpoints. To do this Centrifugo supports several options:  admin_handler_prefix (default &quot;&quot;) - to control Admin panel URL prefixwebsocket_handler_prefix (default &quot;/connection/websocket&quot;) - to control WebSocket URL prefixsockjs_handler_prefix (default &quot;/connection/sockjs&quot;) - to control SockJS URL prefixuni_sse_handler_prefix (default &quot;/connection/uni_sse&quot;) - to control unidirectional Eventsource URL prefixuni_http_stream_handler_prefix (default &quot;/connection/uni_http_stream&quot;) - to control unidirectional HTTP streaming URL prefixuni_websocket_handler_prefix (default &quot;/connection/uni_websocket&quot;) - to control unidirectional WebSocket URL prefixapi_handler_prefix (default &quot;/api&quot;) - to control HTTP API URL prefixprometheus_handler_prefix (default &quot;/metrics&quot;) - to control Prometheus URL prefixhealth_handler_prefix (default &quot;/health&quot;) - to control health check URL prefix  ","version":"v3","tagName":"h3"},{"title":"Signal handling​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#signal-handling","content":" It's possible to send HUP signal to Centrifugo to reload a configuration:  kill -HUP &lt;PID&gt;   Though at moment this will only reload token secrets and channel options (top-level and namespaces).  Centrifugo tries to gracefully shut down client connections when SIGINT or SIGTERM signals are received. By default, the maximum graceful shutdown period is 30 seconds but can be changed using shutdown_timeout (integer, in seconds) configuration option.  ","version":"v3","tagName":"h2"},{"title":"Insecure modes​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#insecure-modes","content":" ","version":"v3","tagName":"h2"},{"title":"Insecure client connection​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#insecure-client-connection","content":" The boolean option client_insecure (default false) allows connecting to Centrifugo without JWT token. In this mode, there is no user authentication involved. This mode can be useful for demo projects based on Centrifugo, local projects, or real-time application prototyping. Don't use it in production.  ","version":"v3","tagName":"h3"},{"title":"Insecure API mode​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#insecure-api-mode","content":" This mode can be enabled using the boolean option api_insecure (default false). When on there is no need to provide API key in HTTP requests. When using this mode everyone that has access to /api endpoint can send any command to server. Enabling this option can be reasonable if /api endpoint is protected by firewall rules.  The option is also useful in development to simplify sending API commands to Centrifugo using CURL for example without specifying Authorization header in requests.  ","version":"v3","tagName":"h3"},{"title":"Insecure admin mode​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#insecure-admin-mode","content":" This mode can be enabled using the boolean option admin_insecure (default false). When on there is no authentication in the admin web interface. Again - this is not secure but can be justified if you protected the admin interface by firewall rules or you want to use basic authentication for the Centrifugo admin interface (configured on proxy level).  ","version":"v3","tagName":"h3"},{"title":"Setting time duration options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#setting-time-duration-options","content":" Time durations in Centrifugo can be set using strings where duration value and unit are both provided. For example, to set 5 seconds duration use &quot;5s&quot;.  The minimal time resolution is 1ms. Some options of Centrifugo only support second precision (for example history_ttl channel option).  Valid time units are &quot;ms&quot; (milliseconds), &quot;s&quot; (seconds), &quot;m&quot; (minutes), &quot;h&quot; (hours).  Some examples:  &quot;1000ms&quot; // 1000 milliseconds &quot;1s&quot; // 1 second &quot;12h&quot; // 12 hours &quot;720h&quot; // 30 days   ","version":"v3","tagName":"h2"},{"title":"Setting namespaces over env​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#setting-namespaces-over-env","content":" While setting most options in Centrifugo over env is pretty straightforward setting namespaces is a bit special:  CENTRIFUGO_NAMESPACES='[{&quot;name&quot;: &quot;ns1&quot;}, {&quot;name&quot;: &quot;ns2&quot;}]' ./centrifugo   I.e. CENTRIFUGO_NAMESPACES environment variable should be a valid JSON string that represents namespaces array.  ","version":"v3","tagName":"h2"},{"title":"Anonymous usage stats​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/3/server/configuration#anonymous-usage-stats","content":" Centrifugo periodically sends anonymous usage information (once in 24 hours). That information is impersonal and does not include sensitive data, passwords, IP addresses, hostnames, etc. Only counters to estimate version and installation size distribution, and feature usage.  Please do not disable usage stats sending without reason. If you depend on Centrifugo – sure you are interested in further project improvements. Usage stats help us understand Centrifugo use cases better, concentrate on widely-used features, and be confident we are moving in the right direction. Developing in the dark is hard, and decisions may be non-optimal.  To disable sending usage stats set usage_stats_disable option:  config.json { &quot;usage_stats_disable&quot;: true }  ","version":"v3","tagName":"h2"},{"title":"Real-time transports","type":0,"sectionRef":"#","url":"/docs/3/transports/overview","content":"","keywords":"","version":"v3"},{"title":"Bidirectional​","type":1,"pageTitle":"Real-time transports","url":"/docs/3/transports/overview#bidirectional","content":" Bidirectional transports are capable to serve all Centrifugo features. These transports are the main Centrifugo focus.  Bidirectional transports come with a cost that developers need to use a special client connector library which speaks Centrifugo client protocol. The reason why we need a special client connector library is that a bidirectional connection is asynchronous – it's required to match requests to responses, properly manage connection state and request queueing/timeouts/errors.  Centrifugo has client SDKs for bidirectional communication for popular environments.  ","version":"v3","tagName":"h2"},{"title":"Unidirectional​","type":1,"pageTitle":"Real-time transports","url":"/docs/3/transports/overview#unidirectional","content":" Unidirectional transports suit well for simple use-cases with stable subscriptions, usually known at connection time.  The advantage is that unidirectional transports do not require special client connectors - developers can use native browser APIs (like WebSocket, EventSource, HTTP streaming), or GRPC generated code to receive real-time updates from Centrifugo – thus avoiding dependency to a client connector that abstracts bidirectional communication.  The drawback is that with unidirectional transports you are not inheriting all Centrifugo features out of the box (like dynamic subscriptions/unsubscriptions, automatic message recovery on reconnect, possibility to send RPC calls over persistent connection). But some of the missing client APIs can be mimicked by using calls to Centrifugo server API (i.e. over client -&gt; application backend -&gt; Centrifugo).  ","version":"v3","tagName":"h2"},{"title":"Unidirectional message types​","type":1,"pageTitle":"Real-time transports","url":"/docs/3/transports/overview#unidirectional-message-types","content":" In case of unidirectional transports Centrifugo will send Push frames to the connection. Push frames defined by client protocol schema. I.e. Centrifugo reuses a part of its bidirectional protocol for unidirectional communication. Push message defined as:  message Push { enum PushType { PUBLICATION = 0; JOIN = 1; LEAVE = 2; UNSUBSCRIBE = 3; MESSAGE = 4; SUBSCRIBE = 5; CONNECT = 6; DISCONNECT = 7; REFRESH = 8; } PushType type = 1; string channel = 2; bytes data = 3; }   So unidirectional connection will receive various pushes. All you need to do is look at Push type and process it or skip it. In most cases you will be most interested in CONNECT and PUBLICATION types.  tip In case of unidirectional WebSocket, EventSource and HTTP-streaming which currently work only with JSON data field of Push will come as an embedded JSON instead of bytes (again – the same mechanism as for Centrifugo bidirectional JSON protocol).  Just try using any unidirectional transport and you will quickly get the idea. ","version":"v3","tagName":"h3"},{"title":"Configure TLS","type":0,"sectionRef":"#","url":"/docs/3/server/tls","content":"","keywords":"","version":"v3"},{"title":"Using crt and key files​","type":1,"pageTitle":"Configure TLS","url":"/docs/3/server/tls#using-crt-and-key-files","content":" In first way you already have cert and key files. For development you can create self-signed certificate - see this instruction as example.  config.json { ... &quot;tls&quot;: true, &quot;tls_key&quot;: &quot;server.key&quot;, &quot;tls_cert&quot;: &quot;server.crt&quot; }   And run:  ./centrifugo --config=config.json   ","version":"v3","tagName":"h3"},{"title":"Automatic certificates​","type":1,"pageTitle":"Configure TLS","url":"/docs/3/server/tls#automatic-certificates","content":" For automatic certificates from Let's Encrypt add into configuration file:  config.json { ... &quot;tls_autocert&quot;: true, &quot;tls_autocert_host_whitelist&quot;: &quot;www.example.com&quot;, &quot;tls_autocert_cache_dir&quot;: &quot;/tmp/certs&quot;, &quot;tls_autocert_email&quot;: &quot;user@example.com&quot;, &quot;tls_autocert_http&quot;: true, &quot;tls_autocert_http_addr&quot;: &quot;:80&quot; }   tls_autocert (boolean) says Centrifugo that you want automatic certificate handling using ACME provider.  tls_autocert_host_whitelist (string) is a string with your app domain address. This can be comma-separated list. It's optional but recommended for extra security.  tls_autocert_cache_dir (string) is a path to a folder to cache issued certificate files. This is optional but will increase performance.  tls_autocert_email (string) is optional - it's an email address ACME provider will send notifications about problems with your certificates.  tls_autocert_http (boolean) is an option to handle http_01 ACME challenge on non-TLS port.  tls_autocert_http_addr (string) can be used to set address for handling http_01 ACME challenge (default is :80)  When configured correctly and your domain is valid (localhost will not work) - certificates will be retrieved on first request to Centrifugo.  Also Let's Encrypt certificates will be automatically renewed.  There are two options that allow Centrifugo to support TLS client connections from older browsers such as Chrome 49 on Windows XP and IE8 on XP:  tls_autocert_force_rsa - this is a boolean option, by default false. When enabled it forces autocert manager generate certificates with 2048-bit RSA keys.tls_autocert_server_name - string option, allows to set server name for client handshake hello. This can be useful to deal with old browsers without SNI support - see comment  grpc_api_tls_disable boolean flag allows to disable TLS for GRPC API server but keep it on for HTTP endpoints.  uni_grpc_tls_disable boolean flag allows to disable TLS for GRPC uni stream server but keep it on for HTTP endpoints.  ","version":"v3","tagName":"h3"},{"title":"TLS for GRPC API​","type":1,"pageTitle":"Configure TLS","url":"/docs/3/server/tls#tls-for-grpc-api","content":" You can provide custom certificate files to configure TLS for GRPC API server.  grpc_api_tls boolean flag enables TLS for GRPC API server, requires an X509 certificate and a key filegrpc_api_tls_cert string provides a path to an X509 certificate file for GRPC API servergrpc_api_tls_key string provides a path to an X509 certificate key for GRPC API server  ","version":"v3","tagName":"h3"},{"title":"TLS for GRPC unidirectional stream​","type":1,"pageTitle":"Configure TLS","url":"/docs/3/server/tls#tls-for-grpc-unidirectional-stream","content":" Starting from Centrifugo v3.0.0 you can provide custom certificate files to configure TLS for GRPC unidirectional stream endpoint.  uni_grpc_tls boolean flag enables TLS for GRPC server, requires an X509 certificate and a key fileuni_grpc_tls_cert string provides a path to an X509 certificate file for GRPC uni stream serveruni_grpc_tls_key string provides a path to an X509 certificate key for GRPC uni stream server ","version":"v3","tagName":"h3"},{"title":"Client protocol","type":0,"sectionRef":"#","url":"/docs/3/transports/client_protocol","content":"","keywords":"","version":"v3"},{"title":"Client implementation feature matrix​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#client-implementation-feature-matrix","content":" First we will look at list of features bidirectional client library should support. If you are an author of client library you can use this list as a checklist.  Our current client feature matrix looks like this:   connect to server (both Centrifugo and Centrifuge-based) using JSON protocol format connect to server (both Centrifugo and Centrifuge-based) using Protobuf protocol format connect with token (JWT in Centrifugo case, any string token in Centrifuge library case) connect to server with custom headers (not available in a browser) automatic reconnect in case of connection problems (server restart, unavailable network) an exponential backoff for reconnect process possibility to set handlers for connect and disconnect events extract and expose disconnect code and reason subscribe to a channel and provide a way to handle asynchronous Publications coming from it handle Join and Leave messages from a channel handle Unsubscribe notifications provide publish method of Subscription object provide unsubscribe method of Subscription provide presence method of Subscription provide presence stats method of Subscription provide history method of Subscription provide publish method on top level provide unsubscribe method on top level provide presence method on top level provide presence stats method on top level provide history method on top level send asynchronous messages to server handle asynchronous messages from server send RPC requests to server publish to channel without being subscribed subscribe to private (token-protected) channels with a token implement client-side connection token refresh mechanism implement private channel subscription token refresh mechanism client protocol level ping/pong to find a broken connection automatic reconnect in case of connect or subscribe command timeouts handle connection expired error handle subscription expired error server-side subscriptions message recovery mechanism for client-side subscriptions message recovery mechanism for server-side subscriptions  This document describes protocol specifics for Websocket transport which supports binary and text formats to transfer data. As Centrifugo and Centrifuge library for Go have various types of messages it serializes protocol messages using JSON or Protobuf formats.  info SockJS works almost the same way as JSON websocket described here but has its own extra framing on top of Centrifuge protocol messages. SockJS can only work with JSON - it's not possible to transfer binary data over it.  ","version":"v3","tagName":"h3"},{"title":"Top level framing​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#top-level-framing","content":" Centrifuge protocol defined in Protobuf schema. That schema is a source of the truth. Below we describe messages from that schema.  In bidirectional case client sends Command to server and server sends Reply to client. I.e. all communication between client and server is a bidirectional exchange of Command and Reply messages.  One request from client to server and one response from server to client can have more than one Command or Reply. This allows reducing number of system calls for writing and reading data.  When JSON format used then many Command can be sent from client to server in JSON streaming line-delimited format. I.e. each individual Command encoded to JSON and then commands joined together using new line symbol \\n:  {&quot;id&quot;: 1, &quot;method&quot;: 1, &quot;params&quot;: {&quot;channel&quot;: &quot;ch1&quot;}} {&quot;id&quot;: 2, &quot;method&quot;: 1, &quot;params&quot;: {&quot;channel&quot;: &quot;ch2&quot;}}   For example here is how we do this in Javascript client when JSON format used:  function encodeCommands(commands) { const encodedCommands = []; for (const i in commands) { if (commands.hasOwnProperty(i)) { encodedCommands.push(JSON.stringify(commands[i])); } } return encodedCommands.join('\\n'); }   info This doc will use JSON format for examples because it's human-readable. Everything said here for JSON is also true for Protobuf encoded case. There is a difference how several individual Command or server Reply joined into one request – see details below. Also, in JSON format bytes fields transformed into embedded JSON by Centrifugo.  When Protobuf format used then many Command can be sent from client to server in length-delimited format where each individual Command marshaled to bytes prepended by varint length. See existing client implementations for encoding example.  The same rules relate to many Reply in one response from server to client. Line-delimited JSON and varint-length prefixed Protobuf also used there.  For example here is how we read server response and extracting individual replies in Javascript client when JSON format used:  function decodeReplies(data) { const replies = []; const encodedReplies = data.split('\\n'); for (const i in encodedReplies) { if (encodedReplies.hasOwnProperty(i)) { if (!encodedReplies[i]) { continue; } const reply = JSON.parse(encodedReplies[i]); replies.push(reply); } } return replies; }   For Protobuf case see existing client implementations for decoding example.  As you can see each Command has id field. This is an incremental uint32 field. This field will be echoed in a server replies to commands so client could match a certain Reply to Command sent before. This is important since Websocket is an asynchronous protocol where server and client both send messages at any moment and there is no builtin request-response matching. Having id allows matching a reply with a command send before.  So you can expect something like this in response after sending commands to server:  {&quot;id&quot;: 1, &quot;result&quot;: {}} {&quot;id&quot;: 2, &quot;result&quot;: {}}   Besides id Reply from server to client have two important fields: result and error.  result contains useful payload object which is different for various Reply messages.  error contains error description if Command processing resulted in some error on a server. error is optional and if Reply does not have error then it means that Command processed successfully and client can parse result object appropriately.  error looks like this in JSON case:  { &quot;code&quot;: 100, &quot;message&quot;: &quot;internal server error&quot; }   We will talk more about error handling below.  The special type of Reply is asynchronous Reply. Such replies have no id field set (or id can be equal to zero). Async replies can come to client in any moment - not as reaction to issued Command but as message from server to client in arbitrary time. For example this can be a message published into channel.  Centrifuge library defines several command types client can issue. A well-written client must be aware of all those commands and client workflow. Communication with Centrifuge/Centrifugo server starts with issuing connect command.  ","version":"v3","tagName":"h3"},{"title":"Connect​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#connect","content":" First of all client must dial with a server and then send connect Command to it.  Default Websocket endpoint in Centrifugo is:  ws://centrifugo.example.com/connection/websocket   In case of using TLS:  wss://centrifugo.example.com/connection/websocket   After a successful dial to WebSocket endpoint client must send connect command to server to authorize itself.  connect command looks like:  { &quot;id&quot;: 1, &quot;method&quot;: 0, &quot;params&quot;: { &quot;token&quot;: &quot;JWT&quot;, &quot;data&quot;: {} } }   All methods defined in Protobuf schema:  message Command { uint32 id = 1; enum MethodType { CONNECT = 0; SUBSCRIBE = 1; UNSUBSCRIBE = 2; PUBLISH = 3; PRESENCE = 4; PRESENCE_STATS = 5; HISTORY = 6; PING = 7; SEND = 8; RPC = 9; REFRESH = 10; SUB_REFRESH = 11; } MethodType method = 2; bytes params = 3; }   So here we are using a enum value for CONNECT (0).  Params fields:  optional string token - connection token. Can be omitted if token-based auth not used.data - can contain custom connect data, for example it can contain client settings.  In response to connect command server sends a connect reply. It looks this way:  { &quot;id&quot;: 1, &quot;result&quot;:{ &quot;client&quot;: &quot;421bf374-dd01-4f82-9def-8c31697e956f&quot;, &quot;version&quot;: &quot;2.0.0&quot; } }   result has some fields:  string client - unique client connection ID server issued to this connectionstring version - server versionoptional bool expires - whether a server will expire connection at some pointoptional int32 ttl - time in seconds until connection expires  ","version":"v3","tagName":"h3"},{"title":"Subscribe​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#subscribe","content":" As soon as client successfully connected and got unique connection ID it is ready to subscribe on channels. To do this it must send subscribe command to server:  { &quot;id&quot;: 2, &quot;method&quot;: 1, &quot;params&quot;: { &quot;channel&quot;: &quot;ch1&quot; } }   Fields that can be set in params are:  string channel - channel to subscribe  In response to subscribe a client receives reply like:  { &quot;id&quot;: 2, &quot;result&quot;: {} }   result can have the following fields that relate to subscription expiration:  optional bool expires - indicates whether subscription expires or not.optional uint32 ttl - number of seconds until subscription expire.  Also several fields that relate to message recovery:  optional bool recoverable - means that messages can be recovered in this subscription.optional uint64 offset - current publication offset inside channeloptional string epoch - current epoch inside channeloptional array publications - this is an array of missed publications in channel. When received client must call general publication event handler for each message in this array.optional bool recovered - this flag set to true when server thinks that all missed publications successfully recovered and send in subscribe reply (in publications array) and false otherwise.  See more about meaning of recovery related fields in special doc chapter.  After a client received a successful reply on subscribe command it will receive asynchronous reply messages published to this channel. Messages can be of several types:  Publication messageJoin messageLeave messageUnsubscribe message  See more about asynchronous messages below.  ","version":"v3","tagName":"h3"},{"title":"Unsubscribe​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#unsubscribe","content":" When client wants to unsubscribe from a channel and therefore stop receiving asynchronous subscription messages from connection related to channel it must call unsubscribe command:  { &quot;id&quot;: 3, &quot;method&quot;: 2, &quot;params&quot;: { &quot;channel&quot;: &quot;ch1&quot; } }   Actually server response does not mean a lot for a client - it must immediately remove channel subscription from internal implementation data structures and ignore all messages related to channel.  ","version":"v3","tagName":"h3"},{"title":"Refresh​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#refresh","content":" It's possible to turn on client connection expiration mechanism on a server. While enabled server will keep track of connections whose time of life is close to the end (connection lifetime set on connection authentication phase). In this case connection will be closed. Client can prevent closing connection refreshing its connection credentials. To do this it must send refresh command to server. refresh command is similar to connect:  { &quot;id&quot;: 4, &quot;method&quot;: 10, &quot;params&quot;: { &quot;token&quot;: &quot;&lt;refreshed token&gt;&quot; } }   The tip whether a connection must be refreshed by a client comes in reply to connect command shown above - fields expires and ttl.  When client connection expire mechanism is on the value of field expires in connect reply is true. In this case client implementation should look at ttl value which is seconds left until connection will be considered expired. Client must send refresh command after this ttl seconds. Server gives client a configured window to refresh token after ttl passed and then closes connection if client have not updated its token.  When connecting with already expired token an error will be returned (with code 109). In this case client should refresh its token and reconnect with exponential backoff.  ","version":"v3","tagName":"h3"},{"title":"RPC-like calls: publish, history, presence​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#rpc-like-calls-publish-history-presence","content":" The mechanics of these calls is simple - client sends command and expects response from server.  publish command allows to publish a message into a channel from a client.  tip To publish from client publish option in Centrifugo configuration must be set to true  history allows asking a server for channel history if enabled.  presence allows asking a server for channel presence information if enabled.  presence_stats allows asking for short presence info (num clients and unique users in a channel).  ","version":"v3","tagName":"h3"},{"title":"Asynchronous server-to-client messages​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#asynchronous-server-to-client-messages","content":" There are several types of asynchronous messages that can come from a server to a client. All of them relate to the current client subscriptions.  The most important message is Publication:  { &quot;result&quot;:{ &quot;channel&quot;:&quot;ch1&quot;, &quot;data&quot;:{ &quot;data&quot;:{&quot;input&quot;:&quot;1&quot;}, &quot;info&quot;:{ &quot;user&quot;:&quot;2694&quot;, &quot;client&quot;:&quot;5c48510e-cf49-4fa8-a9b2-490b22231e74&quot;, &quot;conn_info&quot;:{&quot;name&quot;:&quot;Alexander&quot;}, &quot;chan_info&quot;:{} } } } }   Publication is a message published into channel. Note that there is no id field in this message - this symptom allows to distinguish it from Reply to Command.  Next message is Join message:  { &quot;result&quot;:{ &quot;type&quot;:1, &quot;channel&quot;:&quot;ch1&quot;, &quot;data&quot;:{ &quot;info&quot;:{ &quot;user&quot;:&quot;2694&quot;, &quot;client&quot;:&quot;5c48510e-cf49-4fa8-a9b2-490b22231e74&quot;, &quot;conn_info&quot;:{&quot;name&quot;:&quot;Alexander&quot;}, &quot;chan_info&quot;:{} } } } }   Join messages sent when someone joined (subscribed on) channel.  tip To enable Join and Leave messages join_leave option must be enabled in Centrifugo for a channel namespace.  Leave messages sent when someone left (unsubscribed from) channel.  { &quot;result&quot;:{ &quot;type&quot;:2, &quot;channel&quot;:&quot;ch1&quot;, &quot;data&quot;:{ &quot;info&quot;:{ &quot;user&quot;:&quot;2694&quot;, &quot;client&quot;:&quot;5c48510e-cf49-4fa8-a9b2-490b22231e74&quot;, &quot;conn_info&quot;:{&quot;name&quot;:&quot;Alexander&quot;}, &quot;chan_info&quot;:{} } } } }   Finally Unsubscribe message that means that server unsubscribed current client from a channel:  { &quot;result&quot;:{ &quot;type&quot;:3, &quot;channel&quot;:&quot;ch1&quot;, &quot;data&quot;:{} } }   It's possible to distinguish between different types of asynchronous messages looking at type field (for Publication this field not set or 0).  ","version":"v3","tagName":"h3"},{"title":"Ping Pong​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#ping-pong","content":" To maintain connection alive and detect broken connections client must periodically send ping commands to server and expect replies to it. Ping command looks like:  { &quot;id&quot;:32, &quot;method&quot;:&quot;ping&quot; }   Server just echoes this command back. When client does not receive ping reply for some time it must consider connection broken and try to reconnect. Recommended ping interval is 25 seconds, recommended period to wait for pong is 1-5 seconds. Though those numbers can vary.  ","version":"v3","tagName":"h3"},{"title":"Handle disconnects​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#handle-disconnects","content":" Client should handle disconnect advices from server. In websocket case disconnect advice is sent in reason field of CLOSE Websocket frame. Reason contains string which is disconnect object encoded into JSON (even in case of Protobuf scenario). That objects looks like:  { &quot;reason&quot;: &quot;shutdown&quot;, &quot;reconnect&quot;: true }   It contains string reason of connection closing and advice to reconnect or not. Client should take this reconnect advice into account.  In case of network problems and random disconnect from server without well known reason client should always try to reconnect with exponential intervals.  ","version":"v3","tagName":"h3"},{"title":"Handle errors​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#handle-errors","content":" This section contains advices to error handling in client implementations.  Errors can happen during various operations and can be handled in special way in context of some commands to tolerate network and server problems.  Errors during connect must result in full client reconnect with exponential backoff strategy. The special case is error with code 110 which signals that connection token already expired. As we said above client should update its connection JWT before connecting to server again.  Errors during subscribe must result in full client reconnect in case of internal error (code 100). And be sent to subscribe error event handler of subscription if received error is persistent. Persistent errors are errors like permission denied, bad request, namespace not found etc. Persistent errors in most situation mean a mistake from developers side.  The special corner case is client-side timeout during subscribe operation. As protocol is asynchronous it's possible in this case that server will eventually subscribe client on channel but client will think that it's not subscribed. It's possible to retry subscription request and tolerate already subscribed (code 105) error as expected. But the simplest solution is to reconnect entirely as this is simpler and gives client a chance to connect to working server instance.  Errors during rpc-like operations can be just returned to caller - i.e. user javascript code. Calls like history and presence are idempotent. You should be accurate with non-idempotent operations like publish - in case of client timeout it's possible to send the same message into channel twice if retry publish after timeout - so users of libraries must care about this case – making sure they have some protection from displaying message twice on client side (maybe some sort of unique key in payload).  ","version":"v3","tagName":"h3"},{"title":"Client implementation advices​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#client-implementation-advices","content":" Here are some advices about client public API. Examples here are in Javascript language. This is just an attempt to help in developing a client - but rules here is not obligatorily the best way to implement client.  Create client instance:  const centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, {});   Set connection token (in case of using Centrifugo):  centrifuge.setToken(&quot;XXX&quot;)   Connect to server:  centrifuge.connect();   2 event handlers can be set to centrifuge object: connect and disconnect  centrifuge.on('connect', function(context) { console.log(context); }); centrifuge.on('disconnect', function(context) { console.log(context); });   Client created in disconnected state with reconnect attribute set to true and reconnecting flag set to false . After connect() called state goes to connecting. It's only possible to connect from disconnected state. Every time connect() called reconnect flag of client must be set to true. After each failed connect attempt state must be set to disconnected, disconnect event must be emitted (only if reconnecting flag is false), and then reconnecting flag must be set to true (if client should continue reconnecting) to not emit disconnect event again after next in a row connect attempt failure. In case of failure next connection attempt must be scheduled automatically with backoff strategy. On successful connect reconnecting flag must be set to false, backoff retry must be reset and connect event must be emitted. When connection lost then the same set of actions as when connect failed must be performed.  Client must allow to subscribe on channels:  var subscription = centrifuge.subscribe(&quot;channel&quot;, eventHandlers);   Subscription object created and control immediately returned to caller - subscribing must be performed asynchronously. This is required because client can automatically reconnect later so event-based model better suites for subscriptions.  Subscription should support several event handlers:  handler for publication received from channeljoin message handlerleave message handlererror handlersubscribe success event handlerunsubscribe event handler  Every time client connects to server it must restore all subscriptions.  Every time client disconnects from server it must call unsubscribe handlers for all active subscriptions and then emit disconnect event.  Client must periodically (once in 25 secs, configurable) send ping messages to server. If pong has not beed received in 5 secs (configurable) then client must disconnect from server and try to reconnect with backoff strategy.  Client can automatically batch several requests into one frame to server and also must be able to handle several replies received from server in one frame.  ","version":"v3","tagName":"h3"},{"title":"Server side subscriptions (SSS)​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#server-side-subscriptions-sss","content":" It's also possible to subscribe connection to channels on server side. In this case we call this server-side subscription. Client should only handle asynchronous messages coming from a server without need to create subscriptions on client side.  SSS should be kept separate from client-side subsSSS requires new event handlers on top-level of Client - Subscribe, Publish, Join, Leave, Unsubscribe, event handlers will be called with event context similar to client-side subs case but with channel fieldConnect Reply contains SSS set by a server on connect, on reconnect client has a chance to recover missed PublicationsServer side subscription can happen at any moment - Sub Push will be sent to client  ","version":"v3","tagName":"h3"},{"title":"Message recovery​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#message-recovery","content":" Client should automatically recover messages after being disconnected due to network problems and set appropriate fields in subscribe event context. Two important fields in onSubscribeSuccess event context are isRecovered and isResubscribe. First field let user know what server thinks about subscription state - were all messages recovered or not. The second field must only be true if resubscribe was caused by temporary network connection lost. If user initiated resubscribe himself (calling unsubscribe method and then subscribe method) then recover workflow should not be used and isResubscribe must be false.  ","version":"v3","tagName":"h3"},{"title":"Disconnect code and reason​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#disconnect-code-and-reason","content":" In case of Websocket it is sent by server in CLOSE Websocket frame. This is a string containing JSON object with fields: reason (string) and reconnect (bool). Client should give users access to these fields in disconnect event and automatically follow reconnect advice.  ","version":"v3","tagName":"h3"},{"title":"Additional notes​","type":1,"pageTitle":"Client protocol","url":"/docs/3/transports/client_protocol#additional-notes","content":" Client protocol does not allow one client connection to subscribe to the same channel twice. In this case client will receive already subscribed error in reply to a subscribe command. ","version":"v3","tagName":"h3"},{"title":"Client real-time SDKs","type":0,"sectionRef":"#","url":"/docs/3/transports/client_sdk","content":"Client real-time SDKs The following SDKs allow connecting to Centrifugo from the application frontend: No need in clients for unidirectional approach Client libraries listed here speak Centrifugo bidirectional protocol (WebSocket). If you aim to use unidirectional approach you don't need client connectors – just use standard APIs. See the difference here. centrifuge-js – for browser, NodeJS and React Nativecentrifuge-go - for Go languagecentrifuge-mobile - for iOS/Android with centrifuge-go as basis and gomobilecentrifuge-dart - for Dart and Fluttercentrifuge-swift – for native iOS developmentcentrifuge-java – for native Android development and general Java See a description of client protocol if you want to write a custom client bidirectional connector.","keywords":"","version":"v3"},{"title":"Private channels","type":0,"sectionRef":"#","url":"/docs/3/server/private_channels","content":"","keywords":"","version":"v3"},{"title":"Claims​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#claims","content":" Private channel subscription token claims are: client, channel, info, b64info, exp and expire_at. What do they mean? Let's describe it in detail.  ","version":"v3","tagName":"h2"},{"title":"client​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#client","content":" Required. Client ID which wants to subscribe on a channel (string).  note Centrifugo server generates a unique client ID for each incoming connection. This client ID regenerated for a client on every reconnect. You must use this client ID for a private channel subscription token. If you are using centrifuge-js library then Client ID and channels that the user wants to subscribe will be automatically added to AJAX POST request to application backend. In other cases refer to specific client documentation (in most cases you will have client ID and channel in private subscription event context).  ","version":"v3","tagName":"h3"},{"title":"channel​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#channel","content":" Required. Channel that client tries to subscribe to (string).  ","version":"v3","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#info","content":" Optional. Additional information for connection inside this channel (valid JSON).  ","version":"v3","tagName":"h3"},{"title":"b64info​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#b64info","content":" Optional. Additional information for connection inside this channel in base64 format (string). Will be decoded by Centrifugo to raw bytes.  ","version":"v3","tagName":"h3"},{"title":"exp​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#exp","content":" Optional. This is a standard JWT claim that allows setting private channel subscription token expiration time (a UNIX timestamp in the future, in seconds, as integer) and configures subscription expiration time.  At the moment if the subscription expires client connection will be closed and the client will try to reconnect. In most cases, you don't need this and should prefer using the expiration of the connection JWT to deactivate the connection (see authentication). But if you need more granular per-channel control this may fit your needs.  Once exp is set in token every subscription token must be periodically refreshed. This refresh workflow happens on the client side. Refer to the specific client documentation to see how to refresh subscriptions.  ","version":"v3","tagName":"h3"},{"title":"expire_at​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#expire_at","content":" Optional. By default, Centrifugo looks on exp claim to both check token expiration and configure subscription expiration time. In most cases this is fine, but there could be situations where you want to decouple subscription token expiration check with subscription expiration time. As soon as the expire_at claim is provided (set) in subscription JWT Centrifugo relies on it for setting subscription expiration time (JWT expiration still checked over exp though).  expire_at is a UNIX timestamp seconds when the subscription should expire.  Set it to the future time for expiring subscription at some pointSet it to 0 to disable subscription expiration (but still check token exp claim). This allows implementing a one-time subscription token.  ","version":"v3","tagName":"h3"},{"title":"aud​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#aud","content":" Handled since Centrifugo v3.2.0  By default, Centrifugo does not check JWT audience (rfc7519 aud claim). But if you set token_audience option as described in client authentication then audience for subscription JWT will also be checked.  ","version":"v3","tagName":"h3"},{"title":"iss​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#iss","content":" Handled since Centrifugo v3.2.0  By default, Centrifugo does not check JWT issuer (rfc7519 iss claim). But if you set token_issuer option as described in client authentication then issuer for subscription JWT will also be checked.  ","version":"v3","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Private channels","url":"/docs/3/server/private_channels#example","content":" So to generate a subscription token you can use something like this in Python (assuming client ID is xxxx-xxx-xxx-xxxx and the private channel is $gossips):  import jwt token = jwt.encode({ &quot;client&quot;: &quot;xxxx-xxx-xxx-xxxx&quot;, &quot;channel&quot;: &quot;$gossips&quot; }, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   Where &quot;secret&quot; is the token_hmac_secret_key from Centrifugo configuration (we use HMAC tokens in this example which relies on a shared secret key, for RSA or ECDSA tokens you need to use a private key known only by your backend). ","version":"v3","tagName":"h2"},{"title":"Engines, scalability","type":0,"sectionRef":"#","url":"/docs/3/server/engines","content":"","keywords":"","version":"v3"},{"title":"Memory engine​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#memory-engine","content":" Used by default. Supports only one node. Nice choice to start with. Supports all features keeping everything in Centrifugo node process memory. You don't need to install Redis when using this engine.  Advantages:  Super fast since it does not involve network at allDoes not require separate broker setup  Disadvantages:  Does not allow scaling nodes (actually you still can scale Centrifugo with Memory engine but you have to publish data into each Centrifugo node and you won't have consistent history and presence state throughout Centrifugo nodes)Does not persist message history in channels between Centrifugo restarts  ","version":"v3","tagName":"h2"},{"title":"Memory engine options​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#memory-engine-options","content":" history_meta_ttl​  Duration, default 0s.  history_meta_ttl sets a time in seconds of history stream metadata expiration. Stream metadata is information about the current offset number in the channel and epoch value. By default, metadata for channels does not expire. Though in some cases – when channels are created for а short time and then not used anymore – created metadata can stay in memory while not useful. For example, you can have a personal user channel but after using your app for a while user left it forever. From a long-term perspective, this can be an unwanted memory leak. Setting a reasonable value to this option (usually much bigger than the history retention period) can help. In this case, unused channel metadata will eventually expire.  ","version":"v3","tagName":"h3"},{"title":"Redis engine​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#redis-engine","content":" Redis is an open-source, in-memory data structure store, used as a database, cache, and message broker.  Centrifugo Redis engine allows scaling Centrifugo nodes to different machines. Nodes will use Redis as a message broker (utilizing Redis PUB/SUB for node communication) and keep presence and history data in Redis.  Minimal Redis version is 5.0.1  ","version":"v3","tagName":"h2"},{"title":"Redis engine options​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#redis-engine-options","content":" Several configuration options related to Redis engine.  redis_address​  String, default &quot;127.0.0.1:6379&quot; - Redis server address.  redis_password​  String, default &quot;&quot; - Redis password.  redis_user​  Available since Centrifugo v3.2.0  String, default &quot;&quot; - Redis user for ACL-based auth.  redis_db​  Integer, default 0 - number of Redis db to use.  redis_tls​  Boolean, default false - enable Redis TLS connection.  redis_tls_skip_verify​  Boolean, default false - disable Redis TLS host verification.  redis_prefix​  String, default &quot;centrifugo&quot; – custom prefix to use for channels and keys in Redis.  redis_use_lists​  Boolean, default false – turns on using Redis Lists instead of Stream data structure for keeping history (not recommended, keeping this for backwards compatibility mostly).  history_meta_ttl​  Similar to a Memory engine Redis engine also looks at history_meta_ttl option (duration, default 0) - which sets a time of history stream metadata expiration in Redis Engine (with seconds resolution). Meta key in Redis is a HASH that contains the current offset number in channel and epoch value. By default, metadata for channels does not expire. Though in some cases – when channels are created for а short time and then not used anymore – created stream metadata can stay in memory while not useful. For example, you can have a personal user channel but after using your app for a while user left it forever. From a long-term perspective, this can be an unwanted memory leak. Setting a reasonable value to this option (usually much bigger than the history retention period) can help. In this case, unused channel metadata will eventually expire.  ","version":"v3","tagName":"h3"},{"title":"Scaling with Redis tutorial​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#scaling-with-redis-tutorial","content":" Let's see how to start several Centrifugo nodes using the Redis Engine. We will start 3 Centrifugo nodes and all those nodes will be connected via Redis.  First, you should have Redis running. As soon as it's running - we can launch 3 Centrifugo instances. Open your terminal and start the first one:  centrifugo --config=config.json --port=8000 --engine=redis --redis_address=127.0.0.1:6379   If your Redis is on the same machine and runs on its default port you can omit redis_address option in the command above.  Then open another terminal and start another Centrifugo instance:  centrifugo --config=config.json --port=8001 --engine=redis --redis_address=127.0.0.1:6379   Note that we use another port number (8001) as port 8000 is already busy by our first Centrifugo instance. If you are starting Centrifugo instances on different machines then you most probably can use the same port number (8000 or whatever you want) for all instances.  And finally, let's start the third instance:  centrifugo --config=config.json --port=8002 --engine=redis --redis_address=127.0.0.1:6379   Now you have 3 Centrifugo instances running on ports 8000, 8001, 8002 and clients can connect to any of them. You can also send API requests to any of those nodes – as all nodes connected over Redis PUB/SUB message will be delivered to all interested clients on all nodes.  To load balance clients between nodes you can use Nginx – you can find its configuration here in the documentation.  tip In the production environment you will most probably run Centrifugo nodes on different hosts, so there will be no need to use different port numbers.  Here is a live example where we locally start two Centrifugo nodes both connected to local Redis:  Sorry, your browser doesn't support embedded video.  ","version":"v3","tagName":"h3"},{"title":"Redis Sentinel for high availability​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#redis-sentinel-for-high-availability","content":" Centrifugo supports the official way to add high availability to Redis - Redis Sentinel.  For this you only need to utilize 2 Redis Engine options: redis_sentinel_address and redis_sentinel_master_name:  redis_sentinel_address (string, default &quot;&quot;) - comma separated list of Sentinel addresses for HA. At least one known server required.redis_sentinel_master_name (string, default &quot;&quot;) - name of Redis master Sentinel monitors  Also:  redis_sentinel_password – optional string password for your Sentinel, works with Redis Sentinel &gt;= 5.0.1redis_sentinel_user (available since v3.2.0) - optional string user (used only in Redis ACL-based auth).  So you can start Centrifugo which will use Sentinels to discover Redis master instances like this:  centrifugo --config=config.json   Where config.json:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_sentinel_address&quot;: &quot;127.0.0.1:26379&quot;, &quot;redis_sentinel_master_name&quot;: &quot;mymaster&quot; }   Sentinel configuration files can look like this:  port 26379 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 10000 sentinel failover-timeout mymaster 60000   You can find how to properly set up Sentinels in official documentation.  Note that when your Redis master instance is down there will be a small downtime interval until Sentinels discover a problem and come to a quorum decision about a new master. The length of this period depends on Sentinel configuration.  ","version":"v3","tagName":"h3"},{"title":"Haproxy instead of Sentinel configuration​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#haproxy-instead-of-sentinel-configuration","content":" Alternatively, you can use Haproxy between Centrifugo and Redis to let it properly balance traffic to Redis master. In this case, you still need to configure Sentinels but you can omit Sentinel specifics from Centrifugo configuration and just use Redis address as in a simple non-HA case.  For example, you can use something like this in Haproxy config:  listen redis server redis-01 127.0.0.1:6380 check port 6380 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 server redis-02 127.0.0.1:6381 check port 6381 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 backup bind *:16379 mode tcp option tcpka option tcplog option tcp-check tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK balance roundrobin   And then just point Centrifugo to this Haproxy:  centrifugo --config=config.json --engine=redis --redis_address=&quot;localhost:16379&quot;   ","version":"v3","tagName":"h3"},{"title":"Redis sharding​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#redis-sharding","content":" Centrifugo has built-in Redis sharding support.  This resolves the situation when Redis becoming a bottleneck on a large Centrifugo setup. Redis is a single-threaded server, it's very fast but its power is not infinite so when your Redis approaches 100% CPU usage then the sharding feature can help your application to scale.  At moment Centrifugo supports a simple comma-based approach to configuring Redis shards. Let's just look at examples.  To start Centrifugo with 2 Redis shards on localhost running on port 6379 and port 6380 use config like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: [ &quot;127.0.0.1:6379&quot;, &quot;127.0.0.1:6380&quot;, ] }   To start Centrifugo with Redis instances on different hosts:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: [ &quot;192.168.1.34:6379&quot;, &quot;192.168.1.35:6379&quot;, ] }   If you also need to customize AUTH password, Redis DB number then you can use an extended address notation.  note Due to how Redis PUB/SUB works it's not possible (and it's pretty useless anyway) to run shards in one Redis instance using different Redis DB numbers.  When sharding enabled Centrifugo will spread channels and history/presence keys over configured Redis instances using a consistent hashing algorithm. At moment we use Jump consistent hash algorithm (see paper and implementation).  ","version":"v3","tagName":"h3"},{"title":"Redis cluster​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#redis-cluster","content":" Running Centrifugo with Redis cluster is simple and can be achieved using redis_cluster_address option. This is an array of strings. Each element of the array is a comma-separated Redis cluster seed node. For example:  { ... &quot;redis_cluster_address&quot;: [ &quot;localhost:30001,localhost:30002,localhost:30003&quot; ] }   You don't need to list all Redis cluster nodes in config – only several working nodes are enough to start.  To set the same over environment variable:  CENTRIFUGO_REDIS_CLUSTER_ADDRESS=&quot;localhost:30001&quot; CENTRIFUGO_ENGINE=redis ./centrifugo   If you need to shard data between several Redis clusters then simply add one more string with seed nodes of another cluster to this array:  { ... &quot;redis_cluster_address&quot;: [ &quot;localhost:30001,localhost:30002,localhost:30003&quot;, &quot;localhost:30101,localhost:30102,localhost:30103&quot; ] }   Sharding between different Redis clusters can make sense due to the fact how PUB/SUB works in the Redis cluster. It does not scale linearly when adding nodes as all PUB/SUB messages got copied to every cluster node. See this discussion for more information on topic. To spread data between different Redis clusters Centrifugo uses the same consistent hashing algorithm described above (i.e. Jump).  To reproduce the same over environment variable use space to separate different clusters:  CENTRIFUGO_REDIS_CLUSTER_ADDRESS=&quot;localhost:30001,localhost:30002 localhost:30101,localhost:30102&quot; CENTRIFUGO_ENGINE=redis ./centrifugo   ","version":"v3","tagName":"h3"},{"title":"KeyDB Engine​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#keydb-engine","content":" EXPERIMENTAL  Centrifugo Redis engine seamlessly works with KeyDB. KeyDB server is compatible with Redis and provides several additional features beyond.  caution We can't give any promises about compatibility with KeyDB in the future Centrifugo releases - while KeyDB is fully compatible with Redis things should work just fine. That's why we consider this as EXPERIMENTAL feature.  Use KeyDB instead of Redis only if you are sure you need it. Nothing stops you from running several Redis instances per each core you have, configure sharding, and obtain even better performance than KeyDB can provide (due to lack of synchronization between threads in Redis).  To run Centrifugo with KeyDB all you need to do is use redis engine but run the KeyDB server instead of Redis.  ","version":"v3","tagName":"h2"},{"title":"Tarantool engine​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#tarantool-engine","content":" EXPERIMENTAL  Tarantool is a fast and flexible in-memory storage with different persistence/replication schemes and LuaJIT for writing custom logic on the Tarantool side. It allows implementing Centrifugo engine with unique characteristics.  caution EXPERIMENTAL status of Tarantool integration means that we are still going to improve it and there could be breaking changes as integration evolves.  There are many ways to operate Tarantool in production and it's hard to distribute Centrifugo Tarantool engine in a way that suits everyone. Centrifugo tries to fit generic case by providing centrifugal/tarantool-centrifuge module and by providing ready-to-use centrifugal/rotor project based on centrifugal/tarantool-centrifuge and Tarantool Cartridge.  info To be honest we bet on the community help to push this integration further. Tarantool provides an incredible performance boost for presence and history operations (up to 5x more RPS compared to the Redis Engine) and a pretty fast PUB/SUB (comparable to what Redis Engine provides). Let's see what we can build together.  There are several supported Tarantool topologies to which Centrifugo can connect:  One standalone Tarantool instanceMany standalone Tarantool instances and consistently shard data between themTarantool running in CartridgeTarantool with replica and automatic failover in CartridgeMany Tarantool instances (or leader-follower setup) in Cartridge with consistent client-side sharding between themTarantool with synchronous replication (Raft-based, Tarantool &gt;= 2.7)  After running Tarantool you can point Centrifugo to it (and of course scale Centrifugo nodes):  config.json { ... &quot;engine&quot;: &quot;tarantool&quot;, &quot;tarantool_address&quot;: &quot;127.0.0.1:3301&quot; }   See centrifugal/rotor repo for ready-to-use engine based on Tarantool Cartridge framework.  See centrifugal/tarantool-centrifuge repo for examples on how to run engine with Standalone single Tarantool instance or with Raft-based synchronous replication.  ","version":"v3","tagName":"h2"},{"title":"Tarantool engine options​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#tarantool-engine-options","content":" tarantool_address​  String or array of strings. Default tcp://127.0.0.1:3301.  Connection address to Tarantool.  tarantool_mode​  String, default standalone  A mode how to connect to Tarantool. Default is standalone which connects to a single Tarantool instance address. Possible values are: leader-follower (connects to a setup with Tarantool master and async replicas) and leader-follower-raft (connects to a Tarantool with synchronous Raft-based replication).  All modes support client-side consistent sharding (similar to what Redis engine provides).  tarantool_user​  String, default &quot;&quot;. Allows setting a user.  tarantool_password​  String, default &quot;&quot;. Allows setting a password.  history_meta_ttl​  Duration, default 0s.  Same option as for Memory engine and Redis engine also applies to Tarantool case.  ","version":"v3","tagName":"h3"},{"title":"Nats broker​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#nats-broker","content":" It's possible to scale with Nats PUB/SUB server. Keep in mind, that Nats is called a broker here, not an Engine – Nats integration only implements PUB/SUB part of Engine, so carefully read limitations below.  Limitations:  Nats integration works only for unreliable at most once PUB/SUB. This means that history, presence, and message recovery Centrifugo features won't be available.Nats wildcard channel subscriptions with symbols * and &gt; not supported.  First start Nats server:  $ nats-server [3569] 2020/07/08 20:28:44.324269 [INF] Starting nats-server version 2.1.7 [3569] 2020/07/08 20:28:44.324400 [INF] Git commit [not set] [3569] 2020/07/08 20:28:44.325600 [INF] Listening for client connections on 0.0.0.0:4222 [3569] 2020/07/08 20:28:44.325612 [INF] Server id is NDAM7GEHUXAKS5SGMA3QE6ZSO4IQUJP6EL3G2E2LJYREVMAMIOBE7JT4 [3569] 2020/07/08 20:28:44.325617 [INF] Server is ready   Then start Centrifugo with broker option:  centrifugo --broker=nats --config=config.json   And one more Centrifugo on another port (of course in real life you will start another Centrifugo on another machine):  centrifugo --broker=nats --config=config.json --port=8001   Now you can scale connections over Centrifugo instances, instances will be connected over Nats server.  ","version":"v3","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Engines, scalability","url":"/docs/3/server/engines#options","content":" nats_url​  String, default nats://127.0.0.1:4222.  Connection url in format nats://derek:pass@localhost:4222.  nats_prefix​  String, default centrifugo.  Prefix for channels used by Centrifugo inside Nats.  nats_dial_timeout​  Duration, default 1s.  Timeout for dialing with Nats.  nats_write_timeout​  Duration, default 1s.  Write (and flush) timeout for a connection to Nats. ","version":"v3","tagName":"h3"},{"title":"Proxy to backend","type":0,"sectionRef":"#","url":"/docs/3/server/proxy","content":"","keywords":"","version":"v3"},{"title":"HTTP proxy​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#http-proxy","content":" HTTP proxy in Centrifugo converts client connection events into HTTP call to the application backend.  ","version":"v3","tagName":"h2"},{"title":"HTTP request structure​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#http-request-structure","content":" All proxy calls are HTTP POST requests that will be sent from Centrifugo to configured endpoints with a configured timeout. These requests will have some headers copied from the original client request (see details below) and include JSON body which varies depending on call type (for example data sent by a client in RPC call etc, see more details about JSON bodies below).  ","version":"v3","tagName":"h3"},{"title":"Proxy HTTP headers​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#proxy-http-headers","content":" The good thing about Centrifugo HTTP proxy is that it transparently proxies original HTTP request headers in a request to app backend. In most cases this allows achieving transparent authentication on the application backend side. But it's required to provide an explicit list of HTTP headers you want to be proxied, for example:  config.json { ... &quot;proxy_http_headers&quot;: [ &quot;Origin&quot;, &quot;User-Agent&quot;, &quot;Cookie&quot;, &quot;Authorization&quot;, &quot;X-Real-Ip&quot;, &quot;X-Forwarded-For&quot;, &quot;X-Request-Id&quot; ] }   Alternatively, you can set a list of headers via an environment variable (space separated):  export CENTRIFUGO_PROXY_HTTP_HEADERS=&quot;Cookie User-Agent X-B3-TraceId X-B3-SpanId&quot; ./centrifugo   note Centrifugo forces the Content-Type header to be application/json in all HTTP proxy requests since it sends the body in JSON format to the application backend.  ","version":"v3","tagName":"h3"},{"title":"Proxy GRPC metadata​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#proxy-grpc-metadata","content":" When GRPC unidirectional stream is used as a client transport then you may want to proxy GRPC metadata from the client request. In this case you may configure proxy_grpc_metadata option. This is an array of string metadata keys which will be proxied. These metadata keys transformed to HTTP headers of proxy request. By default no metadata keys are proxied.  See below the table of rules how metadata and headers proxied in transport/proxy different scenarios.  ","version":"v3","tagName":"h3"},{"title":"Connect proxy​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#connect-proxy","content":" With the following options in the configuration file:  { ... &quot;proxy_connect_endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;proxy_connect_timeout&quot;: &quot;1s&quot; }   – connection requests without JWT set will be proxied to proxy_connect_endpoint URL endpoint. On your backend side, you can authenticate the incoming connection and return client credentials to Centrifugo in response to the proxied request.  danger Make sure you properly configured allowed_origins Centrifugo option or check request origin on your backend side upon receiving connect request from Centrifugo. Otherwise, your site can be vulnerable to CSRF attacks if you are using WebSocket transport for client connections.  Yes, this means you don't need to generate JWT and pass it to a client-side and can rely on a cookie while authenticating the user. Centrifugo should work on the same domain in this case so your site cookie could be passed to Centrifugo by browsers.  tip If you want to pass some custom authentication token from a client side (not in Centrifugo JWT format) but force request to be proxied then you may put it in a cookie or use connection request custom data field (available in all our transports). This data can contain arbitrary payload you want to pass from a client to a server.  This also means that every new connection from a user will result in an HTTP POST request to your application backend. While with JWT token you usually generate it once on application page reload, if client reconnects due to Centrifugo restart or internet connection loss it uses the same JWT it had before thus usually no additional requests are generated during reconnect process (until JWT expired).    Payload example that will be sent to app backend when client without token wants to establish a connection with Centrifugo and proxy_connect_endpoint is set to non-empty URL string:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot; }   Expected response example:  {&quot;result&quot;: {&quot;user&quot;: &quot;56&quot;}}   This response allows connecting and tells Centrifugo the ID of a user. See below the full list of supported fields in the result.  Several app examples which use connect proxy can be found in our blog:  With NodeJSWith DjangoWith Laravel  Connect request fields​  This is what sent from Centrifugo to application backend in case of connect proxy request.  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) name\tstring\tyes\toptional name of the client (this field will only be set if provided by a client on connect) version\tstring\tyes\toptional version of the client (this field will only be set if provided by a client on connect) data\tJSON\tyes\toptional data from client (this field will only be set if provided by a client on connect) b64data\tstring\tyes\toptional data from the client in base64 format (if the binary proxy mode is used) channels\tArray of strings\tyes\tlist of server-side channels client want to subscribe to, the application server must check permissions and add allowed channels to result  Connect result fields​  This is what application returns to Centrifugo inside result field in case of connect proxy request.  Field\tType\tOptional\tDescriptionuser\tstring\tno\tuser ID (calculated on app backend based on request cookie header for example). Return it as an empty string for accepting unauthenticated requests expire_at\tinteger\tyes\ta timestamp when connection must be considered expired. If not set or set to 0 connection won't expire at all info\tJSON\tyes\ta connection info JSON b64info\tstring\tyes\tbinary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages data\tJSON\tyes\ta custom data to send to the client in connect command response. b64data\tstring\tyes\ta custom data to send to the client in the connect command response for binary connections, will be decoded to raw bytes on Centrifugo side before sending to client channels\tarray of strings\tyes\tallows providing a list of server-side channels to subscribe connection to. See more details about server-side subscriptions subs\tmap of SubscribeOptions\tyes\tmap of channels with options to subscribe connection to. See more details about server-side subscriptions meta\tJSON object (ex. {&quot;key&quot;: &quot;value&quot;})\tyes\ta custom data to attach to connection (this won't be exposed to client-side)  Options​  proxy_connect_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend.  Example​  Here is the simplest example of the connect handler in Tornado Python framework (note that in a real system you need to authenticate the user on your backend side, here we just return &quot;56&quot; as user ID):  class CentrifugoConnectHandler(tornado.web.RequestHandler): def check_xsrf_cookie(self): pass def post(self): self.set_header('Content-Type', 'application/json; charset=&quot;utf-8&quot;') data = json.dumps({ 'result': { 'user': '56' } }) self.write(data) def main(): options.parse_command_line() app = tornado.web.Application([ (r'/centrifugo/connect', CentrifugoConnectHandler), ]) app.listen(3000) tornado.ioloop.IOLoop.instance().start() if __name__ == '__main__': main()   This example should help you to implement a similar HTTP handler in any language/framework you are using on the backend side.  We also have a tutorial in the blog about Centrifugo integration with NodeJS which uses connect proxy and native session middleware of Express.js to authenticate connections. Even if you are not using NodeJS on a backend a tutorial can help you understand the idea.  ","version":"v3","tagName":"h3"},{"title":"Refresh proxy​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#refresh-proxy","content":" With the following options in the configuration file:  { ... &quot;proxy_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/refresh&quot;, &quot;proxy_refresh_timeout&quot;: &quot;1s&quot; }   – Centrifugo will call proxy_refresh_endpoint when it's time to refresh the connection. Centrifugo itself will ask your backend about connection validity instead of refresh workflow on the client-side.  The payload sent to app backend in refresh request (when the connection is going to expire):  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot; }   Expected response example:  {&quot;result&quot;: {&quot;expire_at&quot;: 1565436268}}   Refresh request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc.) protocol\tstring\tno\tprotocol type used by client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Refresh result fields​  Field\tType\tOptional\tDescriptionexpired\tbool\tyes\ta flag to mark the connection as expired - the client will be disconnected expire_at\tinteger\tyes\ta timestamp in the future when connection must be considered expired info\tJSON\tyes\ta connection info JSON b64info\tstring\tyes\tbinary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages  Options​  proxy_refresh_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend.  ","version":"v3","tagName":"h3"},{"title":"RPC proxy​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#rpc-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_rpc_endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;proxy_rpc_timeout&quot;: &quot;1s&quot; }   RPC calls over client connection will be proxied to proxy_rpc_endpoint. This allows a developer to utilize WebSocket (or SockJS) connection in a bidirectional way.  Payload example sent to app backend in RPC request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;method&quot;: &quot;getCurrentPrice&quot;, &quot;data&quot;:{&quot;params&quot;: {&quot;object_id&quot;: 12}} }   Expected response example:  {&quot;result&quot;: {&quot;data&quot;: {&quot;answer&quot;: &quot;2019&quot;}}}   RPC request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket or sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process method\tstring\tyes\tan RPC method string, if the client does not use named RPC call then method will be omitted data\tJSON\tyes\tRPC custom data sent by client b64data\tstring\tyes\twill be set instead of data field for binary proxy mode meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  RPC result fields​  Field\tType\tOptional\tDescriptiondata\tJSON\tyes\tRPC response - any valid JSON is supported b64data\tstring\tyes\tcan be set instead of data for binary response encoded in base64 format  Options​  proxy_rpc_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend.  See below on how to return a custom error.  ","version":"v3","tagName":"h3"},{"title":"Subscribe proxy​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#subscribe-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot; }   – subscribe requests sent over client connection will be proxied to proxy_subscribe_endpoint. This allows you to check the access of the client to a channel.  tip Subscribe proxy does not proxy private and user-limited channels at the moment. That's because those are already providing a level of security (user-limited channels check current user ID, private channels require subscription token). In some cases you may use subscribe proxy as a replacement for private channels actually: if you prefer to check permissions using the proxy to backend mechanism – just stop using $ prefixes in channels, properly configure subscribe proxy and validate subscriptions upon proxy from Centrifugo to your backend (issued each time user tries to subscribe on a channel for which subscribe proxy enabled).  Unlike proxy types described above subscribe proxy must be enabled per channel namespace. This means that every namespace (including global/default one) has a boolean option proxy_subscribe that enables subscribe proxy for channels in a namespace.  So to enable subscribe proxy for channels without namespace define proxy_subscribe on a top configuration level:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;proxy_subscribe&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;proxy_subscribe&quot;: true }] }   Payload example sent to app backend in subscribe request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;chat:index&quot; }   Expected response example if subscription is allowed:  {&quot;result&quot;: {}}   Subscribe request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket or sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\ta string channel client wants to subscribe to meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true) data\tJSON\tyes\tcustom data from client sent with subscription request (this field will only be set if provided by a client on subscribe). Available since Centrifugo v3.1.1 b64data\tstring\tyes\toptional subscription data from the client in base64 format (if the binary proxy mode is used). Available since Centrifugo v3.1.1  Subscribe result fields​  Field\tType\tOptional\tDescriptioninfo\tJSON\tyes\ta channel info JSON b64info\tstring\tyes\ta binary connection channel info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using data\tJSON\tyes\ta custom data to send to the client in subscribe command reply. b64data\tstring\tyes\ta custom data to send to the client in subscribe command reply, will be decoded to raw bytes on Centrifugo side before sending to client override\tOverride object\tyes\tAllows dynamically override some channel options defined in Centrifugo configuration on a per-connection basis (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave position\tBoolValue\tyes\tOverride position recover\tBoolValue\tyes\tOverride recover  BoolValue is an object like this:  { &quot;value&quot;: true/false }   See below on how to return an error in case you don't want to allow subscribing.  Options​  proxy_subscribe_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend.  ","version":"v3","tagName":"h3"},{"title":"Publish proxy​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#publish-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot; }   – publish calls sent by a client will be proxied to proxy_publish_endpoint.  This request happens BEFORE a message is published to a channel, so your backend can validate whether a client can publish data to a channel. An important thing here is that publication to the channel can fail after your backend successfully validated publish request (for example publish to Redis by Centrifugo returned an error). In this case, your backend won't know about the error that happened but this error will propagate to the client-side.    Like the subscribe proxy, publish proxy must be enabled per channel namespace. This means that every namespace (including the global/default one) has a boolean option proxy_publish that enables publish proxy for channels in the namespace. All other namespace options will be taken into account before making a proxy request, so you also need to turn on the publish option too.  So to enable publish proxy for channels without namespace define proxy_publish and publish on a top configuration level:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true }] }   Keep in mind that this will only work if the publish channel option is on for a channel namespace (or for a global top-level namespace).  Payload example sent to app backend in a publish request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;chat:index&quot;, &quot;data&quot;:{&quot;input&quot;:&quot;hello&quot;} }   Expected response example if publish is allowed:  {&quot;result&quot;: {}}   Publish request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\ta string channel client wants to publish to data\tJSON\tyes\tdata sent by client b64data\tstring\tyes\twill be set instead of data field for binary proxy mode meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Publish result fields​  Field\tType\tOptional\tDescriptiondata\tJSON\tyes\tan optional JSON data to send into a channel instead of original data sent by a client b64data\tstring\tyes\ta binary data encoded in base64 format, the meaning is the same as for data above, will be decoded to raw bytes on Centrifugo side before publishing skip_history\tbool\tyes\twhen set to true Centrifugo won't save publication to the channel history  See below on how to return an error in case you don't want to allow publishing.  Options​  proxy_publish_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend.  ","version":"v3","tagName":"h3"},{"title":"Return custom error​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#return-custom-error","content":" Application backend can return JSON object that contains an error to return it to the client:  { &quot;error&quot;: { &quot;code&quot;: 1000, &quot;message&quot;: &quot;custom error&quot; } }   Application should use error codes &gt;= 1000, error codes in the range 0-999 are reserved by Centrifugo internal protocol. Error code field is uint32 internally.  note Returning custom error does not apply to response on refresh request as there is no sense in returning an error (will not reach client anyway).  ","version":"v3","tagName":"h3"},{"title":"Return custom disconnect​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#return-custom-disconnect","content":" Application backend can return JSON object that contains a custom disconnect object to disconnect client in a custom way:  { &quot;disconnect&quot;: { &quot;code&quot;: 4000, &quot;reconnect&quot;: false, &quot;reason&quot;: &quot;custom disconnect&quot; } }   Application must use numbers in the range 4000-4999 for custom disconnect codes. Code is uint32 internally. Numbers below 4000 are reserved by Centrifugo internal protocol. Keep in mind that due to WebSocket protocol limitations and Centrifugo internal protocol needs you need to keep disconnect reason string no longer than 32 symbols.  note Returning custom disconnect does not apply to response on refresh request as there is no way to control disconnect at moment - the client will always be disconnected with expired disconnect reason.  ","version":"v3","tagName":"h3"},{"title":"GRPC proxy​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#grpc-proxy","content":" Centrifugo can also proxy connection events to your backend over GRPC instead of HTTP. In this case, Centrifugo acts as a GRPC client and your backend acts as a GRPC server.  GRPC service definitions can be found in the Centrifugo repository: proxy.proto.  tip GRPC proxy inherits all the fields for HTTP proxy – so you can refer to field descriptions for HTTP above. Both proxy types in Centrifugo share the same Protobuf schema definitions.  Every proxy call in this case is a unary GRPC call. Centrifugo puts client headers into GRPC metadata (since GRPC doesn't have headers concept).  All you need to do to enable proxying over GRPC instead of HTTP is to use grpc schema in endpoint, for example for the connect proxy:  config.json { ... &quot;proxy_connect_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_connect_timeout&quot;: &quot;1s&quot; }   Refresh proxy:  config.json { ... &quot;proxy_refresh_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_refresh_timeout&quot;: &quot;1s&quot; }   Or for RPC proxy:  config.json { ... &quot;proxy_rpc_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_rpc_timeout&quot;: &quot;1s&quot; }   For publish proxy in namespace chat:  config.json { ... &quot;proxy_publish_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot; &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;chat&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true } ] }   Use subscribe proxy for all channels without namespaces:  config.json { ... &quot;proxy_subscribe_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;proxy_subscribe&quot;: true }   So the same as for HTTP, just the different endpoint scheme.  ","version":"v3","tagName":"h2"},{"title":"GRPC proxy options​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#grpc-proxy-options","content":" Some additional options exist to control GRPC proxy behavior.  proxy_grpc_cert_file​  String, default: &quot;&quot;.  Path to cert file for secure TLS connection. If not set then an insecure connection with the backend endpoint is used.  proxy_grpc_credentials_key​  String, default &quot;&quot; (i.e. not used).  Add custom key to per-RPC credentials.  proxy_grpc_credentials_value​  String, default &quot;&quot; (i.e. not used).  A custom value for proxy_grpc_credentials_key.  ","version":"v3","tagName":"h3"},{"title":"GRPC proxy example​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#grpc-proxy-example","content":" We have an example of backend server (written in Go language) which can react to events from Centrifugo over GRPC. For other programming languages the approach is similar, i.e.:  Copy proxy Protobuf definitionsGenerate GRPC codeRun backend service with you custom business logicPoint Centrifugo to it.  ","version":"v3","tagName":"h3"},{"title":"Header proxy rules​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#header-proxy-rules","content":" Centrifugo not only supports HTTP-based client transports but also GRPC-based (for example GRPC unidirectional stream). Here is a table with rules used to proxy headers/metadata in various scenarios:  Client protocol type\tProxy type\tClient headers\tClient metadataHTTP\tHTTP\tIn proxy request headers\tN/A GRPC\tGRPC\tN/A\tIn proxy request metadata HTTP\tGRPC\tIn proxy request metadata\tN/A GRPC\tHTTP\tN/A\tIn proxy request headers  ","version":"v3","tagName":"h2"},{"title":"Binary mode​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#binary-mode","content":" As you may noticed there are several fields in request/result description of various proxy calls which use base64 encoding.  Centrifugo can work with binary Protobuf protocol (in case of bidirectional WebSocket transport). All our bidirectional clients support this.  Most Centrifugo users use JSON for custom payloads: i.e. for data sent to a channel, for connection info attached while authenticating (which becomes part of presence response, join/leave messages and added to Publication client info when message published from a client side).  But since HTTP proxy works with JSON format (i.e. sends requests with JSON body) – it can not properly pass binary data to application backend. Arbitrary binary data can't be encoded into JSON.  In this case it's possible to turn Centrifugo proxy into binary mode by using:  config.json { ... &quot;proxy_binary_encoding&quot;: true }   Once enabled this option tells Centrifugo to use base64 format in requests and utilize fields like b64data, b64info with payloads encoded to base64 instead of their JSON field analogues.  While this feature is useful for HTTP proxy it's not really required if you are using GRPC proxy – since GRPC allows passing binary data just fine.  Regarding b64 fields in proxy results – just use base64 fields when required – Centrifugo is smart enough to detect that you are using base64 field and will pick payload from it, decode from base64 automatically and will pass further to connections in binary format.  ","version":"v3","tagName":"h2"},{"title":"Granular proxy mode​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#granular-proxy-mode","content":" New in Centrifugo v3.1.0.  By default, with proxy configuration shown above, you can only define a global proxy settings and one endpoint for each type of proxy (i.e. one for connect proxy, one for subscribe proxy, and so on). Also, you can configure only one set of headers to proxy which will be used by each proxy type. This may be sufficient for many use cases, but what if you need a more granular control? For example, use different subscribe proxy endpoints for different channel namespaces (i.e. when using microservice architecture).  Centrifugo v3.1.0 introduced a new mode for proxy configuration called granular proxy mode. In this mode it's possible to configure subscribe and publish proxy behaviour on per-namespace level, use different set of headers passed to the proxy endpoint in each proxy type. Also, Centrifugo v3.1.0 introduced a concept of rpc namespaces (in addition to channel namespaces) – together with granular proxy mode this allows configuring rpc proxies on per rpc namespace basis.  ","version":"v3","tagName":"h2"},{"title":"Enable granular proxy mode​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#enable-granular-proxy-mode","content":" Since the change is rather radical it requires a separate boolean option granular_proxy_mode to be enabled. As soon as this option set Centrifugo does not use proxy configuration rules described above and follows the rules described below.  config.json { ... &quot;granular_proxy_mode&quot;: true }   ","version":"v3","tagName":"h3"},{"title":"Defining a list of proxies​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#defining-a-list-of-proxies","content":" When using granular proxy mode on configuration top level you can define &quot;proxies&quot; array with a list of different proxy objects. Each proxy object in an array should have at least two required fields: name and endpoint.  Here is an example:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [ { &quot;name&quot;: &quot;connect&quot;, &quot;endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;timeout&quot;: &quot;500ms&quot;, &quot;http_headers&quot;: [&quot;Cookie&quot;] }, { &quot;name&quot;: &quot;refresh&quot;, &quot;endpoint&quot;: &quot;http://localhost:3000/centrifugo/refresh&quot;, &quot;timeout&quot;: &quot;500ms&quot; }, { &quot;name&quot;: &quot;subscribe1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/subscribe&quot; }, { &quot;name&quot;: &quot;publish1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/publish&quot; }, { &quot;name&quot;: &quot;rpc1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/rpc&quot; }, { &quot;name&quot;: &quot;subscribe2&quot;, &quot;endpoint&quot;: &quot;http://localhost:3002/centrifugo/subscribe&quot; }, { &quot;name&quot;: &quot;publish2&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3002&quot; } { &quot;name&quot;: &quot;rpc2&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3002&quot; } ] }   Let's look at all fields for a proxy object which is possible to set for each proxy inside &quot;proxies&quot; array.  Field name\tField type\tRequired\tDescriptionname\tstring\tyes\tUnique name of proxy used for referencing in configuration, must match regexp ^[-a-zA-Z0-9_.]{2,}$ endpoint\tstring\tyes\tHTTP or GRPC endpoint in the same format as in default proxy mode. For example, http://localhost:3000/path for HTTP or grpc://localhost:3000 for GRPC. timeout\tduration (string)\tno\tProxy request timeout, default &quot;1s&quot; http_headers\tarray of strings\tno\tList of headers to proxy, by default no headers grpc_metadata\tarray of strings\tno\tList of GRPC metadata keys to proxy, by default no metadata keys binary_encoding\tbool\tno\tUse base64 for payloads include_connection_meta\tbool\tno\tInclude meta information (attached on connect) grpc_cert_file\tstring\tno\tPath to cert file for secure TLS connection. If not set then an insecure connection with the backend endpoint is used. grpc_credentials_key\tstring\tno\tAdd custom key to per-RPC credentials. grpc_credentials_value\tstring\tno\tA custom value for grpc_credentials_key.  ","version":"v3","tagName":"h3"},{"title":"Granular connect and refresh​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#granular-connect-and-refresh","content":" As soon as you defined a list of proxies you can reference them by a name to use a specific proxy configuration for a specific event.  To enable connect proxy:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;connect_proxy_name&quot;: &quot;connect&quot; }   We have an example of Centrifugo integration with NodeJS which uses granular proxy mode. Even if you are not using NodeJS on a backend an example can help you understand the idea.  Let's also add refresh proxy:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;connect_proxy_name&quot;: &quot;connect&quot;, &quot;refresh_proxy_name&quot;: &quot;refresh&quot; }   ","version":"v3","tagName":"h3"},{"title":"Granular subscribe and publish​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#granular-subscribe-and-publish","content":" Subscribe and publish proxy work per-namespace. This means that subscribe_proxy_name and publish_proxy_name are just a channel namespace options. So it's possible to define these options on configuration top-level (for channels in default top-level namespace) or inside namespace object.  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;ns1&quot;, &quot;subscribe_proxy_name&quot;: &quot;subscribe1&quot;, &quot;publish&quot;: true, &quot;publish_proxy_name&quot;: &quot;publish1&quot; }, { &quot;name&quot;: &quot;ns2&quot;, &quot;subscribe_proxy_name&quot;: &quot;subscribe2&quot;, &quot;publish&quot;: true, &quot;publish_proxy_name&quot;: &quot;publish2&quot; } ] }   If namespace does not have &quot;subscribe_proxy_name&quot; or &quot;subscribe_proxy_name&quot; is empty then no subscribe proxy will be used for a namespace.  If namespace does not have &quot;publish_proxy_name&quot; or &quot;publish_proxy_name&quot; is empty then no publish proxy will be used for a namespace.  tip You can define subscribe_proxy_name and publish_proxy_name on configuration top level – and in this case publish and subscribe requests for channels without explicit namespace will be proxied using this proxy. The same mechanics as for other channel options in Centrifugo.  ","version":"v3","tagName":"h3"},{"title":"Granular RPC​","type":1,"pageTitle":"Proxy to backend","url":"/docs/3/server/proxy#granular-rpc","content":" Analogous to channel namespaces it's possible to configure rpc namespaces:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;namespaces&quot;: [...], &quot;rpc_namespaces&quot;: [ { &quot;name&quot;: &quot;rpc_ns1&quot;, &quot;rpc_proxy_name&quot;: &quot;rpc1&quot;, }, { &quot;name&quot;: &quot;rpc_ns2&quot;, &quot;rpc_proxy_name&quot;: &quot;rpc2&quot; } ] }   The mechanics is the same as for channel namespaces. RPC requests with RPC method like rpc_ns1:test will use rpc proxy rpc1, RPC requests with RPC method like rpc_ns2:test will use rpc proxy rpc2. So Centrifugo uses : as RPC namespace boundary in RPC method (just like it does for channel namespaces).  Just like channel namespaces RPC namespaces should have a name which match ^[-a-zA-Z0-9_.]{2,}$ regexp pattern – this is validated on Centrifugo start.  tip The same as for channel namespaces and channel options you can define rpc_proxy_name on configuration top level – and in this case RPC calls without explicit namespace in RPC method will be proxied using this proxy. ","version":"v3","tagName":"h3"},{"title":"Server API","type":0,"sectionRef":"#","url":"/docs/3/server/server_api","content":"","keywords":"","version":"v3"},{"title":"HTTP API​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#http-api","content":" Server HTTP API works on /api endpoint (by default). It has a simple request format: this is an HTTP POST request with application/json Content-Type and with JSON command body.  Here we will look at available methods and parameters  tip In some cases, you can just use one of our available HTTP API libraries or use Centrifugo GRPC API to avoid manually constructing requests.  ","version":"v3","tagName":"h2"},{"title":"HTTP API authorization​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#http-api-authorization","content":" HTTP API protected by api_key set in Centrifugo configuration. I.e. api_key option must be added to config, like:  config.json { ... &quot;api_key&quot;: &quot;&lt;YOUR API KEY&gt;&quot; }   This API key must be set in the request Authorization header in this way:  Authorization: apikey &lt;KEY&gt;   It's also possible to pass API key over URL query param. This solves some edge cases where it's not possible to use the Authorization header. Simply add ?api_key=&lt;YOUR API KEY&gt; query param to the API endpoint. Keep in mind that passing the API key in the Authorization header is a recommended way.  It's possible to disable API key check on Centrifugo side using the api_insecure configuration option. Be sure to protect the API endpoint by firewall rules, in this case, to prevent anyone on the internet to send commands over your unprotected Centrifugo API endpoint. API key auth is not very safe for man-in-the-middle so we also recommended running Centrifugo with TLS.  A command is a JSON object with two properties: method and params.  method is the name of the API command you want to call.params is an object with command arguments. Each method can have its own params  Before looking at all available commands here is a CURL that calls info command:  curl --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;info&quot;, &quot;params&quot;: {}}' \\ http://localhost:8000/api   Here is a live example:  Your browser does not support the video tag.  Now let's investigate each API method in detail.  ","version":"v3","tagName":"h3"},{"title":"publish​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#publish","content":" Publish command allows publishing data into a channel. Most probably this is a command you'll use most.  It looks like this:  { &quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; } } }   Let's apply all information said above and send publish command to Centrifugo. We will send a request using the requests library for Python.  import json import requests command = { &quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;docs&quot;, &quot;data&quot;: { &quot;content&quot;: &quot;1&quot; } } } api_key = &quot;YOUR_API_KEY&quot; data = json.dumps(command) headers = {'Content-type': 'application/json', 'Authorization': 'apikey ' + api_key} resp = requests.post(&quot;https://centrifuge.example.com/api&quot;, data=data, headers=headers) print(resp.json())   The same using httpie console tool:  echo '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: {&quot;text&quot;: &quot;hello&quot;}}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey &lt;YOUR_API_KEY&gt;&quot; -vvv POST /api HTTP/1.1 Accept: application/json, */* Accept-Encoding: gzip, deflate Authorization: apikey KEY Connection: keep-alive Content-Length: 80 Content-Type: application/json Host: localhost:8000 User-Agent: HTTPie/0.9.8 { &quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; } } } HTTP/1.1 200 OK Content-Length: 3 Content-Type: application/json Date: Thu, 17 May 2018 22:01:42 GMT { &quot;result&quot;: {} }   In case of error response object can contain error field (here we artificially publishing to a channel with unknown namespace):  echo '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;unknown:chat&quot;, &quot;data&quot;: {&quot;text&quot;: &quot;hello&quot;}}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey &lt;YOUR_API_KEY&gt;&quot; HTTP/1.1 200 OK Content-Length: 55 Content-Type: application/json Date: Thu, 17 May 2018 22:03:09 GMT { &quot;error&quot;: { &quot;code&quot;: 102, &quot;message&quot;: &quot;namespace not found&quot; } }   error object contains error code and message - this is also the same for other commands described below.  Publish params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to publish data\tany JSON\tyes\tCustom JSON data to publish into a channel skip_history\tbool\tno\tSkip adding publication to history for this request tags\tmap[string]string\tno\tPublication tags - map with arbitrary string keys and values which is attached to publication and will be delivered to clients (available since v3.2.0)  Publish result​  Field name\tField type\tOptional\tDescriptionoffset\tinteger\tyes\tOffset of publication in history stream epoch\tstring\tyes\tEpoch of current stream  ","version":"v3","tagName":"h3"},{"title":"broadcast​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#broadcast","content":" Similar to publish but allows to send the same data into many channels.  { &quot;method&quot;: &quot;broadcast&quot;, &quot;params&quot;: { &quot;channels&quot;: [&quot;CHANNEL_1&quot;, &quot;CHANNEL_2&quot;], &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; } } }   Broadcast params​  Parameter name\tParameter type\tRequired\tDescriptionchannels\tArray of strings\tyes\tList of channels to publish data to data\tany JSON\tyes\tCustom JSON data to publish into each channel skip_history\tbool\tno\tSkip adding publications to channels' history for this request tags\tmap[string]string\tno\tPublication tags (available since v3.2.0) - map with arbitrary string keys and values which is attached to publication and will be delivered to clients  Broadcast result​  Field name\tField type\tOptional\tDescriptionresponses\tArray of publish responses\tno\tResponses for each individual publish (with possible error and publish result)  ","version":"v3","tagName":"h3"},{"title":"subscribe​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#subscribe","content":" subscribe allows subscribing user to a channel.  Subscribe params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to subscribe channel\tstring\tyes\tName of channel to subscribe user to info\tany JSON\tno\tAttach custom data to subscription (will be used in presence and join/leave messages) b64info\tstring\tno\tinfo in base64 for binary mode (will be decoded by Centrifugo) client\tstring\tno\tSpecific client ID to subscribe (user still required to be set, will ignore other user connections with different client IDs) session\tstring\tno\tSpecific client session to subscribe (user still required to be set). Available since Centrifugo v3.2.0 data\tany JSON\tno\tCustom subscription data (will be sent to client in Subscribe push) b64data\tstring\tno\tSame as data but in base64 format (will be decoded by Centrifugo) recover_since\tStreamPosition object\tno\tStream position to recover from override\tOverride object\tno\tAllows dynamically override some channel options defined in Centrifugo configuration (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave position\tBoolValue\tyes\tOverride position recover\tBoolValue\tyes\tOverride recover  BoolValue is an object like this:  { &quot;value&quot;: true/false }   Subscribe result​  Empty object at the moment.  ","version":"v3","tagName":"h3"},{"title":"unsubscribe​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#unsubscribe","content":" unsubscribe allows unsubscribing user from a channel.  { &quot;method&quot;: &quot;unsubscribe&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;CHANNEL NAME&quot;, &quot;user&quot;: &quot;USER ID&quot; } }   Unsubscribe params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to unsubscribe channel\tstring\tyes\tName of channel to unsubscribe user to client\tstring\tno\tSpecific client ID to unsubscribe (user still required to be set) session\tstring\tno\tSpecific client session to disconnect (user still required to be set). Available since Centrifugo v3.2.0  Unsubscribe result​  Empty object at the moment.  ","version":"v3","tagName":"h3"},{"title":"disconnect​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#disconnect","content":" disconnect allows disconnecting a user by ID.  { &quot;method&quot;: &quot;disconnect&quot;, &quot;params&quot;: { &quot;user&quot;: &quot;USER ID&quot; } }   Disconnect params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to disconnect client\tstring\tno\tSpecific client ID to disconnect (user still required to be set) session\tstring\tno\tSpecific client session to disconnect (user still required to be set). Available since Centrifugo v3.2.0 whitelist\tArray of strings\tno\tArray of client IDs to keep disconnect\tDisconnect object\tno\tProvide custom disconnect object, see below  Disconnect object​  Field name\tField type\tRequired\tDescriptioncode\tint\tyes\tDisconnect code reason\tstring\tyes\tDisconnect reason reconnect\tbool\tno\tReconnect advice  Disconnect result​  Empty object at the moment.  ","version":"v3","tagName":"h3"},{"title":"refresh​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#refresh","content":" refresh allows refreshing user connection (mostly useful when unidirectional transports are used).  Refresh params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to refresh client\tstring\tno\tClient ID to refresh (user still required to be set) session\tstring\tno\tSpecific client session to refresh (user still required to be set). Available since Centrifugo v3.2.0 expired\tbool\tno\tMark connection as expired and close with Disconnect Expired reason expire_at\tint\tno\tUnix time (in seconds) in the future when the connection will expire  Refresh result​  Empty object at the moment.  ","version":"v3","tagName":"h3"},{"title":"presence​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#presence","content":" presence allows getting channel online presence information (all clients currently subscribed on this channel).  tip Presence in channels is not enabled by default. See how to enable it over channel options.  { &quot;method&quot;: &quot;presence&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot; } }   Example:  fz@centrifugo: echo '{&quot;method&quot;: &quot;presence&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 127 Content-Type: application/json Date: Thu, 17 May 2018 22:13:17 GMT { &quot;result&quot;: { &quot;presence&quot;: { &quot;c54313b2-0442-499a-a70c-051f8588020f&quot;: { &quot;client&quot;: &quot;c54313b2-0442-499a-a70c-051f8588020f&quot;, &quot;user&quot;: &quot;42&quot; }, &quot;adad13b1-0442-499a-a70c-051f858802da&quot;: { &quot;client&quot;: &quot;adad13b1-0442-499a-a70c-051f858802da&quot;, &quot;user&quot;: &quot;42&quot; } } } }   Presence params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call presence from  Presence result​  Field name\tField type\tOptional\tDescriptionpresence\tMap of client ID (string) to ClientInfo object\tno\tOffset of publication in history stream  ClientInfo​  Field name\tField type\tOptional\tDescriptionclient\tstring\tno\tClient ID user\tstring\tno\tUser ID conn_info\tJSON\tyes\tOptional connection info chan_info\tJSON\tyes\tOptional channel info  ","version":"v3","tagName":"h3"},{"title":"presence_stats​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#presence_stats","content":" presence_stats allows getting short channel presence information - number of clients and number of unique users (based on user ID).  { &quot;method&quot;: &quot;presence_stats&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot; } }   Example:  echo '{&quot;method&quot;: &quot;presence_stats&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;public:chat&quot;}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 43 Content-Type: application/json Date: Thu, 17 May 2018 22:09:44 GMT { &quot;result&quot;: { &quot;num_clients&quot;: 0, &quot;num_users&quot;: 0 } }   Presence stats params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call presence from  Presence stats result​  Field name\tField type\tOptional\tDescriptionnum_clients\tinteger\tno\tTotal number of clients in channel num_users\tinteger\tno\tTotal number of unique users in channel  ","version":"v3","tagName":"h3"},{"title":"history​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#history","content":" history allows getting channel history information (list of last messages published into the channel). By default if no limit parameter set in request history call will only return current stream position information - i.e. offset and epoch fields. To get publications you must explicitly provide limit parameter. See also history API description in special doc chapter.  tip History in channels is not enabled by default. See how to enable it over channel options.  { &quot;method&quot;: &quot;history&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot;, &quot;limit&quot;: 2 } }   Example:  echo '{&quot;method&quot;: &quot;history&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;, &quot;limit&quot;: 2}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 129 Content-Type: application/json Date: Wed, 21 Jul 2021 05:30:48 GMT { &quot;result&quot;: { &quot;epoch&quot;: &quot;qFhv&quot;, &quot;offset&quot;: 4, &quot;publications&quot;: [ { &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; }, &quot;offset&quot;: 2 }, { &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; }, &quot;offset&quot;: 3 } ] } }   History params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call history from limit\tint\tno\tLimit number of returned publications, if not set in request then only current stream position information will present in result (without any publications) since\tStreamPosition object\tno\tTo return publications after this position reverse\tbool\tno\tIterate in reversed order (from latest to earliest)  StreamPosition​  Field name\tField type\tRequired\tDescriptionoffset\tinteger\tyes\tOffset in a stream epoch\tstring\tyes\tStream epoch  History result​  Field name\tField type\tOptional\tDescriptionpublications\tArray of publication objects\tyes\tList of publications in channel offset\tinteger\tyes\tTop offset in history stream epoch\tstring\tyes\tEpoch of current stream  ","version":"v3","tagName":"h3"},{"title":"history_remove​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#history_remove","content":" history_remove allows removing publications in channel history. Current top stream position meta data kept untouched to avoid client disconnects due to insufficient state.  { &quot;method&quot;: &quot;history_remove&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot; } }   Example:  echo '{&quot;method&quot;: &quot;history_remove&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 43 Content-Type: application/json Date: Thu, 17 May 2018 22:09:44 GMT { &quot;result&quot;: {} }   History remove params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to remove history  History remove result​  Empty object at the moment.  ","version":"v3","tagName":"h3"},{"title":"channels​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#channels","content":" channels return active channels (with one or more active subscribers in it).  { &quot;method&quot;: &quot;channels&quot;, &quot;params&quot;: {} }   Channels params​  Parameter name\tParameter type\tRequired\tDescriptionpattern\tstring\tno\tPattern to filter channels  Channels result​  Field name\tField type\tOptional\tDescriptionchannels\tMap of string to ChannelInfo\tno\tMap where key is channel and value is ChannelInfo (see below)  ChannelInfo​  Field name\tField type\tOptional\tDescriptionnum_clients\tinteger\tno\tTotal number of connections currently subscribed to a channel  caution Keep in mind that since the channels method by default returns all active channels it can be really heavy for massive deployments. Centrifugo does not provide a way to paginate over channels list. At the moment we mostly suppose that channels RPC extension will be used in the development process or for administrative/debug purposes, and in not very massive Centrifugo setups (with no more than 10k active channels). A better and scalable approach for huge setups could be a real-time analytics approach described here.  ","version":"v3","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#info","content":" info method allows getting information about running Centrifugo nodes.  Example:  echo '{&quot;method&quot;: &quot;info&quot;, &quot;params&quot;: {}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 184 Content-Type: application/json Date: Thu, 17 May 2018 22:07:58 GMT { &quot;result&quot;: { &quot;nodes&quot;: [ { &quot;name&quot;: &quot;Alexanders-MacBook-Pro.local_8000&quot;, &quot;num_channels&quot;: 0, &quot;num_clients&quot;: 0, &quot;num_users&quot;: 0, &quot;uid&quot;: &quot;f844a2ed-5edf-4815-b83c-271974003db9&quot;, &quot;uptime&quot;: 0, &quot;version&quot;: &quot;&quot; } ] } }   Info params​  Empty object at the moment.  Info result​  Field name\tField type\tOptional\tDescriptionnodes\tArray of Node objects\tno\tInformation about all nodes in a cluster  ","version":"v3","tagName":"h3"},{"title":"Command pipelining​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#command-pipelining","content":" It's possible to combine several commands into one request to Centrifugo. To do this use JSON streaming format. This can improve server throughput and reduce traffic traveling around.  ","version":"v3","tagName":"h3"},{"title":"HTTP API libraries​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#http-api-libraries","content":" Sending an API request to Centrifugo is a simple task to do in any programming language - this is just a POST request with JSON payload in body and Authorization header.  But we have several official HTTP API libraries for different languages, to help developers to avoid constructing proper HTTP requests manually:  cent for Pythonphpcent for PHPgocent for Gorubycent for Ruby  Also, there are API libraries created by community:  crystalcent API client for Crystal languagecent.js API client for NodeJSCentrifugo.AspNetCore API client for ASP.NET Core  tip Also, keep in mind that Centrifugo has GRPC API so you can automatically generate client API code for your language.  ","version":"v3","tagName":"h3"},{"title":"GRPC API​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#grpc-api","content":" Centrifugo also supports GRPC API. With GRPC it's possible to communicate with Centrifugo using a more compact binary representation of commands and use the power of HTTP/2 which is the transport behind GRPC.  GRPC API is also useful if you want to publish binary data to Centrifugo channels.  tip GRPC API allows calling all commands described in HTTP API doc, actually both GRPC and HTTP API in Centrifugo based on the same Protobuf schema definition. So refer to the HTTP API description doc for the parameter and the result field description.  You can enable GRPC API in Centrifugo using grpc_api option:  config.json { ... &quot;grpc_api&quot;: true }   By default, GRPC will be served on port 10000 but you can change it using the grpc_api_port option.  Now, as soon as Centrifugo started – you can send GRPC commands to it. To do this get our API Protocol Buffer definitions from this file.  Then see GRPC docs specific to your language to find out how to generate client code from definitions and use generated code to communicate with Centrifugo.  ","version":"v3","tagName":"h2"},{"title":"GRPC example for Python​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#grpc-example-for-python","content":" For example for Python you need to run sth like this according to GRPC docs:  pip install grpcio-tools python -m grpc_tools.protoc -I ./ --python_out=. --grpc_python_out=. api.proto   As soon as you run the command you will have 2 generated files: api_pb2.py and api_pb2_grpc.py. Now all you need is to write a simple program that uses generated code and sends GRPC requests to Centrifugo:  import grpc import api_pb2_grpc as api_grpc import api_pb2 as api_pb channel = grpc.insecure_channel('localhost:10000') stub = api_grpc.CentrifugoApiStub(channel) try: resp = stub.Info(api_pb.InfoRequest()) except grpc.RpcError as err: # GRPC level error. print(err.code(), err.details()) else: if resp.error.code: # Centrifugo server level error. print(resp.error.code, resp.error.message) else: print(resp.result)   Note that you need to explicitly handle Centrifugo API level error which is not transformed automatically into GRPC protocol-level error.  ","version":"v3","tagName":"h3"},{"title":"GRPC example for Go​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#grpc-example-for-go","content":" Here is a simple example of how to run Centrifugo with the GRPC Go client.  You need protoc, protoc-gen-go and protoc-gen-go-grpc installed.  First start Centrifugo itself with GRPC API enabled:  CENTRIFUGO_GRPC_API=1 centrifugo --config config.json   In another terminal tab:  mkdir centrifugo_grpc_example cd centrifugo_grpc_example/ touch main.go go mod init centrifugo_example mkdir apiproto cd apiproto wget https://raw.githubusercontent.com/centrifugal/centrifugo/master/internal/apiproto/api.proto -O api.proto   Run protoc to generate code:  protoc -I ./ api.proto --go_out=. --go-grpc_out=.   Put the following code to main.go file (created on the last step above):  package main import ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;centrifugo_example/apiproto&quot; &quot;google.golang.org/grpc&quot; ) func main() { conn, err := grpc.Dial(&quot;localhost:10000&quot;, grpc.WithInsecure()) if err != nil { log.Fatalln(err) } defer conn.Close() client := apiproto.NewCentrifugoApiClient(conn) for { resp, err := client.Publish(context.Background(), &amp;apiproto.PublishRequest{ Channel: &quot;chat:index&quot;, Data: []byte(`{&quot;input&quot;: &quot;hello from GRPC&quot;}`), }) if err != nil { log.Printf(&quot;Transport level error: %v&quot;, err) } else { if resp.GetError() != nil { respError := resp.GetError() log.Printf(&quot;Error %d (%s)&quot;, respError.Code, respError.Message) } else { log.Println(&quot;Successfully published&quot;) } } time.Sleep(time.Second) } }   Then run:  go run main.go   The program starts and periodically publishes the same payload into chat:index channel.  ","version":"v3","tagName":"h3"},{"title":"GRPC API key authorization​","type":1,"pageTitle":"Server API","url":"/docs/3/server/server_api#grpc-api-key-authorization","content":" You can also set grpc_api_key (string) in Centrifugo configuration to protect GRPC API with key. In this case, you should set per RPC metadata with key authorization and value apikey &lt;KEY&gt;. For example in Go language:  package main import ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;centrifugo_example/apiproto&quot; &quot;google.golang.org/grpc&quot; ) type keyAuth struct { key string } func (t keyAuth) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) { return map[string]string{ &quot;authorization&quot;: &quot;apikey &quot; + t.key, }, nil } func (t keyAuth) RequireTransportSecurity() bool { return false } func main() { conn, err := grpc.Dial(&quot;localhost:10000&quot;, grpc.WithInsecure(), grpc.WithPerRPCCredentials(keyAuth{&quot;xxx&quot;})) if err != nil { log.Fatalln(err) } defer conn.Close() client := apiproto.NewCentrifugoClient(conn) for { resp, err := client.Publish(context.Background(), &amp;PublishRequest{ Channel: &quot;chat:index&quot;, Data: []byte(`{&quot;input&quot;: &quot;hello from GRPC&quot;}`), }) if err != nil { log.Printf(&quot;Transport level error: %v&quot;, err) } else { if resp.GetError() != nil { respError := resp.GetError() log.Printf(&quot;Error %d (%s)&quot;, respError.Code, respError.Message) } else { log.Println(&quot;Successfully published&quot;) } } time.Sleep(time.Second) } }   For other languages refer to GRPC docs. ","version":"v3","tagName":"h3"},{"title":"Client authentication","type":0,"sectionRef":"#","url":"/docs/3/server/authentication","content":"","keywords":"","version":"v3"},{"title":"Claims​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#claims","content":" Centrifugo uses the following claims in a JWT: sub, exp, iat, jti, info, b64info, channels, subs.  ","version":"v3","tagName":"h2"},{"title":"sub​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#sub","content":" This is a standard JWT claim which must contain an ID of the current application user (as string).  If a user is not currently authenticated in an application, but you want to let him connect to Centrifugo anyway – you can use an empty string as a user ID in sub claim. This is called anonymous access. In this case, the anonymous option must be enabled in Centrifugo configuration for channels that the client will subscribe to.  ","version":"v3","tagName":"h3"},{"title":"exp​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#exp","content":" This is a UNIX timestamp seconds when the token will expire. This is a standard JWT claim - all JWT libraries for different languages provide an API to set it.  If exp claim is not provided then Centrifugo won't expire connection. When provided special algorithm will find connections with exp in the past and activate the connection refresh mechanism. Refresh mechanism allows connection to survive and be prolonged. In case of refresh failure, the client connection will be eventually closed by Centrifugo and won't be accepted until new valid and actual credentials are provided in the connection token.  You can use the connection expiration mechanism in cases when you don't want users of your app to be subscribed on channels after being banned/deactivated in the application. Or to protect your users from token leakage (providing a reasonably short time of expiration).  Choose exp value wisely, you don't need small values because the refresh mechanism will hit your application often with refresh requests. But setting this value too large can lead to slow user connection deactivation. This is a trade-off.  Read more about connection expiration below.  ","version":"v3","tagName":"h3"},{"title":"iat​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#iat","content":" This is a UNIX time when token was issued (seconds). See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v3","tagName":"h3"},{"title":"jti​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#jti","content":" This is a token unique ID. See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v3","tagName":"h3"},{"title":"aud​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#aud","content":" Handled since Centrifugo v3.2.0  By default, Centrifugo does not check JWT audience (rfc7519 aud claim).  But you can force this check by setting token_audience string option:  config.json { &quot;token_audience&quot;: &quot;centrifugo&quot; }   caution Setting token_audience will also affect subscription tokens (used for private channels).  ","version":"v3","tagName":"h3"},{"title":"iss​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#iss","content":" Handled since Centrifugo v3.2.0  By default, Centrifugo does not check JWT issuer (rfc7519 iss claim).  But you can force this check by setting token_issuer string option:  config.json { &quot;token_issuer&quot;: &quot;my_app&quot; }   caution Setting token_issuer will also affect subscription tokens (used for private channels).  ","version":"v3","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#info","content":" This claim is optional - this is additional information about client connection that can be provided for Centrifugo. This information will be included in presence information, join/leave events, and channel publication if it was published from a client-side.  ","version":"v3","tagName":"h3"},{"title":"b64info​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#b64info","content":" If you are using binary Protobuf protocol you may want info to be custom bytes. Use this field in this case.  This field contains a base64 representation of your bytes. After receiving Centrifugo will decode base64 back to bytes and will embed the result into various places described above.  ","version":"v3","tagName":"h3"},{"title":"channels​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#channels","content":" An optional array of strings with server-side channels to subscribe a client to. See more details about server-side subscriptions.  caution By providing a list of channels in JWT with channels claim you are not making them automatically unaccessible by other users. Other users can still call a client-side .subscribe() method and subscribe to these channels if channel permissions allow doing this. If you need to protect channels from being subscribed by other connections then you can use private channels inside this channels array (i.e. starting with $) or turn on protected option for channels namespaces.  ","version":"v3","tagName":"h3"},{"title":"subs​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#subs","content":" An optional map of channels with options. This is like a channels claim but allows more control over server-side subscription since every channel can be annotated with info, data, and so on using options.  tip This claim is called subs as a shortcut from subscriptions. The claim sub described above is a standart JWT claim to provide a user ID (it's a shortcut from subject). While claims have similar names they have different purpose in a connection JWT.  caution By providing a map of channels in JWT with subs claim you are not making channels automatically unaccessible by other users. Other users can still call a client-side .subscribe() method and subscribe to these channels if channel permissions allow doing this. If you need to protect channels from being subscribed by other connections then you can use private channels inside this subs map (i.e. starting with $) or turn on protected option for channels namespaces.  Example:  { ... &quot;subs&quot;: { &quot;channel1&quot;: { &quot;data&quot;: {&quot;welcome&quot;: &quot;welcome to channel1&quot;} }, &quot;channel2&quot;: { &quot;data&quot;: {&quot;welcome&quot;: &quot;welcome to channel2&quot;} } } }   Subscribe options:​  Field\tType\tOptional\tDescriptioninfo\tJSON object\tyes\tCustom channel info b64info\tstring\tyes\tCustom channel info in Base64 - to pass binary channel info data\tJSON object\tyes\tCustom JSON data to return in subscription context inside Connect reply b64data\tstring\tyes\tSame as data but in Base64 to send binary data override\tOverride object\tyes\tAllows dynamically override some channel options defined in Centrifugo configuration on a per-connection basis (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave position\tBoolValue\tyes\tOverride position recover\tBoolValue\tyes\tOverride recover  BoolValue is an object like this:  { &quot;value&quot;: true/false }   ","version":"v3","tagName":"h3"},{"title":"meta​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#meta","content":" Meta is an additional JSON object (ex. {&quot;key&quot;: &quot;value&quot;}) that will be attached to a connection. Unlike info it's never exposed to clients and only accessible on a backend side. It will be included in proxy calls from Centrifugo to the application backend. Also, there is a get_user_connections API method in Centrifugo PRO that returns this data in the user connection object.  ","version":"v3","tagName":"h3"},{"title":"expire_at​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#expire_at","content":" By default, Centrifugo looks on exp claim to configure connection expiration. In most cases this is fine, but there could be situations where you wish to decouple token expiration check with connection expiration time. As soon as the expire_at claim is provided (set) in JWT Centrifugo relies on it for setting connection expiration time (JWT expiration still checked over exp though).  expire_at is a UNIX timestamp seconds when the connection should expire.  Set it to the future time for expiring connection at some pointSet it to 0 to disable connection expiration (but still check token exp claim).  ","version":"v3","tagName":"h3"},{"title":"Connection expiration​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#connection-expiration","content":" As said above exp claim in a connection token allows expiring client connection at some point in time. Let's look in detail at what happens when Centrifugo detects that the connection is going to expire.  First, you should do is enable client expiration mechanism in Centrifugo providing a connection JWT with expiration:  import jwt import time token = jwt.encode({&quot;sub&quot;: &quot;42&quot;, &quot;exp&quot;: int(time.time()) + 10*60}, &quot;secret&quot;).decode() print(token)   Let's suppose that you set exp field to timestamp that will expire in 10 minutes and the client connected to Centrifugo with this token. During 10 minutes the connection will be kept by Centrifugo. When this time passed Centrifugo gives the connection some time (configured, 25 seconds by default) to refresh its credentials and provide a new valid token with new exp.  When a client first connects to Centrifugo it receives the ttl value in connect reply. That ttl value contains the number of seconds after which the client must send the refresh command with new credentials to Centrifugo. Centrifugo clients must handle this ttl field and automatically start the refresh process.  For example, a Javascript browser client will send an AJAX POST request to your application when it's time to refresh credentials. By default, this request goes to /centrifuge/refresh URL endpoint. In response your server must return JSON with a new connection JWT:  { &quot;token&quot;: token }   So you must just return the same connection JWT for your user when rendering the page initially. But with actual valid exp. Javascript client will then send them to Centrifugo server and connection will be refreshed for a time you set in exp.  In this case, you know which user wants to refresh its connection because this is just a general request to your app - so your session mechanism will tell you about the user.  If you don't want to refresh the connection for this user - just return 403 Forbidden on refresh request to your application backend.  Javascript client also has options to hook into a refresh mechanism to implement your custom way of refreshing. Other Centrifugo clients also should have hooks to refresh credentials but depending on client API for this can be different - see specific client docs.  ","version":"v3","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#examples","content":" Let's look at how to generate connection HS256 JWT in Python:  ","version":"v3","tagName":"h2"},{"title":"Simplest token​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#simplest-token","content":"     PythonNodeJS import jwt token = jwt.encode({&quot;sub&quot;: &quot;42&quot;}, &quot;secret&quot;).decode() print(token)   Note that we use the value of token_hmac_secret_key from Centrifugo config here (in this case token_hmac_secret_key value is just secret). The only two who must know the HMAC secret key is your application backend which generates JWT and Centrifugo. You should never reveal the HMAC secret key to your users.  Then you can pass this token to your client side and use it when connecting to Centrifugo:  Using centrifuge-js v2.x var centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;); centrifuge.setToken(token); centrifuge.connect();   ","version":"v3","tagName":"h3"},{"title":"Token with expiration​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#token-with-expiration","content":" HS256 token that will be valid for 5 minutes:  PythonNodeJS import jwt import time claims = {&quot;sub&quot;: &quot;42&quot;, &quot;exp&quot;: int(time.time()) + 5*60} token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   ","version":"v3","tagName":"h3"},{"title":"Token with additional connection info​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#token-with-additional-connection-info","content":" Let's attach user name:  PythonNodeJS import jwt claims = {&quot;sub&quot;: &quot;42&quot;, &quot;info&quot;: {&quot;name&quot;: &quot;Alexander Emelin&quot;}} token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   ","version":"v3","tagName":"h3"},{"title":"Investigating problems with JWT​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#investigating-problems-with-jwt","content":" You can use jwt.io site to investigate the contents of your tokens. Also, server logs usually contain some useful information.  ","version":"v3","tagName":"h3"},{"title":"JSON Web Key support​","type":1,"pageTitle":"Client authentication","url":"/docs/3/server/authentication#json-web-key-support","content":" Centrifugo supports JSON Web Key (JWK) spec. This means that it's possible to improve JWT security by providing an endpoint to Centrifugo from where to load JWK (by looking at kid header of JWT).  A mechanism can be enabled by providing token_jwks_public_endpoint string option to Centrifugo (HTTP address).  As soon as token_jwks_public_endpoint set all tokens will be verified using JSON Web Key Set loaded from JWKS endpoint. This makes it impossible to use non-JWK based tokens to connect and subscribe to private channels.  At the moment Centrifugo caches keys loaded from an endpoint for one hour.  Centrifugo will load keys from JWKS endpoint by issuing GET HTTP request with 1 second timeout and one retry in case of failure (not configurable at the moment).  Only RSA algorithm is supported.  JWKS support enabled both connection and private channel subscription tokens. ","version":"v3","tagName":"h2"},{"title":"SockJS","type":0,"sectionRef":"#","url":"/docs/3/transports/sockjs","content":"","keywords":"","version":"v3"},{"title":"SockJS caveats​","type":1,"pageTitle":"SockJS","url":"/docs/3/transports/sockjs#sockjs-caveats","content":" caution There are several important caveats to know when using SockJS – see below.  ","version":"v3","tagName":"h2"},{"title":"Sticky sessions​","type":1,"pageTitle":"SockJS","url":"/docs/3/transports/sockjs#sticky-sessions","content":" First is that you need to use sticky sessions mechanism if you have more than one Centrifugo nodes running behind a load balancer. This mechanism usually supported by load balancers (for example Nginx). Sticky sessions mean that all requests from the same client will come to the same Centrifugo node. This is necessary because SockJS maintains connection session in process memory thus allowing bidirectional communication between a client and a server. Sticky mechanism not required if you only use one Centrifugo node on a backend.  For example, with Nginx sticky support can be enabled with ip_hash directive for upstream:  upstream centrifugo { ip_hash; server 127.0.0.1:8000; server 127.0.0.2:8000; }   With this configuration Nginx will proxy connections with the same ip address to the same upstream backend.  But ip_hash; is not the best choice in this case, because there could be situations where a lot of different connections are coming with the same IP address (behind proxies) and the load balancing system won't be fair.  So the best solution would be using something like nginx-sticky-module which uses setting a special cookie to track the upstream server for a client.  ","version":"v3","tagName":"h3"},{"title":"Browser only​","type":1,"pageTitle":"SockJS","url":"/docs/3/transports/sockjs#browser-only","content":" SockJS is only supported by centrifuge-js – i.e. our browser client. There is no much sense to use SockJS outside of a browser these days.  ","version":"v3","tagName":"h3"},{"title":"JSON only​","type":1,"pageTitle":"SockJS","url":"/docs/3/transports/sockjs#json-only","content":" One more thing to be aware of is that SockJS does not support binary data, so there is no option to use Centrifugo Protobuf protocol on top of SockJS (unlike WebSocket). Only JSON payloads can be transferred.  ","version":"v3","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"SockJS","url":"/docs/3/transports/sockjs#options","content":" ","version":"v3","tagName":"h2"},{"title":"sockjs​","type":1,"pageTitle":"SockJS","url":"/docs/3/transports/sockjs#sockjs","content":" Boolean, default: false.  Enables SockJS transport.  ","version":"v3","tagName":"h3"},{"title":"sockjs_url​","type":1,"pageTitle":"SockJS","url":"/docs/3/transports/sockjs#sockjs_url","content":" Default: https://cdn.jsdelivr.net/npm/sockjs-client@1/dist/sockjs.min.js  Link to SockJS url which is required when iframe-based HTTP fallbacks are in use. ","version":"v3","tagName":"h3"},{"title":"Unidirectional GRPC","type":0,"sectionRef":"#","url":"/docs/3/transports/uni_grpc","content":"","keywords":"","version":"v3"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#supported-data-formats","content":" JSON and binary.  ","version":"v3","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#options","content":" ","version":"v3","tagName":"h2"},{"title":"uni_grpc​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#uni_grpc","content":" Boolean, default: false.  Enables unidirectional GRPC endpoint.  config.json { ... &quot;uni_grpc&quot;: true }   ","version":"v3","tagName":"h3"},{"title":"uni_grpc_port​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#uni_grpc_port","content":" String, default &quot;11000&quot;.  Port to listen on.  ","version":"v3","tagName":"h3"},{"title":"uni_grpc_address​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#uni_grpc_address","content":" String, default &quot;&quot; (listen on all interfaces)  Address to bind uni GRPC to.  ","version":"v3","tagName":"h3"},{"title":"uni_grpc_max_receive_message_size​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#uni_grpc_max_receive_message_size","content":" Default: 65536 (64KB)  Maximum allowed size of a first connect message received from GRPC connection in bytes.  ","version":"v3","tagName":"h3"},{"title":"uni_grpc_tls​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#uni_grpc_tls","content":" Boolean, default: false  Enable custom TLS for unidirectional GRPC server.  ","version":"v3","tagName":"h3"},{"title":"uni_grpc_tls_cert​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#uni_grpc_tls_cert","content":" String, default: &quot;&quot;.  Path to cert file.  ","version":"v3","tagName":"h3"},{"title":"uni_grpc_tls_key​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#uni_grpc_tls_key","content":" String, default: &quot;&quot;.  Path to key file.  ","version":"v3","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/3/transports/uni_grpc#example","content":" A basic example can be found here. It uses Go language, but for other languages approach is mostly the same:  Copy Protobuf definitionsGenerate GRPC client codeUse generated code to connect to CentrifugoProcess Push messages, drop unknown Push types, handle those necessary for the application. ","version":"v3","tagName":"h2"},{"title":"Unidirectional HTTP streaming","type":0,"sectionRef":"#","url":"/docs/3/transports/uni_http_stream","content":"","keywords":"","version":"v3"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#connect-command","content":" It's possible to pass initial connect command by posting a JSON body to a streaming endpoint.  Refer to the full Connect command description – it's the same as for unidirectional WebSocket.  ","version":"v3","tagName":"h2"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#supported-data-formats","content":" JSON  ","version":"v3","tagName":"h2"},{"title":"Pings​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#pings","content":" Centrifugo will send different message types to a connection. Every message is JSON encoded. A special JSON value null used as a PING message. You can simply ignore it on a client side upon receiving. You can ignore such messages or use them to detect broken connections (nothing received from a server for a long time).  ","version":"v3","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#options","content":" ","version":"v3","tagName":"h2"},{"title":"uni_http_stream​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#uni_http_stream","content":" Boolean, default: false.  Enables unidirectional HTTP streaming endpoint.  config.json { ... &quot;uni_http_stream&quot;: true }   ","version":"v3","tagName":"h3"},{"title":"uni_http_stream_max_request_body_size​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#uni_http_stream_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes.  ","version":"v3","tagName":"h3"},{"title":"Connecting using CURL​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#connecting-using-curl","content":" Let's look how simple it is to connect to Centrifugo using HTTP streaming.  We will start from scratch, generate new configuration file:  centrifugo genconfig   Turn on uni HTTP stream and automatically subscribe users to personal channel upon connect:  config.json { ... &quot;uni_http_stream&quot;: true, &quot;user_subscribe_to_personal&quot;: true }   Run Centrifugo:  centrifugo -c config.json   In separate terminal window create token for a user:  ❯ go run main.go gentoken -u user12 HMAC SHA-256 JWT for user user12 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMTIiLCJleHAiOjE2MjUwNzMyODh9.BxmS4R-X6YXMxLfXNhYRzeHvtu_M2NCaXF6HNu7VnDM   Then connect to Centrifugo uni HTTP stream endpoint with simple CURL POST request:  curl -X POST http://localhost:8000/connection/uni_http_stream \\ -d '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMTIiLCJleHAiOjE2MjUwNzMyODh9.BxmS4R-X6YXMxLfXNhYRzeHvtu_M2NCaXF6HNu7VnDM&quot;}'   Open one more terminal window and publish message to a personal user channel:  curl -X POST http://localhost:8000/api \\ -d '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;#user12&quot;, &quot;data&quot;: {&quot;input&quot;: &quot;hello&quot;}}}' \\ -H &quot;Authorization: apikey 9230f514-34d2-4971-ace2-851c656e81dc&quot;   You should see this message received in a terminal window with established connection to HTTP streaming endpoint:  ❯ curl -X POST http://localhost:8000/connection/uni_http_stream \\ -d '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMTIiLCJleHAiOjE2MjUwNzMyODh9.BxmS4R-X6YXMxLfXNhYRzeHvtu_M2NCaXF6HNu7VnDM&quot;}' {&quot;type&quot;:6,&quot;data&quot;:{&quot;client&quot;:&quot;cf5dc239-83ac-4d0f-b9ed-9733d7f7b61b&quot;,&quot;version&quot;:&quot;dev&quot;,&quot;subs&quot;:{&quot;#user12&quot;:{}}}} null null null null null {&quot;channel&quot;:&quot;#user12&quot;,&quot;data&quot;:{&quot;data&quot;:{&quot;input&quot;: &quot;hello&quot;}}}   null messages are pings from a server.  That's all, happy streaming!  ","version":"v3","tagName":"h2"},{"title":"Browser example​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/3/transports/uni_http_stream#browser-example","content":" A basic browser example can be found here. ","version":"v3","tagName":"h2"},{"title":"Unidirectional SSE (EventSource)","type":0,"sectionRef":"#","url":"/docs/3/transports/uni_sse","content":"","keywords":"","version":"v3"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/3/transports/uni_sse#connect-command","content":" Unfortunately SSE specification does not allow POST requests from a web browser, so the only way to pass initial connect command is over URL params. Centrifugo is looking for cf_connect URL param for connect command. Connect command value expected to be a JSON-encoded string, properly encoded into URL. For example:  const url = new URL('http://localhost:8000/connection/uni_sse'); url.searchParams.append(&quot;cf_connect&quot;, JSON.stringify({ 'token': '&lt;JWT&gt;' })); const eventSource = new EventSource(url);   Refer to the full Connect command description – it's the same as for unidirectional WebSocket.  The length of URL query should be kept less than 2048 characters to work throughout browsers. This should be more than enough for most use cases.  tip Centrifugo unidirectional SSE endpoint also supports POST requests. While it's not very useful for browsers which only allow GET requests for EventSource this can be useful when connecting from a mobile device. In this case you must send the connect object as a JSON body of a POST request (instead of using cf_connect URL parameter), similar to what we have in unidirectional HTTP streaming transport case.  ","version":"v3","tagName":"h2"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/3/transports/uni_sse#supported-data-formats","content":" JSON  ","version":"v3","tagName":"h2"},{"title":"Pings​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/3/transports/uni_sse#pings","content":" Centrifugo sends SSE data like this as pings:  event: ping data:   I.e. with event name ping and empty data. You can ignore such messages or use them to detect broken connections (nothing received from a server for a long time).  ","version":"v3","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/3/transports/uni_sse#options","content":" ","version":"v3","tagName":"h2"},{"title":"uni_sse​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/3/transports/uni_sse#uni_sse","content":" Boolean, default: false.  Enables unidirectional SSE (EventSource) endpoint.  config.json { ... &quot;uni_sse&quot;: true }   ","version":"v3","tagName":"h3"},{"title":"uni_sse_max_request_body_size​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/3/transports/uni_sse#uni_sse_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes (when using HTTP POST requests to connect).  ","version":"v3","tagName":"h3"},{"title":"Browser example​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/3/transports/uni_sse#browser-example","content":" A basic browser example can be found here. ","version":"v3","tagName":"h2"},{"title":"Unidirectional WebSocket","type":0,"sectionRef":"#","url":"/docs/3/transports/uni_websocket","content":"","keywords":"","version":"v3"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#connect-command","content":" It's possible to send connect command as first WebSocket message (as JSON).  Field name\tField type\tRequired\tDescriptiontoken\tstring\tno\tConnection JWT, not required when using the connect proxy feature. data\tany JSON\tno\tCustom JSON connection data name\tstring\tno\tApplication name version\tstring\tno\tApplication version subs\tmap of channel to SubscribeRequest\tno\tPass an information about desired subscriptions to a server  ","version":"v3","tagName":"h2"},{"title":"SubscribeRequest​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#subscriberequest","content":" Field name\tField type\tRequired\tDescriptionrecover\tboolean\tno\tWhether a client wants to recover from a certain position offset\tinteger\tno\tKnown stream position offset when recover is used epoch\tstring\tno\tKnown stream position epoch when recover is used  ","version":"v3","tagName":"h3"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#supported-data-formats","content":" JSON  ","version":"v3","tagName":"h2"},{"title":"Pings​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#pings","content":" Centrifugo uses empty messages (frame with no payload at all) as pings for unidirectional WS. You can ignore such messages or use them to detect broken connections (nothing received from a server for a long time).  ","version":"v3","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#options","content":" ","version":"v3","tagName":"h2"},{"title":"uni_websocket​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#uni_websocket","content":" Boolean, default: false.  Enables unidirectional WebSocket endpoint.  config.json { ... &quot;uni_websocket&quot;: true }   ","version":"v3","tagName":"h3"},{"title":"uni_websocket_message_size_limit​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#uni_websocket_message_size_limit","content":" Default: 65536 (64KB)  Maximum allowed size of a first connect message received from WebSocket connection in bytes.  ","version":"v3","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/3/transports/uni_websocket#example","content":" Let's connect to a unidirectional WebSocket endpoint using wscat tool – it allows connecting to WebSocket servers interactively from a terminal.  First, run Centrifugo with uni_websocket enabled. Also let's enable automatic personal channel subscriptions for users. Configuration example:  config.json { &quot;token_hmac_secret_key&quot;: &quot;secret&quot;, &quot;uni_websocket&quot;:true, &quot;user_subscribe_to_personal&quot;: true }   Run Centrifugo:  ./centrifugo -c config.json   In another terminal:  ❯ ./centrifugo -c config.json -u test_user HMAC SHA-256 JWT for user test_user with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ0ZXN0X3VzZXIiLCJleHAiOjE2MzAxMzAxNzB9.u7anX-VYXywX1p1lv9UC9CAu04vpA6LgG5gsw5lz1Iw   Install wscat and run:  wscat -c &quot;ws://localhost:8000/connection/uni_websocket&quot;   This will establish a connection with a server and you then can send connect command to a server:  ❯ wscat -c &quot;ws://localhost:8000/connection/uni_websocket&quot; Connected (press CTRL+C to quit) &gt; {&quot;token&quot;: &quot;eyJh..5lz1Iw&quot;, &quot;subs&quot;: {&quot;abc&quot;: {}}} &lt; {&quot;type&quot;:6,&quot;data&quot;:{&quot;client&quot;:&quot;8ceaa299-4c7b-4254-9d65-c61b6883833a&quot;,&quot;version&quot;:&quot;3.0.0&quot;,&quot;subs&quot;:{&quot;#test_user&quot;:{&quot;recoverable&quot;:true,&quot;epoch&quot;:&quot;StoH&quot;,&quot;positioned&quot;:true},&quot;abc&quot;:{&quot;recoverable&quot;:true,&quot;epoch&quot;:&quot;nNgd&quot;,&quot;positioned&quot;:true},&quot;expires&quot;:true,&quot;ttl&quot;:604653}}   The connection will receive pings (empty messages) periodically. You can try to publish something to #test_user or abc channels (using Centrifugo server API or using admin UI) – and the message should come to the connection we just established. ","version":"v3","tagName":"h2"},{"title":"WebSocket","type":0,"sectionRef":"#","url":"/docs/3/transports/websocket","content":"","keywords":"","version":"v3"},{"title":"Options​","type":1,"pageTitle":"WebSocket","url":"/docs/3/transports/websocket#options","content":" ","version":"v3","tagName":"h2"},{"title":"websocket_message_size_limit​","type":1,"pageTitle":"WebSocket","url":"/docs/3/transports/websocket#websocket_message_size_limit","content":" Default: 65536 (64KB)  Maximum allowed size of a message received from WebSocket connection in bytes.  ","version":"v3","tagName":"h3"},{"title":"websocket_read_buffer_size​","type":1,"pageTitle":"WebSocket","url":"/docs/3/transports/websocket#websocket_read_buffer_size","content":" In bytes, by default 0 which tells Centrifugo to reuse read buffer from HTTP server for WebSocket connection (usually 4096 bytes in size). If set to a lower value can reduce memory usage per WebSocket connection (but can increase number of system calls depending on average message size).  config.json { ... &quot;websocket_read_buffer_size&quot;: 512 }   ","version":"v3","tagName":"h3"},{"title":"websocket_write_buffer_size​","type":1,"pageTitle":"WebSocket","url":"/docs/3/transports/websocket#websocket_write_buffer_size","content":" In bytes, by default 0 which tells Centrifugo to reuse write buffer from HTTP server for WebSocket connection (usually 4096 bytes in size). If set to a lower value can reduce memory usage per WebSocket connection (but HTTP buffer won't be reused):  config.json { ... &quot;websocket_write_buffer_size&quot;: 512 }   ","version":"v3","tagName":"h3"},{"title":"websocket_use_write_buffer_pool​","type":1,"pageTitle":"WebSocket","url":"/docs/3/transports/websocket#websocket_use_write_buffer_pool","content":" If you have a few writes then websocket_use_write_buffer_pool (boolean, default false) option can reduce memory usage of Centrifugo a bit as there won't be separate write buffer binded to each WebSocket connection.  ","version":"v3","tagName":"h3"},{"title":"websocket_compression​","type":1,"pageTitle":"WebSocket","url":"/docs/3/transports/websocket#websocket_compression","content":" An experimental feature for raw WebSocket endpoint - permessage-deflate compression for websocket messages. Btw look at great article about websocket compression. WebSocket compression can reduce an amount of traffic travelling over the wire.  We consider this experimental because this websocket compression is experimental in Gorilla Websocket library that Centrifugo uses internally.  caution Enabling WebSocket compression will result in much slower Centrifugo performance and more memory usage – depending on your message rate this can be very noticeable.  To enable WebSocket compression for raw WebSocket endpoint set websocket_compression to true in a configuration file. After this clients that support permessage-deflate will negotiate compression with server automatically. Note that enabling compression does not mean that every connection will use it - this depends on client support for this feature.  Another option is websocket_compression_min_size. Default 0. This is a minimal size of message in bytes for which we use deflate compression when writing it to client's connection. Default value 0 means that we will compress all messages when websocket_compression enabled and compression support negotiated with client.  It's also possible to control websocket compression level defined at compress/flate By default when compression with a client negotiated Centrifugo uses compression level 1 (BestSpeed). If you want to set custom compression level use websocket_compression_level configuration option.  ","version":"v3","tagName":"h3"},{"title":"Protobuf binary protocol​","type":1,"pageTitle":"WebSocket","url":"/docs/3/transports/websocket#protobuf-binary-protocol","content":" In most cases you will use Centrifugo with JSON protocol which is used by default. It consists of simple human-readable frames that can be easily inspected. Also it's a very simple task to publish JSON encoded data to HTTP API endpoint. You may want to use binary Protobuf client protocol if:  you want less traffic on wire as Protobuf is very compactyou want maximum performance on server-side as Protobuf encoding/decoding is very efficientyou can sacrifice human-readable JSON for your application  Binary protobuf protocol only works for raw Websocket connections (as SockJS can't deal with binary). With most clients to use binary you just need to provide query parameter format to Websocket URL, so final URL look like:  wss://centrifugo.example.com/connection/websocket?format=protobuf   After doing this Centrifugo will use binary frames to pass data between client and server. Your application specific payload can be random bytes.  tip You still can continue to encode your application specific data as JSON when using Protobuf protocol thus have a possibility to co-exist with clients that use JSON protocol on the same Centrifugo installation inside the same channels. ","version":"v3","tagName":"h2"},{"title":"Attributions","type":0,"sectionRef":"#","url":"/docs/4/attributions","content":"","keywords":"","version":"v4"},{"title":"Landing Page Images​","type":1,"pageTitle":"Attributions","url":"/docs/4/attributions#landing-page-images","content":" The following images have been used in the landing page.  Icons made by kerismaker:  https://www.flaticon.com/packs/web-maintenance-35 ","version":"v4","tagName":"h2"},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/4/faq","content":"","keywords":"","version":"v4"},{"title":"How many connections can one Centrifugo instance handle?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#how-many-connections-can-one-centrifugo-instance-handle","content":" This depends on many factors. Real-time transport choice, hardware, message rate, size of messages, Centrifugo features enabled, client distribution over channels, compression on/off, etc. So no certain answer to this question exists. Common sense, performance measurements, and monitoring can help here.  Generally, we suggest not put more than 50-100k clients on one node - but you should measure for your use case.  You can find a description of a test stand with million WebSocket connections in this blog post. Though the point above is still valid – measure and monitor your setup.  ","version":"v4","tagName":"h3"},{"title":"Memory usage per connection?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#memory-usage-per-connection","content":" Depending on transport used and features enabled the amount of RAM required per each connection can vary.  For example, you can expect that each WebSocket connection will cost about 30-50 KB of RAM, thus a server with 1 GB of RAM can handle about 20-30k connections.  For other real-time transports, the memory usage per connection can differ (for example, SockJS connections will cost ~ 2 times more RAM than pure WebSocket connections). So the best way is again – measure for your custom case since depending on Centrifugo transport/features memory usage can vary.  ","version":"v4","tagName":"h3"},{"title":"Can Centrifugo scale horizontally?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#can-centrifugo-scale-horizontally","content":" Yes, it can do this using built-in engines: Redis, KeyDB, Tarantool, or Nats broker.  See engines and scalability considerations.  ","version":"v4","tagName":"h3"},{"title":"Message delivery model​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#message-delivery-model","content":" See design overview  ","version":"v4","tagName":"h3"},{"title":"Message order guarantees​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#message-order-guarantees","content":" See design overview.  ","version":"v4","tagName":"h3"},{"title":"Should I create channels explicitly?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#should-i-create-channels-explicitly","content":" No. By default, channels are created automatically as soon as the first client subscribed to it. And destroyed automatically when the last client unsubscribes from a channel.  When history inside the channel is on then a window of last messages is kept automatically during the retention period. So a client that comes later and subscribes to a channel can retrieve those messages using the call to the history API (or maybe by using the automatic recovery feature which also uses a history internally).  ","version":"v4","tagName":"h3"},{"title":"What about best practices with the number of channels?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#what-about-best-practices-with-the-number-of-channels","content":" Channel is a very lightweight ephemeral entity - Centrifugo can deal with lots of channels, don't be afraid to have many channels in an application.  But keep in mind that one client should be subscribed to a reasonable number of channels at one moment. Client-side subscription to a channel requires a separate frame from client to server – more frames mean more heavy initial connection, more heavy reconnect, etc.  One example which may lead to channel misusing is a messenger app where user can be part of many groups. In this case, using a separate channel for each group/chat in a messenger may be a bad approach. The problem is that messenger app may have chat list screen – a view that displays all user groups (probably with pagination). If you are using separate channel for each group then this may lead to lots of subscriptions. Also, with pagination, to receive updates from older chats (not visible on a screen due to pagination) – user may need to subscribe on their channels too. In this case, using a single personal channel for each user is a preferred approach. As soon as you need to deliver a message to a group you can use Centrifugo broadcast API to send it to many users. If your chat groups are huge in size then you may also need additional queuing system between your application backend and Centrifugo to broadcast a message to many personal channels.  ","version":"v4","tagName":"h3"},{"title":"Any way to exclude message publisher from receiving a message from a channel?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#any-way-to-exclude-message-publisher-from-receiving-a-message-from-a-channel","content":" Currently, no.  We know that services like Pusher provide a way to exclude current client by providing a client ID (socket ID) in publish request. A couple of problems with this:  Client can reconnect while message travels over wire/Backend/Centrifugo – in this case client has a chance to receive a message unexpectedly since it will have another client ID (socket ID)Client can call a history manually or message recovery process can run upon reconnect – in this case a message will present in a history  Both cases may result in duplicate messages. These reasons prevent us adding such functionality into Centrifugo, the correct application architecture requires having some sort of idempotent identifier which allow dealing with message duplicates.  Once added nobody will think about idempotency and this can lead to hard to catch/fix problems in an application. This can also make enabling channel history harder at some point.  Centrifugo behaves similar to Kafka here – i.e. channel should be considered as immutable stream of events where each channel subscriber simply receives all messages published to a channel.  In the future releases Centrifugo may have some sort of server-side message filtering, but we are searching for a proper and safe way of adding it.  ","version":"v4","tagName":"h3"},{"title":"Can I have both binary and JSON clients in one channel?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#can-i-have-both-binary-and-json-clients-in-one-channel","content":" No. It's not possible to transparently encode binary data into JSON protocol (without converting binary to base64 for example which we don't want to do due to increased complexity and performance penalties). So if you have clients in a channel which work with JSON – you need to use JSON payloads everywhere.  Most Centrifugo bidirectional connectors are using binary Protobuf protocol between a client and Centrifugo. But you can send JSON over Protobuf protocol just fine (since JSON is a UTF-8 encoded sequence of bytes in the end).  To summarize:  if you are using binary Protobuf clients and binary payloads everywhere – you are fine.if you are using binary or JSON clients and valid JSON payloads everywhere – you are fine.if you try to send binary data to JSON protocol based clients – you will get errors from Centrifugo.  ","version":"v4","tagName":"h3"},{"title":"Online presence for chat apps - online status of your contacts​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#online-presence-for-chat-apps---online-status-of-your-contacts","content":" While online presence is a good feature it does not fit well for some apps. For example, if you make a chat app - you may probably use a single personal channel for each user. In this case, you cannot find who is online at moment using the built-in Centrifugo presence feature as users do not share a common channel.  You can solve this using a separate service that tracks the online status of your users (for example in Redis) and has a bulk API that returns online status approximation for a list of users. This way you will have an efficient scalable way to deal with online statuses. This is also available as Centrifugo PRO feature.  ","version":"v4","tagName":"h3"},{"title":"Centrifugo stops accepting new connections, why?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#centrifugo-stops-accepting-new-connections-why","content":" The most popular reason behind this is reaching the open file limit. You can make it higher, we described how to do this nearby in this doc. Also, check out an article in our blog which mentions possible problems when dealing with many persistent connections like WebSocket.  ","version":"v4","tagName":"h3"},{"title":"Can I use Centrifugo without reverse-proxy like Nginx before it?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#can-i-use-centrifugo-without-reverse-proxy-like-nginx-before-it","content":" Yes, you can - Go standard library designed to allow this. Though proxy before Centrifugo can be very useful for load balancing clients.  ","version":"v4","tagName":"h3"},{"title":"Does Centrifugo work with HTTP/2?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#does-centrifugo-work-with-http2","content":" Yes, Centrifugo works with HTTP/2. This is provided by built-in Go http server implementation.  You can disable HTTP/2 running Centrifugo server with GODEBUG environment variable:  GODEBUG=&quot;http2server=0&quot; centrifugo -c config.json   Keep in mind that when using WebSocket you are working only over HTTP/1.1, so HTTP/2 support mostly makes sense for SockJS HTTP transports and unidirectional transports: like EventSource (SSE) and HTTP-streaming.  ","version":"v4","tagName":"h3"},{"title":"Does Centrifugo work with HTTP/3?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#does-centrifugo-work-with-http3","content":" Centrifugo v4 added an experimental HTTP/3 support. As soon as you enabled TLS and provided &quot;http3&quot;: true option all endpoints on external port will be served by HTTP/3 server based on github.com/quic-go/quic-go implementation. This (among other benefits which HTTP/3 can provide) is a step towards a proper WebTransport support. For now we support WebTransport experimentally.  It's worth noting that WebSocket transport does not work over HTTP/3, it still starts with HTTP/1.1 Upgrade request (there is an interesting IETF draft BTW about Bootstrapping WebSockets with HTTP/3). But HTTP-streaming and Eventsource should work just fine with HTTP/3.  HTTP/3 does not work with ACME autocert TLS at the moment - i.e. you need to explicitly provide paths to cert and key files as described here.  ","version":"v4","tagName":"h3"},{"title":"Is there a way to use a single connection to Centrifugo from different browser tabs?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#is-there-a-way-to-use-a-single-connection-to-centrifugo-from-different-browser-tabs","content":" If the underlying transport is HTTP-based, and you use HTTP/2 then this will work automatically. For WebSocket, each browser tab creates a new connection.  ","version":"v4","tagName":"h3"},{"title":"What if I need to send push notifications to mobile or web applications?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#what-if-i-need-to-send-push-notifications-to-mobile-or-web-applications","content":" Sometimes it's confusing to see a difference between real-time messages and push notifications. Centrifugo is a real-time messaging server. It can not send push notifications to devices - to Apple iOS devices via APNS, Android devices via FCM, or browsers over Web Push API.  We are preparing our own push notifications API as part of Centrifugo PRO version. This is under construction though.  The reasonable question here is how can you know when you need to send a real-time message to an online client or push notification to its device for an offline client. The solution is pretty simple. You can keep critical notifications for a client in the database. And when a client reads a message you should send an ack to your backend marking that notification as read by the client. Periodically you can check which notifications were sent to clients but they have not read it (no read ack received). For such notifications, you can send push notifications to its device using your own or another open-source solution. Look at Firebase for example.  ","version":"v4","tagName":"h3"},{"title":"How can I know a message is delivered to a client?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#how-can-i-know-a-message-is-delivered-to-a-client","content":" You can, but Centrifugo does not have such an API. What you have to do to ensure your client has received a message is sending confirmation ack from your client to your application backend as soon as the client processed the message coming from a Centrifugo channel.  ","version":"v4","tagName":"h3"},{"title":"Can I publish new messages over a WebSocket connection from a client?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#can-i-publish-new-messages-over-a-websocket-connection-from-a-client","content":" It's possible to publish messages into channels directly from a client (when publish channel option is enabled). But we strongly discourage this in production usage as those messages just go through Centrifugo without any additional control and validation from the application backend.  We suggest using one of the available approaches:  When a user generates an event it must be first delivered to your app backend using a convenient way (for example AJAX POST request for a web application), processed on the backend (validated, saved into the main application database), and then published to Centrifugo using Centrifugo HTTP or GRPC API.Utilize the RPC proxy feature – in this case, you can call RPC over Centrifugo WebSocket which will be translated to an HTTP request to your backend. After receiving this request on the backend you can publish a message to Centrifugo server API. This way you can utilize WebSocket transport between the client and your server in a bidirectional way. HTTP traffic will be concentrated inside your private network.Utilize the publish proxy feature – in this case client can call publish on the frontend, this publication request will be transformed into HTTP or GRPC call to the application backend. If your backend allows publishing - Centrifugo will pass the payload to the channel (i.e. will publish message to the channel itself).  Sometimes publishing from a client directly into a channel (without any backend involved) can be useful though - for personal projects, for demonstrations (like we do in our examples) or if you trust your users and want to build an application without backend. In all cases when you don't need any message control on your backend.  ","version":"v4","tagName":"h3"},{"title":"How to create a secure channel for two users only (private chat case)?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#how-to-create-a-secure-channel-for-two-users-only-private-chat-case","content":" There are several ways to achieve it:  use a private channel (starting with $) - every time a user subscribes to it your backend should provide a sign to confirm that subscription request. Read more in channels chapternext is user limited channels (with #) - you can create a channel with a name like dialog#42,567 to limit subscribers only to the user with id 42 and user with ID 567, this does not fit well for channels with many or dynamic possible subscribersyou can use subscribe proxy feature to validate subscriptions, see chapter about proxyfinally, you can create a hard-to-guess channel name (based on some secret key and user IDs or just generate and save this long unique name into your main app database) so other users won't know this channel to subscribe on it. This is the simplest but not the safest way - but can be reasonable to consider in many situations  ","version":"v4","tagName":"h3"},{"title":"What's the best way to organize channel configuration?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#whats-the-best-way-to-organize-channel-configuration","content":" In most situations, your application needs several different real-time features. We suggest using namespaces for every real-time feature if it requires some option enabled.  For example, if you need join/leave messages for a chat app - create a special channel namespace with this join_leave option enabled. Otherwise, your other channels will receive join/leave messages too - increasing load and traffic in the system but not used by clients.  The same relates to other channel options.  ","version":"v4","tagName":"h3"},{"title":"Does Centrifugo support webhooks?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#does-centrifugo-support-webhooks","content":" Proxy feature allows integrating Centrifugo with your session mechanism (via connect proxy) and provides a way to react to connection events (rpc, subscribe, publish). Also, it opens a road for bidirectional communication with RPC calls. And periodic connection refresh hooks are also there.  Centrifugo does not support unsubscribe/disconnect hooks – see the reasoning below.  ","version":"v4","tagName":"h3"},{"title":"Why Centrifugo does not have disconnect hooks?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#why-centrifugo-does-not-have-disconnect-hooks","content":" Centrifugo does not support disconnect hooks at this point.  First of all, there is no guarantee that the disconnect process will have a time to execute on the client-side (as the client can just switch off its device or simply lose internet connection). This means that a server may notice a connection loss with some delay (thanks to PING/PONG mechanism).  Centrifugo node can be unexpectedly killed. So there is a chance that disconnect event won't have a chance to be emitted to the backend.  One more reason is that Centrifugo designed to scale to many concurrent connections. Think millions of them. As we mentioned in our blog there are cases when all connections start reconnecting at the same time. In this case Centrifugo could potentially generate lots of disconnect events. To reduce the load during connect process Centrifugo has JWT authentication. Even if disconnect events were queued/rate-limited there could be situations when your app processes disconnect hook while user already reconnected and connect event processed. This is a racy situation which you will need to handle somehow (possibly based on unique client ID attached to each connection).  If you need to know that client disconnected and program your business logic around this fact then the reasonable approach could be periodically call your backend from the client-side and update user status somewhere on the backend (use Redis maybe). This is a pretty robust solution where you can't occasionally miss disconnect events. You can also utilize Centrifugo refresh proxy for the task of periodic backend pinging. In this case you will notice that user (or particular client) left app with some delay – this may be a acceptable trade-off in many cases.  Having said that, processing disconnect events may be reasonable – as a best-effort solution while taking into account everything said above. Centrifuge library for Go language (which is the core of Centrifugo) supports client disconnect callbacks on a server-side – so technically the possibility exists. If someone comes with a use case which definitely wins from having disconnect hooks in Centrifugo we are ready to discuss this and try to design a proper solution together.  ","version":"v4","tagName":"h3"},{"title":"Is it possible to listen to join/leave events on the app backend side?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#is-it-possible-to-listen-to-joinleave-events-on-the-app-backend-side","content":" No, join/leave events are only available in the client protocol. In most cases join event can be handled by using subscribe proxy. Leave events are harder – there is no unsubscribe hook available (mostly the same reasons as for disconnect hook described above). So the workaround here can be similar to one for disconnect – ping an app backend periodically while client is subscribed and thus know that client is currently in a channel with some approximation in time.  ","version":"v4","tagName":"h3"},{"title":"How scalable is the online presence and join/leave features?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#how-scalable-is-the-online-presence-and-joinleave-features","content":" Online presence is good for channels with a reasonably small number of active subscribers. As soon as there are tons of active subscribers, presence information becomes very expensive in terms of bandwidth (as it contains full information about all clients in a channel).  There is presence_stats API method that can be helpful if you only need to know the number of clients (or unique users) in a channel. But in the case of the Redis engine even presence_stats call is not optimized for channels with more than several thousand active subscribers.  You may consider using a separate service to deal with presence status information that provides information in near real-time maybe with some reasonable approximation. Centrifugo PRO provides a user status feature which may fit your needs.  The same is true for join/leave messages - as soon as you turn on join/leave events for a channel with many active subscribers each subscriber starts generating indiviaual join/leave events. This may result in many messages sent to each subscriber in a channel, drastically multiplying amount of messages traveling through the system. Especially when all clients reconnect simulteniously. So be careful and estimate the possible load. There is no magic, unfortunately.  ","version":"v4","tagName":"h3"},{"title":"How to send initial data to channel subscriber?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#how-to-send-initial-data-to-channel-subscriber","content":" Sometimes you need to send some initial state towards channel subscriber. Centrifugo provides a way to attach any data to a successful subscribe reply when using subscribe proxy feature. See data and b64data fields. This data will be part of subscribed event context. And of course, you can always simply send request to get initial data from the application backend before or after subscribing to a channel without Centrifugo connection involved (i.e. using sth like general AJAX/HTTP call or passing data to the template when rendering an application page).  ","version":"v4","tagName":"h3"},{"title":"Does Centrifugo support multitenancy?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#does-centrifugo-support-multitenancy","content":" If you want to use Centrifugo with different projects the recommended approach is to have different Centrifugo installations for each project. Multitenancy is better to solve on infrastructure level in case of Centrifugo.  It's possible to share one Redis setup though by setting unique redis_prefix. But we recommend having completely isolated setups.  ","version":"v4","tagName":"h3"},{"title":"I have not found an answer to my question here:​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/4/faq#i-have-not-found-an-answer-to-my-question-here","content":" Ask in our community rooms:    ","version":"v4","tagName":"h3"},{"title":"flow_diagrams","type":0,"sectionRef":"#","url":"/docs/4/flow_diagrams","content":"flow_diagrams For swimlanes.io: Client &lt;- App Backend: JWT note: The backend generates JWT for a user and passes it to the client side. Client -&gt; Centrifugo: Client connects to Centrifugo with JWT ...: {fas-spinner} Persistent connection established Client -&gt; Centrifugo: Client issues channel subscribe requests Centrifugo --&gt;&gt; Client: Client receives real-time updates from channels Client -&gt; Centrifugo: Connect request note: Client connects to Centrifugo without JWT. Centrifugo -&gt; App backend: Sends request further (via HTTP or GRPC) note: The application backend validates client connection and tells Centrifugo user credentials in Connect reply. App backend -&gt; Centrifugo: Connect reply Centrifugo -&gt; Client: Connect Reply ...: {fas-spinner} Persistent connection established Client -&gt; App Backend: Publish request note: Client sends data to publish to the application backend. Backend validates it, maybe modifies, optionally saves to the main database, constructs real-time update and publishes it to the Centrifugo server API. App Backend -&gt; Centrifugo: Publish over Centrifugo API Centrifugo --&gt;&gt; Client: {far-bolt fa-lg} Real-time notification note: Centrifugo delivers real-time message to active channel subscribers. Client -&gt; App Backend: Publish request note: Client sends data to publish to the application backend. Backend validates it, maybe modifies, optionally saves to the main database, constructs real-time update and publishes it to the Centrifugo server API. App Backend -&gt; Centrifugo: Publish over Centrifugo API Centrifugo --&gt;&gt; Client: {far-bolt fa-lg} Real-time notification note: Centrifugo delivers real-time message to active channel subscribers. ","keywords":"","version":"v4"},{"title":"Client API showcase","type":0,"sectionRef":"#","url":"/docs/4/getting-started/client_api","content":"","keywords":"","version":"v4"},{"title":"Connecting to a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#connecting-to-a-server","content":" Each Centrifugo client allows connecting to a server.  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket'); centrifuge.connect();   In most cases you will need to pass JWT (JSON Web Token) for authentication, so the example above transforms to:  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket'); centrifuge.setToken('&lt;USER-JWT&gt;') centrifuge.connect();   See authentication chapter for more information on how to generate connection JWT.  If you are using connect proxy then you may go without setting JWT.  ","version":"v4","tagName":"h2"},{"title":"Disconnecting from a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#disconnecting-from-a-server","content":" After connecting you can disconnect from a server at any moment.  centrifuge.disconnect();   ","version":"v4","tagName":"h2"},{"title":"Reconnecting to a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#reconnecting-to-a-server","content":" Centrifugo clients automatically reconnect to a server in case of temporary connection loss, also clients periodically ping the server to detect broken connections.  ","version":"v4","tagName":"h2"},{"title":"Connection lifecycle events​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#connection-lifecycle-events","content":" All client implementations allow setting handlers on connect and disconnect events.  For example:  centrifuge.on('connect', function(connectCtx){ console.log('connected', connectCtx) }); centrifuge.on('disconnect', function(disconnectCtx){ console.log('disconnected', disconnectCtx) });   ","version":"v4","tagName":"h2"},{"title":"Subscribe to a channel​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#subscribe-to-a-channel","content":" Another core functionality of client API is the possibility to subscribe to a channel to receive all messages published to that channel.  centrifuge.subscribe('channel', function(messageCtx) { console.log(messageCtx); })   Client can subscribe to different channels. Subscribe method returns the Subscription object. It's also possible to react to different Subscription events: join and leave events, subscribe success and subscribe error events, unsubscribe events.  In idiomatic case messages published to channels from application backend over Centrifugo server API. Though it's not always true.  Centrifugo also provides a message recovery feature to restore missed publications in channels. Publications can be missed due to temporary disconnects (bad network) or server reloads. Recovery happens automatically on reconnect (due to bad network or server reloads) as soon as recovery in the channel properly configured. Client keeps last seen Publication offset and restores missed publications since known offset upon reconnecting. If recovery failed then client implementation provides a flag inside subscribe event to let the application know that some publications were missed – so you may need to load state from scratch from the application backend. Not all Centrifugo clients implement a recovery feature – refer to specific client implementation docs. More details about recovery in a dedicated chapter.  ","version":"v4","tagName":"h2"},{"title":"Server-side subscriptions​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#server-side-subscriptions","content":" To handle publications coming from server-side subscriptions client API allows listening publications simply on Centrifuge client instance:  centrifuge.on('publish', function(messageCtx) { console.log(messageCtx); });   It's also possible to react on different server-side Subscription events: join and leave events, subscribe success, unsubscribe event. There is no subscribe error event here since the subscription was initiated on the server-side.  ","version":"v4","tagName":"h2"},{"title":"Send RPC​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#send-rpc","content":" A client can send RPC to a server. RPC is a call that is not related to channels at all. It's just a way to call the server method from the client-side over the WebSocket or SockJS connection. RPC is only available when RPC proxy configured.  const rpcRequest = {'key': 'value'}; const data = await centrifuge.namedRPC('example_method', rpcRequest);   ","version":"v4","tagName":"h2"},{"title":"Call channel history​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#call-channel-history","content":" Once subscribed client can call publication history inside a channel (only for channels where history configured) to get last publications in channel:  Get stream current top position:  const resp = await subscription.history(); console.log(resp.offset); console.log(resp.epoch);   Get up to 10 publications from history since known stream position:  const resp = await subscription.history({limit: 10, since: {offset: 0, epoch: '...'}}); console.log(resp.publications);   Get up to 10 publications from history since current stream beginning:  const resp = await subscription.history({limit: 10}); console.log(resp.publications);   Get up to 10 publications from history since current stream end in reversed order (last to first):  const resp = await subscription.history({limit: 10, reverse: true}); console.log(resp.publications);   ","version":"v4","tagName":"h2"},{"title":"Presence and presence stats​","type":1,"pageTitle":"Client API showcase","url":"/docs/4/getting-started/client_api#presence-and-presence-stats","content":" Once subscribed client can call presence and presence stats information inside channel (only for channels where presence configured):  For presence (full information about active subscribers in channel):  const resp = await subscription.presence(); // resp contains presence information - a map client IDs as keys // and client information as values.   For presence stats (just a number of clients and unique users in a channel):  const resp = await subscription.presenceStats(); // resp contains a number of clients and a number of unique users.  ","version":"v4","tagName":"h2"},{"title":"Join community","type":0,"sectionRef":"#","url":"/docs/4/getting-started/community","content":"Join community If you find Centrifugo interesting – please welcome to our community rooms in Telegram (the most active) and Discord: We are trying to create a respectful and curious community. In those rooms you may find answers to questions not fully covered by the documentation, share your thoughts and ideas on the real-time messaging topics and Centrifugo in particular. We also have Twitter account and Youtube channel. See you there!","keywords":"","version":"v4"},{"title":"Design overview","type":0,"sectionRef":"#","url":"/docs/4/getting-started/design","content":"","keywords":"","version":"v4"},{"title":"Idiomatic usage​","type":1,"pageTitle":"Design overview","url":"/docs/4/getting-started/design#idiomatic-usage","content":" Originally Centrifugo was built with the unidirectional flow as the main approach. Though Centrifugo itself used a bidirectional protocol between a client and a server to allow client dynamically create subscriptions, Centrifugo did not allow using it for sending data from client to server.  With this approach publications travel only from server to a client. All requests that generate new data first go to the application backend (for example over AJAX call of backend API). The backend can validate the message, process it, save it into a database for long-term persistence – and then publish an event from a backend side to Centrifugo API.  This is a pretty natural workflow for applications since this is how applications traditionally work (without real-time features) and Centrifugo is decoupled from the application in this case.    During Centrifugo v2 life cycle this paradigm evolved a bit. It's now possible to send RPC requests from client to Centrifugo and the request will be then proxied to the application backend. Also, connection attempts and publications to channels can now be proxied. So bidirectional connection between client and Centrifugo is now available for utilizing by developers in both directions. For example, here is how publish diagram could look like when using publish request proxy feature:    So at the moment, the number of possible integration ways increased.  ","version":"v4","tagName":"h2"},{"title":"Message history considerations​","type":1,"pageTitle":"Design overview","url":"/docs/4/getting-started/design#message-history-considerations","content":" Idiomatic Centrifugo usage requires having the main application database from which initial and actual state can be loaded at any point in time.  While Centrifugo has channel history, it has been mostly designed to reduce the load on the main application database when all users reconnect at once (in case of load balancer configuration reload, Centrifugo restart, temporary network problems, etc). This allows to radically reduce the load on the application main database during reconnect storm. Since such disconnects are usually pretty short in time having a reasonably small number of messages cached in history is sufficient.  The addition of history iteration API shifts possible use cases a bit. Manually calling history chunk by chunk allows keeping larger number of publications per channel.  Depending on Engine used and configuration of the underlying storage history stream persistence characteristics can vary. For example, with Memory Engine history will be lost upon Centrifugo restart. With Redis or Tarantool engines history will survive Centrifugo restarts but depending on a storage configuration it can be lost upon storage restart – so you should take into account storage configuration and persistence properties as well. For example, consider enabling Redis RDB and AOF, configure replication for storage high-availability, use Redis Cluster or maybe synchronous replication with Tarantool.  Centrifugo provides ways to distinguish whether the missed messages can't be restored from Centrifugo history upon recovery so a client should restore state from the main application database. So Centrifugo message history can be used as a complementary way to restore messages and thus reduce a load on the main application database most of the time.  ","version":"v4","tagName":"h2"},{"title":"Message delivery model​","type":1,"pageTitle":"Design overview","url":"/docs/4/getting-started/design#message-delivery-model","content":" By default, the message delivery model of Centrifugo is at most once. With history and the positioning/recovery features enabled it's possible to achieve at least once guarantee within history retention time and size. After abnormal disconnect clients have an option to recover missed messages from the publication channel stream history that Centrifugo maintains.  Without the positioning or recovery features enabled a message sent to Centrifugo can be theoretically lost while moving towards clients. Centrifugo tries to do its best to prevent message loss on a way to online clients, but the application should tolerate a loss.  As noted Centrifugo has a feature called message recovery to automatically recover messages missed due to short network disconnections. Also, it compensates at most once delivery of broker PUB/SUB system (Redis, Tarantool) by using additional publication offset checks and periodic offset synchronization. So publication loss missed in PUB/SUB layer will be detected eventually and client may catch up the state loading it from history.  At this moment Centrifugo message recovery is designed for a short-term disconnect period (think no more than one hour for a typical chat application, but this can vary). After this period (which can be configured per channel basis) Centrifugo removes messages from the channel history cache. In this case, Centrifugo may tell the client that some messages can not be recovered, so your application state should be loaded from the main database.  ","version":"v4","tagName":"h2"},{"title":"Message order guarantees​","type":1,"pageTitle":"Design overview","url":"/docs/4/getting-started/design#message-order-guarantees","content":" Message order in channels is guaranteed to be the same while you publish messages into channel one after another or publish them in one request. If you do parallel publications into the same channel then Centrifugo can't guarantee message order since those may be processed concurrently by Centrifugo.  ","version":"v4","tagName":"h2"},{"title":"Graceful degradation​","type":1,"pageTitle":"Design overview","url":"/docs/4/getting-started/design#graceful-degradation","content":" It is recommended to design an application in a way that users don't even notice when Centrifugo does not work. Use graceful degradation. For example, if a user posts a new comment over AJAX to your application backend - you should not rely only on Centrifugo to receive a new comment from a channel and display it. You should return new comment data in AJAX call response and render it. This way user that posts a comment will think that everything works just fine. Be careful to not draw comments twice in this case - think about idempotent identifiers for your entities.  ","version":"v4","tagName":"h2"},{"title":"Online presence considerations​","type":1,"pageTitle":"Design overview","url":"/docs/4/getting-started/design#online-presence-considerations","content":" Online presence in a channel is designed to be eventually consistent. It will return the correct state most of the time. But when using Redis or Tarantool engines, due to the network failures and unexpected shut down of Centrifugo node, there are chances that clients can be presented in a presence up to one minute more (until presence entry expiration).  Also, channel presence does not scale well for channels with lots of active subscribers. This is due to the fact that presence returns the entire snapshot of all clients in a channel – as soon as the number of active subscribers grows the response size becomes larger. In some cases, presence_stats API call can be sufficient to avoid receiving the entire presence state.  ","version":"v4","tagName":"h2"},{"title":"Scalability considerations​","type":1,"pageTitle":"Design overview","url":"/docs/4/getting-started/design#scalability-considerations","content":" Centrifugo can scale horizontally with built-in engines (Redis, Tarantool, KeyDB) or with Nats broker. See engines.  All supported brokers are fast – they can handle hundreds of thousands of requests per second. This should be enough for most applications.  But, if you approach broker resource limits (CPU or memory) then it's possible:  Use Centrifugo consistent sharding support to balance queries between different broker instances (supported for Redis, KeyDB, Tarantool)Use Redis Cluster (it's also possible to consistently shard data between different Redis Clusters)Nats broker should scale well itself in cluster setup  All brokers can be set up in highly available way so there won't be a single point of failure.  All Centrifugo data (history, online presence) is designed to be ephemeral and have an expiration time. Due to this fact and the fact that Centrifugo provides hooks for the application to understand history loss makes the process of resharding mostly automatic. As soon as you need to add additional broker shard (when using client-side sharding) you can just add it to the configuration and restart Centrifugo. Since data is sharded consistently part of the data will stay on the same broker nodes. Applications should handle cases that channel data moved to another shard and restore a state from the main application database when needed. ","version":"v4","tagName":"h2"},{"title":"Ecosystem notes","type":0,"sectionRef":"#","url":"/docs/4/getting-started/ecosystem","content":"","keywords":"","version":"v4"},{"title":"Centrifuge library for Go​","type":1,"pageTitle":"Ecosystem notes","url":"/docs/4/getting-started/ecosystem#centrifuge-library-for-go","content":" Centrifugo is built on top of Centrifuge library for Go language.  Due to its standalone language-agnostic nature Centrifugo dictates some rules developers should follow when integrating. If you need more freedom and a more tight integration of real-time server with application business logic you may consider using Centrifuge library to build something similar to Centrifugo but with customized behavior. Centrifuge library can be considered as Socket.IO analogue in Go language ecosystem.  Library README has detailed description, link to examples and introduction post.  Many Centrifugo features should be re-implemented when using Centrifuge - like API layer, admin web UI, proxy etc (if you need those of course). And you need to write in Go language. But the core functionality like a client-server protocol (all Centrifugo client SDKs work with Centrifuge library based server) and Redis engine to scale come out of the box – in most cases this is enough to start building an app.  tip Many things said in Centrifugo doc can be considered as an extra documentation for Centrifuge library (for example part about infrastructure tuning or transport description). But not all of them.  ","version":"v4","tagName":"h2"},{"title":"Framework integrations​","type":1,"pageTitle":"Ecosystem notes","url":"/docs/4/getting-started/ecosystem#framework-integrations","content":" There are some community-driven projects that provide integration with frameworks for more native experience.  tip In general, integrating Centrifugo can be done in several steps even without third-party libraries – see our integration guide. Integrating directly may allow using all Centrifugo features without limitations which can be introduced by third-party wrapper.  laravel-centrifugo integration with Laravel frameworklaravel-centrifugo-broadcaster one more integration with Laravel framework to considerCentrifugoBundle integration with Symfony frameworkDjango-instant integration with Django frameworkroadrunner-php/centrifugo integration with RoadRunnerspiral/roadrunner-bridge integration with Spiral Framework ","version":"v4","tagName":"h2"},{"title":"Main highlights","type":0,"sectionRef":"#","url":"/docs/4/getting-started/highlights","content":"","keywords":"","version":"v4"},{"title":"Simple integration​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#simple-integration","content":" Centrifugo was originally designed to be used in conjunction with frameworks without built-in concurrency support (like Django, Laravel, etc.).  It works as a standalone service with well-defined communication contracts. It nicely fits both monolithic and microservice architectures. Application developers should not change backend philosophy and technology stack at all – just integrate with Centrifugo HTTP or GRPC API and let users enjoy real-time updates.  ","version":"v4","tagName":"h3"},{"title":"Great performance​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#great-performance","content":" Centrifugo is fast. It's written in Go language, built on top of fast and battle-tested open-source libraries, has some smart internal optimizations like message queuing on broadcasts, smart batching to reduce the number of RTT with broker, connection hub sharding to avoid lock contention, JSON and Protobuf encoding speedups over code generation and other.  See a Million WebSocket with Centrifugo post in our blog to see some real-world numbers.  ","version":"v4","tagName":"h3"},{"title":"Built-in scalability​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#built-in-scalability","content":" Centrifugo scales well to many machines with a help of PUB/SUB brokers. So as soon as you have more client connections in the application – you can spread them over different Centrifugo nodes which will be connected together into a cluster.  The main PUB/SUB engine Centrifugo integrates with is Redis. It supports client-side consistent sharding and Redis Cluster – so a single Redis instance won't be a bottleneck also.  There are other options to scale: KeyDB, Nats, Tarantool. See docs.  ","version":"v4","tagName":"h3"},{"title":"Strict client protocol​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#strict-client-protocol","content":" Centrifugo supports JSON and binary Protobuf protocol for client-server communication. The bidirectional protocol is defined by a strict schema and several ready-to-use SDKs wrap this protocol, handle asynchronous message passing, timeouts, reconnects, and various Centrifugo client API features. See the detailed information about client real-time transports in a dedicated section.  ","version":"v4","tagName":"h3"},{"title":"Variety of real-time transports​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#variety-of-real-time-transports","content":" The main transport in Centrifugo is WebSocket. For browsers that do not support WebSocket Centrifugo provides its own bidirectional WebSocket emulation layer based on HTTP-streaming and SSE (EventSource), and also supports SockJS as an older but battle-tested WebSocket polyfill option, and WebTransport in experimental form.  Centrifugo also supports unidirectional transports for real-time updates: like SSE (EventSource), HTTP streaming, GRPC unidirectional stream. Using unidirectional transport is sufficient for many real-time applications and does not require using our client SDKs – just native standards or GRPC-generated code.  See the detailed information about client real-time transports in a dedicated section.  ","version":"v4","tagName":"h3"},{"title":"Flexible authentication​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#flexible-authentication","content":" Centrifugo can authenticate connections using JWT (JSON Web Token) or by issuing an HTTP/GRPC request to your application backend upon client connection to Centrifugo. It's possible to proxy original request headers or request metadata (in the case of GRPC connection).  It supports the JWK specification.  ","version":"v4","tagName":"h3"},{"title":"Connection management​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#connection-management","content":" Connections can expire, developers can choose a way to handle connection refresh – using a client-side refresh workflow, or a server-side call from Centrifugo to the application backend.  ","version":"v4","tagName":"h3"},{"title":"Channel (room) concept​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#channel-room-concept","content":" Centrifugo is a PUB/SUB server – users subscribe on channels to receive real-time updates. Message sent to a channel is delivered to all online channel subscribers.  ","version":"v4","tagName":"h3"},{"title":"Different types of subscriptions​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#different-types-of-subscriptions","content":" Centrifugo supports client-side (initiated by a client) and server-side (forced by a server) channel subscriptions.  ","version":"v4","tagName":"h3"},{"title":"RPC over bidirectional connection​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#rpc-over-bidirectional-connection","content":" You can fully utilize bidirectional connections by sending RPC calls from the client-side to a configured endpoint on your backend. Calling RPC over WebSocket avoids sending headers on each request – thus reducing external traffic and, in most cases, provides better latency characteristics.  ","version":"v4","tagName":"h3"},{"title":"Online presence information​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#online-presence-information","content":" Online presence feature for channels provides information about active channel subscribers. Also, channel join and leave events (when someone subscribes/unsubscribes) can be received on the client side.  ","version":"v4","tagName":"h3"},{"title":"Message history in channels​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#message-history-in-channels","content":" Optionally Centrifugo allows turning on history for publications in channels. This publication history has a limited size and retention period (TTL). With a channel history, Centrifugo can help to survive the mass reconnect scenario – clients can automatically catch up missed state from a fast cache thus reducing the load on your primary database. It's also possible to manually iterate over a stream from a client or a server-side.  ","version":"v4","tagName":"h3"},{"title":"Embedded admin web UI​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#embedded-admin-web-ui","content":" Built-in admin UI allows publishing messages to channels, look at Centrifugo cluster information, and more.  ","version":"v4","tagName":"h3"},{"title":"Cross-platform​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#cross-platform","content":" Centrifugo works on Linux, macOS, and Windows.  ","version":"v4","tagName":"h3"},{"title":"Ready to deploy​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#ready-to-deploy","content":" Centrifugo supports various deploy ways: in Docker, using prepared RPM or DEB packages, via Kubernetes Helm chart. It supports automatic TLS with Let's Encrypt TLS, outputs Prometheus/Graphite metrics, has an official Grafana dashboard for Prometheus data source.  ","version":"v4","tagName":"h3"},{"title":"Open-source​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#open-source","content":" Centrifugo stands on top of open-source library Centrifuge (MIT license). The OSS version of Centrifugo is based on the permissive open-source license (Apache 2.0). All our official client SDKs and API libraries are MIT-licensed.  ","version":"v4","tagName":"h3"},{"title":"Pro features​","type":1,"pageTitle":"Main highlights","url":"/docs/4/getting-started/highlights#pro-features","content":" Centrifugo PRO extends Centrifugo with several unique features which can give interesting advantages for business adopters. For additional details, refer to the Centrifugo PRO documentation. ","version":"v4","tagName":"h3"},{"title":"Install Centrifugo","type":0,"sectionRef":"#","url":"/docs/4/getting-started/installation","content":"","keywords":"","version":"v4"},{"title":"Install from the binary release​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/4/getting-started/installation#install-from-the-binary-release","content":" For a local development you can download prebuilt Centrifugo binary release (i.e. single all-contained executable file) for your system.  Binary releases available on Github. Download latest release for your operating system, unpack it and you are done. Centrifugo is pre-built for:  Linux 64-bit (linux_amd64)Linux 32-bit (linux_386)Linux ARM 64-bit (linux_arm64)MacOS (darwin_amd64)MacOS on Apple Silicon (darwin_arm64)Windows (windows_amd64)FreeBSD (freebsd_amd64)ARM v6 (linux_armv6)  Archives contain a single statically compiled binary centrifugo file that is ready to run:  ./centrifugo   If you doubt which distribution you need, then on Linux or MacOS you can use the following command to download and unpack centrifugo binary to your current working directory:  curl -sSLf https://centrifugal.dev/install.sh | sh   See the version of Centrifugo:  ./centrifugo version   Centrifugo requires a configuration file with several secret keys. If you are new to Centrifugo then there is genconfig command which generates a minimal configuration file to get started:  ./centrifugo genconfig   It creates a configuration file config.json with some auto-generated option values in a current directory (by default).  tip It's possible to generate file in YAML or TOML format, i.e. ./centrifugo genconfig -c config.toml  Having a configuration file you can finally run Centrifugo instance:  ./centrifugo --config=config.json   We will talk about a configuration in detail in the next sections.  You can also put or symlink centrifugo into your bin OS directory and run it from anywhere:  centrifugo --config=config.json   ","version":"v4","tagName":"h2"},{"title":"Docker image​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/4/getting-started/installation#docker-image","content":" Centrifugo server has a docker image available on Docker Hub.  docker pull centrifugo/centrifugo   Run:  docker run --ulimit nofile=262144:262144 -v /host/dir/with/config/file:/centrifugo -p 8000:8000 centrifugo/centrifugo centrifugo -c config.json   Note that docker allows setting nofile limits in command-line arguments which is pretty important to handle lots of simultaneous persistent connections and not run out of open file limit (each connection requires one file descriptor). See also infrastructure tuning chapter.  caution Pin to the exact Docker Image tag in production, for example: centrifugo/centrifugo:v4.0.0, this will help to avoid unexpected problems during re-deploy process.  ","version":"v4","tagName":"h2"},{"title":"Docker-compose example​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/4/getting-started/installation#docker-compose-example","content":" Create configuration file config.json:  { &quot;token_hmac_secret_key&quot;: &quot;my_secret&quot;, &quot;api_key&quot;: &quot;my_api_key&quot;, &quot;admin_password&quot;: &quot;password&quot;, &quot;admin_secret&quot;: &quot;secret&quot;, &quot;admin&quot;: true }   Create docker-compose.yml:  version: &quot;3.9&quot; services: centrifugo: container_name: centrifugo image: centrifugo/centrifugo:v4 volumes: - ./config.json:/centrifugo/config.json command: centrifugo -c config.json ports: - 8000:8000 ulimits: nofile: soft: 65535 hard: 65535   Run with:  docker-compose up   ","version":"v4","tagName":"h2"},{"title":"Kubernetes Helm chart​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/4/getting-started/installation#kubernetes-helm-chart","content":" See our official Kubernetes Helm chart. Follow instructions in a Centrifugo chart README to bootstrap Centrifugo inside your Kubernetes cluster.  ","version":"v4","tagName":"h2"},{"title":"RPM and DEB packages for Linux​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/4/getting-started/installation#rpm-and-deb-packages-for-linux","content":" Every time we make a new Centrifugo release we upload rpm and deb packages for popular Linux distributions on packagecloud.io.  At moment, we support versions of the following distributions:  64-bit Debian 8 Jessie64-bit Debian 9 Stretch64-bit Debian 10 Buster64-bit Debian 11 Bullseye64-bit Ubuntu 16.04 Xenial64-bit Ubuntu 18.04 Bionic64-bit Ubuntu 20.04 Focal Fossa64-bit Centos 764-bit Centos 8  See full list of available packages and installation instructions.  Centrifugo also works on 32-bit architecture, but we don't support packaging for it since 64-bit is more convenient for servers today.  ","version":"v4","tagName":"h2"},{"title":"With brew on macOS​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/4/getting-started/installation#with-brew-on-macos","content":" If you are developing on macOS then you can install Centrifugo over brew:  brew tap centrifugal/centrifugo brew install centrifugo   ","version":"v4","tagName":"h2"},{"title":"Build from source​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/4/getting-started/installation#build-from-source","content":" You need Go language installed:  git clone https://github.com/centrifugal/centrifugo.git cd centrifugo go build ./centrifugo  ","version":"v4","tagName":"h2"},{"title":"Integration guide","type":0,"sectionRef":"#","url":"/docs/4/getting-started/integration","content":"","keywords":"","version":"v4"},{"title":"0. Install​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#0-install","content":" First, you need to do is download/install Centrifugo server. See install chapter for details.  ","version":"v4","tagName":"h2"},{"title":"1. Configure Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#1-configure-centrifugo","content":" Create basic configuration file with token_hmac_secret_key (or token_rsa_public_key) and api_key set and then run Centrifugo. See this chapter for details about token_hmac_secret_key/token_rsa_public_key and chapter about server API for API description. The simplest way to do this automatically is by using genconfig command:  ./centrifugo genconfig   – which will generate config.json file for you with a minimal set of fields to start from.  Properly configure allowed_origins option.  ","version":"v4","tagName":"h2"},{"title":"2. Configure your backend​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#2-configure-your-backend","content":" In the configuration file of your application backend register several variables: Centrifugo token secret (if you decided to stick with JWT authentication) and Centrifugo API key you set on a previous step, also Centrifugo API endpoint address. By default, the API address is http://localhost:8000/api. You must never reveal token secret and API key to your users.  ","version":"v4","tagName":"h2"},{"title":"3. Connect to Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#3-connect-to-centrifugo","content":" Now your users can start connecting to Centrifugo. You should get a client library (see list of available client SDKs) for your application frontend. Every library has a method to connect to Centrifugo. See information about Centrifugo connection endpoints here.  Every client should provide a connection token (JWT) on connect. You must generate this token on your backend side using Centrifugo secret key you set to backend configuration (note that in the case of RSA tokens you are generating JWT with a private key). See how to generate this JWT in special chapter.  You pass this token from the backend to your frontend app (pass it in template context or use separate request from client-side to get user-specific JWT from backend side). And use this token when connecting to Centrifugo (for example browser client has a special method setToken).  There is also a way to authenticate connections without using JWT - see chapter about proxying to backend.  You are connecting to Centrifugo using one of the available transports.  ","version":"v4","tagName":"h2"},{"title":"4. Subscribe to channels​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#4-subscribe-to-channels","content":" After connecting to Centrifugo subscribe clients to channels they are interested in. See more about channels in special chapter. All bidirectional client SDKs provide a way to handle messages coming to a client from a channel after subscribing to it. Learn more about client SDK possibilities from client SDK API spec.  There is also a way to subscribe connection to a list of channels on the server side at the moment of connection establishment. See chapter about server-side subscriptions.  ","version":"v4","tagName":"h2"},{"title":"5. Publish to channel​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#5-publish-to-channel","content":" Everything should work now – as soon as a user opens some page of your application it must successfully connect to Centrifugo and subscribe to a channel (or channels).  Now let's imagine you want to send a real-time message to users subscribed on a specific channel. This message can be a reaction to some event that happened in your app: someone posted a new comment, the administrator just created a new post, the user pressed the &quot;like&quot; button, etc. Anyway, this is an event your backend just got, and you want to immediately send it to interested users.  You can do this using Centrifugo HTTP API. To simplify your life we have several API libraries for different languages. You can publish messages into a channel using one of those libraries or you can simply follow API description to construct API requests yourself - this is very simple. Also Centrifugo supports GRPC API. As soon as you published a message to the channel it must be delivered to your online client subscribed to that channel.  ","version":"v4","tagName":"h2"},{"title":"6. Deploy to production​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#6-deploy-to-production","content":" To put this all into production you need to deploy Centrifugo on your production server. To help you with this we have many things like Docker image, rpm and deb packages, Nginx configuration. See Infrastructure tuning chapter for some actions you have to do to prepare your server infrastructure for handling many persistent connections.  ","version":"v4","tagName":"h2"},{"title":"7. Monitor Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#7-monitor-centrifugo","content":" Don't forget to configure metrics monitoring your production Centrifugo setup. This may help you to understand what's going on with Centrifugo setup, understand number of connections, operation count and latency distributions, etc.  ","version":"v4","tagName":"h2"},{"title":"8. Scale Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#8-scale-centrifugo","content":" As soon as you are close to machine resource limits you may want to scale Centrifugo – you can run many Centrifugo instances and load-balance clients between them using Redis engine, or with KeyDB, or with Tarantool, or with Nats broker. Engines and scalability chapter describes available options in detail.  ","version":"v4","tagName":"h2"},{"title":"9. Read FAQ​","type":1,"pageTitle":"Integration guide","url":"/docs/4/getting-started/integration#9-read-faq","content":" That's all for basics. The documentation actually covers lots of other concepts Centrifugo server has: scalability, private channels, admin web interface, SockJS fallback, Protobuf support, and more. And don't forget to read our FAQ – it contains lot of useful information. ","version":"v4","tagName":"h2"},{"title":"Centrifugo introduction","type":0,"sectionRef":"#","url":"/docs/4/getting-started/introduction","content":"","keywords":"","version":"v4"},{"title":"Background​","type":1,"pageTitle":"Centrifugo introduction","url":"/docs/4/getting-started/introduction#background","content":"   Centrifugo was born a decade ago to help applications with a server-side written in a language or a framework without built-in concurrency support. In this case, dealing with persistent connections is a real headache that usually can only be resolved by introducing a shift in the technology stack and spending time to create a production-ready solution.  For example, frameworks like Django, Flask, Yii, Laravel, Ruby on Rails, and others have poor or not really performant support of working with many persistent connections for the real-time messaging tasks.  In this case, Centrifugo is a straightforward and non-obtrusive way to introduce real-time updates and handle lots of persistent connections without radical changes in the application backend architecture. Developers could proceed writing the application backend with a favorite language or favorite framework, keep existing architecture – and just let Centrifugo deal with persistent connections and be a real-time messaging transport layer.  These days Centrifugo provides some advanced and unique features that can simplify a developer's life and save months of development. Even if the application backend is built with the asynchronous concurrent language. One example is that Centrifugo has built-in support for scalability to many machines to handle more connections and still making sure channel subscribers on different Centrifugo nodes receive all the publications.  Centrifugo fits well modern architectures and may be a universal real-time component regardless of the application technology stack. There are more things to mention, the documentation uncovers them step by step. ","version":"v4","tagName":"h2"},{"title":"Migrating to v4","type":0,"sectionRef":"#","url":"/docs/4/getting-started/migration_v4","content":"","keywords":"","version":"v4"},{"title":"Client SDK migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#client-sdk-migration","content":" New generation of client protocol requires using the latest versions of client SDKs. During the next several days we will release the following SDK versions which are compatible with Centrifugo v4:  centrifuge-js &gt;= v3.0.0centrifuge-go &gt;= v0.9.0centrifuge-dart &gt;= v0.9.0centrifuge-swift &gt;= v0.5.0centrifuge-java &gt;= v0.2.0  New client SDKs support only new client protocol – you can not connect to Centrifugo v3 with them.  If you have a production system where you want to upgrade Centrifugo from v3 to v4 then the plan is:  danger If you are using private channels (starting with $) or user-limited channels (containing #) then carefully read about subscription token migration and user-limited channels migration below.  Upgrade Centrifugo and its configuration to adopt changes in v4.In Centrifugo v4 config turn on use_client_protocol_v1_by_default.Run Centrifugo v4 – all current clients should continue working with it.Then on the client-side uprade client SDK version to the one which works with Centrifugo v4, adopt changes in SDK API dictated by our new client SDK API spec. Important thing – add ?cf_protocol_version=v2 URL param to the connection endpoint to tell Centrifugo that modern generation of protocol is being used by the connection (otherwise, it assumes old protocol since we have use_client_protocol_v1_by_default option enabled).As soon as all your clients migrated to use new protocol generation you can remove use_client_protocol_v1_by_default option from the server configuration.After that you can remove ?cf_protocol_version=v2 from connection endpoint on the client-side.  tip If you are using mobile client SDKs then most probably some time must pass while clients update their apps to use an updated Centrifugo SDK version.  tip Starting from Centrifugo v4.1.1 it's possible to completely turn off client protocol v1 by setting disable_client_protocol_v1 boolean option to true.  ","version":"v4","tagName":"h2"},{"title":"Unidirectional transport migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#unidirectional-transport-migration","content":" Client protocol framing also changed in unidirectional transports. The good news is that Centrifugo v4 still supports previous format for unidirectional transports.  When you are enabling use_client_protocol_v1_by_default option described above you also make unidirectional transports to work over old protocol format. So your existing clients will continue working just fine with Centrifugo v4. Then the same steps to migrate described above can be applied to unidirectional transport case. The only difference that in unidirectional approach you are not using Centrifugo SDKs.  ","version":"v4","tagName":"h2"},{"title":"SockJS migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#sockjs-migration","content":" SockJS is now DEPRECATED in Centrifugo. Centrifugo v4 may be the last release which supports it. We now offer our own bidirectional emulation layer on top of HTTP-streaming and EventSource. See additional information in Centrifugo v4 introduction post.  ","version":"v4","tagName":"h2"},{"title":"Channel ASCII enforced​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#channel-ascii-enforced","content":" Centrifugo v2 and v3 docs mentioned the fact that channels must contain only ASCII characters. But it was not actually enforced by a server. Now Centrifugo is more strict. If a channel has non-ASCII characters then the 107: bad request error will be returned to the client. Please reach us out if this behavior is not suitable for your use case – we can discuss the use case and think on a proper solution together.  ","version":"v4","tagName":"h2"},{"title":"Subscription token migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#subscription-token-migration","content":" Subscription token now requires sub claim (current user ID) to be set.  In most cases the only change which is required to smoothly migrate to v4 without breaking things is to add a boolean option &quot;skip_user_check_in_subscription_token&quot;: true to a Centrifugo v4 configuration. This skips the check of sub claim to contain the current user ID set to a connection during authentication.  After that start adding sub claim (with current user ID) to subscription tokens. As soon as all subscription tokens in your system contain user ID in sub claim you can remove the skip_user_check_in_subscription_token from a server configuration.  One more important note is that client claim in subscription token in Centrifugo v4 only supported for backwards compatibility. It must not be included into new subscription tokens.  It's worth mentioning that Centrifugo v4 does not allow subscribing on channels starting with $ without token even if namespace marked as available for subscribing using sth like allow_subscribe_for_client option. This is done to prevent potential security risk during v3 -&gt; v4 migration when client previously not available to subscribe to channels starting with $ in any case may get permissions to do so.  ","version":"v4","tagName":"h2"},{"title":"User-limited channel migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#user-limited-channel-migration","content":" User-limited channel support should now be allowed over a separate channel namespace option allow_user_limited_channels. See below the namespace option converter which takes this change into account.  ","version":"v4","tagName":"h2"},{"title":"Namespace configuration migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#namespace-configuration-migration","content":" In Centrifugo v4 namespace configuration options have been changed. Centrifugo now has secure by default namespaces. First thing to do is to read the new docs about channels and namespaces.  Then you can use the following converter which will transform your old namespace configuration to a new one. This converter tries to keep backwards compatibility – i.e. it should be possible to deploy Centrifugo with namespace configuration from converter output and have the same behaviour as before regarding channel permissions. We believe that new option names should provide a more readable configuration and may help to reveal some potential security improvements in your namespace configuration – i.e. making it more strict and protective.  caution Do not blindly deploy things to production – test your system first, go through the possible usage scenarios and/or test cases.  tip It's fully client-side: your data won't be sent anywhere.      Convert Here will be configuration for v4 Here will be log of changes made in your config  ","version":"v4","tagName":"h2"},{"title":"Proxy disconnect code changes​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#proxy-disconnect-code-changes","content":" reconnect flag from custom disconnect code is removed. Reconnect advice is now determined by disconnect code value. This allowed us avoiding using JSON in WebSocket CLOSE frame reason. See proxy docs docs for more details.  ","version":"v4","tagName":"h2"},{"title":"Other configuration option changes​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#other-configuration-option-changes","content":" Several other non-namespace related options have been renamed or removed:  client_anonymous option renamed to allow_anonymous_connect_without_token – new name better describes the purpose of this option which was previously not clear. Converter above takes this into account.use_unlimited_history_by_default option was removed. It was used to help migrating from Centrifugo v2 to v3.  ","version":"v4","tagName":"h2"},{"title":"Server API changes​","type":1,"pageTitle":"Migrating to v4","url":"/docs/4/getting-started/migration_v4#server-api-changes","content":" The only breaking change is that user_connections API method (which is available in Centrifugo PRO only) was renamed to connections. The method is more generic now with a broader possibilities – so previous name does not match the current behavior. ","version":"v4","tagName":"h2"},{"title":"Quickstart tutorial ⏱️","type":0,"sectionRef":"#","url":"/docs/4/getting-started/quickstart","content":"Quickstart tutorial ⏱️ In this tutorial we will build a very simple browser application with Centrifugo. Users will connect to Centrifugo over WebSocket, subscribe to a channel, and start receiving all channel publications (messages published to that channel). In our case, we will send a counter value to all channel subscribers to update counter widget in all open browser tabs in real-time. First you need to install Centrifugo. In this example, we are using a binary file release which is fine for development. Once you have Centrifugo binary available on your machine you can generate minimal required configuration file with the following command: ./centrifugo genconfig This helper command will generate config.json file in the working directory with a content like this: config.json { &quot;token_hmac_secret_key&quot;: &quot;bbe7d157-a253-4094-9759-06a8236543f9&quot;, &quot;admin_password&quot;: &quot;d0683813-0916-4c49-979f-0e08a686b727&quot;, &quot;admin_secret&quot;: &quot;4e9eafcf-0120-4ddd-b668-8dc40072c78e&quot;, &quot;api_key&quot;: &quot;d7627bb6-2292-4911-82e1-615c0ed3eebb&quot;, &quot;allowed_origins&quot;: [] } Now we can start a server. Let's start Centrifugo with a built-in admin web interface: ./centrifugo --config=config.json --admin We could also enable the admin web interface by not using --admin flag but by adding &quot;admin&quot;: true option to the JSON configuration file: config.json { &quot;token_hmac_secret_key&quot;: &quot;bbe7d157-a253-4094-9759-06a8236543f9&quot;, &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;d0683813-0916-4c49-979f-0e08a686b727&quot;, &quot;admin_secret&quot;: &quot;4e9eafcf-0120-4ddd-b668-8dc40072c78e&quot;, &quot;api_key&quot;: &quot;d7627bb6-2292-4911-82e1-615c0ed3eebb&quot;, &quot;allowed_origins&quot;: [] } And then running Centrifugo only with a path to a configuration file: ./centrifugo --config=config.json Now open http://localhost:8000. You should see Centrifugo admin web panel. Enter admin_password value from the configuration file to log in (in our case it's d0683813-0916-4c49-979f-0e08a686b727, but you will have a different value). Inside the admin panel, you should see that one Centrifugo node is running, and it does not have connected clients: Now let's create index.html file with our simple app: index.html &lt;html&gt; &lt;head&gt; &lt;title&gt;Centrifugo quick start&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;counter&quot;&gt;-&lt;/div&gt; &lt;script src=&quot;https://unpkg.com/centrifuge@3.1.0/dist/centrifuge.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; const container = document.getElementById('counter'); const centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, { token: &quot;&lt;TOKEN&gt;&quot; }); centrifuge.on('connecting', function (ctx) { console.log(`connecting: ${ctx.code}, ${ctx.reason}`); }).on('connected', function (ctx) { console.log(`connected over ${ctx.transport}`); }).on('disconnected', function (ctx) { console.log(`disconnected: ${ctx.code}, ${ctx.reason}`); }).connect(); const sub = centrifuge.newSubscription(&quot;channel&quot;); sub.on('publication', function (ctx) { container.innerHTML = ctx.data.value; document.title = ctx.data.value; }).on('subscribing', function (ctx) { console.log(`subscribing: ${ctx.code}, ${ctx.reason}`); }).on('subscribed', function (ctx) { console.log('subscribed', ctx); }).on('unsubscribed', function (ctx) { console.log(`unsubscribed: ${ctx.code}, ${ctx.reason}`); }).subscribe(); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; Note that we are using centrifuge-js 3.1.0 in this example, getting it from CDN, you better use its latest version at the moment of reading this tutorial. In real Javascript app you most probably will load centrifuge from NPM. In index.html above we created an instance of a Centrifuge client passing Centrifugo server default WebSocket endpoint address to it, then we subscribed to a channel called channel and provided a callback function to process incoming real-time messages (publications). Upon receiving a new publication we update page HTML and setting counter value to page title. We call .subscribe() to initialte subscription and .connect() method of Client to start a WebSocket connection. We also handle Client state transitions (disconnected, connecting, connected) and Subscription state transitions (unsubscribed, subscribing, subscribed) – see detailed description in client SDK spec. Now you need to serve this file with an HTTP server. In a real-world Javascript application, you will serve your HTML files with a web server of your choice – but for this simple example we can use a simple built-in Centrifugo static file server: ./centrifugo serve --port 3000 Alternatively, if you have Python 3 installed: python3 -m http.server 3000 These commands start a simple static file web server that serves the current directory on port 3000. Make sure you still have Centrifugo server running. Open http://localhost:3000/. Now if you look at browser developer tools or in Centrifugo logs you will notice that a connection can not be successfully established: 2021-09-01 10:17:33 [INF] request Origin is not authorized due to empty allowed_origins origin=http://localhost:3000 That's because we have not set allowed_origins in the configuration. Modify allowed_origins like this: config.json { ... &quot;allowed_origins&quot;: [&quot;http://localhost:3000&quot;] } Allowed origins is a security option for request originating from web browsers – see more details in server configuration docs. Restart Centrifugo after modifying allowed_origins in a configuration file. Now if you reload a browser window with an application you should see new information logs in server output: 2022-06-10 09:44:21 [INF] invalid connection token error=&quot;invalid token: token format is not valid&quot; client=a65a8463-6a36-421d-814a-0083c8836529 2022-06-10 09:44:21 [INF] disconnect after handling command client=a65a8463-6a36-421d-814a-0083c8836529 command=&quot;id:1 connect:{token:\\&quot;&lt;TOKEN&gt;\\&quot; name:\\&quot;js\\&quot;}&quot; reason=&quot;invalid token&quot; user= We still can not connect. That's because the client should provide a valid JWT (JSON Web Token) to authenticate itself. This token must be generated on your backend and passed to a client-side (over template variables or using separate AJAX call – whatever way you prefer). Since in our simple example we don't have an application backend we can quickly generate an example token for a user using centrifugo sub-command gentoken. Like this: ./centrifugo gentoken -u 123722 – where -u flag sets user ID. The output should be like this: HMAC SHA-256 JWT for user &quot;123722&quot; with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE2NTU0NDgyOTl9.mUU9s5kj3yqp-SAEqloGy8QBgsLg0llA7lKUNwtHRnw – you will have another token value since this one is based on randomly generated token_hmac_secret_key from the configuration file we created at the beginning of this tutorial. See token authentication docs for information about proper token generation in a real application. Now we can copy generated HMAC SHA-256 JWT and paste it into Centrifugo constructor instead of &lt;TOKEN&gt; placeholder in index.html file. I.e.: const centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, { token: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE2NTU0NDgyOTl9.mUU9s5kj3yqp-SAEqloGy8QBgsLg0llA7lKUNwtHRnw&quot; }); If you reload your browser tab – the connection will be successfully established, but the client still can not subscribe to a channel: 2022-06-10 09:45:49 [INF] client command error error=&quot;permission denied&quot; client=88116489-350f-447f-9ff3-ab61c9341efe code=103 command=&quot;id:2 subscribe:{channel:\\&quot;channel\\&quot;}&quot; reply=&quot;id:2 error:{code:103 message:\\&quot;permission denied\\&quot;}&quot; user=123722 We need to give client a permission to subscribe on the channel channel. There are several ways to do this. For example, client can provide subscription JWT for a channel. But here we will use an option to allow all authenticated clients subscribe to any channel. To do this let's extend a server configuration with allow_subscribe_for_client option: config.json { &quot;token_hmac_secret_key&quot;: &quot;bbe7d157-a253-4094-9759-06a8236543f9&quot;, &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;d0683813-0916-4c49-979f-0e08a686b727&quot;, &quot;admin_secret&quot;: &quot;4e9eafcf-0120-4ddd-b668-8dc40072c78e&quot;, &quot;api_key&quot;: &quot;d7627bb6-2292-4911-82e1-615c0ed3eebb&quot;, &quot;allowed_origins&quot;: [&quot;http://localhost:3000&quot;], &quot;allow_subscribe_for_client&quot;: true } tip A good practice with Centrifugo is configuring channel namespaces for different types of real-time features you have in the application. By defining namespaces you can achieve a granular control over channel behavior and permissions. Restart Centrifugo – and after doing this everything should start working. Client can successfully connect and successfully subscribe to a channel now. Open developer tools and look at WebSocket frames panel, you should see sth like this: Note, that in this example we generated connection JWT – but it has expiration time, so after some time Centrifugo stops accepting those tokens. In real-life you need to add a token refresh function to a client to rotate tokens. See out client API SDK spec. OK, the last thing we need to do here is to publish a new counter value to a channel and make sure our app works properly. We can do this over Centrifugo API sending an HTTP request to default API endpoint http://localhost:8000/api, but let's do this over the admin web panel first. Open Centrifugo admin web panel in another browser tab (http://localhost:8000/) and go to Actions section. Select publish action, insert channel name that you want to publish to – in our case this is a string channel and insert into data area JSON like this: { &quot;value&quot;: 1 } Click PUBLISH button and check out the application browser tab – counter value must be immediately received and displayed. Open several browser tabs with our app and make sure all tabs receive a message as soon as you publish it. BTW, let's also look at how you can publish data to a channel over Centrifugo server API from a terminal using curl tool: curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey d7627bb6-2292-4911-82e1-615c0ed3eebb&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;channel&quot;, &quot;data&quot;: {&quot;value&quot;: 2}}}' \\ http://localhost:8000/api – where for Authorization header we set api_key value from Centrifugo config file generated above. We did it! We built the simplest browser real-time app with Centrifugo and its Javascript client. It does not have a backend, it's not very useful, to be honest, but it should give you an insight on how to start working with Centrifugo server. Read more about Centrifugo server in the next documentations chapters – it can do much much more than we just showed here. Integration guide describes a process of idiomatic Centrifugo integration with your application backend.","keywords":"","version":"v4"},{"title":"Channel capabilities","type":0,"sectionRef":"#","url":"/docs/4/pro/capabilities","content":"","keywords":"","version":"v4"},{"title":"Connection capabilities​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#connection-capabilities","content":" Connection capabilities can be set:  in connection JWT (in caps claim)in connect proxy result (caps field)  For example, here we are issuing permissions to subscribe on channel news and channel user_42 to a client:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;, &quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;] } ] }   Known capabilities:  sub - subscribe to a channel to receive publications from itpub - publish into a channel (your backend won't be able to process the publication in this case)prs - call presence and presence stats API, also consume join/leave events upon subscribinghst - call history API, also make Subscription positioned or recoverable upon subscribing  ","version":"v4","tagName":"h2"},{"title":"Caps processing behavior​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#caps-processing-behavior","content":" Centrifugo processes caps objects till it finds a match to a channel. At this point it applies permissions in the matched object and stops processing remaining caps. If no match found – then 103 permission denied returned to a client (of course if namespace does not have other permission-related options enabled). Let's consider example like this:  WRONG! { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;], &quot;allow&quot;: [&quot;pub&quot;] }, { &quot;channels&quot;: [&quot;news&quot;], &quot;allow&quot;: [&quot;sub&quot;] }, ] }   Here we have two entries for channel news, but when client subscribes on news only the first entry will be taken into considiration by Centrifugo – so Subscription attempt will be rejected (since first cap object does not have sub capability). In real life you don't really want to have cap objects with identical channels – but below we will introduce wildcard matching where understanding how caps processed becomes important.  Another example:  WRONG! { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;, &quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;] }, { &quot;channels&quot;: [&quot;user_42&quot;], &quot;allow&quot;: [&quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] }, ] }   One could expect that client will have [&quot;sub&quot;, &quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] capabilities for a channel user_42. But it's not true since Centrifugo processes caps objects and channels inside caps object in order – it finds a match to user_42 in first caps object, it contains only &quot;sub&quot; capability, processing stops. So user can subscribe to a channel, but can not publish, can not call history and presence APIs even though those capabilities are mentioned in caps object. The correct way to give all caps to the channel user_42 would be to split channels into different caps objects:  CORRECT { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;], &quot;allow&quot;: [&quot;sub&quot;] }, { &quot;channels&quot;: [&quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;, &quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] }, ] }   The processing behaves like this to avoid potential problems with possibly conflicting matches (mostly when using wildcard and regex matching – see below) and still allow overriding capabilities for specific channels.  ","version":"v4","tagName":"h3"},{"title":"Expiration considirations​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#expiration-considirations","content":" In JWT auth case – capabilities in JWT will work till token expiration, that's why it's important to keep reasonably small token expiration times. We can recommend using sth like 5-10 mins as a good expiration value, but of course this is application specific.In connect proxy case – capabilities will work until client connection close (disconnect) or connection refresh triggered (with refresh proxy you can provide an updated set of capabilities).  ","version":"v4","tagName":"h3"},{"title":"Revoking connection caps​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#revoking-connection-caps","content":" If at some point you need to revoke some capability from a client:  Simplest way is to wait for a connection expiration, then upon refresh: if using proxy – provide new caps in refresh proxy result, Centrifugo will update caps and unsubscribe a client from channels it does not have permissions anymore (only those obtained due to previous connection-wide capabilities).if JWT auth - provide new caps in connection token, Centrifugo will update caps and unsubscribe a client from channels it does not have permissions anymore (only those obtained due to previous connection-wide capabilities). In case of using connect proxy – you can disconnect a user (or client) with a reconnect code. New capabilities will be asked upon reconnection.In case of using token auth – revoke token (Centrifugo PRO feature) and disconnect user (or client) with reconnect code. Upon reconnection user will receive an error that token revoked and will try to load a new one.  ","version":"v4","tagName":"h3"},{"title":"Example: wildcard match​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#example-wildcard-match","content":" It's possible to use wildcards in channel resource names. For example, let's give a permission to subscribe on all channels in news namespace.  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news:*&quot;], &quot;match&quot;: &quot;wildcard&quot;, &quot;allow&quot;: [&quot;sub&quot;] } ] }   note Match type is used for all channels in caps object. If you need different matching behavior for different channels then split them on different caps objects.  ","version":"v4","tagName":"h3"},{"title":"Example: regex match​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#example-regex-match","content":" Or regex:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;^posts_[\\d]+$&quot;], &quot;match&quot;: &quot;regex&quot;, &quot;allow&quot;: [&quot;sub&quot;] } ] }   ","version":"v4","tagName":"h3"},{"title":"Example: different types of match​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#example-different-types-of-match","content":" Of course it's possible to combine different types of match inside one caps array:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;^posts_[\\d]+$&quot;], &quot;match&quot;: &quot;regex&quot;, &quot;allow&quot;: [&quot;sub&quot;] } { &quot;channels&quot;: [&quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;] } ] }   ","version":"v4","tagName":"h3"},{"title":"Example: full access to all channels​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#example-full-access-to-all-channels","content":" Let's look how to allow all permissions to a client:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;*&quot;], &quot;match&quot;: &quot;wildcard&quot;, &quot;allow&quot;: [&quot;sub&quot;, &quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] } ] }   Full access warn Should we mention that giving full access to a client is something to wisely consider? 🤔  ","version":"v4","tagName":"h3"},{"title":"Subscription capabilities​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#subscription-capabilities","content":" Subscription capabilities can be set:  in subscription JWT (in allow claim)in subscribe proxy result (allow field)  Subscription token already belongs to a channel (it has a channel claim). So users with a valid subscription token can subscribe to a channel. But it's possible to additionally grant channel permissions to a user for publishing and calling presence and history using allow claim:  { &quot;allow&quot;: [&quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] }   Putting sub permission to the Subscription token does not make much sense – Centrifugo only expects valid token for a subscription permission check.  ","version":"v4","tagName":"h2"},{"title":"Expiration considirations​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#expiration-considirations-1","content":" In JWT auth case – capabilities in subscription JWT will work till token expiration, that's why it's important to keep reasonably small token expiration times. We can recommend using sth like 5-10 mins as a good expiration value, but of course this is application specific.In subscribe proxy case – capabilities will work until client unsubscribe (or connection close).  ","version":"v4","tagName":"h3"},{"title":"Revoking subscription permissions​","type":1,"pageTitle":"Channel capabilities","url":"/docs/4/pro/capabilities#revoking-subscription-permissions","content":" If at some point you need to revoke some capability from a client:  Simplest way is to wait for a subscription expiration, then upon refresh: provide new caps in subscription token, Centrifugo will update channel caps. In case of using subscribe proxy – you can unsubscribe a user (or client) with a resubscribe code. Or disconnect with reconnect code. New capabilities will be set up upon resubscription/reconnection.In case of using JWT auth – revoke token (Centrifugo PRO feature) and unsubscribe/disconnect user (or client) with resubscribe/reconnect code. Upon resubscription/reconnection user will receive an error that token revoked and will try to load a new one. ","version":"v4","tagName":"h3"},{"title":"CEL expressions","type":0,"sectionRef":"#","url":"/docs/4/pro/cel_expressions","content":"","keywords":"","version":"v4"},{"title":"subscribe_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/4/pro/cel_expressions#subscribe_cel","content":" We suppose that the main operation for which developers may use CEL expressions in Centrifugo is a subscribe operation. Let's look at it in detail.  It's possible to configure subscribe_cel for a channel namespace (subscribe_cel is just an additional namespace channel option, with same rules applied). This expression should be a valid CEL expression.  config.json { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;admin&quot;, &quot;subscribe_cel&quot;: &quot;'admin' in meta.roles&quot; } ] }   In the example we are using custom meta information (must be an object) attached to the connection. As mentioned before in the doc this meta may be attached to the connection:  when set in the connect proxy resultor provided in JWT as meta claim  An expression is evaluated for every subscription attempt to a channel in a namespace. So if meta attached to the connection is sth like this:  { &quot;roles&quot;: [&quot;admin&quot;] }   – then for every channel in the admin namespace defined above expression will be evaluated to True and subscription will be accepted by Centrifugo.  tip meta must be JSON object (any {}) for CEL expressions to work.  ","version":"v4","tagName":"h2"},{"title":"Expression variables​","type":1,"pageTitle":"CEL expressions","url":"/docs/4/pro/cel_expressions#expression-variables","content":" Inside the expression developers can use some variables which are injected by Centrifugo to the CEL runtime.  Information about current user ID, meta information attached to the connection, all the variables defined in matched channel pattern will be available for CEL expression evaluation.  Say client with user ID 123 subscribes to a channel /users/4 which matched the channel pattern /users/:user:  Variable\tType\tExample\tDescriptionsubscribed\tbool\tfalse\tWhether client is subscribed to channel, always false for subscribe operation user\tstring\t&quot;123&quot;\tCurrent authenticated user ID (known from from JWT or connect proxy result) meta\tmap[string]any\t{&quot;roles&quot;: [&quot;admin&quot;]}\tMeta information attached to the connection by the apllication backend (in JWT or over connect proxy result) channel\tstring\t&quot;/users/4&quot;\tChannel client tries to subscribe vars\tmap[string]string\t{&quot;user&quot;: &quot;4&quot;}\tExtracted variables from the matched channel pattern. It's empty in case of using channels without variables.  In this case, to allow admin to subscribe on any user's channel or allow non-admin user to subscribe only on its own channel, you may construct an expression like this:  { ... &quot;subscribe_cel&quot;: &quot;vars.user == user or 'admin' in meta.roles&quot; }   Let's look at one more example. Say client with user ID 123 subscribes to a channel /example.com/users/4 which matched the channel pattern /:tenant/users/:user. The permission check may be transformed into sth like this (assuming meta information has information about current connection tenant):  { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;/:tenant/users/:user&quot;, &quot;subscribe_cel&quot;: &quot;vars.tenant == meta.tenant &amp;&amp; (vars.user == user or 'admin' in meta.roles)&quot; } ] }   ","version":"v4","tagName":"h3"},{"title":"publish_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/4/pro/cel_expressions#publish_cel","content":" CEL expression to check permissions to publish into a channel. Same expression variables are available.  ","version":"v4","tagName":"h2"},{"title":"history_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/4/pro/cel_expressions#history_cel","content":" CEL expression to check permissions for channel history. Same expression variables are available.  ","version":"v4","tagName":"h2"},{"title":"presence_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/4/pro/cel_expressions#presence_cel","content":" CEL expression to check permissions for channel presence. Same expression variables are available. ","version":"v4","tagName":"h2"},{"title":"Channel patterns","type":0,"sectionRef":"#","url":"/docs/4/pro/channel_patterns","content":"","keywords":"","version":"v4"},{"title":"Configuration​","type":1,"pageTitle":"Channel patterns","url":"/docs/4/pro/channel_patterns#configuration","content":" Let's look at the example:  { // rest of the config ... &quot;channel_patterns&quot;: true, // required to turn on the feature. &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;/users/:name&quot; // namespace options may go here ... }, { &quot;name&quot;: &quot;/events/:project/:type&quot; // namespace options may go here ... } ] }   As soon as namespace name starts with / - it's considered a channel pattern. Just like an HTTP path it consists of segments delimited by /. The : symbol in the segment beginning defines a variable part – more information below.  In this case a channel to be used must be sth like /users/mario - i.e. start with / and match one of the patterns defined in the configuration. So this channel pattern matching mechanics behaves mostly like HTTP route matching in many frameworks.  Given the configuration example above:  if channel is /users/mario, then the namespace with the name /users/:name will match and we apply all the options defined for it to the channel.if channel is /events/42/news, then the namespace with the name /events/:project/:type will match.if channel is /events/42, then no namespace will match and the unknown channel error will be returned.  Basic example demonstrating use of pattern channels in JS const client := new Centrifuge(&quot;ws://...&quot;, {}); const sub = client.newSubscription('/users/mario'); sub.subscribe(); client.connect();   ","version":"v4","tagName":"h3"},{"title":"Implementation details​","type":1,"pageTitle":"Channel patterns","url":"/docs/4/pro/channel_patterns#implementation-details","content":" Some implementation restrictions and details to know about:  When using channel patterns feature : symbol in a namespace name defines a variable part. It's not related to a namespace separator anymore – the entire channel is matched over the channel pattern. Similar to the HTTP routes semantics. So namespace separator is not needed at all when using channel patterns.Centrifugo only allows explicit channel pattern matching which do not result into channel pattern conflicts in runtime, this is checked during configuration validation on server start. Explicitly defined static patterns (without variables) have precedence over patterns with variables.There is no analogue of top-level namespace (like we have for standard namespace configuration) for channels starting with /. If a channel does not match any explicitly defined pattern then Centrifugo returns the 102: unknown channel error.If you define channel_regex inside channel pattern options – then regex matches over the entire channel (since variable parts are located in the namespace name in this case).Channel pattern must only contain ASCII characters.Duplicate variable names are not allowed inside an individual pattern, i.e. defining /users/:user/:user will result into validation error on start.  ","version":"v4","tagName":"h3"},{"title":"Variables​","type":1,"pageTitle":"Channel patterns","url":"/docs/4/pro/channel_patterns#variables","content":" : in the channel pattern name helps to define a variable to match against. Named parameters only match a single segment of the channel:  Channel pattern &quot;/users/:name&quot;: /users/mary ✅ match /users/john ✅ match /users/mary/info ❌ no match /users ❌ no match   Another example for channel pattern /news/:type/:subtype, i.e. with multiple variables:  Channel pattern &quot;/news/:type/:subtype&quot;: /news/sport/football ✅ match /news/sport/volleyball ✅ match /news/sport ❌ no match /news ❌ no match   Channel patterns support mid-segment variables, so the following is possible:  Channel pattern &quot;/personal/user_:user&quot;: /personal/user_mary ✅ match /personal/user_john ✅ match /personal/user_ ❌ no match   ","version":"v4","tagName":"h3"},{"title":"Using varibles​","type":1,"pageTitle":"Channel patterns","url":"/docs/4/pro/channel_patterns#using-varibles","content":" Additional benefits of using channel patterns may be achieved together with Centrifugo PRO CEL expressions. Channel pattern variables are available inside CEL expressions for evaluation in a custom way. ","version":"v4","tagName":"h3"},{"title":"Message batching control","type":0,"sectionRef":"#","url":"/docs/4/pro/client_message_batching","content":"","keywords":"","version":"v4"},{"title":"client_write_delay​","type":1,"pageTitle":"Message batching control","url":"/docs/4/pro/client_message_batching#client_write_delay","content":" The client_write_delay is a duration option, it is a time Centrifugo will try to collect messages inside each connection message write loop before sending them towards the connection.  Enabling client_write_delay may reduce CPU usage of both server and client in case of high message rate inside individual connections. The reduction happens due to the lesser number of system calls to execute. Enabling client_write_delay limits the maximum throughput of messages towards the connection which may be achieved. For example, if client_write_delay is 100ms then the max throughput per second will be (1000 / 100) * client_max_messages_in_frame (16 by default), i.e. 160 messages per second. Though this should be more than enough for target Centrifugo use cases (frontend apps).  Example:  config.json { // Rest of config here ... &quot;client_write_delay&quot;: &quot;100ms&quot; }   ","version":"v4","tagName":"h2"},{"title":"client_reply_without_queue​","type":1,"pageTitle":"Message batching control","url":"/docs/4/pro/client_message_batching#client_reply_without_queue","content":" The client_reply_without_queue is a boolean option to not use client queue for replies to commands. When true replies are written to the transport without going through the connection message queue.  ","version":"v4","tagName":"h2"},{"title":"client_max_messages_in_frame​","type":1,"pageTitle":"Message batching control","url":"/docs/4/pro/client_message_batching#client_max_messages_in_frame","content":" The client_max_messages_in_frame is an integer option which controls the maximum number of messages which may be joined by Centrifugo into one transport frame. By default, 16. Use -1 for unlimited number. ","version":"v4","tagName":"h2"},{"title":"Connections API","type":0,"sectionRef":"#","url":"/docs/4/pro/connections","content":"","keywords":"","version":"v4"},{"title":"Example​","type":1,"pageTitle":"Connections API","url":"/docs/4/pro/connections#example","content":" Let's look at the quick example. First, generate a JWT for user 42:  $ centrifugo genconfig   Generate token for some user to be used in the example connections:  $ centrifugo gentoken -u 42 HMAC SHA-256 JWT for user 42 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y   Run Centrifugo with uni_http_stream transport enabled (it will allow us connecting from the terminal with curl):  CENTRIFUGO_UNI_HTTP_STREAM=1 centrifugo -c config.json   Create new terminal window and run:  curl -X POST http://localhost:8000/connection/uni_http_stream --data '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y&quot;, &quot;name&quot;: &quot;terminal&quot;}'   In another terminal create one more connection:  curl -X POST http://localhost:8000/connection/uni_http_stream --data '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y&quot;, &quot;name&quot;: &quot;terminal&quot;}'   Now let's call connections over HTTP API:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;connections&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;42&quot;}}' \\ http://localhost:8000/api   The result:  { &quot;result&quot;: { &quot;connections&quot;: { &quot;db8bc772-2654-4283-851a-f29b888ace74&quot;: { &quot;app_name&quot;: &quot;terminal&quot;, &quot;transport&quot;: &quot;uni_http_stream&quot;, &quot;protocol&quot;: &quot;json&quot; }, &quot;4bc3ca70-ecc5-439d-af14-a78ae18e31c7&quot;: { &quot;app_name&quot;: &quot;terminal&quot;, &quot;transport&quot;: &quot;uni_http_stream&quot;, &quot;protocol&quot;: &quot;json&quot; } } } }   Here we can see that user has 2 connections from terminal app.  Each connection can be annotated with meta JSON information which is set during connection establishment (over meta claim of JWT or by returning meta in the connect proxy result).  ","version":"v4","tagName":"h3"},{"title":"connections​","type":1,"pageTitle":"Connections API","url":"/docs/4/pro/connections#connections","content":" Returns information about active connections according to the request.  connections params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tno\tfast filter by User ID expression\tstring\tno\tCEL expression to filter users  connections result​  Field name\tField type\tOptional\tDescriptionconnections\tmap[string]ConnectionInfo\tno\tactive user connections map where key is client ID and value is ConnectionInfo  ConnectionInfo​  Field name\tField type\tOptional\tDescriptionapp_name\tstring\tyes\tclient app name (if provided by client) app_version\tstring\tyes\tclient app version (if provided by client) transport\tstring\tno\tclient connection transport protocol\tstring\tno\tclient connection protocol (json or protobuf) user\tstring\tyes\tclient user ID state\tConnectionState\tyes\tconnection state  ConnectionState object​  Field name\tField type\tOptional\tDescriptionchannels\tmap[string]ChannelContext\tyes\tChannels client subscribed to connection_token\tConnectionTokenInfo\tyes\tinformation about connection token subscription_tokens\tmap&lt;string, SubscriptionTokenInfo&gt;\tyes\tinformation about channel tokens used to subscribe meta\tJSON object\tyes\tmeta information attached to a connection  ChannelContext object​  Field name\tField type\tOptional\tDescriptionsource\tint\tyes\tThe source of channel subscription  ConnectionTokenInfo object​  Field name\tField type\tOptional\tDescriptionuid\tstring\tyes\tunique token ID (jti) issued_at\tint\tyes\ttime (Unix seconds) when token was issued  SubscriptionTokenInfo object​  Field name\tField type\tOptional\tDescriptionuid\tstring\tyes\tunique token ID (jti) issued_at\tint\tyes\ttime (Unix seconds) when token was issued ","version":"v4","tagName":"h3"},{"title":"Install and run PRO version","type":0,"sectionRef":"#","url":"/docs/4/pro/install_and_run","content":"","keywords":"","version":"v4"},{"title":"Binary release​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/4/pro/install_and_run#binary-release","content":" Centrifugo PRO binary releases available on Github. Note that we use a separate repo for PRO releases. Download latest release for your operating system, unpack it and run (see how to set license key below).  ","version":"v4","tagName":"h3"},{"title":"Docker image​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/4/pro/install_and_run#docker-image","content":" Centrifugo PRO uses a different image from OSS version – centrifugo/centrifugo-pro:  docker run --ulimit nofile=262144:262144 -v /host/dir/with/config/file:/centrifugo -p 8000:8000 centrifugo/centrifugo-pro:v4.0.0-beta.10 centrifugo -c config.json   ","version":"v4","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/4/pro/install_and_run#kubernetes","content":" You can use our official Helm chart but make sure you changed Docker image to use PRO version and point to the correct image tag:  values.yaml ... image: registry: docker.io repository: centrifugo/centrifugo-pro tag: v4.0.0-beta.10   ","version":"v4","tagName":"h3"},{"title":"Debian and Ubuntu​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/4/pro/install_and_run#debian-and-ubuntu","content":" DEB package available in release assets.  wget https://github.com/centrifugal/centrifugo-pro/releases/download/v4.0.0-beta.10/centrifugo-pro_4.0.0-beta.10_amd64.deb sudo dpkg -i centrifugo-pro_4.0.0-beta.10_amd64.deb   ","version":"v4","tagName":"h3"},{"title":"Centos​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/4/pro/install_and_run#centos","content":" RPM package available in release assets.  wget https://github.com/centrifugal/centrifugo-pro/releases/download/v4.0.0-beta.10/centrifugo-pro-4.0.0-beta.10.x86_64.rpm sudo yum install centrifugo-pro-4.0.0-beta.10.x86_64.rpm   ","version":"v4","tagName":"h3"},{"title":"Setting PRO license key​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/4/pro/install_and_run#setting-pro-license-key","content":" Centrifugo PRO inherits all features and configuration options from open-source version. The only difference is that it expects a valid license key on start to avoid sandbox mode limits.  Once you have installed a PRO version and have a license key you can set it in configuration over license field, or pass over environment variables as CENTRIFUGO_LICENSE. Like this:  config.json { ... &quot;license&quot;: &quot;&lt;YOUR_LICENSE_KEY&gt;&quot; }   tip If license properly set then on Centrifugo PRO start you should see license information in logs: owner, license type and expiration date. All PRO features should be unlocked at this point. Warning about sandbox mode in logs on server start must disappear. ","version":"v4","tagName":"h2"},{"title":"Centrifugo PRO overview","type":0,"sectionRef":"#","url":"/docs/4/pro/overview","content":"","keywords":"","version":"v4"},{"title":"Features​","type":1,"pageTitle":"Centrifugo PRO overview","url":"/docs/4/pro/overview#features","content":" Centrifugo PRO is packed with the following features:  Everything from Centrifugo OSS🔍 Channel and user tracing allows watching client protocol frames in channel or per user ID in real time.💹 Real-time analytics with ClickHouse for a great system observability, reporting and trending.🛡️ Operation throttling to protect server from the real-time API misusing and frontend bugs.🔥 Push notification API to manage device tokens and send mobile and browser push notifications.🟢 User status API feature allows understanding activity state for a list of users.🔌 Connections API to query, filter and inspect active connections.✋ User blocking API to block/unblock abusive users by ID.🛑 JWT revoking and invalidation API to revoke tokens by ID and invalidate user's tokens based on issue time.💪 Channel capabilities for controlling channel permissions per connection or per subscription.📜 Channel patterns allow defining channel configuration like HTTP routes with parameters.✍️ CEL expressions to write custom efficient permission rules for channel operations.🚀 Faster performance to reduce resource usage on server side.🔮 Singleflight for online presence and history to reduce load on the broker.🍔 Message batching control for advanced tuning of client connection write behaviour.🪵 CPU and RSS memory usage stats of Centrifugo nodes in admin UI.  info PRO features can change with time. We reserve a right to move features from PRO to OSS version if there is a clear signal that this is required to do for the ecosystem.  ","version":"v4","tagName":"h2"},{"title":"Try for free in sandbox mode​","type":1,"pageTitle":"Centrifugo PRO overview","url":"/docs/4/pro/overview#try-for-free-in-sandbox-mode","content":" You can try out Centrifugo PRO for free. When you start Centrifugo PRO without license key then it's running in a sandbox mode. Sandbox mode limits the usage of Centrifigo PRO in several ways. For example:  Centrifugo handles up to 20 concurrent connectionsup to 2 server nodes supportedup to 10 API requests per second allowed  This mode should be enough for development and trying out PRO features, but must not be used in production environment as we can introduce additional limitations in the future.  Centrifugo PRO license agreement Centrifugo PRO is distributed by Centrifugal Labs LTD under commercial license which is different from OSS version. By downloading Centrifugo PRO you automatically accept commercial license terms.  ","version":"v4","tagName":"h2"},{"title":"Pricing​","type":1,"pageTitle":"Centrifugo PRO overview","url":"/docs/4/pro/overview#pricing","content":" To run without limits Centrifugo PRO requires a license key.  At this point we are not issuing license keys for Centrifugo PRO as we are in the process of defining pricing strategy and distribution model for it. Please contact us over sales@centrifugal.dev – so we can add you to the list of interested customers. Will appreciate if you share which PRO features you are mostly interested in. ","version":"v4","tagName":"h2"},{"title":"Faster performance","type":0,"sectionRef":"#","url":"/docs/4/pro/performance","content":"","keywords":"","version":"v4"},{"title":"Faster HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#faster-http-api","content":" Centrifugo PRO has an optimized JSON serialization/deserialization for HTTP API.  The effect can be noticeable under load. The exact numbers heavily depend on usage scenario. According to our benchmarks you can expect 10-15% more requests/sec for small message publications over HTTP API, and up to several times throughput boost when you are frequently get lots of messages from a history, see a couple of examples below.  ","version":"v4","tagName":"h2"},{"title":"Faster GRPC API​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#faster-grpc-api","content":" Centrifugo PRO has an optimized Protobuf serialization/deserialization for GRPC API. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v4","tagName":"h2"},{"title":"Faster HTTP proxy​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#faster-http-proxy","content":" Centrifugo PRO has an optimized JSON serialization/deserialization for HTTP proxy. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v4","tagName":"h2"},{"title":"Faster GRPC proxy​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#faster-grpc-proxy","content":" Centrifugo PRO has an optimized Protobuf serialization/deserialization for GRPC API. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v4","tagName":"h2"},{"title":"Faster JWT decoding​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#faster-jwt-decoding","content":" Centrifugo PRO has an optimized decoding of JWT claims.  ","version":"v4","tagName":"h2"},{"title":"Faster GRPC unidirectional stream​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#faster-grpc-unidirectional-stream","content":" Centrifugo PRO has an optimized Protobuf deserialization for GRPC unidirectional stream. This only affects deserialization of initial connect command.  ","version":"v4","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#examples","content":" Let's look at quick live comparisons of Centrifugo OSS and Centrifugo PRO regarding HTTP API performance.  ","version":"v4","tagName":"h2"},{"title":"Publish HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#publish-http-api","content":" Sorry, your browser doesn't support embedded video.  In this video you can see a 13% speed up for publish operation. But for more complex API calls with larger payloads the difference can be much bigger. See next example that demonstrates this.  ","version":"v4","tagName":"h3"},{"title":"History HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/4/pro/performance#history-http-api","content":" Sorry, your browser doesn't support embedded video.  In this video you can see an almost 2x overall speed up while asking 100 messages from Centrifugo history API. ","version":"v4","tagName":"h3"},{"title":"CPU and RSS stats","type":0,"sectionRef":"#","url":"/docs/4/pro/process_stats","content":"CPU and RSS stats A useful addition of Centrifugo PRO is an ability to show CPU and RSS memory usage of each node in admin web UI. Here is how this looks like: The information updated in near real-time (with several seconds delay). It's also available as part of info API.","keywords":"","version":"v4"},{"title":"Push notification API","type":0,"sectionRef":"#","url":"/docs/4/pro/push_notifications","content":"","keywords":"","version":"v4"},{"title":"Motivation and design choices​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#motivation-and-design-choices","content":" We tried to be practical with our Push Notification API, let's look at its design choices and implementation properties we were able to achieve.  ","version":"v4","tagName":"h2"},{"title":"Storage for tokens​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#storage-for-tokens","content":" To start delivering push notifications in the application, developers usually need to integrate with providers such as FCM, HMS, and APNs. This integration typically requires the storage of device tokens in the application database and the implementation of sending push messages to provider push services.  Centrifugo PRO simplifies the process by providing a backend for device token storage, following best practices in token management. It reacts to errors and periodically removes stale devices/tokens to maintain a working set of device tokens based on provider recommendations.  ","version":"v4","tagName":"h3"},{"title":"Efficient queuing​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#efficient-queuing","content":" Additionally, Centrifugo PRO provides an efficient, scalable queuing mechanism for sending push notifications. Developers can send notifications from the app backend to Centrifugo API with minimal latency and let Centrifugo process sending to FCM, HMS, APNs concurrently using built-in workers. In our tests, we achieved hundreds of thousands of pushes in tens of seconds.  ","version":"v4","tagName":"h3"},{"title":"Unified secure topics​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#unified-secure-topics","content":" FCM and HMS have a built-in way of sending notification to large groups of devices over topics mechanism (the same for HMS). One problem with native FCM or HMS topics though is that client can subscribe to any topic from the frontend side without any permission check. In today's world this is usually not desired. So Centrifugo PRO re-implements FCM, HMS topics by introducing an additional API to manage device subscriptions to topics.  tip In some cases you may have real-time channels and device subscription topics with matching names – to send messages to both online and offline users. Though it's up to you.  Centrifugo PRO device topic subscriptions also add a way to introduce the missing topic semantics for APNs.  Centrifugo PRO additionally provides an API to create persistent bindings of user to notification topics. Then – as soon as user registers a device – it will be automatically subscribed to its own topics. As soon as user logs out from the app and you update user ID of the device - user topics binded to the device automatically removed/switched. This design solves one of the issues with FCM – if two different users use the same device it's becoming problematic to unsubscribe the device from large number of topics upon logout. Also, as soon as user to topic binding added (using user_topic_update API) – it will be synchronized across all user active devices. You can still manage such persistent subscriptions on the application backend side if you prefer and provide the full list inside device_register call.  ","version":"v4","tagName":"h3"},{"title":"Non-obtrusive proxying​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#non-obtrusive-proxying","content":" Unlike other solutions that combine different provider push sending APIs into a unified API, Centrifugo PRO provides a non-obtrusive proxy for all the mentioned providers. Developers can send notification payloads in a format defined by each provider.  It's also possible to send notifications into native FCM, HMS topics or send to raw FCM, HMS, APNs tokens using Centrifugo PRO's push API, allowing them to combine native provider primitives with those added by Centrifugo (i.e., sending to a list of device IDs or to a list of topics).  ","version":"v4","tagName":"h3"},{"title":"Builtin analytics​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#builtin-analytics","content":" Furthermore, Centrifugo PRO offers the ability to inspect sent push notifications using ClickHouse analytics. Providers may also offer their own analytics, such as FCM, which provides insight into push notification delivery. Centrifugo PRO also offers a way to analyze push notification delivery and interaction using the update_push_status API.  ","version":"v4","tagName":"h3"},{"title":"Steps to integrate​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#steps-to-integrate","content":" Add provider SDK on the frontend side, follow provider instructions for your platform to obtain a push token for a device. For example, for FCM see instructions for iOS, Android, Flutter, Web Browser). The same for HMS or APNs – frontend part should be handled by their native SDKs.Call Centrifugo PRO backend API with the obtained token. From the application backend call Centrifugo device_register API to register the device in Centrifugo PRO storage. Optionally provide list of topics to subscribe device to.Centrifugo returns a registered device object. Pass a generated device ID to the frontend and save it on the frontend together with a token received from FCM.Call Centrifugo send_push_notification API whenever it's time to deliver a push notification.  At any moment you can inspect device storage by calling device_list API.  Once user logs out from the app, you can detach user ID from device by using device_update or remove device with device_remove API.  ","version":"v4","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#configuration","content":" In Centrifugo PRO you can configure one push provider or use all of them – this choice is up to you.  ","version":"v4","tagName":"h2"},{"title":"FCM​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#fcm","content":" As mentioned above Centrifigo uses PostgreSQL for token storage. To enable push notifications make sure database section defined in the configration and fcm is in the push_notifications.enabled_providers list. Centrifugo PRO uses Redis for queuing push notification requests, so Redis address should be configured also. Finally, to integrate with FCM a path to the credentials file must be provided (see how to create one in this instruction). So the full configuration to start sending push notifications over FCM may look like this:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;enabled_providers&quot;: [&quot;fcm&quot;], &quot;fcm_credentials_file_path&quot;: &quot;/path/to/service/account/credentials.json&quot; } }   tip Actually, PostgreSQL database configuration is optional here – you can use push notifications API without it. In this case you will be able to send notifications to FCM, HMS, APNs raw tokens, FCM and HMS native topics and conditions. I.e. using Centrifugo as an efficient proxy for push notifications (for example if you already keep tokens in your database). But sending to device ids and topics, and token/topic management APIs won't be available for usage.  ","version":"v4","tagName":"h3"},{"title":"HMS​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#hms","content":" { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;enabled_providers&quot;: [&quot;hms&quot;], &quot;hms_app_id&quot;: &quot;&lt;your_app_id&gt;&quot;, &quot;hms_app_secret&quot;: &quot;&lt;your_app_secret&gt;&quot;, } }   tip See example how to get app id and app secret here.  ","version":"v4","tagName":"h3"},{"title":"APNs​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#apns","content":" { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;enabled_providers&quot;: [&quot;apns&quot;], &quot;apns_endpoint&quot;: &quot;development&quot;, &quot;apns_bundle_id&quot;: &quot;com.example.your_app&quot;, &quot;apns_auth&quot;: &quot;token&quot;, &quot;apns_token_auth_key_path&quot;: &quot;/path/to/auth/key/file.p8&quot;, &quot;apns_token_key_id&quot;: &quot;&lt;your_key_id&gt;&quot;, &quot;apns_token_team_id&quot;: &quot;your_team_id&quot;, } }   We also support auth over p12 certificates with the following options:  push_notifications.apns_cert_p12_pathpush_notifications.apns_cert_p12_b64push_notifications.apns_cert_p12_password  ","version":"v4","tagName":"h3"},{"title":"Other options​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#other-options","content":" push_notifications.max_inactive_device_days​  This option configures the number of days to keep device without updates. By default Centrifugo does not remove inactive devices.  ","version":"v4","tagName":"h3"},{"title":"Use PostgreSQL as queue​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#use-postgresql-as-queue","content":" Coming soon 🚧  Centrifugo PRO utilizes Redis Streams as the default queue engine for push notifications. However, it also offers the option to employ PostgreSQL for queuing. It's as simple as:  config.json { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;queue_engine&quot;: &quot;database&quot;, // rest of the options... } }   tip Queue based on Redis streams is faster, so if you start with PostgreSQL based queue – you have an option to switch to a more performant implementation later. Though active push notifications will be lost during a switch.  ","version":"v4","tagName":"h3"},{"title":"API description​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#api-description","content":" ","version":"v4","tagName":"h2"},{"title":"device_register​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#device_register","content":" Registers or updates device information.  device_register request​  Field\tType\tRequired\tDescriptionid\tstring\tNo\tID of the device being registered (provide it when updating). provider\tstring\tYes\tProvider of the device token (valid choices: fcm, hms, apns). token\tstring\tYes\tPush notification token for the device. platform\tstring\tYes\tPlatform of the device (valid choices: ios, android, web). user\tstring\tNo\tUser associated with the device. topics\tarray of strings\tNo\tDevice topic subscriptions. This should be a full list which replaces all the topics previously accociated with the device. User topics managed by UserTopic model will be automatically attached. tags\tmap&lt;string, string&gt;\tNo\tAdditional tags for the device (indexed key-value data). meta\tmap&lt;string, string&gt;\tNo\tAdditional metadata for the device (not indexed).  device_register result​  Field Name\tType\tRequired\tDescriptionid\tstring\tYes\tThe device ID that was registered/updated.  ","version":"v4","tagName":"h3"},{"title":"device_update​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#device_update","content":" Call this method to update device. For example, when user logs out the app and you need to detach user ID from the device.  device_update request​  Field\tType\tRequired\tDescriptionids\trepeated string\tNo\tDevice ids to filter users\trepeated string\tNo\tDevice users filter provider_tokens\trepeated DeviceProviderTokens\tNo\tProvider tokens filter user_update\tDeviceUserUpdate\tNo\tOptional user update object meta_update\tDeviceMetaUpdate\tNo\tOptional device meta update object tags_update\tDeviceTagsUpdate\tNo\tOptional device tags update object topics_update\tDeviceChannelsUpdate\tNo\tOptional topics update object  DeviceUserUpdate:  Field\tType\tRequired\tDescriptionuser\tstring\tYes\tUser to set  DeviceMetaUpdate:  Field\tType\tRequired\tDescriptionmeta\tmap&lt;string, string&gt;\tYes\tMeta to set  DeviceTagsUpdate:  Field\tType\tRequired\tDescriptiontags\tmap&lt;string, string&gt;\tYes\tTags to set  DeviceChannelsUpdate:  Field\tType\tRequired\tDescriptiontopics\trepeated string\tYes\tChannels to set  device_update result​  Empty object.  ","version":"v4","tagName":"h3"},{"title":"device_remove​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#device_remove","content":" Removes device from storage. This may be also called when user logs out the app and you don't need its device token after that.  device_remove request​  Field Name\tType\tRequired\tDescriptionids\trepeated string\tNo\tA list of device IDs to be removed users\trepeated string\tNo\tA list of device user IDs to filter devices to remove provider_tokens\tProviderTokens\tNo\tProvider tokens to remove  device_remove result​  Empty object.  ","version":"v4","tagName":"h3"},{"title":"device_list​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#device_list","content":" Returns a paginated list of registered devices according to request filter conditions.  device_list request​  Field\tType\tRequired\tDescriptionids\trepeated string\tNo\tList of device IDs to filter results. providers\trepeated string\tNo\tList of device token providers to filter results. provider_tokens\trepeated ProviderTokens\tNo\tProvider tokens to filter results. platforms\trepeated string\tNo\tList of device platforms to filter results. users\trepeated string\tNo\tList of device users to filter results. since\tstring\tNo\tCursor for pagination (last device id in previous batch, empty for first page). limit\tint32\tNo\tMaximum number of devices to retrieve. include_topics\tbool\tNo\tFlag indicating whether to include topics information for each device. include_tags\tbool\tNo\tFlag indicating whether to include tags information for each device. include_meta\tbool\tNo\tFlag indicating whether to include meta information for each device.  device_list result​  Field Name\tType\tRequired\tDescriptionitems\trepeated Device\tYes\tA list of devices has_more\tbool\tYes\tA flag indicating whether there are more devices available  Device:  Field Name\tType\tDescriptionid\tstring\tThe device's ID. provider\tstring\tThe device's token provider. token\tstring\tThe device's token. platform\tstring\tThe device's platform. user\tstring\tThe user associated with the device. topics\tarray of strings\tOnly included if include_topics was true tags\tmap&lt;string, string&gt;\tOnly included if include_tags was true meta\tmap&lt;string, string&gt;\tOnly included if include_meta was true  ","version":"v4","tagName":"h3"},{"title":"device_topic_update​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#device_topic_update","content":" Manage mapping of device to topics.  device_topic_update request​  Field\tType\tRequired\tDescriptiondevice_id\tstring\tYes\tDevice ID. op\tstring\tYes\tadd or remove or set topics\trepeated string\tNo\tList of topics.  device_topic_update result​  Empty object.  ","version":"v4","tagName":"h3"},{"title":"device_topic_list​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#device_topic_list","content":" List device to topic mapping.  device_topic_list request​  Field\tType\tRequired\tDescriptiondevice_ids\trepeated string\tNo\tList of device IDs to filter results. device_providers\trepeated string\tNo\tList of device token providers to filter results. device_provider_tokens\trepeated ProviderTokens\tNo\tProvider tokens to filter results. device_platforms\trepeated string\tNo\tList of device platforms to filter results. device_users\trepeated string\tNo\tList of device users to filter results. topics\trepeated string\tNo\tList of topics to filter results. topic_prefix\tstring\tNo\tChannel prefix to filter results. since\tstring\tNo\tCursor for pagination (last device id in previous batch, empty for first page). limit\tint32\tNo\tMaximum number of devices to retrieve. include_device\tbool\tNo\tFlag indicating whether to include Device information for each object.  device_topic_list result​  Field Name\tType\tRequired\tDescriptionitems\trepeated DeviceChannel\tYes\tA list of DeviceChannel objects has_more\tbool\tYes\tA flag indicating whether there are more devices available  DeviceChannel:  Field\tType\tRequired\tDescriptionid\tstring\tYes\tID of DeviceChannel device_id\tstring\tYes\tDevice ID topic\tstring\tYes\tChannel  ","version":"v4","tagName":"h3"},{"title":"user_topic_update​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#user_topic_update","content":" Manage mapping of topics with users. These user topics will be automatically attached to user devices upon registering. And removed from device upon deattaching user.  user_topic_update request​  Field\tType\tRequired\tDescriptionuser\tstring\tYes\tUser ID. op\tstring\tYes\tadd or remove or set topics\trepeated string\tNo\tList of topics.  user_topic_update result​  Empty object.  ","version":"v4","tagName":"h3"},{"title":"user_topic_list​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#user_topic_list","content":" List user to topic mapping.  user_topic_list request​  Field\tType\tRequired\tDescriptionusers\trepeated string\tNo\tList of users to filter results. topics\trepeated string\tNo\tList of topics to filter results. topic_prefix\tstring\tNo\tChannel prefix to filter results. since\tstring\tNo\tCursor for pagination (last id in previous batch, empty for first page). limit\tint32\tNo\tMaximum number of UserTopic objects to retrieve.  user_topic_list result​  Field Name\tType\tDescriptionitems\trepeated UserTopic\tA list of UserTopic objects has_more\tbool\tA flag indicating whether there are more devices available  UserTopic:  Field\tType\tRequired\tDescriptionid\tstring\tYes\tID of UserTopic user\tstring\tYes\tUser ID topic\tstring\tYes\tChannel  ","version":"v4","tagName":"h3"},{"title":"send_push_notification​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#send_push_notification","content":" Send push notification to specific device_ids, or to topics, or native provider identifiers like fcm_tokens, or to fcm_topic. Request will be queued by Centrifugo, consumed by Centrifugo built-in workers and sent to the provider API.  send_push_notification request​  Field name\tType\tRequired\tDescriptionrecipient\tPushRecipient\tYes\tRecipient of push notification notification\tPushNotification\tYes\tPush notification to send  PushRecipient (you must set only one of the following fields):  Field\tType\tRequired\tDescriptiondevice_ids\trepeated string\tNo\tSend to a list of device IDs (managed by Centrifugo) topics\trepeated string\tNo\tSend to topics (managed by Centrifugo) fcm_tokens\trepeated string\tNo\tSend to a list of FCM native tokens fcm_topic\tstring\tNo\tSend to a FCM native topic fcm_condition\tstring\tNo\tSend to a FCM native condition hms_tokens\trepeated string\tNo\tSend to a list of HMS native tokens hms_topic\tstring\tNo\tSend to a HMS native topic hms_condition\tstring\tNo\tSend to a HMS native condition apns_tokens\trepeated string\tNo\tSend to a list of APNs native tokens  PushNotification:  Field\tType\tRequired\tDescriptionuid\tstring\tNo\tUnique send id, used for Centrifugo builtin analytics expire_at\tint64\tNo\tUnix timestamp when Centrifugo stops attempting to send this notification (this does not relate to notification TTL fields) fcm\tFcmPushNotification\tNo\tNotification for FCM hms\tHmsPushNotification\tNo\tNotification for HMS apns\tApnsPushNotification\tNo\tNotification for APNs  FcmPushNotification:  Field\tType\tRequired\tDescriptionmessage\tJSON object\tYes\tFCM Message described in FCM docs.  HmsPushNotification:  Field\tType\tRequired\tDescriptionmessage\tJSON object\tYes\tHMS Message described in HMS Push Kit docs.  ApnsPushNotification:  Field\tType\tRequired\tDescriptionheaders\tmap&lt;string, string&gt;\tNo\tAPNs headers payload\tJSON object\tYes\tAPNs payload  send_push_notification result​  Field Name\tType\tDescriptionuid\tstring\tUnique send id, matches uid in request if it was provided  ","version":"v4","tagName":"h3"},{"title":"update_push_status​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#update_push_status","content":" This API call is experimental, some changes may happen here.  Centrifugo PRO also allows tracking status of push notification delivery and interaction. It's possible to use update_push_status API to save the updated status of push notification to the notifications analytics table. Then it's possible to build insights into push notification effectiveness by querying the table.  The update_push_status API supposes that you are using uid field with each notification sent and you are using Centrifugo PRO generated device IDs (as described in steps to integrate).  This is a part of server API at the moment, so you need to proxy requests to this endpoint over your backend. We can consider making this API suitable for requests from the client side – please reach out if your use case requires it.  update_push_status request​  Field\tType\tRequired\tDescriptionuid\tstring\tYes\tuid (unique send id) from send_push_notification status\tstring\tYes\tStatus of push notification - delivered or interacted device_id\tstring\tYes\tDevice ID msg_id\tstring\tNo\tMessage ID  update_push_status result​  Empty object.  ","version":"v4","tagName":"h3"},{"title":"Metrics​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#metrics","content":" Several metrics are available to monitor the state of Centrifugo push worker system:  centrifugo_push_notification_count - counter, shows total count of push notifications sent to providers (splitted by provider, recipient type, platform, success, error code).centrifugo_push_queue_consuming_lag - gauge, shows the lag of queues, should be close to zero most of the time. Splitted by provider and name of queue.centrifugo_push_consuming_inflight_jobs - gauge, shows immediate number of workers proceccing pushes. Splitted by provider and name of queue.centrifugo_push_job_duration_seconds - summary, provides insights about worker job duration timings. Splitted by provider and recipient type.  ","version":"v4","tagName":"h2"},{"title":"Further reading and tutorials​","type":1,"pageTitle":"Push notification API","url":"/docs/4/pro/push_notifications#further-reading-and-tutorials","content":" Coming soon. ","version":"v4","tagName":"h2"},{"title":"Singleflight","type":0,"sectionRef":"#","url":"/docs/4/pro/singleflight","content":"Singleflight Centrifugo PRO provides an additional boolean option use_singleflight (default false). When this option enabled Centrifugo will automatically try to merge identical requests to history, online presence or presence stats issued at the same time into one real network request. It will do this by using in-memory component called singleflight. tip While it can seem similar, singleflight is not a cache. It only combines identical parallel requests into one. If requests come one after another – they will be sent separately to the broker or presence storage. This option can radically reduce a load on a broker in the following situations: Many clients subscribed to the same channel and in case of massive reconnect scenario try to access history simultaneously to restore a state (whether manually using history API or over automatic recovery feature)Many clients subscribed to the same channel and positioning feature is on so Centrifugo tracks client positionMany clients subscribed to the same channel and in case of massive reconnect scenario try to call presence or presence stats simultaneously Using this option only makes sense with remote engine (Redis, KeyDB, Tarantool), it won't provide a benefit in case of using a Memory engine. To enable: config.json { ... &quot;use_singleflight&quot;: true } Or via CENTRIFUGO_USE_SINGLEFLIGHT environment variable.","keywords":"","version":"v4"},{"title":"Analytics with ClickHouse","type":0,"sectionRef":"#","url":"/docs/4/pro/analytics","content":"","keywords":"","version":"v4"},{"title":"Configuration​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#configuration","content":" To enable integration with ClickHouse add the following section to a configuration file:  config.json { ... &quot;clickhouse_analytics&quot;: { &quot;enabled&quot;: true, &quot;clickhouse_dsn&quot;: [ &quot;tcp://127.0.0.1:9000&quot;, &quot;tcp://127.0.0.1:9001&quot;, &quot;tcp://127.0.0.1:9002&quot;, &quot;tcp://127.0.0.1:9003&quot; ], &quot;clickhouse_database&quot;: &quot;centrifugo&quot;, &quot;clickhouse_cluster&quot;: &quot;centrifugo_cluster&quot;, &quot;export_connections&quot;: true, &quot;export_subscriptions&quot;: true, &quot;export_operations&quot;: true, &quot;export_publications&quot;: true, &quot;export_notifications&quot;: true, &quot;export_http_headers&quot;: [ &quot;User-Agent&quot;, &quot;Origin&quot;, &quot;X-Real-Ip&quot; ] } }   All ClickHouse analytics options scoped to clickhouse_analytics section of configuration.  Toggle this feature using enabled boolean option.  tip While we have a nested configuration here it's still possible to use environment variables to set options. For example, use CENTRIFUGO_CLICKHOUSE_ANALYTICS_ENABLED env var name for configure enabled option mentioned above. I.e. nesting expressed as _ in Centrifugo.  Centrifugo can export data to different ClickHouse instances, addresses of ClickHouse can be set over clickhouse_dsn option.  You also need to set a ClickHouse cluster name (clickhouse_cluster) and database name clickhouse_database.  export_connections tells Centrifugo to export connection information snapshots. Information about connection will be exported once a connection established and then periodically while connection alive. See below on table structure to see which fields are available.  export_subscriptions tells Centrifugo to export subscription information snapshots. Information about subscription will be exported once a subscription established and then periodically while connection alive. See below on table structure to see which fields are available.  export_operations tells Centrifugo to export individual client operation information. See below on table structure to see which fields are available.  export_publications tells Centrifugo to export publications for channels to a separate ClickHouse table.  export_notifications tells Centrifugo to export push notifications to a separate ClickHouse table.  export_http_headers is a list of HTTP headers to export for connection information.  export_grpc_metadata is a list of metadata keys to export for connection information for GRPC unidirectional transport.  skip_schema_initialization - boolean, default false. By default Centrifugo tries to initialize table schema on start (if not exists). This flag allows skipping initialization process.  skip_ping_on_start - boolean, default false. Centrifugo pings Clickhouse servers by default on start, if any of servers is unavailable – Centrifugo fails to start. This option allow skipping this check thus Centrifugo is able to start even if Clickhouse cluster not working correctly.  ","version":"v4","tagName":"h2"},{"title":"Connections table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#connections-table","content":" SHOW CREATE TABLE centrifugo.connections; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.connections ( `client` String, `user` String, `name` String, `version` String, `transport` String, `headers` Map(String, Array(String)), `metadata` Map(String, Array(String)), `time` DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/connections', '{replica}') PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.connections_distributed; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.connections_distributed ( `client` String, `user` String, `name` String, `version` String, `transport` String, `headers` Map(String, Array(String)), `metadata` Map(String, Array(String)), `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'connections', murmurHash3_64(client)) │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v4","tagName":"h2"},{"title":"Subscriptions table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#subscriptions-table","content":" SHOW CREATE TABLE centrifugo.subscriptions ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.subscriptions ( `client` String, `user` String, `channels` Array(String), `time` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.subscriptions_distributed; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.subscriptions_distributed ( `client` String, `user` String, `channels` Array(String), `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'subscriptions', murmurHash3_64(client)) │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v4","tagName":"h2"},{"title":"Operations table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#operations-table","content":" SHOW CREATE TABLE centrifugo.operations; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations ( `client` String, `user` String, `op` String, `channel` String, `method` String, `error` UInt32, `disconnect` UInt32, `duration` UInt64, `time` DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/operations', '{replica}') PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.operations_distributed; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations_distributed ( `client` String, `user` String, `op` String, `channel` String, `method` String, `error` UInt32, `disconnect` UInt32, `duration` UInt64, `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'operations', murmurHash3_64(client)) │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v4","tagName":"h2"},{"title":"Publications table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#publications-table","content":" SHOW CREATE TABLE centrifugo.publications ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.publications ( `channel` String, `source` String, `size` UInt64, `client` String, `user` String, `time` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.publications_distributed; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations_distributed ( `channel` String, `source` String, `size` UInt64, `client` String, `user` String, `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'publications', murmurHash3_64(channel)) │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v4","tagName":"h2"},{"title":"Notifications table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#notifications-table","content":" 🚧 This PRO feature is under construction together with push notification API.  SHOW CREATE TABLE centrifugo.notifications ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.notifications ( `uid` String, `provider` String, `type` String, `recipient` String, `device_id` String, `platform` String, `user` String, `msg_id` String, `status` String, `error_message` String, `error_code` String, `time` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.notifications_distributed; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations_distributed ( `uid` String, `provider` String, `type` String, `recipient` String, `device_id` String, `platform` String, `user` String, `msg_id` String, `status` String, `error_message` String, `error_code` String, `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'notifications', murmurHash3_64(uid)) │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v4","tagName":"h2"},{"title":"Query examples​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#query-examples","content":" Show unique users which were connected:  SELECT DISTINCT user FROM centrifugo.connections_distributed; ┌─user─────┐ │ user_1 │ │ user_2 │ │ user_3 │ │ user_4 │ │ user_5 │ └──────────┘   Show total number of publication attempts which were throttled by Centrifugo (received Too many requests error with code 111):  SELECT COUNT(*) FROM centrifugo.operations_distributed WHERE (error = 111) AND (op = 'publish'); ┌─count()─┐ │ 4502 │ └─────────┘   The same for a specific user:  SELECT COUNT(*) FROM centrifugo.operations_distributed WHERE (error = 111) AND (op = 'publish') AND (user = 'user_200'); ┌─count()─┐ │ 1214 │ └─────────┘   Show number of unique users subscribed to a specific channel in last 5 minutes (this is approximate since subscriptions table contain periodic snapshot entries, clients could unsubscribe in between snapshots – this is reflected in operations table):  SELECT COUNT(Distinct(user)) FROM centrifugo.subscriptions_distributed WHERE arrayExists(x -&gt; (x = 'chat:index'), channels) AND (time &gt;= (now() - toIntervalMinute(5))); ┌─uniqExact(user)─┐ │ 101 │ └─────────────────┘   Show top 10 users which called publish operation during last one minute:  SELECT COUNT(op) AS num_ops, user FROM centrifugo.operations_distributed WHERE (op = 'publish') AND (time &gt;= (now() - toIntervalMinute(1))) GROUP BY user ORDER BY num_ops DESC LIMIT 10; ┌─num_ops─┬─user─────┐ │ 56 │ user_200 │ │ 11 │ user_75 │ │ 6 │ user_87 │ │ 6 │ user_65 │ │ 6 │ user_39 │ │ 5 │ user_28 │ │ 5 │ user_63 │ │ 5 │ user_89 │ │ 3 │ user_32 │ │ 3 │ user_52 │ └─────────┴──────────┘   Show total number of push notifications to iOS devices sent during last 24 hours:  SELECT COUNT(*) FROM centrifugo.notifications WHERE (time &gt; (now() - toIntervalHour(24))) AND (platform = 'ios') ┌─count()─┐ │ 31200 │ └─────────┘   ","version":"v4","tagName":"h2"},{"title":"Development​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#development","content":" The recommended way to run ClickHouse in production is with cluster. See an example of such cluster configuration made with Docker Compose.  But during development you may want to run Centrifugo with single instance ClickHouse.  To do this set only one ClickHouse dsn and do not set cluster name:  config.json { ... &quot;clickhouse_analytics&quot;: { &quot;enabled&quot;: true, &quot;clickhouse_dsn&quot;: [ &quot;tcp://127.0.0.1:9000&quot; ], &quot;clickhouse_database&quot;: &quot;centrifugo&quot;, &quot;clickhouse_cluster&quot;: &quot;&quot;, &quot;export_connections&quot;: true, &quot;export_subscriptions&quot;: true, &quot;export_publications&quot;: true, &quot;export_operations&quot;: true, &quot;export_http_headers&quot;: [ &quot;Origin&quot;, &quot;User-Agent&quot; ] } }   Run ClickHouse locally:  docker run -it --rm -v /tmp/clickhouse:/var/lib/clickhouse -p 9000:9000 --name click clickhouse/clickhouse-server   Run ClickHouse client:  docker run -it --rm --link click:clickhouse-server --entrypoint clickhouse-client clickhouse/clickhouse-server --host clickhouse-server   Issue queries:  :) SELECT * FROM centrifugo.operations ┌─client───────────────────────────────┬─user─┬─op──────────┬─channel─────┬─method─┬─error─┬─disconnect─┬─duration─┬────────────────time─┐ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ connecting │ │ │ 0 │ 0 │ 217894 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ connect │ │ │ 0 │ 0 │ 0 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ $chat:index │ │ 0 │ 0 │ 92714 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ presence │ $chat:index │ │ 0 │ 0 │ 3539 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test1 │ │ 0 │ 0 │ 2402 │ 2021-07-31 08:15:12 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test2 │ │ 0 │ 0 │ 634 │ 2021-07-31 08:15:12 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test3 │ │ 0 │ 0 │ 412 │ 2021-07-31 08:15:12 │ └──────────────────────────────────────┴──────┴─────────────┴─────────────┴────────┴───────┴────────────┴──────────┴─────────────────────┘   ","version":"v4","tagName":"h2"},{"title":"How export works​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/4/pro/analytics#how-export-works","content":" When ClickHouse analytics enabled Centrifugo nodes start exporting events to ClickHouse. Each node issues insert with events once in 10 seconds (flushing collected events in batches thus making insertion in ClickHouse efficient). Maximum batch size is 100k for each table at the momemt. If insert to ClickHouse failed Centrifugo retries it once and then buffers events in memory (up to 1 million entries). If ClickHouse still unavailable after collecting 1 million events then new events will be dropped until buffer has space. These limits are configurable. Centrifugo PRO uses very efficient code for writing data to ClickHouse, so analytics feature should only add a little overhead for Centrifugo node.  Several metrics are exposed to monitor export process health:  centrifugo_clickhouse_analytics_flush_duration_seconds summarycentrifugo_clickhouse_analytics_batch_size summarycentrifugo_clickhouse_analytics_drop_count counter ","version":"v4","tagName":"h2"},{"title":"Operation throttling","type":0,"sectionRef":"#","url":"/docs/4/pro/throttling","content":"","keywords":"","version":"v4"},{"title":"In-memory per connection throttling​","type":1,"pageTitle":"Operation throttling","url":"/docs/4/pro/throttling#in-memory-per-connection-throttling","content":" In-memory throttling is an efficient way to limit number of operations allowed on a per-connection basis – i.e. inside each individual real-time connection. Our throttling implementation uses token bucket algorithm internally.  The list of operations which can be throttled on a per-connection level is:  subscribepublishhistorypresencepresence_statsrefreshsub_refreshrpc (with optional method resolution)  In addition, Centrifugo allows defining two special buckets containers:  total – define it to limit the total number of commands per interval (all commands sent from client count), these buckets will always be checked if defined, every command from the client always consumes token from total bucketsdefault - define it if you don't want to configure some command buckets explicitly, default buckets will be used in case command buckets is not configured explicitly.  config.json { ... &quot;client_command_throttling&quot;: { &quot;enabled&quot;: true, &quot;default&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 60 }, ] }, &quot;total&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 20 }, { &quot;interval&quot;: &quot;60s&quot;, &quot;rate&quot;: 50 }, ] }, &quot;publish&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1 }, ] }, &quot;rpc&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 10 } ], &quot;method_override&quot;: [ { &quot;method&quot;: &quot;updateActiveStatus&quot;, &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;20s&quot;, &quot;rate&quot;: 1 } ] } ] } } }   tip Centrifugo real-time SDKs written in a way that if client receives an error during connect – it will try to reconnect to a server with backoff algorithm. The same for subscribing to channels (i.e. error from subscribe command) – subscription request will be retried with a backoff. Refresh and subscription refresh will be also retried automatically by SDK upon errors after in several seconds. Retries of other commands should be handled manually from the client side if needed – though usually you should choose throttling limits in a way that normal users of your app never hit the limits.  ","version":"v4","tagName":"h2"},{"title":"In-memory per user throttling​","type":1,"pageTitle":"Operation throttling","url":"/docs/4/pro/throttling#in-memory-per-user-throttling","content":" Another type of throttling in Centrifugo PRO is a per user ID in-memory throttling. Like per client throttling this one is also very efficient since also uses in-memory token buckets. The difference is that instead of throttling per individual client this type of throttling takes user ID into account.  This type of throttling only checks commands coming from authenticated users – i.e. with non-empty user ID set. Requests from anonymous users can't be throttled with it.  The list of operations which can be throttled is similar to the in-memory throttling described above. But with additional connect method:  totaldefaultconnectsubscribepublishhistorypresencepresence_statsrefreshsub_refreshrpc (with optional method resolution)  The configuration is very similar:  config.json { ... &quot;user_command_throttling&quot;: { &quot;enabled&quot;: true, &quot;default&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 60 }, ] }, &quot;publish&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1 } ] }, &quot;rpc&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 10 } ], &quot;method_override&quot;: [ { &quot;method&quot;: &quot;updateActiveStatus&quot;, &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;20s&quot;, &quot;rate&quot;: 1 } ] } ] } } }   ","version":"v4","tagName":"h2"},{"title":"Redis per user throttling​","type":1,"pageTitle":"Operation throttling","url":"/docs/4/pro/throttling#redis-per-user-throttling","content":" The next type of throttling in Centrifugo PRO is a distributed per user ID throttling with Redis as a bucket state storage. In this case limits are global for the entire Centrifugo cluster. If one user executed two commands on different Centrifugo nodes, Centrifugo consumes two tokens from the same bucket kept in Redis. Since this throttling goes to Redis to check limits, it adds some latency to a command processing. Our implementation tries to provide good throughput characteristics though – in our tests single Redis instance can handle more than 100k limit check requests per second. And it's possible to scale Redis in the same ways as for Centrifugo Redis Engine.  This type of throttling only checks commands coming from authenticated users – i.e. with non-empty user ID set. Requests from anonymous users can't be throttled with it. The implementation also uses token bucket algorithm internally.  The list of operations which can be throttled is similar to the in-memory user command throttling described above. But without special bucket total:  defaultconnectsubscribepublishhistorypresencepresence_statsrefreshsub_refreshrpc (with optional method resolution)  The configuration is very similar:  config.json { ... &quot;redis_user_command_throttling&quot;: { &quot;enabled&quot;: true, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;default&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 60 }, ] }, &quot;publish&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1 } ] }, &quot;rpc&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 10 } ], &quot;method_override&quot;: [ { &quot;method&quot;: &quot;updateActiveStatus&quot;, &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;20s&quot;, &quot;rate&quot;: 1 } ] } ] } } }   Redis configuration for throttling feature matches Centrifugo Redis engine configuration. So Centrifugo supports client-side consistent sharding to scale Redis, Redis Sentinel, Redis Cluster for throttling feature too.  It's also possible to reuse Centrifugo Redis engine by setting use_redis_from_engine option instead of custom throttling Redis configuration declaration, like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;redis_user_command_throttling&quot;: { &quot;enabled&quot;: true, &quot;use_redis_from_engine&quot;: true, ... } }   In this case throttling will simply connect to Redis instances configured for an Engine.  ","version":"v4","tagName":"h2"},{"title":"Disconnecting abusive or misbehaving connections​","type":1,"pageTitle":"Operation throttling","url":"/docs/4/pro/throttling#disconnecting-abusive-or-misbehaving-connections","content":" Above we showed how you can define throttling strategies to protect server resources and prevent execution of many commands inside the connection and from certain user.  But there are scenarios when abusive or broken connections may generate a significant load on the server just by calling commands and getting error responses due to throttling or due to other reasons (like malformed command). Centrifugo PRO provides a way to configure error limits per connection to deal with this case.  Error limits are configured as in-memory buckets operating on a per-connection level. When these buckets are full due to lots of errors for an individual connection Centrifugo disconnects the client (with advice to not reconnect, so our SDKs may follow it). This way it's possible to get rid of the connection and rely on HTTP infrastracture tools to deal with client reconnections. Since WebSocket or other our transports (except unidirectional GRPC, but it's usually not available to the public port) are HTTP-based (or start with HTTP request in WebSocket Upgrade case) – developers can use Nginx limit_req_zone directive, Cloudflare rules, iptables, and so on, to protect Centrifugo from unwanted connections.  tip Centrifugo PRO does not count internal errors for the error limit buckets – as internal errors is usually not a client's fault.  The configuration on error limits per connection may look like this:  config.json { ... &quot;client_error_limits&quot;: { &quot;enabled&quot;: true, &quot;total&quot;: { &quot;buckets&quot; : [ { &quot;interval&quot;: &quot;5s&quot;, &quot;rate&quot;: 20 } ] } } }  ","version":"v4","tagName":"h2"},{"title":"Token revocation API","type":0,"sectionRef":"#","url":"/docs/4/pro/token_revocation","content":"","keywords":"","version":"v4"},{"title":"How it works​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#how-it-works","content":" By default, information about token revocations shared throughout Centrifugo cluster and kept in a process memory. So token revocation information will be lost upon Centrifugo restart.  But it's possible to enable revocation information persistence by configuring a persistence storage – in this case token revocation information will survive Centrifugo restarts.  Centrifugo also automatically expires entries in the storage to keep working set reasonably small. Keeping pool of revoked tokens small allows avoiding expensive database lookups on every check – information is loaded periodically from the database and all checks performed over in-memory data structure – thus token revocation checks are cheap and have a small impact on the overall system performance.  ","version":"v4","tagName":"h2"},{"title":"Configure​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#configure","content":" Token revocation features (both revocation by token ID and user token invalidation by issue time) are enabled by default in Centrifugo PRO (as soon as your JWTs has jti and iat claims you will be able to use revocation APIs). By default revocation information kept in a process memory.  There are two types of persistent engines supported at the moment:  redisdatabase  ","version":"v4","tagName":"h2"},{"title":"Redis persistence engine​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#redis-persistence-engine","content":" Revocation data can be kept in Redis. To enable this configuration should be:  { ... &quot;token_revoke&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; }, &quot;user_tokens_invalidate&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; } }   danger Unlike many other Redis features in Centrifugo consistent sharding is not supported for revocation data. The reason is that we don't want to loose revocation information when additional Redis node added. So only one Redis shard can be provided for token_revoke and user_tokens_invalidate features. This should be fine given that working set of revoked entities should be reasonably small and old entries expire. If you try to set several Redis shards here Centrifugo will exit with an error on start.  caution One more thing you may notice is that Redis configuration here does not have use_redis_from_engine option. The reason is that since Redis is not shardable here reusing Redis configuration here could cause problems at the moment of main Redis scaling – which we want to avoid thus require explicit configuration here.  ","version":"v4","tagName":"h3"},{"title":"Database persistence engine​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#database-persistence-engine","content":" Revocation data can be kept in the relational database. Only PostgreSQL is supported.  To enable this configuration should be like:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;token_revoke&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; }, &quot;user_tokens_invalidate&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; } }   ","version":"v4","tagName":"h3"},{"title":"Revoke token API​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#revoke-token-api","content":" ","version":"v4","tagName":"h2"},{"title":"revoke_token​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#revoke_token","content":" Allows revoking individual tokens. For example, this may be useful when token leakage has been detected and you want to revoke access for a particular tokens. BTW Centrifugo PRO provides user_connections API which has an information about tokens for active users connections (if set in JWT).  caution This API assumes that JWTs you are using contain &quot;jti&quot; claim which is a unique token ID (according to RFC).  Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;revoke_token&quot;, &quot;params&quot;: {&quot;uid&quot;: &quot;xxx-xxx-xxx&quot;, &quot;expire_at&quot;: 1635845122}}' \\ http://localhost:8000/api   revoke_token params​  Parameter name\tParameter type\tRequired\tDescriptionuid\tstring\tyes\tToken unique ID (JTI claim in case of JWT) expire_at\tint\tno\tUnix time in the future when revocation information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time (matching the expiration time of your JWTs) to keep working set of revocations small (since Centrifugo nodes periodically load all entries from the database table to construct in-memory cache).  revoke_token result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"Invalidate user tokens API​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#invalidate-user-tokens-api","content":" ","version":"v4","tagName":"h2"},{"title":"invalidate_user_tokens​","type":1,"pageTitle":"Token revocation API","url":"/docs/4/pro/token_revocation#invalidate_user_tokens","content":" Allows revoking all tokens for a user which were issued before a certain time. For example, this may be useful after user changed a password in an application.  caution This API assumes that JWTs you are using contain &quot;iat&quot; claim which is a time token was issued at (according to RFC).  Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;invalidate_user_tokens&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;test&quot;, &quot;issued_before&quot;: 1635845022, &quot;expire_at&quot;: 1635845122}}' \\ http://localhost:8000/api   invalidate_user_tokens params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to invalidate tokens for issued_before\tint\tno\tAll tokens issued at before this Unix time will be considered revoked (in case of JWT this requires iat to be properly set in JWT), if not provided server uses current time expire_at\tint\tno\tUnix time in the future when revocation information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time (matching the expiration time of your JWTs) to keep working set of revocations small (since Centrifugo nodes periodically load all entries from the database table to construct in-memory cache).  invalidate_user_tokens result​  Empty object. ","version":"v4","tagName":"h3"},{"title":"User and channel tracing","type":0,"sectionRef":"#","url":"/docs/4/pro/tracing","content":"","keywords":"","version":"v4"},{"title":"Save to a file​","type":1,"pageTitle":"User and channel tracing","url":"/docs/4/pro/tracing#save-to-a-file","content":" It's possible to connect to the admin tracing endpoint with CURL using the admin session token. And then save tracing output to a file for later processing.  curl -X POST http://localhost:8000/admin/trace -H &quot;Authorization: token &lt;ADMIN_AUTH_TOKEN&gt;&quot; -d '{&quot;type&quot;: &quot;user&quot;, &quot;entity&quot;: &quot;56&quot;}' -o trace.txt   Currently, you should copy the admin auth token from browser developer tools, this may be improved in the future as PRO version evolves. ","version":"v4","tagName":"h3"},{"title":"User blocking API","type":0,"sectionRef":"#","url":"/docs/4/pro/user_block","content":"","keywords":"","version":"v4"},{"title":"How it works​","type":1,"pageTitle":"User blocking API","url":"/docs/4/pro/user_block#how-it-works","content":" By default, information about user block/unblock requests shared throughout Centrifugo cluster and kept in memory. So user will be blocked until Centrifugo restart.  But it's possible to enable blocking information persistence by configuring a persistence storage – in this case information will survive Centrifugo restarts.  Centrifugo also automatically expires entries in the storage to keep working set of blocked users reasonably small. Keeping pool of blocked users small allows avoiding expensive database lookups on every check – information is loaded periodically from the storage and all checks performed over in-memory data structure – thus user blocking checks are cheap and have a small impact on the overall system performance.  ","version":"v4","tagName":"h2"},{"title":"Configure​","type":1,"pageTitle":"User blocking API","url":"/docs/4/pro/user_block#configure","content":" User block feature is enabled by default in Centrifugo PRO (blocking information will be stored in process memory). To keep blocking information persistently you need to configure persistence engine.  There are two types of persistent engines supported at the moment:  redisdatabase  ","version":"v4","tagName":"h2"},{"title":"Redis persistence engine​","type":1,"pageTitle":"User blocking API","url":"/docs/4/pro/user_block#redis-persistence-engine","content":" Blocking data can be kept in Redis. To enable this configuration should be:  { ... &quot;user_block&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; } }   danger Unlike many other Redis features in Centrifugo consistent sharding is not supported for blocking data. The reason is that we don't want to loose blocking information when additional Redis node added. So only one Redis shard can be provided for user_block feature. This should be fine given that working set of blocked users should be reasonably small and old entries expire. If you try to set several Redis shards here Centrifugo will exit with an error on start.  caution One more thing you may notice is that Redis configuration here does not have use_redis_from_engine option. The reason is that since Redis is not shardable here reusing Redis configuration here could cause problems at the moment of Redis scaling – which we want to avoid thus require explicit configuration here.  ","version":"v4","tagName":"h3"},{"title":"Database persistence engine​","type":1,"pageTitle":"User blocking API","url":"/docs/4/pro/user_block#database-persistence-engine","content":" Blocking data can be kept in the relational database. Only PostgreSQL is supported.  To enable this configuration should be like:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;user_block&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; } }   tip To quickly start local PostgreSQL database: docker run -it --rm -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=pass -p 5432:5432 postgres:15   ","version":"v4","tagName":"h3"},{"title":"Block API​","type":1,"pageTitle":"User blocking API","url":"/docs/4/pro/user_block#block--api","content":" ","version":"v4","tagName":"h2"},{"title":"block_user​","type":1,"pageTitle":"User blocking API","url":"/docs/4/pro/user_block#block_user","content":" Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;block_user&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;2695&quot;, &quot;expire_at&quot;: 1635845122}}' \\ http://localhost:8000/api   block_user params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to block expire_at\tint\tno\tUnix time in the future when user blocking information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time to keep working set of blocked users small (since Centrifugo nodes periodically load all entries from the storage to construct in-memory cache).  block_user result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"unblock_user​","type":1,"pageTitle":"User blocking API","url":"/docs/4/pro/user_block#unblock_user","content":" Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;unblock_user&quot;, &quot;params&quot;: {&quot;user&quot;: &quot;2695&quot;}}' \\ http://localhost:8000/api   unblock_user params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to unblock  unblock_user result​  Empty object at the moment. ","version":"v4","tagName":"h3"},{"title":"User status API","type":0,"sectionRef":"#","url":"/docs/4/pro/user_status","content":"","keywords":"","version":"v4"},{"title":"Client-side status update RPC​","type":1,"pageTitle":"User status API","url":"/docs/4/pro/user_status#client-side-status-update-rpc","content":" Centrifugo PRO provides a built-in RPC method of client API called update_user_status. Call it with empty parameters from a client side whenever user performs a useful action that proves it's active status in your app. For example, in Javascript:  await centrifuge.rpc('update_user_status', {});   note Don't forget to debounce this method calls on a client side to avoid exposing RPC on every mouse move event for example.  This RPC call sets user's last active time value in Redis (with sharding and Cluster support). Information about active status will be kept in Redis for a configured time interval, then expire.  ","version":"v4","tagName":"h3"},{"title":"update_user_status server API​","type":1,"pageTitle":"User status API","url":"/docs/4/pro/user_status#update_user_status-server-api","content":" It's also possible to call update_user_status using Centrifugo server API (for example if you want to force status during application development or you want to proxy status updates over your app backend when using unidirectional transports):  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;update_user_status&quot;, &quot;params&quot;: {&quot;users&quot;: [&quot;42&quot;]}}' \\ http://localhost:8000/api   Update user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to update status for  Update user status result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"get_user_status server API​","type":1,"pageTitle":"User status API","url":"/docs/4/pro/user_status#get_user_status-server-api","content":" Now on a backend side you have access to a bulk API to effectively get status of particular users.  Call RPC method of server API (over HTTP or GRPC):  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;get_user_status&quot;, &quot;params&quot;: {&quot;users&quot;: [&quot;42&quot;]}}' \\ http://localhost:8000/api   You should get a response like this:  { &quot;result&quot;:{ &quot;statuses&quot;:[ { &quot;user&quot;:&quot;42&quot;, &quot;active&quot;:1627107289, &quot;online&quot;:1627107289 } ] } }   In case information about last status update time not available the response will be like this:  { &quot;result&quot;:{ &quot;statuses&quot;:[ { &quot;user&quot;:&quot;42&quot; } ] } }   I.e. status object will present in a response but active field won't be set for status object.  Note that Centrifugo also maintains online field inside user status object. This field updated periodically by Centrifugo itself while user has active connection with a server. So you can draw away statuses in your application: i.e. when user connected (online time) but not using application for a long time (active time).  Get user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to get status for  Get user status result​  Field name\tField type\tOptional\tDescriptionstatuses\tarray of UserStatus\tno\tStatuses for each user in params (same order)  UserStatus​  Field name\tField type\tOptional\tDescriptionuser\tstring\tno\tUser ID active\tinteger\tyes\tLast active time (Unix seconds) online\tinteger\tyes\tLast online time (Unix seconds)  ","version":"v4","tagName":"h3"},{"title":"delete_user_status server API​","type":1,"pageTitle":"User status API","url":"/docs/4/pro/user_status#delete_user_status-server-api","content":" If you need to clear user status information for some reason there is a delete_user_status server API call:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;delete_user_status&quot;, &quot;params&quot;: {&quot;users&quot;: [&quot;42&quot;]}}' \\ http://localhost:8000/api   Delete user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to delete status for  Delete user status result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"User status API","url":"/docs/4/pro/user_status#configuration","content":" To enable Redis user status feature:  config.json { ... &quot;user_status&quot;: { &quot;enabled&quot;: true, &quot;redis_address&quot;: &quot;127.0.0.1:6379&quot; } }   Redis configuration for user status feature matches Centrifugo Redis engine configuration. So Centrifugo supports client-side consistent sharding to scale Redis, Redis Sentinel, Redis Cluster for user status feature too.  It's also possible to reuse Centrifugo Redis engine by setting use_redis_from_engine option instead of custom throttling Redis address declaration, like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;user_status&quot;: { &quot;enabled&quot;: true, &quot;use_redis_from_engine&quot;: true, } }   In this case Redis active status will simply connect to Redis instances configured for Centrifugo Redis engine.  expire_interval is a duration for how long Redis keys will be kept for each user. Expiration time extended on every update. By default expiration time is 31 day. To set it to 1 day:  config.json { ... &quot;user_status&quot;: { ... &quot;expire_interval&quot;: &quot;24h&quot; } }  ","version":"v4","tagName":"h3"},{"title":"Admin web UI","type":0,"sectionRef":"#","url":"/docs/4/server/admin_web","content":"","keywords":"","version":"v4"},{"title":"Options​","type":1,"pageTitle":"Admin web UI","url":"/docs/4/server/admin_web#options","content":" admin (boolean, default: false) – enables/disables admin web UIadmin_password (string, default: &quot;&quot;) – this is a password to log into admin web interfaceadmin_secret (string, default: &quot;&quot;) - this is a secret key for authentication token set on successful login.  Make both admin_password and admin_secret strong and keep them in secret.  After configuring, restart Centrifugo and go to http://localhost:8000 (by default) - you should see web interface.  tip Although there is a password based authentication a good advice is to protect web interface by firewall rules in production.    ","version":"v4","tagName":"h2"},{"title":"Using custom web interface​","type":1,"pageTitle":"Admin web UI","url":"/docs/4/server/admin_web#using-custom-web-interface","content":" If you want to use custom web interface you can specify path to web interface directory dist:  config.json { ..., &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;&lt;PASSWORD&gt;&quot;, &quot;admin_secret&quot;: &quot;&lt;SECRET&gt;&quot;, &quot;admin_web_path&quot;: &quot;&lt;PATH_TO_WEB_DIST&gt;&quot; }   This can be useful if you want to modify official web interface code in some way and test it with Centrifugo.  ","version":"v4","tagName":"h2"},{"title":"Admin insecure mode​","type":1,"pageTitle":"Admin web UI","url":"/docs/4/server/admin_web#admin-insecure-mode","content":" There is also an option to run Centrifugo in insecure admin mode - in this case you don't need to set admin_password and admin_secret in config – in web interface you will be logged in automatically without any password. Note that this is only an option for production if you protected admin web interface with firewall rules. Otherwise anyone in internet will have full access to admin functionality described above. To enable insecure admin mode:  config.json { ..., &quot;admin&quot;: true, &quot;admin_insecure&quot;: true, &quot;admin_password&quot;: &quot;&lt;PASSWORD&gt;&quot;, &quot;admin_secret&quot;: &quot;&lt;SECRET&gt;&quot; }  ","version":"v4","tagName":"h2"},{"title":"Client JWT authentication","type":0,"sectionRef":"#","url":"/docs/4/server/authentication","content":"","keywords":"","version":"v4"},{"title":"Connection JWT claims​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#connection-jwt-claims","content":" For connection JWT Centrifugo uses the some standart claims defined in rfc7519, also some custom Centrifugo-specific.  ","version":"v4","tagName":"h2"},{"title":"sub​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#sub","content":" This is a standard JWT claim which must contain an ID of the current application user (as string).  If a user is not currently authenticated in an application, but you want to let him connect to Centrifugo anyway – you can use an empty string as a user ID in sub claim. This is called anonymous access. In this case, you may need to enable corresponding channel namespace options which enable access to protocol features for anonymous users.  ","version":"v4","tagName":"h3"},{"title":"exp​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#exp","content":" This is a UNIX timestamp seconds when the token will expire. This is a standard JWT claim - all JWT libraries for different languages provide an API to set it.  If exp claim is not provided then Centrifugo won't expire connection. When provided special algorithm will find connections with exp in the past and activate the connection refresh mechanism. Refresh mechanism allows connection to survive and be prolonged. In case of refresh failure, the client connection will be eventually closed by Centrifugo and won't be accepted until new valid and actual credentials are provided in the connection token.  You can use the connection expiration mechanism in cases when you don't want users of your app to be subscribed on channels after being banned/deactivated in the application. Or to protect your users from token leakage (providing a reasonably short time of expiration).  Choose exp value wisely, you don't need small values because the refresh mechanism will hit your application often with refresh requests. But setting this value too large can lead to slow user connection deactivation. This is a trade-off.  Read more about connection expiration below.  ","version":"v4","tagName":"h3"},{"title":"iat​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#iat","content":" This is a UNIX time when token was issued (seconds). See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v4","tagName":"h3"},{"title":"jti​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#jti","content":" This is a token unique ID. See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v4","tagName":"h3"},{"title":"aud​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#aud","content":" By default, Centrifugo does not check JWT audience (rfc7519 aud claim).  But you can force this check by setting token_audience string option:  config.json { &quot;token_audience&quot;: &quot;centrifugo&quot; }   caution Setting token_audience will also affect subscription tokens (used for channel token authorization). Please read this issue and reach out if your use case requires separate configuration for subscription tokens.  ","version":"v4","tagName":"h3"},{"title":"iss​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#iss","content":" By default, Centrifugo does not check JWT issuer (rfc7519 iss claim).  But you can force this check by setting token_issuer string option:  config.json { &quot;token_issuer&quot;: &quot;my_app&quot; }   caution Setting token_issuer will also affect subscription tokens (used for channel token authorization). Please read this issue and reach out if your use case requires separate configuration for subscription tokens.  ","version":"v4","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#info","content":" This claim is optional - this is additional information about client connection that can be provided for Centrifugo. This information will be included in presence information, join/leave events, and channel publication if it was published from a client-side.  ","version":"v4","tagName":"h3"},{"title":"b64info​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#b64info","content":" If you are using binary Protobuf protocol you may want info to be custom bytes. Use this field in this case.  This field contains a base64 representation of your bytes. After receiving Centrifugo will decode base64 back to bytes and will embed the result into various places described above.  ","version":"v4","tagName":"h3"},{"title":"channels​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#channels","content":" An optional array of strings with server-side channels to subscribe a client to. See more details about server-side subscriptions.  ","version":"v4","tagName":"h3"},{"title":"subs​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#subs","content":" An optional map of channels with options. This is like a channels claim but allows more control over server-side subscription since every channel can be annotated with info, data, and so on using options.  tip This claim is called subs as a shortcut from subscriptions. The claim sub described above is a standart JWT claim to provide a user ID (it's a shortcut from subject). While claims have similar names they have different purpose in a connection JWT.  Example:  { ... &quot;subs&quot;: { &quot;channel1&quot;: { &quot;data&quot;: {&quot;welcome&quot;: &quot;welcome to channel1&quot;} }, &quot;channel2&quot;: { &quot;data&quot;: {&quot;welcome&quot;: &quot;welcome to channel2&quot;} } } }   Subscribe options:​  Field\tType\tOptional\tDescriptioninfo\tJSON object\tyes\tCustom channel info b64info\tstring\tyes\tCustom channel info in Base64 - to pass binary channel info data\tJSON object\tyes\tCustom JSON data to return in subscription context inside Connect reply b64data\tstring\tyes\tSame as data but in Base64 to send binary data override\tOverride object\tyes\tAllows dynamically override some channel options defined in Centrifugo configuration on a per-connection basis (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave position\tBoolValue\tyes\tOverride position recover\tBoolValue\tyes\tOverride recover  BoolValue is an object like this:  { &quot;value&quot;: true/false }   ","version":"v4","tagName":"h3"},{"title":"meta​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#meta","content":" Meta is an additional JSON object (ex. {&quot;key&quot;: &quot;value&quot;}) that will be attached to a connection. Unlike info it's never exposed to clients inside presence and join/leave payloads and only accessible on a backend side. It may be included in proxy calls from Centrifugo to the application backend (see proxy_include_connection_meta option). Also, there is a connections API method in Centrifugo PRO that returns this data in the connection description object.  ","version":"v4","tagName":"h3"},{"title":"expire_at​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#expire_at","content":" By default, Centrifugo looks on exp claim to configure connection expiration. In most cases this is fine, but there could be situations where you wish to decouple token expiration check with connection expiration time. As soon as the expire_at claim is provided (set) in JWT Centrifugo relies on it for setting connection expiration time (JWT expiration still checked over exp though).  expire_at is a UNIX timestamp seconds when the connection should expire.  Set it to the future time for expiring connection at some pointSet it to 0 to disable connection expiration (but still check token exp claim).  ","version":"v4","tagName":"h3"},{"title":"Connection expiration​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#connection-expiration","content":" As said above exp claim in a connection token allows expiring client connection at some point in time. Let's look in detail at what happens when Centrifugo detects that the connection is going to expire.  First, you should do is enable client expiration mechanism in Centrifugo providing a connection JWT with expiration:  import jwt import time token = jwt.encode({&quot;sub&quot;: &quot;42&quot;, &quot;exp&quot;: int(time.time()) + 10*60}, &quot;secret&quot;).decode() print(token)   Let's suppose that you set exp field to timestamp that will expire in 10 minutes and the client connected to Centrifugo with this token. During 10 minutes the connection will be kept by Centrifugo. When this time passed Centrifugo gives the connection some time (configured, 25 seconds by default) to refresh its credentials and provide a new valid token with new exp.  When a client first connects to Centrifugo it receives the ttl value in connect reply. That ttl value contains the number of seconds after which the client must send the refresh command with new credentials to Centrifugo. Centrifugo clients must handle this ttl field and automatically start the refresh process.  For example, a Javascript browser client will send an AJAX POST request to your application when it's time to refresh credentials. By default, this request goes to /centrifuge/refresh URL endpoint. In response your server must return JSON with a new connection JWT:  { &quot;token&quot;: token }   So you must just return the same connection JWT for your user when rendering the page initially. But with actual valid exp. Javascript client will then send them to Centrifugo server and connection will be refreshed for a time you set in exp.  In this case, you know which user wants to refresh its connection because this is just a general request to your app - so your session mechanism will tell you about the user.  If you don't want to refresh the connection for this user - just return 403 Forbidden on refresh request to your application backend.  Javascript client also has options to hook into a refresh mechanism to implement your custom way of refreshing. Other Centrifugo clients also should have hooks to refresh credentials but depending on client API for this can be different - see specific client docs.  ","version":"v4","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#examples","content":" Let's look at how to generate connection HS256 JWT in Python:  ","version":"v4","tagName":"h2"},{"title":"Simplest token​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#simplest-token","content":"     PythonNodeJS import jwt token = jwt.encode({&quot;sub&quot;: &quot;42&quot;}, &quot;secret&quot;).decode() print(token)   Note that we use the value of token_hmac_secret_key from Centrifugo config here (in this case token_hmac_secret_key value is just secret). The only two who must know the HMAC secret key is your application backend which generates JWT and Centrifugo. You should never reveal the HMAC secret key to your users.  Then you can pass this token to your client side and use it when connecting to Centrifugo:  Using centrifuge-js v3 var centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, { token: token }); centrifuge.connect();   See more details about working with connection tokens and handling token expiration on the client-side in the real-time SDK API spec.  ","version":"v4","tagName":"h3"},{"title":"Token with expiration​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#token-with-expiration","content":" HS256 token that will be valid for 5 minutes:  PythonNodeJS import jwt import time claims = {&quot;sub&quot;: &quot;42&quot;, &quot;exp&quot;: int(time.time()) + 5*60} token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   ","version":"v4","tagName":"h3"},{"title":"Token with additional connection info​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#token-with-additional-connection-info","content":" Let's attach user name:  PythonNodeJS import jwt claims = {&quot;sub&quot;: &quot;42&quot;, &quot;info&quot;: {&quot;name&quot;: &quot;Alexander Emelin&quot;}} token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   ","version":"v4","tagName":"h3"},{"title":"Investigating problems with JWT​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#investigating-problems-with-jwt","content":" You can use jwt.io site to investigate the contents of your tokens. Also, server logs usually contain some useful information.  ","version":"v4","tagName":"h3"},{"title":"JSON Web Key support​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#json-web-key-support","content":" Centrifugo supports JSON Web Key (JWK) spec. This means that it's possible to improve JWT security by providing an endpoint to Centrifugo from where to load JWK (by looking at kid header of JWT).  A mechanism can be enabled by providing token_jwks_public_endpoint string option to Centrifugo (HTTP address).  As soon as token_jwks_public_endpoint set all tokens will be verified using JSON Web Key Set loaded from JWKS endpoint. This makes it impossible to use non-JWK based tokens to connect and subscribe to private channels.  tip Read a tutorial in our blog about using Centrifugo with Keycloak SSO. In that case connection tokens are verified using public key loaded from the JWKS endpoint of Keycloak.  At the moment Centrifugo caches keys loaded from an endpoint for one hour.  Centrifugo will load keys from JWKS endpoint by issuing GET HTTP request with 1 second timeout and one retry in case of failure (not configurable at the moment).  Only RSA algorithm is supported.  Once enabled JWKS used for both connection and channel subscription tokens.  ","version":"v4","tagName":"h2"},{"title":"Dynamic JWKs endpoint​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/4/server/authentication#dynamic-jwks-endpoint","content":" Available since Centrifugo v4.1.3  It's possible to extract variables from iss and aud JWT claims using Go regexp named groups, then use variables extracted during iss or aud matching to construct a JWKS endpoint dynamically upon token validation. In this case JWKS endpoint may be set in config as template.  To achieve this Centrifugo provides two additional options:  token_issuer_regex - match JWT issuer (iss claim) against this regex, extract named groups to variables, variables are then available for jwks endpoint construction.token_audience_regex - match JWT audience (aud claim) against this regex, extract named groups to variables, variables are then available for jwks endpoint construction.  Let's look at the example:  { &quot;token_issuer_regex&quot;: &quot;https://example.com/auth/realms/(?P&lt;realm&gt;[A-z]+)&quot;, &quot;token_jwks_public_endpoint&quot;: &quot;https://keycloak:443/{{realm}}/protocol/openid-connect/certs&quot;, }   To use variable in token_jwks_public_endpoint it must be wrapped in {{ }}.  When using token_issuer_regex and token_audience_regex make sure token_issuer and token_audience not used in the config - otherwise and error will be returned on Centrifugo start.  caution Setting token_issuer_regex and token_audience_regex will also affect subscription tokens (used for channel token authorization). Please read this issue and reach out if your use case requires separate configuration for subscription tokens. ","version":"v4","tagName":"h2"},{"title":"Channel permission model","type":0,"sectionRef":"#","url":"/docs/4/server/channel_permissions","content":"","keywords":"","version":"v4"},{"title":"Subscribe permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/4/server/channel_permissions#subscribe-permission-model","content":" By default, client's attempt to subscribe on a channel will be rejected by a server with 103: permission denied error. There are several approaches how to control channel subscribe permissions:  Provide subscription tokenConfigure subscribe proxyUse user-limited channelsUse subscribe_allowed_for_client namespace optionSubscribe capabilities in connection tokenSubscribe capabilities in connect proxy  Below, we are describing those in detail.  Provide subscription token​  A client can provide a subscription token in subscribe request. See the format of the token.  If client provides a valid token then subscription will be accepted. In Centrifugo PRO subscription token can additionally grant publish, history and presence permissions to a client.  caution For namespaces with allow_subscribe_for_client option ON Centrifugo does not allow subscribing on channels starting with private_channel_prefix ($ by default) without token. This limitation exists to help users migrate to Centrifugo v4 without security risks.  Configure subscribe proxy​  If client subscribes on a namespace with configured subscribe proxy then depending on proxy response subscription will be accepted or not.  If a namespace has configured subscribe proxy, but user came with a token – then subscribe proxy is not used, we are relying on token in this case. If a namespace has subscribe proxy, but user subscribes on a user-limited channel – then subscribe proxy is not used also.  Use user-limited channels​  If client subscribes on a user-limited channel and there is a user ID match then subscription will be accepted.  caution User-limited channels must be enabled in a namespace using allow_user_limited_channels option.  Use allow_subscribe_for_client namespace option​  allow_subscribe_for_client allows all authenticated non-anonymous connections to subscribe on all channels in a namespace.  caution Turning this option on effectively makes namespace public – no subscribe permissions will be checked (only the check that current connection is authenticated - i.e. has non-empty user ID). Make sure this is really what you want in terms of channels security.  To additionally allow subscribing to anonymous connections take a look at allow_subscribe_for_anonymous option.  Subscribe capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow user subscribe to channels.  Subscribe capabilities in connect proxy​  Centrifugo PRO only  Connect proxy can return capability object to allow user subscribe to channels.  ","version":"v4","tagName":"h3"},{"title":"Publish permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/4/server/channel_permissions#publish-permission-model","content":" tip In idiomatic Centrifugo use case data should be published to channels from the application backend (over server API). In this case backend can validate data, save it into persistent storage before publishing in real-time towards connections. When publishing from the client-side backend does not receive publication data at all – it just goes through Centrifugo (except using publish proxy). There are cases when direct publications from the client-side are desired (like typing indicators in chat applications) though.  By default, client's attempt to publish data into a channel will be rejected by a server with 103: permission denied error. There are several approaches how to control channel publish permissions:  Configure publish proxyUse allow_publish_for_subscriber namespace optionUse allow_publish_for_client namespace optionPublish capabilities in connection tokenPublish capability in subscription tokenPublish capabilities in connect proxyPublish capability in subscribe proxy  Use allow_publish_for_client namespace option​  allow_publish_for_client allows publications to channels of a namespace for all client connections.  Use allow_publish_for_subscriber namespace option​  allow_publish_for_subscriber allows publications to channels of a namespace for all connections subscribed on a channel they want to publish data into.  Configure publish proxy​  If client publishes to a namespace with configured publish proxy then depending on proxy response publication will be accepted or not.  Configured publish proxy always used??? (what if user has permission in token or allow_publish_for_client?)  Publish capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow client to publish to channels.  Publish capability in subscription token​  Centrifugo PRO only  Connection token can contain a capability object to allow client to publish to a channel.  Publish capabilities in connect proxy​  Centrifugo PRO only  Connect proxy can return capability object to allow client publish to certain channels.  Publish capability in subscribe proxy​  Centrifugo PRO only  Subscribe proxy can return capability object to allow subscriber publish to channel.  ","version":"v4","tagName":"h3"},{"title":"History permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/4/server/channel_permissions#history-permission-model","content":" By default, client's attempt to call history from a channel (with history retention configured) will be rejected by a server with 103: permission denied error. There are several approaches how to control channel history permissions.  Use allow_history_for_subscriber namespace option​  allow_history_for_subscriber allows history requests to all channels in a namespace for all client connections subscribed on a channel they want to call history for.  Use allow_history_for_client namespace option​  allow_history_for_client allows history requests to all channels in a namespace for all client connections.  History capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call history for channels.  History capabilities in subscription token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call history from a channel.  History capabilities in connect proxy​  This is a Centrifugo PRO feature.  Connect proxy can return capability object to allow client call history from certain channels.  History capability in subscribe proxy response​  Centrifugo PRO only  Subscribe proxy can return capability object to allow subscriber call history from channel.  ","version":"v4","tagName":"h3"},{"title":"Presence permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/4/server/channel_permissions#presence-permission-model","content":" By default, client's attempt to call presence from a channel (with channel presence configured) will be rejected by a server with 103: permission denied error. There are several approaches how to control channel presence permissions.  Presence capability in subscribe proxy response​  Subscribe proxy can return capability object to allow subscriber call presence from channel.  Use allow_presence_for_subscriber namespace option​  allow_presence_for_subscriber allows presence requests to all channels in a namespace for all client connections subscribed on a channel they want to call presence for.  Use allow_presence_for_client namespace option​  allow_presence_for_client allows presence requests to all channels in a namespace for all client connections.  Presence capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call presence for channels.  Presence capabilities in subscription token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call presence of a channel.  Presence capabilities in connect proxy​  Centrifugo PRO only  Connect proxy can return capability object to allow client call presence from certain channels.  ","version":"v4","tagName":"h3"},{"title":"Positioning permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/4/server/channel_permissions#positioning-permission-model","content":" Server can whether turn on positioning for all channels in a namespace using &quot;force_positioning&quot;: true option or client can create positioned subscriptions (but in this case client must have access to history capability).  ","version":"v4","tagName":"h3"},{"title":"Recovery permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/4/server/channel_permissions#recovery-permission-model","content":" Server can whether turn on automatic recovery for all channels in a namespace using &quot;force_recovery&quot;: true option or client can create recoverable subscriptions (but in this case client must have access to history capability).  ","version":"v4","tagName":"h3"},{"title":"Join/Leave permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/4/server/channel_permissions#joinleave-permission-model","content":" Server can whether force sending join/leave messages to all subscribers for all channels in a namespace using &quot;force_push_join_leave&quot;: true option or client can ask server to include join/leave messages upon subscribing (but in this case client must have access to presence capability). ","version":"v4","tagName":"h3"},{"title":"Channel JWT authorization","type":0,"sectionRef":"#","url":"/docs/4/server/channel_token_auth","content":"","keywords":"","version":"v4"},{"title":"Subscription JWT claims​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#subscription-jwt-claims","content":" For subscription JWT Centrifugo uses some standard claims defined in rfc7519, also some custom Centrifugo-specific.  ","version":"v4","tagName":"h2"},{"title":"sub​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#sub","content":" This is a standard JWT claim which must contain an ID of the current application user (as string).  The value must match a user in connection JWT – since it's the same real-time connection. The missing claim will mean that token issued for anonymous user (i.e. with empty user ID).  ","version":"v4","tagName":"h3"},{"title":"channel​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#channel","content":" Required. Channel that client tries to subscribe to with this token (string).  ","version":"v4","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#info","content":" Optional. Additional information for connection inside this channel (valid JSON).  ","version":"v4","tagName":"h3"},{"title":"b64info​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#b64info","content":" Optional. Additional information for connection inside this channel in base64 format (string). Will be decoded by Centrifugo to raw bytes.  ","version":"v4","tagName":"h3"},{"title":"exp​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#exp","content":" Optional. This is a standard JWT claim that allows setting private channel subscription token expiration time (a UNIX timestamp in the future, in seconds, as integer) and configures subscription expiration time.  At the moment if the subscription expires client connection will be closed and the client will try to reconnect. In most cases, you don't need this and should prefer using the expiration of the connection JWT to deactivate the connection (see authentication). But if you need more granular per-channel control this may fit your needs.  Once exp is set in token every subscription token must be periodically refreshed. This refresh workflow happens on the client side. Refer to the specific client documentation to see how to refresh subscriptions.  ","version":"v4","tagName":"h3"},{"title":"expire_at​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#expire_at","content":" Optional. By default, Centrifugo looks on exp claim to both check token expiration and configure subscription expiration time. In most cases this is fine, but there could be situations where you want to decouple subscription token expiration check with subscription expiration time. As soon as the expire_at claim is provided (set) in subscription JWT Centrifugo relies on it for setting subscription expiration time (JWT expiration still checked over exp though).  expire_at is a UNIX timestamp seconds when the subscription should expire.  Set it to the future time for expiring subscription at some pointSet it to 0 to disable subscription expiration (but still check token exp claim). This allows implementing a one-time subscription token.  ","version":"v4","tagName":"h3"},{"title":"aud​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#aud","content":" By default, Centrifugo does not check JWT audience (rfc7519 aud claim). But if you set token_audience option as described in client authentication then audience for subscription JWT will also be checked.  ","version":"v4","tagName":"h3"},{"title":"iss​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#iss","content":" By default, Centrifugo does not check JWT issuer (rfc7519 iss claim). But if you set token_issuer option as described in client authentication then issuer for subscription JWT will also be checked.  ","version":"v4","tagName":"h3"},{"title":"iat​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#iat","content":" This is a UNIX time when token was issued (seconds). See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v4","tagName":"h3"},{"title":"jti​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#jti","content":" This is a token unique ID. See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v4","tagName":"h3"},{"title":"override​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#override","content":" One more claim is override. This is an object which allows overriding channel options for the particular channel subscriber which comes with subscription token.  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\toverride presence channel option join_leave\tBoolValue\tyes\toverride join_leave channel option force_push_join_leave\tBoolValue\tyes\toverride force_push_join_leave channel option force_recovery\tBoolValue\tyes\toverride force_recovery channel option force_positioning\tBoolValue\tyes\toverride force_positioning channel option  BoolValue is an object like this:  { &quot;value&quot;: true/false }   So for example, you want to turn off emitting a presence information for a particular subscriber in a channel:  { ... &quot;override&quot;: { &quot;presence&quot;: { &quot;value&quot;: false } } }   ","version":"v4","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#example","content":" So to generate a subscription token you can use something like this in Python (assuming user ID is 42 and the channel is gossips):  import jwt token = jwt.encode({ &quot;sub&quot;: &quot;42&quot;, &quot;channel&quot;: &quot;$gossips&quot; }, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   Where &quot;secret&quot; is the token_hmac_secret_key from Centrifugo configuration (we use HMAC tokens in this example which relies on a shared secret key, for RSA or ECDSA tokens you need to use a private key known only by your backend).  ","version":"v4","tagName":"h2"},{"title":"With gensubtoken cli command​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/4/server/channel_token_auth#with-gensubtoken-cli-command","content":" During development you can quickly generate valid subscription token using Centrifugo gensubtoken cli command.  ./centrifugo gensubtoken -u 123722 -s channel   You should see an output like this:  HMAC SHA-256 JWT for user &quot;123722&quot; and channel &quot;channel&quot; with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE2NTU0NDg0MzgsImNoYW5uZWwiOiJjaGFubmVsIn0.JyRI3ovNV-abV8VxCmZCD556o2F2mNL1UoU58gNR-uI   But in real app subscription JWT must be generated by your application backend. ","version":"v4","tagName":"h2"},{"title":"Channels and namespaces","type":0,"sectionRef":"#","url":"/docs/4/server/channels","content":"","keywords":"","version":"v4"},{"title":"What is channel​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#what-is-channel","content":" Centrifugo is a PUB/SUB system - it has publishers and subscribers. Channel is a route for publications. Clients can subscribe to a channel to receive all real-time messages published to a channel. A channel subscriber can also ask for a channel online presence or channel history information.    Channel is just a string - news, comments, personal_feed are valid channel names. Though this string has some predefined rules as we will see below. You can define different channel behavior using a set of available channel options.  Channels are ephemeral – you don't need to create them explicitly. Channels created automatically by Centrifugo as soon as the first client subscribes to a channel. As soon as the last subscriber leaves a channel - it's automatically cleaned up.  Channel can belong to a channel namespace. Channel namespacing is a mechanism to define different behavior for different channels in Centrifugo. Using namespaces is a recommended way to manage channels – to turn on only those channel options which are required for a specific real-time feature you are implementing on top of Centrifugo.  caution When using channel namespaces make sure you defined a namespace in configuration. Subscription attempts to a channel within a non-defined namespace will result into 102: unknown channel errors.  ","version":"v4","tagName":"h2"},{"title":"Channel name rules​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#channel-name-rules","content":" Only ASCII symbols must be used in a channel string.  Channel name length limited by 255 characters by default (controlled by configuration option channel_max_length).  Several symbols in channel names reserved for Centrifugo internal needs:  : – for namespace channel boundary (see below)# – for user channel boundary (see below)$ – for private channel prefix (see below)* – for the future Centrifugo needs&amp; – for the future Centrifugo needs/ – for the future Centrifugo needs  ","version":"v4","tagName":"h2"},{"title":"namespace boundary (:)​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#namespace-boundary-","content":" : – is a channel namespace boundary. Namespaces are used to set custom options to a group of channels. Each channel belonging to the same namespace will have the same channel options. Read more about about namespaces and channel options below.  If the channel is public:chat - then Centrifugo will apply options to this channel from the channel namespace with the name public.  info A namespace is part of the channel name. If a user subscribed to a channel with namespace, like public:chat – then you need to publish messages into public:chat channel to be delivered to the user. We often see some confusion from developers trying to publish messages into chat and thinking that namespace is somehow stripped upon subscription. It's not true.  ","version":"v4","tagName":"h3"},{"title":"user channel boundary (#)​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#user-channel-boundary-","content":" # – is a user channel boundary. This is a separator to create personal channels for users (we call this user-limited channels) without the need to provide a subscription token.  For example, if the channel is news#42 then the only user with ID 42 can subscribe to this channel (Centrifugo knows user ID because clients provide it in connection credentials with connection JWT).  If you want to create a user-limited channel in namespace personal then you can use a name like personal:user#42 for example.  Moreover, you can provide several user IDs in channel name separated by a comma: dialog#42,43 – in this case only the user with ID 42 and user with ID 43 will be able to subscribe on this channel.  This is useful for channels with a static list of allowed users, for example for single user personal messages channel, for dialog channel between certainly defined users. As soon as you need to manage access to a channel dynamically for many users this channel type does not suit well.  tip User-limited channels must be enabled for a channel namespace using allow_user_limited_channels option. See below more information about channel options and channel namespaces.  ","version":"v4","tagName":"h3"},{"title":"private channel prefix ($)​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#private-channel-prefix-","content":" Centrifugo v4 has this option to achieve compatibility with previous Centrifugo versions. Previously (in Centrifugo v1, v2 and v3) only channels starting with $ could be subscribed with a subscription JWT. In Centrifugo v4 that's not the case anymore – clients can subscribe to any channel with a subscription token (if the token is valid – then subscription to a channel is accepted).  But for namespaces with allow_subscribe_for_client option enabled Centrifugo does not allow subscribing on channels starting with private_channel_prefix ($ by default) without a subscription token. This limitation exists to help users migrate to Centrifugo v4 without security risks.  ","version":"v4","tagName":"h3"},{"title":"Channel is just a string​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#channel-is-just-a-string","content":" Keep in mind that a channel is uniquely identified by its string representation. Do not expect that channels $news and news are the same. They are different because strings are not equal. So if a user subscribed to $news then user won't receive messages published to news.  Channels dialog#42,43 and dialog#43,42 are two different channels too. Centrifugo only applies permission checks when a user subscribes to a channel. So if user-limited channels are enabled then the user with ID 42 will be able to subscribe on both dialog#42,43 and dialog#43,42. But Centrifugo does no magic regarding channel strings when keeping channel-&gt;to-&gt;subscribers map. So if the user subscribed on dialog#42,43 you must publish messages to exactly that channel: dialog#42,43.  The same applies to channels with namespaces. Do not expect that channels chat:index and index are the same – they are different, moreover, belong to different namespaces. We'll look at the concept of channel namespaces in Centrifugo shortly.  ","version":"v4","tagName":"h3"},{"title":"Channel namespaces​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#channel-namespaces","content":" It's possible to configure a list of channel namespaces. Namespaces are optional but very useful.  A namespace allows setting custom options for channels starting with the namespace name. This provides great control over channel behavior so you have a flexible way to define different channel options for different real-time features in the application.  Namespace has a name, and the same channel options (with the same defaults) as described above.  name - unique namespace name (name must consist of letters, numbers, underscores, or hyphens and be more than 2 symbols length i.e. satisfy regexp ^[-a-zA-Z0-9_]{2,}$).  If you want to use namespace options for a channel - you must include namespace name into channel name with : as a separator:  public:messages  gossips:messages  Where public and gossips are namespace names. Centrifugo looks for : symbol in the channel name, if found – extracts the namespace name, and applies namespace options while processing protocol commands from a client.  All things together here is an example of config.json which includes some top-level channel options set and has 2 additional channel namespaces configured:  config.json { &quot;token_hmac_secret_key&quot;: &quot;very-long-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;presence&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;30s&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;facts&quot;, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot; }, { &quot;name&quot;: &quot;gossips&quot; } ] }   Channel news will use globally defined channel options.Channel facts:sport will use facts namespace options.Channel gossips:sport will use gossips namespace options.Channel xxx:hello will result into subscription error since there is no xxx namespace defined in the configuration above.  Channel namespaces also work with private channels and user-limited channels. For example, if you have a namespace called dialogs then the private channel can be constructed as $dialogs:gossips, user-limited channel can be constructed as dialogs:dialog#1,2.  note There is no inheritance in channel options and namespaces – for example, you defined presence: true on a top level of configuration and then defined a namespace – that namespace won't have online presence enabled - you must enable it for a namespace explicitly.  There are many options which can be set for channel namespace (on top-level and to named one) to modify behavior of channels belonging to a namespace. Below we describe all these options.  ","version":"v4","tagName":"h2"},{"title":"Channel options​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#channel-options","content":" Channel behavior can be modified by using channel options. Channel options can be defined on configuration top-level and for every namespace.  ","version":"v4","tagName":"h2"},{"title":"presence​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#presence","content":" presence (boolean, default false) – enable/disable online presence information for channels in a namespace.  Online presence is information about clients currently subscribed to the channel. It contains each subscriber's client ID, user ID, connection info, and channel info. By default, this option is off so no presence information will be available for channels.  Let's say you have a channel chat:index and 2 users (with ID 2694 and 56) subscribed to it. And user 2694 has 2 connections to Centrifugo in different browser tabs. In presence data you may see sth like this:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;presence&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat:index&quot;}}' \\ http://localhost:8000/api { &quot;result&quot;: { &quot;presence&quot;: { &quot;66fdf8d1-06f0-4375-9fac-db959d6ee8d6&quot;: { &quot;user&quot;: &quot;2694&quot;, &quot;client&quot;: &quot;66fdf8d1-06f0-4375-9fac-db959d6ee8d6&quot;, &quot;conn_info&quot;: {&quot;name&quot;: &quot;Alex&quot;} }, &quot;d4516dd3-0b6e-4cfe-84e8-0342fd2bb20c&quot;: { &quot;user&quot;: &quot;2694&quot;, &quot;client&quot;: &quot;d4516dd3-0b6e-4cfe-84e8-0342fd2bb20c&quot;, &quot;conn_info&quot;: {&quot;name&quot;: &quot;Alex&quot;} } &quot;g3216dd3-1b6e-tcfe-14e8-1342fd2bb20c&quot;: { &quot;user&quot;: &quot;56&quot;, &quot;client&quot;: &quot;g3216dd3-1b6e-tcfe-14e8-1342fd2bb20c&quot;, &quot;conn_info&quot;: {&quot;name&quot;: &quot;Alice&quot;} } } } }   To call presence API from the client connection side client must have permission to do so. See presence permission model.  caution Enabling channel online presence adds some overhead since Centrifugo needs to maintain an additional data structure (in a process memory or in a broker memory/disk). So only use it for channels where presence is required.  See more details about online presence design.  ","version":"v4","tagName":"h3"},{"title":"join_leave​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#join_leave","content":" join_leave (boolean, default false) – enable/disable sending join and leave messages when the client subscribes to a channel (unsubscribes from a channel). Join/leave event includes information about the connection that triggered an event – client ID, user ID, connection info, and channel info (similar to entry inside presence information).  Enabling join_leave means that Join/Leave messages will start being emitted, but by default they are not delivered to clients subscribed to a channel. You need to force this using namespace option force_push_join_leave or explicitly provide intent from a client-side (in this case client must have permission to call presence API).  caution Keep in mind that join/leave messages can generate a huge number of messages in a system if turned on for channels with a large number of active subscribers. If you have channels with a large number of subscribers consider avoiding using this feature. It's hard to say what is &quot;large&quot; for you though – just estimate the load based on the fact that each subscribe/unsubscribe event in a channel with N subscribers will result into N messages broadcasted to all. If all clients reconnect at the same time the amount of generated messages is N^2.  Join/leave messages distributed only with at most once delivery guarantee.  ","version":"v4","tagName":"h3"},{"title":"force_push_join_leave​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#force_push_join_leave","content":" Boolean, default false.  When on all clients will receive join/leave events for a channel in a namespace automatically – without explicit intent to consume join/leave messages from the client side.  If pushing join/leave is not forced then client can provide a corresponding Subscription option to enable it – but it should have permissions to access channel presence (by having an explicit capability or if allowed on a namespace level).  ","version":"v4","tagName":"h3"},{"title":"history_size​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#history_size","content":" history_size (integer, default 0) – history size (amount of messages) for channels. As Centrifugo keeps all history messages in process memory (or in a broker memory) it's very important to limit the maximum amount of messages in channel history with a reasonable value. history_size defines the maximum amount of messages that Centrifugo will keep for each channel in the namespace. As soon as history has more messages than defined by history size – old messages will be evicted.  Setting only history_size is not enough to enable history in channels – you also need to wisely configure history_ttl option (see below).  caution Enabling channel history adds some overhead (both memory and CPU) since Centrifugo needs to maintain an additional data structure (in a process memory or a broker memory/disk). So only use history for channels where it's required.  ","version":"v4","tagName":"h3"},{"title":"history_ttl​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#history_ttl","content":" history_ttl (duration, default 0s) – interval how long to keep channel history messages (with seconds precision).  As all history is storing in process memory (or in a broker memory) it is also very important to get rid of old history data for unused (inactive for a long time) channels.  By default history TTL duration is zero – this means that channel history is disabled.  Again – to turn on history you should wisely configure both history_size and history_ttl options.  For example for top-level channels (which do not belong to a namespace):  config.json { ... &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;60s&quot; }   Let's look at example. You enabled history for a namespace chat and sent two messages in channel chat:index. Then history will contain sth like this:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;history&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat:index&quot;, &quot;limit&quot;: 100}}' \\ http://localhost:8000/api { &quot;result&quot;: { &quot;publications&quot;: [ { &quot;data&quot;: { &quot;input&quot;: &quot;1&quot; }, &quot;offset&quot;: 1 }, { &quot;data&quot;: { &quot;input&quot;: &quot;2&quot; }, &quot;offset&quot;: 2 } ], &quot;epoch&quot;: &quot;gWuY&quot;, &quot;offset&quot;: 2 } }   To call history API from the client connection side client must have permission to do so. See history permission model.  See additional information about offsets and epoch in History and recovery chapter.  tip History persistence properties are dictated by Centrifugo engine used. For example, when using memory engine history is only kept till Centrifugo node restart. In Redis engine case persistence is determined by a Redis server persistence configuration (same for KeyDB and Tarantool).  ","version":"v4","tagName":"h3"},{"title":"force_positioning​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#force_positioning","content":" force_positioning (boolean, default false) – when the force_positioning option is on Centrifugo forces all subscriptions in a namespace to be positioned. I.e. Centrifugo will try to compensate at most once delivery of PUB/SUB broker checking client position inside a stream.  If Centrifugo detects a bad position of the client (i.e. potential message loss) it disconnects a client with the Insufficient state disconnect code. Also, when the position option is enabled Centrifugo exposes the current stream top offset and current epoch in subscribe reply making it possible for a client to manually recover its state upon disconnect using history API.  force_positioning option must be used in conjunction with reasonably configured message history for a channel i.e. history_size and history_ttl must be set (because Centrifugo uses channel history to check client position in a stream).  If positioning is not forced then client can provide a corresponding Subscription option to enable it – but it should have permissions to access channel history (by having an explicit capability or if allowed on a namespace level).  ","version":"v4","tagName":"h3"},{"title":"force_recovery​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#force_recovery","content":" force_recovery (boolean, default false) – when the position option is on Centrifugo forces all subscriptions in a namespace to be recoverable. When enabled Centrifugo will try to recover missed publications in channels after a client reconnects for some reason (bad internet connection for example). Also when the recovery feature is on Centrifugo automatically enables properties of the force_positioning option described above.  force_recovery option must be used in conjunction with reasonably configured message history for channel i.e. history_size and history_ttl must be set (because Centrifugo uses channel history to recover messages).  If recovery is not forced then client can provide a corresponding Subscription option to enable it – but it should have permissions to access channel history (by having an explicit capability or if allowed on a namespace level).  tip Not all real-time events require this feature turned on so think wisely when you need this. When this option is turned on your application should be designed in a way to tolerate duplicate messages coming from a channel (currently Centrifugo returns recovered publications in order and without duplicates but this is an implementation detail that can be theoretically changed in the future). See more details about how recovery works in special chapter.  ","version":"v4","tagName":"h3"},{"title":"allow_subscribe_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_subscribe_for_client","content":" allow_subscribe_for_client (boolean, default false) – when on all non-anonymous clients will be able to subscribe to any channel in a namespace. To additionally allow anonymous users to subscribe turn on allow_subscribe_for_anonymous (see below).  caution Turning this option on effectively makes namespace public – no subscribe permissions will be checked (only the check that current connection is authenticated - i.e. has non-empty user ID). Make sure this is really what you want in terms of channels security.  ","version":"v4","tagName":"h3"},{"title":"allow_subscribe_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_subscribe_for_anonymous","content":" allow_subscribe_for_anonymous (boolean, default false) – turn on if anonymous clients (with empty user ID) should be able to subscribe on channels in a namespace.  ","version":"v4","tagName":"h3"},{"title":"allow_publish_for_subscriber​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_publish_for_subscriber","content":" allow_publish_for_subscriber (boolean, default false) - when the allow_publish_for_subscriber option is enabled client can publish into a channel in namespace directly from the client side over real-time connection but only if client subscribed to that channel.  danger Keep in mind that in this case subscriber can publish any payload to a channel – Centrifugo does not validate input at all. Your app backend won't receive those messages - publications just go through Centrifugo towards channel subscribers. Consider always validate messages which are being published to channels (i.e. using server API to publish after validating input on the backend side, or using publish proxy - see idiomatic usage).  allow_publish_for_subscriber (or allow_publish_for_client mentioned below) option still can be useful to send something without backend-side validation and saving it into a database – for example, this option may be handy for demos and quick prototyping real-time app ideas.  ","version":"v4","tagName":"h3"},{"title":"allow_publish_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_publish_for_client","content":" allow_publish_for_client (boolean, default false) – when on allows clients to publish messages into channels directly (from a client-side). It's like allow_publish_for_subscriber – but client should not be a channel subscriber to publish.  danger Keep in mind that in this case client can publish any payload to a channel – Centrifugo does not validate input at all. Your app backend won't receive those messages - publications just go through Centrifugo towards channel subscribers. Consider always validate messages which are being published to channels (i.e. using server API to publish after validating input on the backend side, or using publish proxy - see idiomatic usage).  ","version":"v4","tagName":"h3"},{"title":"allow_publish_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_publish_for_anonymous","content":" allow_publish_for_anonymous (boolean, default false) – turn on if anonymous clients should be able to publish into channels in a namespace.  ","version":"v4","tagName":"h3"},{"title":"allow_history_for_subscriber​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_history_for_subscriber","content":" allow_history_for_subscriber (boolean, default false) – allows clients who subscribed on a channel to call history API from that channel.  ","version":"v4","tagName":"h3"},{"title":"allow_history_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_history_for_client","content":" allow_history_for_client (boolean, default false) – allows all clients to call history information in a namespace.  ","version":"v4","tagName":"h3"},{"title":"allow_history_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_history_for_anonymous","content":" allow_history_for_anonymous (boolean, default false) – turn on if anonymous clients should be able to call history from channels in a namespace.  ","version":"v4","tagName":"h3"},{"title":"allow_presence_for_subscriber​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_presence_for_subscriber","content":" allow_presence_for_subscriber (boolean, default false) – allows clients who subscribed on a channel to call presence information from that channel.  ","version":"v4","tagName":"h3"},{"title":"allow_presence_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_presence_for_client","content":" allow_presence_for_client (boolean, default false) – allows all clients to call presence information in a namespace.  ","version":"v4","tagName":"h3"},{"title":"allow_presence_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_presence_for_anonymous","content":" allow_presence_for_anonymous (boolean, default false) – turn on if anonymous clients should be able to call presence from channels in a namespace.  ","version":"v4","tagName":"h3"},{"title":"allow_user_limited_channels​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#allow_user_limited_channels","content":" allow_user_limited_channels (boolean, default false) - allows using user-limited channels in a namespace for checking subscribe permission.  note If client subscribes to a user-limited channel while this option is off then server rejects subscription with 103: permission denied error.  ","version":"v4","tagName":"h3"},{"title":"channel_regex​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#channel_regex","content":" channel_regex (string, default &quot;&quot;) – is an option to set a regular expression for channels allowed in the namespace. By default Centrifugo does not limit channel name variations. For example, if you have a namespace chat, then channel names inside this namespace are not really limited, it can be chat:index, chat:1, chat:2, chat:zzz and so on. But if you want to be strict and know possible channel patterns you can use channel_regex option. This is especially useful in namespaces where all clients can subscribe to channels.  For example, let's only allow digits after chat: for channel names in a chat namespace:  { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;chat&quot;, &quot;allow_subscribe_for_client&quot;: true, &quot;channel_regex&quot;: &quot;^[\\d+]$&quot; } ] }   danger Note, that we are skipping chat: part in regex. Since namespace prefix is the same for all channels in a namespace we only match the rest (after the prefix) of channel name.  Channel regex only checked for client-side subscriptions, if you are using server-side subscriptions Centrifugo won't check the regex.  Centrifugo uses Go language regexp package for regular expressions.  ","version":"v4","tagName":"h3"},{"title":"proxy_subscribe​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#proxy_subscribe","content":" proxy_subscribe (boolean, default false) – turns on subscribe proxy, more info in proxy chapter  ","version":"v4","tagName":"h3"},{"title":"proxy_publish​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#proxy_publish","content":" proxy_publish (boolean, default false) – turns on publish proxy, more info in proxy chapter  ","version":"v4","tagName":"h3"},{"title":"proxy_sub_refresh​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#proxy_sub_refresh","content":" proxy_sub_refresh (boolean, default false) – turns on sub refresh proxy, more info in proxy chapter  ","version":"v4","tagName":"h3"},{"title":"subscribe_proxy_name​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#subscribe_proxy_name","content":" subscribe_proxy_name (string, default &quot;&quot;) – turns on subscribe proxy when granular proxy mode is used. Note that proxy_subscribe option defined above is ignored in granular proxy mode.  ","version":"v4","tagName":"h3"},{"title":"publish_proxy_name​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#publish_proxy_name","content":" publish_proxy_name (string, default &quot;&quot;) – turns on publish proxy when granular proxy mode is used. Note that proxy_publish option defined above is ignored in granular proxy mode.  ","version":"v4","tagName":"h3"},{"title":"sub_refresh_proxy_name​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#sub_refresh_proxy_name","content":" sub_refresh_proxy_name (string, default &quot;&quot;) – turns on sub refresh proxy when granular proxy mode is used. Note that proxy_sub_refresh option defined above is ignored in granular proxy mode.  ","version":"v4","tagName":"h3"},{"title":"Channel config examples​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/4/server/channels#channel-config-examples","content":" Let's look at how to set some of these options in a config. In this example we turning on presence, history features, forcing publication recovery. Also allowing all client connections (including anonymous users) to subscribe to channels and call publish, history, presence APIs if subscribed.  config.json { &quot;token_hmac_secret_key&quot;: &quot;my-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;presence&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot;, &quot;force_recovery&quot;: true, &quot;allow_subscribe_for_client&quot;: true, &quot;allow_subscribe_for_anonymous&quot;: true, &quot;allow_publish_for_subscriber&quot;: true, &quot;allow_publish_for_anonymous&quot;: true, &quot;allow_history_for_subscriber&quot;: true, &quot;allow_history_for_anonymous&quot;: true, &quot;allow_presence_for_subscriber&quot;: true, &quot;allow_presence_for_anonymous&quot;: true }   Here we set channel options on config top-level – these options will affect channels without namespace. In many cases defining namespaces is a recommended approach so you can manage options for every real-time feature separately. With namespaces the above config may transform to:  config.json { &quot;token_hmac_secret_key&quot;: &quot;my-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;feed&quot;, &quot;presence&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot;, &quot;force_recovery&quot;: true, &quot;allow_subscribe_for_client&quot;: true, &quot;allow_subscribe_for_anonymous&quot;: true, &quot;allow_publish_for_subscriber&quot;: true, &quot;allow_publish_for_anonymous&quot;: true, &quot;allow_history_for_subscriber&quot;: true, &quot;allow_history_for_anonymous&quot;: true, &quot;allow_presence_for_subscriber&quot;: true, &quot;allow_presence_for_anonymous&quot;: true } ] }   In this case channels should be prefixed with feed: to follow the behavior configured for a feed namespace. ","version":"v4","tagName":"h2"},{"title":"Error and disconnect codes","type":0,"sectionRef":"#","url":"/docs/4/server/codes","content":"","keywords":"","version":"v4"},{"title":"Client error codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#client-error-codes","content":" Client errors are errors that can be returned to a client in replies to commands. This is specific for bidirectional client protocol only. For example, an error can be returned inside a reply to a subscribe command issued by a client.  Here is the list of Centrifugo built-in client error codes (with proxy feature you have a way to use custom error codes in replies or reuse existing).  ","version":"v4","tagName":"h2"},{"title":"Internal​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#internal","content":" Code: 100 Message: &quot;internal server error&quot; Temporary: true   Error Internal means server error, if returned this is a signal that something went wrong with a server itself and client most probably not guilty.  ","version":"v4","tagName":"h3"},{"title":"Unauthorized​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#unauthorized","content":" Code: 101 Message: &quot;unauthorized&quot;   Error Unauthorized says that request is unauthorized.  ","version":"v4","tagName":"h3"},{"title":"Unknown Channel​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#unknown-channel","content":" Code: 102 Message: &quot;unknown channel&quot;   Error Unknown Channel means that channel name does not exist.  Usually this is returned when client uses channel with a namespace which is not defined in Centrifugo configuration.  ","version":"v4","tagName":"h3"},{"title":"Permission Denied​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#permission-denied","content":" Code: 103 Message: &quot;permission denied&quot;   Error Permission Denied means that access to resource not allowed.  ","version":"v4","tagName":"h3"},{"title":"Method Not Found​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#method-not-found","content":" Code: 104 Message: &quot;method not found&quot;   Error Method Not Found means that method sent in command does not exist.  ","version":"v4","tagName":"h3"},{"title":"Already Subscribed​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#already-subscribed","content":" Code: 105 Message: &quot;already subscribed&quot;   Error Already Subscribed returned when client wants to subscribe on channel it already subscribed to.  ","version":"v4","tagName":"h3"},{"title":"Limit Exceeded​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#limit-exceeded","content":" Code: 106 Message: &quot;limit exceeded&quot;   Error Limit Exceeded says that some sort of limit exceeded, server logs should give more detailed information. See also ErrorTooManyRequests which is more specific for rate limiting purposes.  ","version":"v4","tagName":"h3"},{"title":"Bad Request​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#bad-request","content":" Code: 107 Message: &quot;bad request&quot;   Error Bad Request says that server can not process received data because it is malformed. Retrying request does not make sense.  ","version":"v4","tagName":"h3"},{"title":"Not Available​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#not-available","content":" Code: 108 Message: &quot;not available&quot;   Error Not Available means that resource is not enabled.  For example, this can be returned when trying to access history or presence in a channel that is not configured for having history or presence features.  ","version":"v4","tagName":"h3"},{"title":"Token Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#token-expired","content":" Code: 109 Message: &quot;token expired&quot;   Error Token Expired indicates that connection token expired. Our SDKs handle it in a special way by updating token.  ","version":"v4","tagName":"h3"},{"title":"Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#expired","content":" Code: 110 Message: &quot;expired&quot;   Error Expired indicates that connection expired (no token involved).  ","version":"v4","tagName":"h3"},{"title":"Too Many Requests​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#too-many-requests","content":" Code: 111 Message: &quot;too many requests&quot; Temporary: true   Error Too Many Requests means that server rejected request due to rate limiting strategies.  ","version":"v4","tagName":"h3"},{"title":"Unrecoverable Position​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#unrecoverable-position","content":" Code: 112 Message: &quot;unrecoverable position&quot;   Error Unrecoverable Position means that stream does not contain required range of publications to fulfill a history query.  This can happen due to wrong epoch passed.  ","version":"v4","tagName":"h3"},{"title":"Client disconnect codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#client-disconnect-codes","content":" Client can be disconnected by a Centrifugo server with custom code and string reason. Here is the list of Centrifugo built-in disconnect codes (with proxy feature you have a way to use custom disconnect codes).  note We expect that in most situations developers don't need to programmatically deal with handling various disconnect codes, but since Centrifugo sends them and codes shown in server metrics – they are documented. We expect these codes are mostly useful for logs and metrics.  ","version":"v4","tagName":"h2"},{"title":"DisconnectConnectionClosed​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#disconnectconnectionclosed","content":" Code: 3000 Reason: &quot;connection closed&quot;   DisconnectConnectionClosed is a special Disconnect object used when client connection was closed without any advice from a server side. This can be a clean disconnect, or temporary disconnect of the client due to internet connection loss. Server can not distinguish the actual reason of disconnect.  ","version":"v4","tagName":"h3"},{"title":"Non-terminal disconnect codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#non-terminal-disconnect-codes","content":" Client will reconnect after receiving such codes.  Shutdown​  Code: 3001 Reason: &quot;shutdown&quot;   Disconnect Shutdown may be sent when node is going to shut down.  DisconnectServerError​  Code: 3004 Reason: &quot;internal server error&quot;   DisconnectServerError issued when internal error occurred on server.  DisconnectExpired​  Code: 3005 Reason: &quot;connection expired&quot;   DisconnectSubExpired​  Code: 3006 Reason: &quot;subscription expired&quot;   DisconnectSubExpired issued when client subscription expired.  DisconnectSlow​  Code: 3008 Reason: &quot;slow&quot;   DisconnectSlow issued when client can't read messages fast enough.  DisconnectWriteError​  Code: 3009 Reason: &quot;write error&quot;   DisconnectWriteError issued when an error occurred while writing to client connection.  DisconnectInsufficientState​  Code: 3010 Reason: &quot;insufficient state&quot;   DisconnectInsufficientState issued when server detects wrong client position in channel Publication stream. Disconnect allows clien to restore missed publications on reconnect.  DisconnectForceReconnect​  Code: 3011 Reason: &quot;force reconnect&quot;   DisconnectForceReconnect issued when server disconnects connection for some reason and whants it to reconnect.  DisconnectNoPong​  Code: 3012 Reason: &quot;no pong&quot;   DisconnectNoPong may be issued when server disconnects bidirectional connection due to no pong received to application-level server-to-client pings in a configured time.  DisconnectTooManyRequests​  Code: 3013 Reason: &quot;too many requests&quot;   DisconnectTooManyRequests may be issued when client sends too many commands to a server.  ","version":"v4","tagName":"h3"},{"title":"Terminal disconnect codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/4/server/codes#terminal-disconnect-codes","content":" Client won't reconnect upon receiving such code.  DisconnectInvalidToken​  Code: 3500 Reason: &quot;invalid token&quot;   DisconnectInvalidToken issued when client came with invalid token.  DisconnectBadRequest​  Code: 3501 Reason: &quot;bad request&quot;   DisconnectBadRequest issued when client uses malformed protocol frames.  DisconnectStale​  Code: 3502 Reason: &quot;stale&quot;   DisconnectStale issued to close connection that did not become authenticated in configured interval after dialing.  DisconnectForceNoReconnect​  Code: 3503 Reason: &quot;force disconnect&quot;   DisconnectForceNoReconnect issued when server disconnects connection and asks it to not reconnect again.  DisconnectConnectionLimit​  Code: 3504 Reason: &quot;connection limit&quot;   DisconnectConnectionLimit can be issued when client connection exceeds a configured connection limit (per user ID or due to other rule).  DisconnectChannelLimit​  Code: 3505 Reason: &quot;channel limit&quot;   DisconnectChannelLimit can be issued when client connection exceeds a configured channel limit.  DisconnectInappropriateProtocol​  Code: 3506 Reason: &quot;inappropriate protocol&quot;   DisconnectInappropriateProtocol can be issued when client connection format can not handle incoming data. For example, this happens when JSON-based clients receive binary data in a channel. This is usually an indicator of programmer error, JSON clients can not handle binary.  DisconnectPermissionDenied​  Code: 3507 Reason: &quot;permission denied&quot;   DisconnectPermissionDenied may be issued when client attempts accessing a server without enough permissions.  DisconnectNotAvailable​  Code: 3508 Reason: &quot;not available&quot;   DisconnectNotAvailable may be issued when ErrorNotAvailable does not fit message type, for example we issue DisconnectNotAvailable when client sends asynchronous message without MessageHandler set on server side.  DisconnectTooManyErrors​  Code: 3509 Reason: &quot;too many errors&quot;   DisconnectTooManyErrors may be issued when client generates too many errors. ","version":"v4","tagName":"h3"},{"title":"Configure Centrifugo","type":0,"sectionRef":"#","url":"/docs/4/server/configuration","content":"","keywords":"","version":"v4"},{"title":"Configuration sources​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#configuration-sources","content":" Centrifugo can be configured in several ways.  ","version":"v4","tagName":"h2"},{"title":"Command-line flags​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#command-line-flags","content":" Centrifugo supports several command-line flags. See centrifugo -h for available flags. Command-line flags limited to most frequently used. In general, we suggest to avoid using flags for configuring Centrifugo in a production environment – prefer environment or configuration file sources.  Command-line options have the highest priority when set than other ways to configure Centrifugo.  ","version":"v4","tagName":"h3"},{"title":"OS environment variables​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#os-environment-variables","content":" All Centrifugo options can be set over env in the format CENTRIFUGO_&lt;OPTION_NAME&gt; (i.e. option name with CENTRIFUGO_ prefix, all in uppercase).  Setting options over env is mostly straightforward except namespaces – see how to set namespaces via env. Environment variables have the second priority after flags.  Boolean options can be set using strings according to Go language ParseBool function. I.e. to set true you can just use &quot;true&quot; value for an environment variable (or simply &quot;1&quot;). To set false use &quot;false&quot; or &quot;0&quot;. Example:  export CENTRIFUGO_PROMETHEUS=&quot;1&quot;   Also, array options, like allowed_origins can be set over environment variables as a single string where values separated by a space. For example:  export CENTRIFUGO_ALLOWED_ORIGINS=&quot;https://mysite1.example.com https://mysite2.example.com&quot;   For a nested object configuration (which we have, for example, in Centrifugo PRO ClickHouse analytics) it's still possible to use environment variables to set options. In this case replace nesting with _ when constructing environment variable name.  Empty environment variables are considered unset (!) and will fall back to the next configuration source.  ","version":"v4","tagName":"h3"},{"title":"Configuration file​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#configuration-file","content":" Configuration file supports all options mentioned in Centrifugo documentation and can be in one of three supported formats: JSON, YAML, or TOML. Config file options have the lowest priority among configuration sources (i.e. option set over environment variable prevails over the same option in config file).  A simple way to start with Centrifugo is to run:  centrifugo genconfig   This command generates config.json configuration file in a current directory. This file already has the minimal number of options set. So it's then possible to start Centrifugo:  centrifugo -c config.json   ","version":"v4","tagName":"h3"},{"title":"Config file formats​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#config-file-formats","content":" Centrifugo supports three configuration file formats: JSON, YAML, or TOML.  ","version":"v4","tagName":"h2"},{"title":"JSON config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#json-config-format","content":" Here is an example of Centrifugo JSON configuration file:  config.json { &quot;allowed_origins&quot;: [&quot;http://localhost:3000&quot;], &quot;token_hmac_secret_key&quot;: &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot;, &quot;api_key&quot;: &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; }   token_hmac_secret_key used to check JWT signature (more info about JWT in authentication chapter). If you are using connect proxy then you may use Centrifugo without JWT.  api_key used for Centrifugo API endpoint authorization, see more in chapter about server HTTP API. Keep both values secret and never reveal them to clients.  allowed_origins option described below.  ","version":"v4","tagName":"h3"},{"title":"TOML config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#toml-config-format","content":" Centrifugo also supports TOML format for configuration file:  centrifugo --config=config.toml   Where config.toml contains:  config.toml allowed_origins: [ &quot;http://localhost:3000&quot; ] token_hmac_secret_key = &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot; api_key = &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; log_level = &quot;debug&quot;   In the example above we also defined logging level to be debug which is useful to have while developing an application. In the production environment debug logging can be too chatty.  ","version":"v4","tagName":"h3"},{"title":"YAML config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#yaml-config-format","content":" YAML format is also supported:  config.yaml allowed_origins: - &quot;http://localhost:3000&quot; token_hmac_secret_key: &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot; api_key: &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; log_level: debug   With YAML remember to use spaces, not tabs when writing a configuration file.  ","version":"v4","tagName":"h3"},{"title":"Important options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#important-options","content":" Let's describe some important options you can configure when running Centrifugo.  ","version":"v4","tagName":"h2"},{"title":"allowed_origins​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#allowed_origins","content":" This option allows setting an array of allowed origin patterns (array of strings) for WebSocket and SockJS endpoints to prevent CSRF or WebSocket hijacking attacks. Also, it's used for HTTP-based unidirectional transports to enable CORS for configured origins.  As soon as allowed_origins is defined every connection request with Origin set will be checked against each pattern in an array.  Connection requests without Origin header set are passing through without any checks (i.e. always allowed).  For example, a client connects to Centrifugo from a web browser application on http://localhost:3000. In this case, allowed_origins should be configured in this way:  &quot;allowed_origins&quot;: [ &quot;http://localhost:3000&quot; ]   When connecting from https://example.com:  &quot;allowed_origins&quot;: [ &quot;https://example.com&quot; ]   Origin pattern can contain wildcard symbol * to match subdomains:  &quot;allowed_origins&quot;: [ &quot;https://*.example.com&quot; ]   – in this case requests with Origin header like https://foo.example.com or https://bar.example.com will pass the check.  It's also possible to allow all origins in the following way (but this is discouraged and insecure when using connect proxy feature):  &quot;allowed_origins&quot;: [ &quot;*&quot; ]   ","version":"v4","tagName":"h3"},{"title":"address​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#address","content":" Bind your Centrifugo to a specific interface address (string, by default &quot;&quot; - listen on all available interfaces).  ","version":"v4","tagName":"h3"},{"title":"port​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#port","content":" Port to bind Centrifugo to (string, by default &quot;8000&quot;).  ","version":"v4","tagName":"h3"},{"title":"engine​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#engine","content":" Engine to use - memory, redis or tarantool. It's a string option, by default memory. Read more about engines in special chapter.  ","version":"v4","tagName":"h3"},{"title":"Advanced options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#advanced-options","content":" These options allow tweaking server behavior, in most cases default values are good to start with.  ","version":"v4","tagName":"h2"},{"title":"client_channel_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#client_channel_limit","content":" Default: 128  Sets the maximum number of different channel subscriptions a single client can have.  tip When designing an application avoid subscribing to an unlimited number of channels per one client. Keep number of subscriptions for each client reasonably small – this will help keeping handshake process lightweight and fast.  ","version":"v4","tagName":"h3"},{"title":"channel_max_length​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#channel_max_length","content":" Default: 255  Sets the maximum length of the channel name.  ","version":"v4","tagName":"h3"},{"title":"client_user_connection_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#client_user_connection_limit","content":" Default: 0  The maximum number of connections from a user (with known user ID) to Centrifugo node. By default, unlimited.  The important thing to emphasize is that client_user_connection_limit works only per one Centrifugo node and exists mostly to protect Centrifugo from many connections from a single user – but not for business logic limitations. This means that if you set this to 1 and scale nodes – say run 10 Centrifugo nodes – then a user will be able to create 10 connections (one to each node).  ","version":"v4","tagName":"h3"},{"title":"client_connection_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#client_connection_limit","content":" Added in Centrifugo v4.0.1  Default: 0  When set to a value &gt; 0 client_connection_limit limits the max number of connections single Centrifugo node can handle. It acts on HTTP middleware level and stops processing request if the condition met. It logs a warning into logs in this case and increments centrifugo_node_client_connection_limit Prometheus counter. Client SDKs will attempt reconnecting.  Some motivation behind this option may be found in this issue.  Note, that at this point client_connection_limit does not affect connections coming over GRPC unidirectional transport.  ","version":"v4","tagName":"h3"},{"title":"client_connection_rate_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#client_connection_rate_limit","content":" Added in Centrifugo v4.1.1  Default: 0  client_connection_rate_limit sets the maximum number of HTTP requests to establish a new real-time connection a single Centrifugo node will accept per second (on real-time transport endpoints). All requests outside the limit will get 503 Service Unavailable code in response. Our SDKs handle this with backoff reconnection.  By default, no limit is used.  Note, that at this point client_connection_rate_limit does not affect connections coming over GRPC unidirectional transport.  ","version":"v4","tagName":"h3"},{"title":"client_queue_max_size​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#client_queue_max_size","content":" Default: 1048576  Maximum client message queue size in bytes to close slow reader connections. By default - 1mb.  ","version":"v4","tagName":"h3"},{"title":"client_concurrency​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#client_concurrency","content":" Default: 0  client_concurrency when set tells Centrifugo that commands from a client must be processed concurrently.  By default, concurrency disabled – Centrifugo processes commands received from a client one by one. This means that if a client issues two RPC requests to a server then Centrifugo will process the first one, then the second one. If the first RPC call is slow then the client will wait for the second RPC response much longer than it could (even if the second RPC is very fast). If you set client_concurrency to some value greater than 1 then commands will be processed concurrently (in parallel) in separate goroutines (with maximum concurrency level capped by client_concurrency value). Thus, this option can effectively reduce the latency of individual requests. Since separate goroutines are involved in processing this mode adds some performance and memory overhead – though it should be pretty negligible in most cases. This option applies to all commands from a client (including subscribe, publish, presence, etc).  ","version":"v4","tagName":"h3"},{"title":"client_stale_close_delay​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#client_stale_close_delay","content":" Duration, default: 10s  This option allows tuning the maximum time Centrifugo will wait for the connect frame (which contains authentication information) from the client after establishing connection. Default value should be reasonable for most use cases.  ","version":"v4","tagName":"h3"},{"title":"allow_anonymous_connect_without_token​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#allow_anonymous_connect_without_token","content":" Default: false  Enable a mode when all clients can connect to Centrifugo without JWT. In this case, all connections without a token will be treated as anonymous (i.e. with empty user ID). Access to channel operations should be explicitly enabled for anonymous connections.  ","version":"v4","tagName":"h3"},{"title":"disallow_anonymous_connection_tokens​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#disallow_anonymous_connection_tokens","content":" Added in Centrifugo v4.1.1  Default: false  When the option is set Centrifugo won't accept connections from anonymous users even if they provided a valid JWT. I.e. if token is valid, but sub claim is empty – then Centrifugo closes connection with advice to not reconnect again.  ","version":"v4","tagName":"h3"},{"title":"gomaxprocs​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#gomaxprocs","content":" Default: 0  By default, Centrifugo runs on all available CPU cores (also Centrifugo can look at cgroup limits when rnning in Docker/Kubernetes). To limit the number of cores Centrifugo can utilize in one moment use this option.  ","version":"v4","tagName":"h3"},{"title":"Endpoint configuration.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#endpoint-configuration","content":" After Centrifugo started there are several endpoints available.  ","version":"v4","tagName":"h2"},{"title":"Default endpoints.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#default-endpoints","content":" Bidirectional WebSocket default endpoint:  ws://localhost:8000/connection/websocket   Bidirectional emulation with HTTP-streaming (disabled by default):  ws://localhost:8000/connection/http_stream   Bidirectional emulation with SSE (EventSource) (disabled by default):  ws://localhost:8000/connection/sse   Bidirectional SockJS default endpoint (disabled by default):  http://localhost:8000/connection/sockjs   Unidirectional EventSource endpoint (disabled by default):  http://localhost:8000/connection/uni_sse   Unidirectional HTTP streaming endpoint (disabled by default):  http://localhost:8000/connection/uni_http_stream   Unidirectional WebSocket endpoint (disabled by default):  http://localhost:8000/connection/uni_websocket   Unidirectional SSE (EventSource) endpoint (disabled by default):  http://localhost:8000/connection/uni_sse   Server HTTP API endpoint:  http://localhost:8000/api   By default, all endpoints work on port 8000. This can be changed with port option:  { &quot;port&quot;: 9000 }   In production setup, you may have a proper domain name in endpoint addresses above instead of localhost. While domain name and port parts can differ depending on setup – URL paths stay the same: /connection/sockjs, /connection/websocket, /api etc.  ","version":"v4","tagName":"h3"},{"title":"Admin endpoints.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#admin-endpoints","content":" Admin web UI endpoint works on root path by default, i.e. http://localhost:8000.  For more details about admin web UI, refer to the Admin web UI documentation.  ","version":"v4","tagName":"h3"},{"title":"Debug endpoints.​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#debug-endpoints","content":" Next, when Centrifugo started in debug mode some extra debug endpoints become available. To start in debug mode add debug option to config:  { ... &quot;debug&quot;: true }   And endpoint:  http://localhost:8000/debug/pprof/   – will show useful information about the internal state of Centrifugo instance. This info is especially helpful when troubleshooting. See wiki page for more info.  ","version":"v4","tagName":"h3"},{"title":"Health check endpoint​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#health-check-endpoint","content":" Use health boolean option (by default false) to enable the health check endpoint which will be available on path /health. Also available over command-line flag:  centrifugo -c config.json --health   ","version":"v4","tagName":"h3"},{"title":"Custom internal ports​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#custom-internal-ports","content":" We strongly recommend not expose API, admin, debug, health, and Prometheus endpoints to the Internet. The following Centrifugo endpoints are considered internal:  API endpoint (/api) - for HTTP API requestsAdmin web interface endpoints (/, /admin/auth, /admin/api) - used by web interfacePrometheus endpoint (/metrics) - used for exposing server metrics in Prometheus formatHealth check endpoint (/health) - used to do health checksDebug endpoints (/debug/pprof) - used to inspect internal server state  It's a good practice to protect all these endpoints with a firewall. For example, it's possible to configure in location section of the Nginx configuration.  Though sometimes you don't have access to a per-location configuration in your proxy/load balancer software. For example when using Amazon ELB. In this case, you can change ports on which your internal endpoints work.  To run internal endpoints on custom port use internal_port option:  { ... &quot;internal_port&quot;: 9000 }   So admin web interface will work on address:  http://localhost:9000   Also, debug page will be available on a new custom port too:  http://localhost:9000/debug/pprof/   The same for API and Prometheus endpoints.  ","version":"v4","tagName":"h3"},{"title":"Disable default endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#disable-default-endpoints","content":" To disable websocket endpoint set websocket_disable boolean option to true.  To disable API endpoint set api_disable boolean option to true.  ","version":"v4","tagName":"h3"},{"title":"Customize handler endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#customize-handler-endpoints","content":" It's possible to customize server HTTP handler endpoints. To do this Centrifugo supports several options:  admin_handler_prefix (default &quot;&quot;) - to control Admin panel URL prefixwebsocket_handler_prefix (default &quot;/connection/websocket&quot;) - to control WebSocket URL prefixhttp_stream_handler_prefix (default &quot;/connection/http_stream&quot;) - to control HTTP-streaming URL prefixsse_handler_prefix (default &quot;/connection/sse&quot;) - to control SSE/EventSource URL prefixsockjs_handler_prefix (default &quot;/connection/sockjs&quot;) - to control SockJS URL prefixuni_sse_handler_prefix (default &quot;/connection/uni_sse&quot;) - to control unidirectional Eventsource URL prefixuni_http_stream_handler_prefix (default &quot;/connection/uni_http_stream&quot;) - to control unidirectional HTTP streaming URL prefixuni_websocket_handler_prefix (default &quot;/connection/uni_websocket&quot;) - to control unidirectional WebSocket URL prefixapi_handler_prefix (default &quot;/api&quot;) - to control HTTP API URL prefixprometheus_handler_prefix (default &quot;/metrics&quot;) - to control Prometheus URL prefixhealth_handler_prefix (default &quot;/health&quot;) - to control health check URL prefix  ","version":"v4","tagName":"h3"},{"title":"Signal handling​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#signal-handling","content":" It's possible to send HUP signal to Centrifugo to reload a configuration:  kill -HUP &lt;PID&gt;   Though at moment this will only reload token secrets and channel options (top-level and namespaces).  Centrifugo tries to gracefully shut down client connections when SIGINT or SIGTERM signals are received. By default, the maximum graceful shutdown period is 30 seconds but can be changed using shutdown_timeout (integer, in seconds) configuration option.  ","version":"v4","tagName":"h2"},{"title":"Insecure modes​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#insecure-modes","content":" ","version":"v4","tagName":"h2"},{"title":"Insecure client connection​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#insecure-client-connection","content":" The boolean option client_insecure (default false) allows connecting to Centrifugo without JWT token. In this mode, there is no user authentication involved. It also disables permission checks on client API level - for presence and history calls. This mode can be useful for demo projects based on Centrifugo, integration tests, local projects, or real-time application prototyping. Don't use it in production until you 100% know what you are doing.  ","version":"v4","tagName":"h3"},{"title":"Insecure API mode​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#insecure-api-mode","content":" This mode can be enabled using the boolean option api_insecure (default false). When on there is no need to provide API key in HTTP requests. When using this mode everyone that has access to /api endpoint can send any command to server. Enabling this option can be reasonable if /api endpoint is protected by firewall rules.  The option is also useful in development to simplify sending API commands to Centrifugo using CURL for example without specifying Authorization header in requests.  ","version":"v4","tagName":"h3"},{"title":"Insecure admin mode​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#insecure-admin-mode","content":" This mode can be enabled using the boolean option admin_insecure (default false). When on there is no authentication in the admin web interface. Again - this is not secure but can be justified if you protected the admin interface by firewall rules or you want to use basic authentication for the Centrifugo admin interface (configured on proxy level).  ","version":"v4","tagName":"h3"},{"title":"Setting time duration options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#setting-time-duration-options","content":" Time durations in Centrifugo can be set using strings where duration value and unit are both provided. For example, to set 5 seconds duration use &quot;5s&quot;.  The minimal time resolution is 1ms. Some options of Centrifugo only support second precision (for example history_ttl channel option).  Valid time units are &quot;ms&quot; (milliseconds), &quot;s&quot; (seconds), &quot;m&quot; (minutes), &quot;h&quot; (hours).  Some examples:  &quot;1000ms&quot; // 1000 milliseconds &quot;1s&quot; // 1 second &quot;12h&quot; // 12 hours &quot;720h&quot; // 30 days   ","version":"v4","tagName":"h2"},{"title":"Setting namespaces over env​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#setting-namespaces-over-env","content":" While setting most options in Centrifugo over env is pretty straightforward setting namespaces is a bit special:  CENTRIFUGO_NAMESPACES='[{&quot;name&quot;: &quot;ns1&quot;}, {&quot;name&quot;: &quot;ns2&quot;}]' ./centrifugo   I.e. CENTRIFUGO_NAMESPACES environment variable should be a valid JSON string that represents namespaces array.  ","version":"v4","tagName":"h2"},{"title":"Anonymous usage stats​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/4/server/configuration#anonymous-usage-stats","content":" Centrifugo periodically sends anonymous usage information (once in 24 hours). That information is impersonal and does not include sensitive data, passwords, IP addresses, hostnames, etc. Only counters to estimate version and installation size distribution, and feature usage.  Please do not disable usage stats sending without reason. If you depend on Centrifugo – sure you are interested in further project improvements. Usage stats help us understand Centrifugo use cases better, concentrate on widely-used features, and be confident we are moving in the right direction. Developing in the dark is hard, and decisions may be non-optimal.  To disable sending usage stats set usage_stats_disable option:  config.json { &quot;usage_stats_disable&quot;: true }  ","version":"v4","tagName":"h2"},{"title":"Helper CLI commands","type":0,"sectionRef":"#","url":"/docs/4/server/console_commands","content":"","keywords":"","version":"v4"},{"title":"version​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/4/server/console_commands#version","content":" To show Centrifugo version and exit run:  centrifugo version   ","version":"v4","tagName":"h2"},{"title":"genconfig​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/4/server/console_commands#genconfig","content":" Another command is genconfig:  centrifugo genconfig -c config.json   It will automatically generate the minimal required configuration file. This is mostly useful for development.  If any errors happen – program will exit with error message and exit code 1.  genconfig also supports generation of YAML and TOML configuration file formats - just provide an extension to a file:  centrifugo genconfig -c config.toml   ","version":"v4","tagName":"h2"},{"title":"checkconfig​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/4/server/console_commands#checkconfig","content":" Centrifugo has special command to check configuration file checkconfig:  centrifugo checkconfig --config=config.json   If any errors found during validation – program will exit with error message and exit code 1.  ","version":"v4","tagName":"h2"},{"title":"gentoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/4/server/console_commands#gentoken","content":" Another command is gentoken:  centrifugo gentoken -c config.json -u 28282   It will automatically generate HMAC SHA-256 based token for user with ID 28282 (which expires in 1 week).  You can change token TTL with -t flag (number of seconds):  centrifugo gentoken -c config.json -u 28282 -t 3600   This way generated token will be valid for 1 hour.  If any errors happen – program will exit with error message and exit code 1.  This command is mostly useful for development.  ","version":"v4","tagName":"h2"},{"title":"gensubtoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/4/server/console_commands#gensubtoken","content":" Another command is gensubtoken:  centrifugo gensubtoken -c config.json -u 28282 -s channel   It will automatically generate HMAC SHA-256 based subscription token for channel channel and user with ID 28282 (which expires in 1 week).  You can change token TTL with -t flag (number of seconds):  centrifugo gentoken -c config.json -u 28282 -s channel -t 3600   This way generated token will be valid for 1 hour.  If any errors happen – program will exit with error message and exit code 1.  This command is mostly useful for development.  ","version":"v4","tagName":"h2"},{"title":"checktoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/4/server/console_commands#checktoken","content":" One more command is checktoken:  centrifugo checktoken -c config.json &lt;TOKEN&gt;   It will validate your connection JWT, so you can test it before using while developing application.  If any errors happen or validation failed – program will exit with error message and exit code 1.  This is mostly useful for development.  ","version":"v4","tagName":"h2"},{"title":"checksubtoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/4/server/console_commands#checksubtoken","content":" One more command is checksubtoken:  centrifugo checksubtoken -c config.json &lt;TOKEN&gt;   It will validate your subscription JWT, so you can test it before using while developing application.  If any errors happen or validation failed – program will exit with error message and exit code 1.  This is mostly useful for development. ","version":"v4","tagName":"h2"},{"title":"Engines and scalability","type":0,"sectionRef":"#","url":"/docs/4/server/engines","content":"","keywords":"","version":"v4"},{"title":"Memory engine​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#memory-engine","content":" Used by default. Supports only one node. Nice choice to start with. Supports all features keeping everything in Centrifugo node process memory. You don't need to install Redis when using this engine.  Advantages:  Super fast since it does not involve network at allDoes not require separate broker setup  Disadvantages:  Does not allow scaling nodes (actually you still can scale Centrifugo with Memory engine but you have to publish data into each Centrifugo node and you won't have consistent history and presence state throughout Centrifugo nodes)Does not persist message history in channels between Centrifugo restarts.  ","version":"v4","tagName":"h2"},{"title":"Memory engine options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#memory-engine-options","content":" history_meta_ttl​  Duration, default 2160h (90 days).  history_meta_ttl sets a time of history stream metadata expiration.  When using a history in a channel, Centrifugo keeps some metadata for it. Metadata includes the latest stream offset and its epoch value. In some cases, when channels are created for а short time and then not used anymore, created metadata can stay in memory while not useful. For example, you can have a personal user channel but after using your app for a while user left it forever. From a long-term perspective, this can be an unwanted memory growth. Setting a reasonable value to this option can help to expire metadata faster (or slower if you need it). The rule of thumb here is to keep this value much bigger than maximum history TTL used in Centrifugo configuration.  ","version":"v4","tagName":"h3"},{"title":"Redis engine​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#redis-engine","content":" Redis is an open-source, in-memory data structure store, used as a database, cache, and message broker.  Centrifugo Redis engine allows scaling Centrifugo nodes to different machines. Nodes will use Redis as a message broker (utilizing Redis PUB/SUB for node communication) and keep presence and history data in Redis.  Minimal Redis version is 5.0.1  With Redis it's possible to come to the architecture like this:    ","version":"v4","tagName":"h2"},{"title":"Redis engine options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#redis-engine-options","content":" Several configuration options related to Redis engine.  redis_address​  String, default &quot;127.0.0.1:6379&quot; - Redis server address.  redis_password​  String, default &quot;&quot; - Redis password.  redis_user​  String, default &quot;&quot; - Redis user for ACL-based auth.  redis_db​  Integer, default 0 - number of Redis db to use.  redis_prefix​  String, default &quot;centrifugo&quot; – custom prefix to use for channels and keys in Redis.  redis_use_lists​  Boolean, default false – turns on using Redis Lists instead of Stream data structure for keeping history (not recommended, keeping this for backwards compatibility mostly).  redis_force_resp2​  Available since Centrifugo v4.1.3  Boolean, default false. If set to true it forces using RESP2 protocol for communicating with Redis. By default, Redis client used by Centrifugo tries to detect supported Redis protocol automatically trying RESP3 first.  history_meta_ttl​  Duration, default 2160h (90 days).  history_meta_ttl sets a time of history stream metadata expiration.  Similar to a Memory engine Redis engine also looks at history_meta_ttl option. Meta key in Redis is a HASH that contains the current offset number in channel and the stream epoch value.  When using a history in a channel, Centrifugo saves metadata for it. Metadata includes the latest stream offset and its epoch value. In some cases, when channels are created for а short time and then not used anymore, created metadata can stay in memory while not useful. For example, you can have a personal user channel but after using your app for a while user left it forever. From a long-term perspective, this can be an unwanted memory growth. Setting a reasonable value to this option can help. The rule of thumb here is to keep this value much bigger than maximum history TTL used in Centrifugo configuration.  ","version":"v4","tagName":"h3"},{"title":"Configuring Redis TLS​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#configuring-redis-tls","content":" Some options may help you configuring TLS-protected communication between Centrifugo and Redis.  redis_tls​  Boolean, default false - enable Redis TLS connection.  redis_tls_insecure_skip_verify​  Boolean, default false - disable Redis TLS host verification. Centrifugo v4 also supports alias for this option – redis_tls_skip_verify – but it will be removed in v5.  redis_tls_cert​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – path to TLS cert file. If you prefer passing certificate as a string instead of path to the file then use redis_tls_cert_pem option.  redis_tls_key​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – path to TLS key file. If you prefer passing cert key as a string instead of path to the file then use redis_tls_key_pem option.  redis_tls_root_ca​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – path to TLS root CA file (in PEM format) to use. If you prefer passing root CA PEM as a string instead of path to the file then use redis_tls_root_ca_pem option.  redis_tls_server_name​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – used to verify the hostname on the returned certificates. It is also included in the client's handshake to support virtual hosting unless it is an IP address.  ","version":"v4","tagName":"h3"},{"title":"Scaling with Redis tutorial​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#scaling-with-redis-tutorial","content":" Let's see how to start several Centrifugo nodes using the Redis Engine. We will start 3 Centrifugo nodes and all those nodes will be connected via Redis.  First, you should have Redis running. As soon as it's running - we can launch 3 Centrifugo instances. Open your terminal and start the first one:  centrifugo --config=config.json --port=8000 --engine=redis --redis_address=127.0.0.1:6379   If your Redis is on the same machine and runs on its default port you can omit redis_address option in the command above.  Then open another terminal and start another Centrifugo instance:  centrifugo --config=config.json --port=8001 --engine=redis --redis_address=127.0.0.1:6379   Note that we use another port number (8001) as port 8000 is already busy by our first Centrifugo instance. If you are starting Centrifugo instances on different machines then you most probably can use the same port number (8000 or whatever you want) for all instances.  And finally, let's start the third instance:  centrifugo --config=config.json --port=8002 --engine=redis --redis_address=127.0.0.1:6379   Now you have 3 Centrifugo instances running on ports 8000, 8001, 8002 and clients can connect to any of them. You can also send API requests to any of those nodes – as all nodes connected over Redis PUB/SUB message will be delivered to all interested clients on all nodes.  To load balance clients between nodes you can use Nginx – you can find its configuration here in the documentation.  tip In the production environment you will most probably run Centrifugo nodes on different hosts, so there will be no need to use different port numbers.  Here is a live example where we locally start two Centrifugo nodes both connected to local Redis:  Sorry, your browser doesn't support embedded video.  ","version":"v4","tagName":"h3"},{"title":"Redis Sentinel for high availability​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#redis-sentinel-for-high-availability","content":" Centrifugo supports the official way to add high availability to Redis - Redis Sentinel.  For this you only need to utilize 2 Redis Engine options: redis_sentinel_address and redis_sentinel_master_name:  redis_sentinel_address (string, default &quot;&quot;) - comma separated list of Sentinel addresses for HA. At least one known server required.redis_sentinel_master_name (string, default &quot;&quot;) - name of Redis master Sentinel monitors  Also:  redis_sentinel_password – optional string password for your Sentinel, works with Redis Sentinel &gt;= 5.0.1redis_sentinel_user - optional string user (used only in Redis ACL-based auth).  So you can start Centrifugo which will use Sentinels to discover Redis master instances like this:  centrifugo --config=config.json   Where config.json:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_sentinel_address&quot;: &quot;127.0.0.1:26379&quot;, &quot;redis_sentinel_master_name&quot;: &quot;mymaster&quot; }   Sentinel configuration file may look like this (for 3-node Sentinel setup with quorum 2):  port 26379 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 10000 sentinel failover-timeout mymaster 60000   You can find how to properly set up Sentinels in official documentation.  Note that when your Redis master instance is down there will be a small downtime interval until Sentinels discover a problem and come to a quorum decision about a new master. The length of this period depends on Sentinel configuration.  ","version":"v4","tagName":"h3"},{"title":"Redis Sentinel TLS​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#redis-sentinel-tls","content":" To configure TLS for Redis Sentinel use the following options.  redis_sentinel_tls​  Boolean, default false - enable Redis TLS connection.  redis_sentinel_tls_insecure_skip_verify​  Boolean, default false - disable Redis TLS host verification. Centrifugo v4 also supports alias for this option – redis_sentinel_tls_skip_verify – but it will be removed in v5.  redis_sentinel_tls_cert​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – path to TLS cert file. If you prefer passing certificate as a string instead of path to the file then use redis_sentinel_tls_cert_pem option.  redis_sentinel_tls_key​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – path to TLS key file. If you prefer passing cert key as a string instead of path to the file then use redis_sentinel_tls_key_pem option.  redis_sentinel_tls_root_ca​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – path to TLS root CA file (in PEM format) to use. If you prefer passing root CA PEM as a string instead of path to the file then use redis_sentinel_tls_root_ca_pem option.  redis_sentinel_tls_server_name​  Added in Centrifugo v4.1.0  String, default &quot;&quot; – used to verify the hostname on the returned certificates. It is also included in the client's handshake to support virtual hosting unless it is an IP address.  ","version":"v4","tagName":"h3"},{"title":"Haproxy instead of Sentinel configuration​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#haproxy-instead-of-sentinel-configuration","content":" Alternatively, you can use Haproxy between Centrifugo and Redis to let it properly balance traffic to Redis master. In this case, you still need to configure Sentinels but you can omit Sentinel specifics from Centrifugo configuration and just use Redis address as in a simple non-HA case.  For example, you can use something like this in Haproxy config:  listen redis server redis-01 127.0.0.1:6380 check port 6380 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 server redis-02 127.0.0.1:6381 check port 6381 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 backup bind *:16379 mode tcp option tcpka option tcplog option tcp-check tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK balance roundrobin   And then just point Centrifugo to this Haproxy:  centrifugo --config=config.json --engine=redis --redis_address=&quot;localhost:16379&quot;   ","version":"v4","tagName":"h3"},{"title":"Redis sharding​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#redis-sharding","content":" Centrifugo has built-in Redis sharding support.  This resolves the situation when Redis becoming a bottleneck on a large Centrifugo setup. Redis is a single-threaded server, it's very fast but its power is not infinite so when your Redis approaches 100% CPU usage then the sharding feature can help your application to scale.  At moment Centrifugo supports a simple comma-based approach to configuring Redis shards. Let's just look at examples.  To start Centrifugo with 2 Redis shards on localhost running on port 6379 and port 6380 use config like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: [ &quot;127.0.0.1:6379&quot;, &quot;127.0.0.1:6380&quot;, ] }   To start Centrifugo with Redis instances on different hosts:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: [ &quot;192.168.1.34:6379&quot;, &quot;192.168.1.35:6379&quot;, ] }   If you also need to customize AUTH password, Redis DB number then you can use an extended address notation.  note Due to how Redis PUB/SUB works it's not possible (and it's pretty useless anyway) to run shards in one Redis instance using different Redis DB numbers.  When sharding enabled Centrifugo will spread channels and history/presence keys over configured Redis instances using a consistent hashing algorithm. At moment we use Jump consistent hash algorithm (see paper and implementation).  ","version":"v4","tagName":"h3"},{"title":"Redis cluster​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#redis-cluster","content":" Running Centrifugo with Redis cluster is simple and can be achieved using redis_cluster_address option. This is an array of strings. Each element of the array is a comma-separated Redis cluster seed node. For example:  { ... &quot;redis_cluster_address&quot;: [ &quot;localhost:30001,localhost:30002,localhost:30003&quot; ] }   You don't need to list all Redis cluster nodes in config – only several working nodes are enough to start.  To set the same over environment variable:  CENTRIFUGO_REDIS_CLUSTER_ADDRESS=&quot;localhost:30001&quot; CENTRIFUGO_ENGINE=redis ./centrifugo   If you need to shard data between several Redis clusters then simply add one more string with seed nodes of another cluster to this array:  { ... &quot;redis_cluster_address&quot;: [ &quot;localhost:30001,localhost:30002,localhost:30003&quot;, &quot;localhost:30101,localhost:30102,localhost:30103&quot; ] }   Sharding between different Redis clusters can make sense due to the fact how PUB/SUB works in the Redis cluster. It does not scale linearly when adding nodes as all PUB/SUB messages got copied to every cluster node. See this discussion for more information on topic. To spread data between different Redis clusters Centrifugo uses the same consistent hashing algorithm described above (i.e. Jump).  To reproduce the same over environment variable use space to separate different clusters:  CENTRIFUGO_REDIS_CLUSTER_ADDRESS=&quot;localhost:30001,localhost:30002 localhost:30101,localhost:30102&quot; CENTRIFUGO_ENGINE=redis ./centrifugo   ","version":"v4","tagName":"h3"},{"title":"KeyDB Engine​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#keydb-engine","content":" EXPERIMENTAL  Centrifugo Redis engine seamlessly works with KeyDB. KeyDB server is compatible with Redis and provides several additional features beyond.  caution We can't give any promises about compatibility with KeyDB in the future Centrifugo releases - while KeyDB is fully compatible with Redis things should work just fine. That's why we consider this as EXPERIMENTAL feature.  Use KeyDB instead of Redis only if you are sure you need it. Nothing stops you from running several Redis instances per each core you have, configure sharding, and obtain even better performance than KeyDB can provide (due to lack of synchronization between threads in Redis).  To run Centrifugo with KeyDB all you need to do is use redis engine but run the KeyDB server instead of Redis.  ","version":"v4","tagName":"h2"},{"title":"Other Redis compatible​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#other-redis-compatible","content":" Other storages which are compatible with Centrifugo may work, but we did not make enough testing with them. Some of them still evolving and do not fully support Redis protocol. So if you want to use these storages with Centrifugo – please read carefully the notes below:  AWS Elasticache – it was reported to work, but we suggest you testing the setup including failover tests and work under load.DragonflyDB - it's mostly compatible, the only problem with DragonflyDB v1.0.0 we observed is failing test regarding history iteration in reversed order (not very common). We have not tested a Redis Cluster emulation mode provided by DragonflyDB yet. We suggest you testing the setup including failover tests and work under load.  ","version":"v4","tagName":"h2"},{"title":"Tarantool engine​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#tarantool-engine","content":" EXPERIMENTAL  Tarantool is a fast and flexible in-memory storage with different persistence/replication schemes and LuaJIT for writing custom logic on the Tarantool side. It allows implementing Centrifugo engine with unique characteristics.  caution EXPERIMENTAL status of Tarantool integration means that we are still going to improve it and there could be breaking changes as integration evolves.  There are many ways to operate Tarantool in production and it's hard to distribute Centrifugo Tarantool engine in a way that suits everyone. Centrifugo tries to fit generic case by providing centrifugal/tarantool-centrifuge module and by providing ready-to-use centrifugal/rotor project based on centrifugal/tarantool-centrifuge and Tarantool Cartridge.  info To be honest we bet on the community help to push this integration further. Tarantool provides an incredible performance boost for presence and history operations (up to 5x more RPS compared to the Redis Engine) and a pretty fast PUB/SUB (comparable to what Redis Engine provides). Let's see what we can build together.  There are several supported Tarantool topologies to which Centrifugo can connect:  One standalone Tarantool instanceMany standalone Tarantool instances and consistently shard data between themTarantool running in CartridgeTarantool with replica and automatic failover in CartridgeMany Tarantool instances (or leader-follower setup) in Cartridge with consistent client-side sharding between themTarantool with synchronous replication (Raft-based, Tarantool &gt;= 2.7)  After running Tarantool you can point Centrifugo to it (and of course scale Centrifugo nodes):  config.json { ... &quot;engine&quot;: &quot;tarantool&quot;, &quot;tarantool_address&quot;: &quot;127.0.0.1:3301&quot; }   See centrifugal/rotor repo for ready-to-use engine based on Tarantool Cartridge framework.  See centrifugal/tarantool-centrifuge repo for examples on how to run engine with Standalone single Tarantool instance or with Raft-based synchronous replication.  ","version":"v4","tagName":"h2"},{"title":"Tarantool engine options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#tarantool-engine-options","content":" tarantool_address​  String or array of strings. Default tcp://127.0.0.1:3301.  Connection address to Tarantool.  tarantool_mode​  String, default standalone  A mode how to connect to Tarantool. Default is standalone which connects to a single Tarantool instance address. Possible values are: leader-follower (connects to a setup with Tarantool master and async replicas) and leader-follower-raft (connects to a Tarantool with synchronous Raft-based replication).  All modes support client-side consistent sharding (similar to what Redis engine provides).  tarantool_user​  String, default &quot;&quot;. Allows setting a user.  tarantool_password​  String, default &quot;&quot;. Allows setting a password.  history_meta_ttl​  Duration, default 2160h.  Same option as for Memory engine and Redis engine also applies to Tarantool case.  ","version":"v4","tagName":"h3"},{"title":"Nats broker​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#nats-broker","content":" It's possible to scale with Nats PUB/SUB server. Keep in mind, that Nats is called a broker here, not an Engine – Nats integration only implements PUB/SUB part of Engine, so carefully read limitations below.  Limitations:  Nats integration works only for unreliable at most once PUB/SUB. This means that history, presence, and message recovery Centrifugo features won't be available.Nats wildcard channel subscriptions with symbols * and &gt; not supported.  First start Nats server:  $ nats-server [3569] 2020/07/08 20:28:44.324269 [INF] Starting nats-server version 2.1.7 [3569] 2020/07/08 20:28:44.324400 [INF] Git commit [not set] [3569] 2020/07/08 20:28:44.325600 [INF] Listening for client connections on 0.0.0.0:4222 [3569] 2020/07/08 20:28:44.325612 [INF] Server id is NDAM7GEHUXAKS5SGMA3QE6ZSO4IQUJP6EL3G2E2LJYREVMAMIOBE7JT4 [3569] 2020/07/08 20:28:44.325617 [INF] Server is ready   Then start Centrifugo with broker option:  centrifugo --broker=nats --config=config.json   And one more Centrifugo on another port (of course in real life you will start another Centrifugo on another machine):  centrifugo --broker=nats --config=config.json --port=8001   Now you can scale connections over Centrifugo instances, instances will be connected over Nats server.  ","version":"v4","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/4/server/engines#options","content":" nats_url​  String, default nats://127.0.0.1:4222.  Connection url in format nats://derek:pass@localhost:4222.  nats_prefix​  String, default centrifugo.  Prefix for channels used by Centrifugo inside Nats.  nats_dial_timeout​  Duration, default 1s.  Timeout for dialing with Nats.  nats_write_timeout​  Duration, default 1s.  Write (and flush) timeout for a connection to Nats. ","version":"v4","tagName":"h3"},{"title":"History and recovery","type":0,"sectionRef":"#","url":"/docs/4/server/history_and_recovery","content":"","keywords":"","version":"v4"},{"title":"History design​","type":1,"pageTitle":"History and recovery","url":"/docs/4/server/history_and_recovery#history-design","content":" History properties configured on a namespace level, to enable history both history_size and history_ttl should be set to a value greater than zero.  Centrifugo is designed with an idea that history streams are ephemeral (can be created on the fly without explicit create call from Centrifugo users) and can expire or can be lost at any moment. Centrifugo provides a way for a client to understand that channel history lost. In this case, the main application database should be the source of truth and state recovery.  When history is on every message published into a channel is saved into a history stream. The persistence properties of history are dictated by a Centrifugo engine used. For example, in the case of the Memory engine, all history is stored in process memory. So as soon as Centrifugo restarted all history is cleared. When using Redis engine history is kept in Redis Stream data structure - persistence properties are then inherited from Redis persistence configuration (the same for KeyDB engine). For Tarantool history is kept inside Tarantool spaces.  Each publication when added to history has an offset field. This is an incremental uint64 field. Each stream is identified by the epoch field - which is an arbitrary string. As soon as the underlying engine loses data epoch field will change for a stream thus letting consumers know that stream can't be used as the source of state recovery anymore.  tip History in channels is not enabled by default. See how to enable it over channel options. History is available in both server and client API.  ","version":"v4","tagName":"h2"},{"title":"History iteration API​","type":1,"pageTitle":"History and recovery","url":"/docs/4/server/history_and_recovery#history-iteration-api","content":" History iteration based on three fields:  limitsincereverse  Combining these fields you can iterate over a stream in both directions.  Get current stream top offset and epoch:  history(limit: 0, since: null, reverse: false)   Get full history from the current beginning (but up to client_history_max_publication_limit, which is 300 by default):  history(limit: -1, since: null, reverse: false)   Get full history from the current end (but up to client_history_max_publication_limit, which is 300 by default):  history(limit: -1, since: null, reverse: true)   Get history from the current beginning (up to 10):  history(limit: 10, since: null, reverse: false)   Get history from the current end in reversed direction (up to 10):  history(limit: 10, since: null, reverse: true)   Get up to 10 publications since known stream position (described by offset and epoch values):  history(limit: 10, since: {offset: 0, epoch: &quot;epoch&quot;}, reverse: false)   Get up to 10 publications since known stream position (described by offset and epoch values) but in reversed direction (from last to earliest):  history(limit: 10, since: {offset: 11, epoch: &quot;epoch&quot;}, reverse: true)   Here is an example program in Go which endlessly iterates over stream both ends (using gocent API library), upon reaching the end of stream the iteration goes in reversed direction (not really useful in real world but fun):  // Iterate by 10. limit := 10 // Paginate in reversed order first, then invert it. reverse := true // Start with nil StreamPosition, then fill it with value while paginating. var sp *gocent.StreamPosition for { historyResult, err = c.History( ctx, channel, gocent.WithLimit(limit), gocent.WithReverse(reverse), gocent.WithSince(sp), ) if err != nil { log.Fatalf(&quot;Error calling history: %v&quot;, err) } for _, pub := range historyResult.Publications { log.Println(pub.Offset, &quot;=&gt;&quot;, string(pub.Data)) sp = &amp;gocent.StreamPosition{ Offset: pub.Offset, Epoch: historyResult.Epoch, } } if len(historyResult.Publications) &lt; limit { // Got all pubs, invert pagination direction. reverse = !reverse log.Println(&quot;end of stream reached, change iteration direction&quot;) } }   ","version":"v4","tagName":"h2"},{"title":"Automatic message recovery​","type":1,"pageTitle":"History and recovery","url":"/docs/4/server/history_and_recovery#automatic-message-recovery","content":" One of the most interesting features of Centrifugo is automatic message recovery after short network disconnects. This mechanism allows a client to automatically restore missed publications on successful resubscribe to a channel after being disconnected for a while.  In general, you could query your application backend for the actual state on every client reconnect - but the message recovery feature allows Centrifugo to deal with this and restore missed publications from the history cache thus radically reducing the load on your application backend and your main application database in some scenarios (when many clients reconnect at the same time).  danger Message recovery protocol feature designed to be used together with reasonably small history stream size as all missed publications sent towards the client in one protocol frame on resubscribing to a channel. Thus, it is mostly suitable for short-time disconnects. It helps a lot to survive a reconnect storm when many clients reconnect at one moment (balancer reload, network glitch) - but it's not a good idea to recover a long list of missed messages after clients being offline for a long time.  To enable recovery mechanism for channels set the force_recovery boolean configuration option to true on the configuration file top-level or for a channel namespace. Make sure to enable this option in namespaces where history is on. It's also possible to ask for enabling recovery from the client-side when configuring Subscription object – in this case client must have a permission to call history API.  When re-subscribing on channels Centrifugo will return missed publications to a client in a subscribe Reply, also it will return a special recovered boolean flag to indicate whether all missed publications successfully recovered after a disconnect or not.  The number of publications that is possible to automatically recover is controlled by the client_recovery_max_publication_limit option which is 300 by default.  Centrifugo recovery model based on two fields in the protocol: offset and epoch. All fields are managed automatically by Centrifugo client SDKs (for bidirectional transport).  The recovery process works this way:  Let's suppose client subscribes on a channel with recovery on.Client receives subscribe reply from Centrifugo with two values: stream epoch and stream top offset, those values are saved by an SDK implementation.Every received publication has incremental offset, client SDK increments locally saved offset on each publication from a channel.Let's say at this point client disconnected for a while.Upon resubscribing to a channel SDK provides last seen epoch anf offset to Centrifugo.Centrifugo tries to load all the missed publications starting from the stream position provided by a client.If Centrifugo decides it can successfully recover client's state – then all missed publications returned in subscribe reply and client receives recovered: true in subscribed event context, and publication event handler of Subscription is called for every missed publication. Otherwise no publications returned and recovered flag of subscribed event context is set to false.  epoch is useful for cases when history storage is temporary and can lose the history stream entirely. In this case comparing epoch values gives Centrifugo a tip that while publications exist and theoretically have same offsets as before - the stream is actually different, so it's impossible to use it for the recovery process.  To summarize, here is a list of possible scenarios when Centrifugo can't recover client's state for a channel and provides recovered: false flag in subscribed event context:  number of missed publications exceeds client_recovery_max_publication_limit optionnumber of missed publications exceeds history_size namespace optionclient was away for a long time and history stream expired according to history_ttl namespace optionstorage used by Centrifugo engine lost the stream (restart, number of shards changed, cleared by the administrator, etc.)  Having said this all, Centrifugo recovery is nice to keep the continuity of the connection and subscription. This speed-ups resubscribe in many cases and helps the backend to survive mass reconnect scenario since you avoid lots of requests for state loading. For setups with millions of connections this can be a life-saver. But we recommend applications to always have a way to load the state from the main application database. For example, on app reload.  You can also manually implement your own recovery logic on top of the basic PUB/SUB possibilities that Centrifugo provides. As we said above you can simply ask your backend for an actual state after every client resubscribe completely bypassing the recovery mechanism described here. Also, it's possible to manually iterate over the Centrifugo stream using the history iteration API described above. ","version":"v4","tagName":"h2"},{"title":"Infrastructure tuning","type":0,"sectionRef":"#","url":"/docs/4/server/infra_tuning","content":"","keywords":"","version":"v4"},{"title":"Open files limit​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/4/server/infra_tuning#open-files-limit","content":" You should increase a max number of open files Centrifugo process can open if you want to handle more connections.  To get the current open files limit run:  ulimit -n   On Linux you can check limits for a running process using:  cat /proc/&lt;PID&gt;/limits   The open file limit shows approximately how many clients your server can handle. Each connection consumes one file descriptor. On most operating systems this limit is 128-256 by default.  See this document to get more info on how to increase this number.  If you install Centrifugo using RPM from repo then it automatically sets max open files limit to 65536.  You may also need to increase max open files for Nginx (or any other proxy before Centrifugo).  ","version":"v4","tagName":"h3"},{"title":"Ephemeral port exhaustion​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/4/server/infra_tuning#ephemeral-port-exhaustion","content":" Ephemeral ports exhaustion problem can happen between your load balancer and Centrifugo server. If your clients connect directly to Centrifugo without any load balancer or reverse proxy software between then you are most likely won't have this problem. But load balancing is a very common thing.  The problem arises due to the fact that each TCP connection uniquely identified in the OS by the 4-part-tuple:  source ip | source port | destination ip | destination port   On load balancer/server boundary you are limited in 65536 possible variants by default. Actually due to some OS limits not all 65536 ports are available for usage (usually about 15k ports available by default).  In order to eliminate a problem you can:  Increase the ephemeral port range by tuning ip_local_port_range optionDeploy more Centrifugo server instances to load balance acrossDeploy more load balancer instancesUse virtual network interfaces  See a post in Pusher blog about this problem and more detailed solution steps.  ","version":"v4","tagName":"h3"},{"title":"Sockets in TIME_WAIT state​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/4/server/infra_tuning#sockets-in-time_wait-state","content":" On load balancer/server boundary one more problem can arise: sockets in TIME_WAIT state.  Under load when lots of connections and disconnections happen socket descriptors can stay in TIME_WAIT state. Those descriptors can not be reused for a while. So you can get various errors when using Centrifugo. For example something like (99: Cannot assign requested address) while connecting to upstream in Nginx error log and 502 on client side.  Look how many socket descriptors in TIME_WAIT state.  netstat -an |grep TIME_WAIT | grep &lt;CENTRIFUGO_PID&gt; | wc -l   Nice article about TIME_WAIT sockets: http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html  The advices here are similar to ephemeral port exhaustion problem:  Increase the ephemeral port range by tuning ip_local_port_range optionDeploy more Centrifugo server instances to load balance acrossDeploy more load balancer instancesUse virtual network interfaces  ","version":"v4","tagName":"h3"},{"title":"Proxy max connections​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/4/server/infra_tuning#proxy-max-connections","content":" Proxies like Nginx and Envoy have default limits on maximum number of connections which can be established.  Make sure you have a reasonable limit for max number of incoming and outgoing connections in your proxy configuration.  ","version":"v4","tagName":"h3"},{"title":"Conntrack table​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/4/server/infra_tuning#conntrack-table","content":" More rare (since default limit is usually sufficient) your possible number of connections can be limited by conntrack table. Netfilter framework which is part of iptables keeps information about all connections and has limited size for this information. See how to see its limits and instructions to increase in this article.  ","version":"v4","tagName":"h3"},{"title":"Additional server protection​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/4/server/infra_tuning#additional-server-protection","content":" You should also consider adding additional protection to your Centrifugo endpoints. Centrifugo itself provides several options (described in configuration section) regarding server protection from the malicious behavior. Though an additional layer of DDOS protection on network or infrastructure level is highly recommended. For example, you may want to limit the number of connections coming from particular IP address.  Here we list some possible ways you can use to protect your Centrifugo installation:  Adding Nginx limit_conn_zone configurationUsing stick tables of HaproxyConfiguring rate limiting rules with Cloudflare  The list is not exhaustive of course. ","version":"v4","tagName":"h3"},{"title":"Load balancing","type":0,"sectionRef":"#","url":"/docs/4/server/load_balancing","content":"","keywords":"","version":"v4"},{"title":"Nginx configuration​","type":1,"pageTitle":"Load balancing","url":"/docs/4/server/load_balancing#nginx-configuration","content":" Although it's possible to use Centrifugo without any reverse proxy before it, it's still a good idea to keep Centrifugo behind mature reverse proxy to deal with edge cases when handling HTTP/Websocket connections from the wild. Also you probably want some sort of load balancing eventually between Centrifugo nodes so that proxy can be such a balancer too.  In this section we will look at Nginx configuration to deploy Centrifugo.  Minimal Nginx version – 1.3.13 because it was the first version that can proxy Websocket connections.  There are 2 ways: running Centrifugo server as separate service on its own domain or embed it to a location of your web site (for example to /centrifugo).  ","version":"v4","tagName":"h2"},{"title":"Separate domain for Centrifugo​","type":1,"pageTitle":"Load balancing","url":"/docs/4/server/load_balancing#separate-domain-for-centrifugo","content":" upstream centrifugo { # uncomment ip_hash if using SockJS transport with many upstream servers. #ip_hash; server 127.0.0.1:8000; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } #server { #\tlisten 80; #\tserver_name centrifugo.example.com; #\trewrite ^(.*) https://$server_name$1 permanent; #} server { server_name centrifugo.example.com; listen 80; #listen 443 ssl; #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #ssl_certificate /etc/nginx/ssl/server.crt; #ssl_certificate_key /etc/nginx/ssl/server.key; include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; tcp_nodelay on; gzip on; gzip_min_length 1000; gzip_proxied any; # Only retry if there was a communication error, not a timeout # on the Centrifugo server (to avoid propagating &quot;queries of death&quot; # to all frontends) proxy_next_upstream error; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; location /connection { proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_http_version 1.1; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } location / { proxy_pass http://centrifugo; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } }   ","version":"v4","tagName":"h3"},{"title":"Embed to a location of web site​","type":1,"pageTitle":"Load balancing","url":"/docs/4/server/load_balancing#embed-to-a-location-of-web-site","content":" upstream centrifugo { # uncomment ip_hash if using SockJS transport with many upstream servers. #ip_hash; server 127.0.0.1:8000; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { # ... your web site Nginx config location /centrifugo/ { rewrite ^/centrifugo/(.*) /$1 break; proxy_pass http://centrifugo; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; } location /centrifugo/connection { rewrite ^/centrifugo(.*) $1 break; proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } }   ","version":"v4","tagName":"h3"},{"title":"worker_connections​","type":1,"pageTitle":"Load balancing","url":"/docs/4/server/load_balancing#worker_connections","content":" You may also need to update worker_connections option of Nginx:  events { worker_connections 65535; }  ","version":"v4","tagName":"h3"},{"title":"Metrics monitoring","type":0,"sectionRef":"#","url":"/docs/4/server/monitoring","content":"","keywords":"","version":"v4"},{"title":"Prometheus​","type":1,"pageTitle":"Metrics monitoring","url":"/docs/4/server/monitoring#prometheus","content":" To enable Prometheus endpoint start Centrifugo with prometheus option on:  config.json { ... &quot;prometheus&quot;: true }   This will enable /metrics endpoint so the Centrifugo instance can be monitored by your Prometheus server.  ","version":"v4","tagName":"h3"},{"title":"Graphite​","type":1,"pageTitle":"Metrics monitoring","url":"/docs/4/server/monitoring#graphite","content":" To enable automatic export to Graphite (via TCP):  config.json { ... &quot;graphite&quot;: true, &quot;graphite_host&quot;: &quot;localhost&quot;, &quot;graphite_port&quot;: 2003 }   By default, stats will be aggregated over 10 seconds intervals inside Centrifugo and then pushed to Graphite over TCP connection.  If you need to change this aggregation interval use the graphite_interval option (in seconds, default 10).  ","version":"v4","tagName":"h3"},{"title":"Grafana dashboard​","type":1,"pageTitle":"Metrics monitoring","url":"/docs/4/server/monitoring#grafana-dashboard","content":" Check out Centrifugo official Grafana dashboard for Prometheus storage. You can import that dashboard to your Grafana, point to Prometheus storage – and enjoy visualized metrics.   ","version":"v4","tagName":"h3"},{"title":"Online presence","type":0,"sectionRef":"#","url":"/docs/4/server/presence","content":"","keywords":"","version":"v4"},{"title":"Overview​","type":1,"pageTitle":"Online presence","url":"/docs/4/server/presence#overview","content":" Online presence provides an instantaneous snapshot of users currently connected to a specific channel. This information includes the user's unique ID and the connection timestamp.  ","version":"v4","tagName":"h2"},{"title":"Enabling Online Presence​","type":1,"pageTitle":"Online presence","url":"/docs/4/server/presence#enabling-online-presence","content":" To enable Online Presence, you need to set the presence option to true for the specific channel in your server configuration.  { &quot;namespaces&quot;: [{ &quot;namespace&quot;: &quot;public&quot;, &quot;presence&quot;: true }] }   After enabling this you can query presence information over server HTTP/GRPC presence call:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;Authorization: apikey YOUR_API_KEY&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;presence&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;public:test&quot;}}' \\ http://localhost:8000/api   See description of presence API.  ","version":"v4","tagName":"h2"},{"title":"Retrieving presence on the client side​","type":1,"pageTitle":"Online presence","url":"/docs/4/server/presence#retrieving-presence-on-the-client-side","content":" Once presence enabled, you can retrieve the presence information on the client side too by calling the presence method on the channel.  To do this you need to give the client permission to call presence. Once done, presence may be retrieved from the subscription:  const presenceData = await subscription.presence(channel);   It's also available on the top-level of the client (for example, if you need to call presence for server-side subscription):  const presenceData = await client.presence(channel);   If the permission check has passed successfully – both methods will return an object containing information about currently subscribed clients.  ","version":"v4","tagName":"h2"},{"title":"Join and leave events​","type":1,"pageTitle":"Online presence","url":"/docs/4/server/presence#join-and-leave-events","content":" Online Presence feature also allows real-time tracking of users joining or leaving a channel by subscribing to join and leave events:  subscription.on('join', function(joinCtx) { console.log('client joined:', joinCtx); }); subscription.on('leave', function(leaveCtx) { console.log('client left:', leaveCtx); });   And the same on client top-level for the needs of server-side subscriptions (analogous to the presence call described above).  These events provide real-time updates and can be used to keep track of user activity and manage live interactions.  ","version":"v4","tagName":"h2"},{"title":"Implementation notes​","type":1,"pageTitle":"Online presence","url":"/docs/4/server/presence#implementation-notes","content":" The Online Presence feature might increase the load on your Centrifugo server, since Centrifugo need to maintain an addition data structure. Therefore, it is recommended to use this feature judiciously based on your server's capability and the necessity of real-time presence data in your application.  Always make sure to secure the presence data, as it could expose sensitive information about user activity in your application. Ensure appropriate security measures are in place.  Join and leave events delivered with at most once guarantee.  See more about presence design in design overview chapter.  Also check out FAQ.  ","version":"v4","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Online presence","url":"/docs/4/server/presence#conclusion","content":" The Online Presence feature of Centrifugo is a highly useful tool for real-time applications. It provides instant and live data about user activity, which can be critical for interactive features like chats, collaborative tools, multiplayer games, or live tracking systems. Make sure to configure and use this feature appropriately to get the most out of your real-time application. ","version":"v4","tagName":"h2"},{"title":"Proxy events to the backend","type":0,"sectionRef":"#","url":"/docs/4/server/proxy","content":"","keywords":"","version":"v4"},{"title":"HTTP proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#http-proxy","content":" HTTP proxy in Centrifugo converts client connection events into HTTP calls to the application backend.  ","version":"v4","tagName":"h2"},{"title":"HTTP request structure​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#http-request-structure","content":" All proxy calls are HTTP POST requests that will be sent from Centrifugo to configured endpoints with a configured timeout. These requests will have some headers copied from the original client request (see details below) and include JSON body which varies depending on call type (for example data sent by a client in RPC call etc, see more details about JSON bodies below).  ","version":"v4","tagName":"h3"},{"title":"Proxy HTTP headers​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#proxy-http-headers","content":" The good thing about Centrifugo HTTP proxy is that it transparently proxies original HTTP request headers in a request to app backend. In most cases this allows achieving transparent authentication on the application backend side. But it's required to provide an explicit list of HTTP headers you want to be proxied, for example:  config.json { ... &quot;proxy_http_headers&quot;: [ &quot;Origin&quot;, &quot;User-Agent&quot;, &quot;Cookie&quot;, &quot;Authorization&quot;, &quot;X-Real-Ip&quot;, &quot;X-Forwarded-For&quot;, &quot;X-Request-Id&quot; ] }   Alternatively, you can set a list of headers via an environment variable (space separated):  export CENTRIFUGO_PROXY_HTTP_HEADERS=&quot;Cookie User-Agent X-B3-TraceId X-B3-SpanId&quot; ./centrifugo   note Centrifugo forces the Content-Type header to be application/json in all HTTP proxy requests since it sends the body in JSON format to the application backend.  ","version":"v4","tagName":"h3"},{"title":"Proxy GRPC metadata​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#proxy-grpc-metadata","content":" When GRPC unidirectional stream is used as a client transport then you may want to proxy GRPC metadata from the client request. In this case you may configure proxy_grpc_metadata option. This is an array of string metadata keys which will be proxied. These metadata keys transformed to HTTP headers of proxy request. By default no metadata keys are proxied.  See below the table of rules how metadata and headers proxied in transport/proxy different scenarios.  ","version":"v4","tagName":"h3"},{"title":"Connect proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#connect-proxy","content":" With the following options in the configuration file:  { ... &quot;proxy_connect_endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;proxy_connect_timeout&quot;: &quot;1s&quot; }   – connection requests without JWT set will be proxied to proxy_connect_endpoint URL endpoint. On your backend side, you can authenticate the incoming connection and return client credentials to Centrifugo in response to the proxied request.  danger Make sure you properly configured allowed_origins Centrifugo option or check request origin on your backend side upon receiving connect request from Centrifugo. Otherwise, your site can be vulnerable to CSRF attacks if you are using WebSocket transport for client connections.  Yes, this means you don't need to generate JWT and pass it to a client-side and can rely on a cookie while authenticating the user. Centrifugo should work on the same domain in this case so your site cookie could be passed to Centrifugo by browsers. In many cases your existing session mechanism will provide user authentication details to the connect proxy handler.  tip If you want to pass some custom authentication token from a client side (not in Centrifugo JWT format) but force request to be proxied then you may put it in a cookie or use connection request custom data field (available in all our transports). This data can contain arbitrary payload you want to pass from a client to a server.  This also means that every new connection from a user will result in an HTTP POST request to your application backend. While with JWT token you usually generate it once on application page reload, if client reconnects due to Centrifugo restart or internet connection loss it uses the same JWT it had before thus usually no additional requests are generated during reconnect process (until JWT expired).    Payload example that will be sent to app backend when client without token wants to establish a connection with Centrifugo and proxy_connect_endpoint is set to non-empty URL string:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot; }   Expected response example:  {&quot;result&quot;: {&quot;user&quot;: &quot;56&quot;}}   This response allows connecting and tells Centrifugo the ID of a user. See below the full list of supported fields in the result.  Several app examples which use connect proxy can be found in our blog:  With NodeJSWith DjangoWith Laravel  Connect request fields​  This is what sent from Centrifugo to application backend in case of connect proxy request.  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) name\tstring\tyes\toptional name of the client (this field will only be set if provided by a client on connect) version\tstring\tyes\toptional version of the client (this field will only be set if provided by a client on connect) data\tJSON\tyes\toptional data from client (this field will only be set if provided by a client on connect) b64data\tstring\tyes\toptional data from the client in base64 format (if the binary proxy mode is used) channels\tArray of strings\tyes\tlist of server-side channels client want to subscribe to, the application server must check permissions and add allowed channels to result  Connect result fields​  This is what application returns to Centrifugo inside result field in case of connect proxy request.  Field\tType\tOptional\tDescriptionuser\tstring\tno\tuser ID (calculated on app backend based on request cookie header for example). Return it as an empty string for accepting unauthenticated requests expire_at\tinteger\tyes\ta timestamp when connection must be considered expired. If not set or set to 0 connection won't expire at all info\tJSON\tyes\ta connection info JSON b64info\tstring\tyes\tbinary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages data\tJSON\tyes\ta custom data to send to the client in connect command response. b64data\tstring\tyes\ta custom data to send to the client in the connect command response for binary connections, will be decoded to raw bytes on Centrifugo side before sending to client channels\tarray of strings\tyes\tallows providing a list of server-side channels to subscribe connection to. See more details about server-side subscriptions subs\tmap of SubscribeOptions\tyes\tmap of channels with options to subscribe connection to. See more details about server-side subscriptions meta\tJSON object (ex. {&quot;key&quot;: &quot;value&quot;})\tyes\ta custom data to attach to connection (this won't be exposed to client-side)  Options​  proxy_connect_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  Example​  Here is the simplest example of the connect handler in Tornado Python framework (note that in a real system you need to authenticate the user on your backend side, here we just return &quot;56&quot; as user ID):  class CentrifugoConnectHandler(tornado.web.RequestHandler): def check_xsrf_cookie(self): pass def post(self): self.set_header('Content-Type', 'application/json; charset=&quot;utf-8&quot;') data = json.dumps({ 'result': { 'user': '56' } }) self.write(data) def main(): options.parse_command_line() app = tornado.web.Application([ (r'/centrifugo/connect', CentrifugoConnectHandler), ]) app.listen(3000) tornado.ioloop.IOLoop.instance().start() if __name__ == '__main__': main()   This example should help you to implement a similar HTTP handler in any language/framework you are using on the backend side.  We also have a tutorial in the blog about Centrifugo integration with NodeJS which uses connect proxy and native session middleware of Express.js to authenticate connections. Even if you are not using NodeJS on a backend a tutorial can help you understand the idea.  What if connection is unauthenticated/unauthorized to connect?​  In this case return a disconnect object as a response. See Return custom disconnect section. Depending on whether you want connection to reconnect or not (usually not) you can select the appropriate disconnect code. Sth like this in response:  { &quot;disconnect&quot;: { &quot;code&quot;: 4501, &quot;reason&quot;: &quot;unauthorized&quot; } }   – may be sufficient enough. Choosing codes and reason is up to the developer, but follow the rules described in Return custom disconnect section.  ","version":"v4","tagName":"h3"},{"title":"Refresh proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#refresh-proxy","content":" With the following options in the configuration file:  { ... &quot;proxy_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/refresh&quot;, &quot;proxy_refresh_timeout&quot;: &quot;1s&quot; }   – Centrifugo will call proxy_refresh_endpoint when it's time to refresh the connection. Centrifugo itself will ask your backend about connection validity instead of refresh workflow on the client-side.  The payload sent to app backend in refresh request (when the connection is going to expire):  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot; }   Expected response example:  {&quot;result&quot;: {&quot;expire_at&quot;: 1565436268}}   Refresh request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc.) protocol\tstring\tno\tprotocol type used by client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Refresh result fields​  Field\tType\tOptional\tDescriptionexpired\tbool\tyes\ta flag to mark the connection as expired - the client will be disconnected expire_at\tinteger\tyes\ta timestamp in the future when connection must be considered expired info\tJSON\tyes\ta connection info JSON b64info\tstring\tyes\tbinary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages  Options​  proxy_refresh_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  ","version":"v4","tagName":"h3"},{"title":"RPC proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#rpc-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_rpc_endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;proxy_rpc_timeout&quot;: &quot;1s&quot; }   RPC calls over client connection will be proxied to proxy_rpc_endpoint. This allows a developer to utilize WebSocket (or SockJS) connection in a bidirectional way.  Payload example sent to app backend in RPC request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;method&quot;: &quot;getCurrentPrice&quot;, &quot;data&quot;:{&quot;params&quot;: {&quot;object_id&quot;: 12}} }   Expected response example:  {&quot;result&quot;: {&quot;data&quot;: {&quot;answer&quot;: &quot;2019&quot;}}}   RPC request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket or sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process method\tstring\tyes\tan RPC method string, if the client does not use named RPC call then method will be omitted data\tJSON\tyes\tRPC custom data sent by client b64data\tstring\tyes\twill be set instead of data field for binary proxy mode meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  RPC result fields​  Field\tType\tOptional\tDescriptiondata\tJSON\tyes\tRPC response - any valid JSON is supported b64data\tstring\tyes\tcan be set instead of data for binary response encoded in base64 format  Options​  proxy_rpc_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  See below on how to return a custom error.  ","version":"v4","tagName":"h3"},{"title":"Subscribe proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#subscribe-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot; }   – subscribe requests sent over client connection will be proxied to proxy_subscribe_endpoint. This allows you to check the access of the client to a channel.  tip Subscribe proxy does not proxy private and user-limited channels at the moment. That's because those are already providing a level of security (user-limited channels check current user ID, private channels require subscription token). In some cases you may use subscribe proxy as a replacement for private channels actually: if you prefer to check permissions using the proxy to backend mechanism – just stop using $ prefixes in channels, properly configure subscribe proxy and validate subscriptions upon proxy from Centrifugo to your backend (issued each time user tries to subscribe on a channel for which subscribe proxy enabled).  Unlike proxy types described above subscribe proxy must be enabled per channel namespace. This means that every namespace (including global/default one) has a boolean option proxy_subscribe that enables subscribe proxy for channels in a namespace.  So to enable subscribe proxy for channels without namespace define proxy_subscribe on a top configuration level:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;proxy_subscribe&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;proxy_subscribe&quot;: true }] }   Payload example sent to app backend in subscribe request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;chat:index&quot; }   Expected response example if subscription is allowed:  {&quot;result&quot;: {}}   Subscribe request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket or sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\ta string channel client wants to subscribe to meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true) data\tJSON\tyes\tcustom data from client sent with subscription request (this field will only be set if provided by a client on subscribe). b64data\tstring\tyes\toptional subscription data from the client in base64 format (if the binary proxy mode is used).  Subscribe result fields​  Field\tType\tOptional\tDescriptioninfo\tJSON\tyes\ta channel info JSON b64info\tstring\tyes\ta binary connection channel info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using data\tJSON\tyes\ta custom data to send to the client in subscribe command reply. b64data\tstring\tyes\ta custom data to send to the client in subscribe command reply, will be decoded to raw bytes on Centrifugo side before sending to client override\tOverride object\tyes\tAllows dynamically override some channel options defined in Centrifugo configuration on a per-connection basis (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave force_push_join_leave\tBoolValue\tyes\tOverride force_push_join_leave force_positioning\tBoolValue\tyes\tOverride force_positioning force_recovery\tBoolValue\tyes\tOverride force_recovery  BoolValue is an object like this:  { &quot;value&quot;: true/false }   See below on how to return an error in case you don't want to allow subscribing.  Options​  proxy_subscribe_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  What if connection is not allowed to subscribe?​  In this case you can return error object as a subscribe handler response. See return custom error section.  In general, frontend applications should not try to subscribe to channels for which access is not allowed. But these situations can happen or malicious user can try to subscribe to a channel. In most scenarios returning:  { &quot;error&quot;: { &quot;code&quot;: 403, &quot;message&quot;: &quot;permission denied&quot; } }   – is sufficient enough. Error code may be not 403 actually, no real reason to force HTTP semantics here - so it's up to Centrifugo user to decide. Just keep it in range [400, 1999] as described here.  If case of returning response above, on client side unsubscribed event of Subscription object will be called with error code 403. Subscription won't resubscribe automatically after that.  ","version":"v4","tagName":"h3"},{"title":"Publish proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#publish-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot; }   – publish calls sent by a client will be proxied to proxy_publish_endpoint.  This request happens BEFORE a message is published to a channel, so your backend can validate whether a client can publish data to a channel. An important thing here is that publication to the channel can fail after your backend successfully validated publish request (for example publish to Redis by Centrifugo returned an error). In this case, your backend won't know about the error that happened but this error will propagate to the client-side.    Like the subscribe proxy, publish proxy must be enabled per channel namespace. This means that every namespace (including the global/default one) has a boolean option proxy_publish that enables publish proxy for channels in the namespace. All other namespace options will be taken into account before making a proxy request, so you also need to turn on the publish option too.  So to enable publish proxy for channels without namespace define proxy_publish and publish on a top configuration level:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true }] }   Keep in mind that this will only work if the publish channel option is on for a channel namespace (or for a global top-level namespace).  Payload example sent to app backend in a publish request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;chat:index&quot;, &quot;data&quot;:{&quot;input&quot;:&quot;hello&quot;} }   Expected response example if publish is allowed:  {&quot;result&quot;: {}}   Publish request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\ta string channel client wants to publish to data\tJSON\tyes\tdata sent by client b64data\tstring\tyes\twill be set instead of data field for binary proxy mode meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Publish result fields​  Field\tType\tOptional\tDescriptiondata\tJSON\tyes\tan optional JSON data to send into a channel instead of original data sent by a client b64data\tstring\tyes\ta binary data encoded in base64 format, the meaning is the same as for data above, will be decoded to raw bytes on Centrifugo side before publishing skip_history\tbool\tyes\twhen set to true Centrifugo won't save publication to the channel history  See below on how to return an error in case you don't want to allow publishing.  Options​  proxy_publish_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  ","version":"v4","tagName":"h3"},{"title":"Sub refresh proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#sub-refresh-proxy","content":" Added in Centrifugo v4.1.1  With the following options in the configuration file:  { ... &quot;proxy_sub_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/sub_refresh&quot;, &quot;proxy_sub_refresh_timeout&quot;: &quot;1s&quot; }   – Centrifugo will call proxy_sub_refresh_endpoint when it's time to refresh the subscription. Centrifugo itself will ask your backend about subscription validity instead of subscription refresh workflow on the client-side.  Like subscribe and publish proxy types, sub refresh proxy must be enabled per channel namespace. This means that every namespace (including the global/default one) has a boolean option proxy_sub_refresh that enables sub refresh proxy for channels in the namespace. Only subscriptions which have expiration time will be validated over sub refresh proxy endpoint.  Sub refresh proxy may be used as a periodical Subscription liveness callback from Centrifugo to app backend.  caution In the current implementation the delay of Subscription refresh requests from Centrifugo to application backend may be up to one minute (was implemented this way from a simplicity and efficiency perspective). We assume this should be enough for many scenarios. But this may be improved if needed. Please reach us out with a detailed description of your use case where you want more accurate requests to refresh subscriptions.  So to enable sub refresh proxy for channels without namespace define proxy_sub_refresh on a top configuration level:  { ... &quot;proxy_sub_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/sub_refresh&quot;, &quot;proxy_sub_refresh&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_sub_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;proxy_sub_refresh&quot;: true }] }   The payload sent to app backend in sub refresh request (when the subscription is going to expire):  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;channel&quot; }   Expected response example:  {&quot;result&quot;: {&quot;expire_at&quot;: 1565436268}}   Sub refresh request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc.) protocol\tstring\tno\tprotocol type used by client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\tchannel for which Subscription is going to expire meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Sub refresh result fields​  Field\tType\tOptional\tDescriptionexpired\tbool\tyes\ta flag to mark the connection as expired - the client will be disconnected expire_at\tinteger\tyes\ta timestamp in the future when connection must be considered expired info\tJSON\tyes\ta channel info JSON b64info\tstring\tyes\tbinary channel info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages  Options​  proxy_sub_refresh_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  ","version":"v4","tagName":"h3"},{"title":"Return custom error​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#return-custom-error","content":" Application backend can return JSON object that contains an error to return it to the client:  { &quot;error&quot;: { &quot;code&quot;: 1000, &quot;message&quot;: &quot;custom error&quot; } }   Applications must use error codes in range [400, 1999]. Error code field is uint32 internally.  note Returning custom error does not apply to response for refresh and sub refresh proxy requests as there is no sense in returning an error (will not reach client anyway).  ","version":"v4","tagName":"h3"},{"title":"Return custom disconnect​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#return-custom-disconnect","content":" Application backend can return JSON object that contains a custom disconnect object to disconnect client in a custom way:  { &quot;disconnect&quot;: { &quot;code&quot;: 4500, &quot;reason&quot;: &quot;disconnect reason&quot; } }   Application must use numbers in the range 4000-4999 for custom disconnect codes:  codes in range [4000, 4499] give client an advice to reconnectcodes in range [4500, 4999] are terminal codes – client won't reconnect upon receiving it.  Code is uint32 internally. Numbers outside of 4000-4999 range are reserved by Centrifugo internal protocol. Keep in mind that due to WebSocket protocol limitations and Centrifugo internal protocol needs you need to keep disconnect reason string no longer than 32 ASCII symbols (i.e. 32 bytes max).  note Returning custom disconnect does not apply to response for refresh and sub refresh proxy requests as there is no way to control disconnect at moment - the client will always be disconnected with expired disconnect reason.  ","version":"v4","tagName":"h3"},{"title":"GRPC proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#grpc-proxy","content":" Centrifugo can also proxy connection events to your backend over GRPC instead of HTTP. In this case, Centrifugo acts as a GRPC client and your backend acts as a GRPC server.  GRPC service definitions can be found in the Centrifugo repository: proxy.proto.  tip GRPC proxy inherits all the fields for HTTP proxy – so you can refer to field descriptions for HTTP above. Both proxy types in Centrifugo share the same Protobuf schema definitions.  Every proxy call in this case is a unary GRPC call. Centrifugo puts client headers into GRPC metadata (since GRPC doesn't have headers concept).  All you need to do to enable proxying over GRPC instead of HTTP is to use grpc schema in endpoint, for example for the connect proxy:  config.json { ... &quot;proxy_connect_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_connect_timeout&quot;: &quot;1s&quot; }   Refresh proxy:  config.json { ... &quot;proxy_refresh_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_refresh_timeout&quot;: &quot;1s&quot; }   Or for RPC proxy:  config.json { ... &quot;proxy_rpc_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_rpc_timeout&quot;: &quot;1s&quot; }   For publish proxy in namespace chat:  config.json { ... &quot;proxy_publish_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot; &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;chat&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true } ] }   Use subscribe proxy for all channels without namespaces:  config.json { ... &quot;proxy_subscribe_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;proxy_subscribe&quot;: true }   So the same as for HTTP, just the different endpoint scheme.  ","version":"v4","tagName":"h2"},{"title":"GRPC proxy options​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#grpc-proxy-options","content":" Some additional options exist to control GRPC proxy behavior.  proxy_grpc_cert_file​  String, default: &quot;&quot;.  Path to cert file for secure TLS connection. If not set then an insecure connection with the backend endpoint is used.  proxy_grpc_credentials_key​  String, default &quot;&quot; (i.e. not used).  Add custom key to per-RPC credentials.  proxy_grpc_credentials_value​  String, default &quot;&quot; (i.e. not used).  A custom value for proxy_grpc_credentials_key.  ","version":"v4","tagName":"h3"},{"title":"GRPC proxy example​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#grpc-proxy-example","content":" We have an example of backend server (written in Go language) which can react to events from Centrifugo over GRPC. For other programming languages the approach is similar, i.e.:  Copy proxy Protobuf definitionsGenerate GRPC codeRun backend service with you custom business logicPoint Centrifugo to it.  ","version":"v4","tagName":"h3"},{"title":"Header proxy rules​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#header-proxy-rules","content":" Centrifugo not only supports HTTP-based client transports but also GRPC-based (for example GRPC unidirectional stream). Here is a table with rules used to proxy headers/metadata in various scenarios:  Client protocol type\tProxy type\tClient headers\tClient metadataHTTP\tHTTP\tIn proxy request headers\tN/A GRPC\tGRPC\tN/A\tIn proxy request metadata HTTP\tGRPC\tIn proxy request metadata\tN/A GRPC\tHTTP\tN/A\tIn proxy request headers  ","version":"v4","tagName":"h2"},{"title":"Binary mode​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#binary-mode","content":" As you may noticed there are several fields in request/result description of various proxy calls which use base64 encoding.  Centrifugo can work with binary Protobuf protocol (in case of bidirectional WebSocket transport). All our bidirectional clients support this.  Most Centrifugo users use JSON for custom payloads: i.e. for data sent to a channel, for connection info attached while authenticating (which becomes part of presence response, join/leave messages and added to Publication client info when message published from a client side).  But since HTTP proxy works with JSON format (i.e. sends requests with JSON body) – it can not properly pass binary data to application backend. Arbitrary binary data can't be encoded into JSON.  In this case it's possible to turn Centrifugo proxy into binary mode by using:  config.json { ... &quot;proxy_binary_encoding&quot;: true }   Once enabled this option tells Centrifugo to use base64 format in requests and utilize fields like b64data, b64info with payloads encoded to base64 instead of their JSON field analogues.  While this feature is useful for HTTP proxy it's not really required if you are using GRPC proxy – since GRPC allows passing binary data just fine.  Regarding b64 fields in proxy results – just use base64 fields when required – Centrifugo is smart enough to detect that you are using base64 field and will pick payload from it, decode from base64 automatically and will pass further to connections in binary format.  ","version":"v4","tagName":"h2"},{"title":"Granular proxy mode​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#granular-proxy-mode","content":" By default, with proxy configuration shown above, you can only define a global proxy settings and one endpoint for each type of proxy (i.e. one for connect proxy, one for subscribe proxy, and so on). Also, you can configure only one set of headers to proxy which will be used by each proxy type. This may be sufficient for many use cases, but what if you need a more granular control? For example, use different subscribe proxy endpoints for different channel namespaces (i.e. when using microservice architecture).  Centrifugo v3.1.0 introduced a new mode for proxy configuration called granular proxy mode. In this mode it's possible to configure subscribe and publish proxy behaviour on per-namespace level, use different set of headers passed to the proxy endpoint in each proxy type. Also, Centrifugo v3.1.0 introduced a concept of rpc namespaces (in addition to channel namespaces) – together with granular proxy mode this allows configuring rpc proxies on per rpc namespace basis.  ","version":"v4","tagName":"h2"},{"title":"Enable granular proxy mode​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#enable-granular-proxy-mode","content":" Since the change is rather radical it requires a separate boolean option granular_proxy_mode to be enabled. As soon as this option set Centrifugo does not use proxy configuration rules described above and follows the rules described below.  config.json { ... &quot;granular_proxy_mode&quot;: true }   ","version":"v4","tagName":"h3"},{"title":"Defining a list of proxies​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#defining-a-list-of-proxies","content":" When using granular proxy mode on configuration top level you can define &quot;proxies&quot; array with a list of different proxy objects. Each proxy object in an array should have at least two required fields: name and endpoint.  Here is an example:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [ { &quot;name&quot;: &quot;connect&quot;, &quot;endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;timeout&quot;: &quot;500ms&quot;, &quot;http_headers&quot;: [&quot;Cookie&quot;] }, { &quot;name&quot;: &quot;refresh&quot;, &quot;endpoint&quot;: &quot;http://localhost:3000/centrifugo/refresh&quot;, &quot;timeout&quot;: &quot;500ms&quot; }, { &quot;name&quot;: &quot;subscribe1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/subscribe&quot; }, { &quot;name&quot;: &quot;publish1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/publish&quot; }, { &quot;name&quot;: &quot;rpc1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/rpc&quot; }, { &quot;name&quot;: &quot;subscribe2&quot;, &quot;endpoint&quot;: &quot;http://localhost:3002/centrifugo/subscribe&quot; }, { &quot;name&quot;: &quot;publish2&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3002&quot; } { &quot;name&quot;: &quot;rpc2&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3002&quot; } ] }   Let's look at all fields for a proxy object which is possible to set for each proxy inside &quot;proxies&quot; array.  Field name\tField type\tRequired\tDescriptionname\tstring\tyes\tUnique name of proxy used for referencing in configuration, must match regexp ^[-a-zA-Z0-9_.]{2,}$ endpoint\tstring\tyes\tHTTP or GRPC endpoint in the same format as in default proxy mode. For example, http://localhost:3000/path for HTTP or grpc://localhost:3000 for GRPC. timeout\tduration (string)\tno\tProxy request timeout, default &quot;1s&quot; http_headers\tarray of strings\tno\tList of headers to proxy, by default no headers grpc_metadata\tarray of strings\tno\tList of GRPC metadata keys to proxy, by default no metadata keys binary_encoding\tbool\tno\tUse base64 for payloads include_connection_meta\tbool\tno\tInclude meta information (attached on connect) grpc_cert_file\tstring\tno\tPath to cert file for secure TLS connection. If not set then an insecure connection with the backend endpoint is used. grpc_credentials_key\tstring\tno\tAdd custom key to per-RPC credentials. grpc_credentials_value\tstring\tno\tA custom value for grpc_credentials_key.  ","version":"v4","tagName":"h3"},{"title":"Granular connect and refresh​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#granular-connect-and-refresh","content":" As soon as you defined a list of proxies you can reference them by a name to use a specific proxy configuration for a specific event.  To enable connect proxy:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;connect_proxy_name&quot;: &quot;connect&quot; }   We have an example of Centrifugo integration with NodeJS which uses granular proxy mode. Even if you are not using NodeJS on a backend an example can help you understand the idea.  Let's also add refresh proxy:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;connect_proxy_name&quot;: &quot;connect&quot;, &quot;refresh_proxy_name&quot;: &quot;refresh&quot; }   ","version":"v4","tagName":"h3"},{"title":"Granular subscribe, publish, sub refresh​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#granular-subscribe-publish-sub-refresh","content":" Subscribe, publish and sub refresh proxies work per-namespace. This means that subscribe_proxy_name, publish_proxy_name and sub_refresh_proxy_name are just channel namespace options. So it's possible to define these options on configuration top-level (for channels in default top-level namespace) or inside namespace object.  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;ns1&quot;, &quot;subscribe_proxy_name&quot;: &quot;subscribe1&quot;, &quot;publish&quot;: true, &quot;publish_proxy_name&quot;: &quot;publish1&quot; }, { &quot;name&quot;: &quot;ns2&quot;, &quot;subscribe_proxy_name&quot;: &quot;subscribe2&quot;, &quot;publish&quot;: true, &quot;publish_proxy_name&quot;: &quot;publish2&quot; } ] }   If namespace does not have &quot;subscribe_proxy_name&quot; or &quot;subscribe_proxy_name&quot; is empty then no subscribe proxy will be used for a namespace.  If namespace does not have &quot;publish_proxy_name&quot; or &quot;publish_proxy_name&quot; is empty then no publish proxy will be used for a namespace.  If namespace does not have &quot;sub_refresh_proxy_name&quot; or &quot;sub_refresh_proxy_name&quot; is empty then no sub refresh proxy will be used for a namespace.  tip You can define subscribe_proxy_name, publish_proxy_name, sub_refresh_proxy_name on configuration top level – and in this case publish, subscribe and sub refresh requests for channels without explicit namespace will be proxied using this proxy. The same mechanics as for other channel options in Centrifugo.  ","version":"v4","tagName":"h3"},{"title":"Granular RPC​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/4/server/proxy#granular-rpc","content":" Analogous to channel namespaces it's possible to configure rpc namespaces:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;namespaces&quot;: [...], &quot;rpc_namespaces&quot;: [ { &quot;name&quot;: &quot;rpc_ns1&quot;, &quot;rpc_proxy_name&quot;: &quot;rpc1&quot;, }, { &quot;name&quot;: &quot;rpc_ns2&quot;, &quot;rpc_proxy_name&quot;: &quot;rpc2&quot; } ] }   The mechanics is the same as for channel namespaces. RPC requests with RPC method like rpc_ns1:test will use rpc proxy rpc1, RPC requests with RPC method like rpc_ns2:test will use rpc proxy rpc2. So Centrifugo uses : as RPC namespace boundary in RPC method (just like it does for channel namespaces).  Just like channel namespaces RPC namespaces should have a name which match ^[-a-zA-Z0-9_.]{2,}$ regexp pattern – this is validated on Centrifugo start.  tip The same as for channel namespaces and channel options you can define rpc_proxy_name on configuration top level – and in this case RPC calls without explicit namespace in RPC method will be proxied using this proxy. ","version":"v4","tagName":"h3"},{"title":"Server API walkthrough","type":0,"sectionRef":"#","url":"/docs/4/server/server_api","content":"","keywords":"","version":"v4"},{"title":"HTTP API​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#http-api","content":" Server HTTP API works on /api endpoint (by default). It has a simple request format: this is an HTTP POST request with application/json Content-Type and with JSON command body.  Here we will look at available command methods and parameters.  tip In some cases, you can just use one of our available HTTP API libraries or use Centrifugo GRPC API to avoid manually constructing requests.  ","version":"v4","tagName":"h2"},{"title":"HTTP API authorization​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#http-api-authorization","content":" HTTP API is protected by api_key set in Centrifugo configuration. I.e. api_key option must be added to config, like:  config.json { ... &quot;api_key&quot;: &quot;&lt;YOUR API KEY&gt;&quot; }   This API key must be set in the request Authorization header in this way:  Authorization: apikey &lt;KEY&gt;   It's also possible to pass API key over URL query param. This solves some edge cases where it's not possible to use the Authorization header. Simply add ?api_key=&lt;YOUR API KEY&gt; query param to the API endpoint. Keep in mind that passing the API key in the Authorization header is a recommended way.  It's possible to disable API key check on Centrifugo side using the api_insecure configuration option. Be sure to protect the API endpoint by firewall rules in this case – to prevent anyone on the internet to send commands over your unprotected Centrifugo API endpoint. API key auth is not very safe for man-in-the-middle so we also recommended running Centrifugo with TLS.  A command is a JSON object with two properties: method and params.  method is the name of the API command you want to call.params is an object with command arguments. Each method can have its own params  Before looking at all available commands here is a CURL that calls info command:  curl --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;method&quot;: &quot;info&quot;, &quot;params&quot;: {}}' \\ http://localhost:8000/api   Here is a live example:  Your browser does not support the video tag.  Now let's investigate each API method in detail.  ","version":"v4","tagName":"h3"},{"title":"publish​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#publish","content":" Publish command allows publishing data into a channel (we call this message publication in Centrifugo). Most probably this is a command you'll use most of the time.  It looks like this:  { &quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; } } }   Let's apply all information said above and send publish command to Centrifugo. We will send a request using the requests library for Python.  import json import requests command = { &quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;docs&quot;, &quot;data&quot;: { &quot;content&quot;: &quot;1&quot; } } } api_key = &quot;YOUR_API_KEY&quot; data = json.dumps(command) headers = {'Content-type': 'application/json', 'Authorization': 'apikey ' + api_key} resp = requests.post(&quot;https://centrifuge.example.com/api&quot;, data=data, headers=headers) print(resp.json())   The same using httpie console tool:  echo '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: {&quot;text&quot;: &quot;hello&quot;}}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey &lt;YOUR_API_KEY&gt;&quot; -vvv POST /api HTTP/1.1 Accept: application/json, */* Accept-Encoding: gzip, deflate Authorization: apikey KEY Connection: keep-alive Content-Length: 80 Content-Type: application/json Host: localhost:8000 User-Agent: HTTPie/0.9.8 { &quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; } } } HTTP/1.1 200 OK Content-Length: 3 Content-Type: application/json Date: Thu, 17 May 2018 22:01:42 GMT { &quot;result&quot;: {} }   In case of error response object can contain error field. For example, let's publish to a channel with unknown namespace:  echo '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;unknown:chat&quot;, &quot;data&quot;: {&quot;text&quot;: &quot;hello&quot;}}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey &lt;YOUR_API_KEY&gt;&quot; HTTP/1.1 200 OK Content-Length: 55 Content-Type: application/json Date: Thu, 17 May 2018 22:03:09 GMT { &quot;error&quot;: { &quot;code&quot;: 102, &quot;message&quot;: &quot;namespace not found&quot; } }   error object contains error code and message - this is also the same for other commands described below.  Publish params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to publish data\tany JSON\tyes\tCustom JSON data to publish into a channel skip_history\tbool\tno\tSkip adding publication to history for this request tags\tmap[string]string\tno\tPublication tags - map with arbitrary string keys and values which is attached to publication and will be delivered to clients b64data\tstring\tno\tCustom binary data to publish into a channel encoded to base64 so it's possible to use HTTP API to send binary to clients. Centrifugo will decode it from base64 before publishing. In case of GRPC you can publish binary using data field.  Publish result​  Field name\tField type\tOptional\tDescriptionoffset\tinteger\tyes\tOffset of publication in history stream epoch\tstring\tyes\tEpoch of current stream  ","version":"v4","tagName":"h3"},{"title":"broadcast​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#broadcast","content":" Similar to publish but allows to send the same data into many channels.  { &quot;method&quot;: &quot;broadcast&quot;, &quot;params&quot;: { &quot;channels&quot;: [&quot;CHANNEL_1&quot;, &quot;CHANNEL_2&quot;], &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; } } }   Broadcast params​  Parameter name\tParameter type\tRequired\tDescriptionchannels\tArray of strings\tyes\tList of channels to publish data to data\tany JSON\tyes\tCustom JSON data to publish into each channel skip_history\tbool\tno\tSkip adding publications to channels' history for this request tags\tmap[string]string\tno\tPublication tags - map with arbitrary string keys and values which is attached to publication and will be delivered to clients b64data\tstring\tno\tCustom binary data to publish into a channel encoded to base64 so it's possible to use HTTP API to send binary to clients. Centrifugo will decode it from base64 before publishing. In case of GRPC you can publish binary using data field.  Broadcast result​  Field name\tField type\tOptional\tDescriptionresponses\tArray of publish responses\tno\tResponses for each individual publish (with possible error and publish result)  ","version":"v4","tagName":"h3"},{"title":"subscribe​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#subscribe","content":" subscribe allows subscribing user to a channel.  Subscribe params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to subscribe channel\tstring\tyes\tName of channel to subscribe user to info\tany JSON\tno\tAttach custom data to subscription (will be used in presence and join/leave messages) b64info\tstring\tno\tinfo in base64 for binary mode (will be decoded by Centrifugo) client\tstring\tno\tSpecific client ID to subscribe (user still required to be set, will ignore other user connections with different client IDs) session\tstring\tno\tSpecific client session to subscribe (user still required to be set) data\tany JSON\tno\tCustom subscription data (will be sent to client in Subscribe push) b64data\tstring\tno\tSame as data but in base64 format (will be decoded by Centrifugo) recover_since\tStreamPosition object\tno\tStream position to recover from override\tOverride object\tno\tAllows dynamically override some channel options defined in Centrifugo configuration (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave force_push_join_leave\tBoolValue\tyes\tOverride force_push_join_leave force_positioning\tBoolValue\tyes\tOverride force_positioning force_recovery\tBoolValue\tyes\tOverride force_recovery  BoolValue is an object like this:  { &quot;value&quot;: true/false }   Subscribe result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"unsubscribe​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#unsubscribe","content":" unsubscribe allows unsubscribing user from a channel.  { &quot;method&quot;: &quot;unsubscribe&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;CHANNEL NAME&quot;, &quot;user&quot;: &quot;USER ID&quot; } }   Unsubscribe params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to unsubscribe channel\tstring\tyes\tName of channel to unsubscribe user to client\tstring\tno\tSpecific client ID to unsubscribe (user still required to be set) session\tstring\tno\tSpecific client session to disconnect (user still required to be set).  Unsubscribe result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"disconnect​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#disconnect","content":" disconnect allows disconnecting a user by ID.  { &quot;method&quot;: &quot;disconnect&quot;, &quot;params&quot;: { &quot;user&quot;: &quot;USER ID&quot; } }   Disconnect params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to disconnect client\tstring\tno\tSpecific client ID to disconnect (user still required to be set) session\tstring\tno\tSpecific client session to disconnect (user still required to be set). whitelist\tArray of strings\tno\tArray of client IDs to keep disconnect\tDisconnect object\tno\tProvide custom disconnect object, see below  Disconnect object​  Field name\tField type\tRequired\tDescriptioncode\tint\tyes\tDisconnect code reason\tstring\tyes\tDisconnect reason  Disconnect result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"refresh​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#refresh","content":" refresh allows refreshing user connection (mostly useful when unidirectional transports are used).  Refresh params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to refresh client\tstring\tno\tClient ID to refresh (user still required to be set) session\tstring\tno\tSpecific client session to refresh (user still required to be set). expired\tbool\tno\tMark connection as expired and close with Disconnect Expired reason expire_at\tint\tno\tUnix time (in seconds) in the future when the connection will expire  Refresh result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"presence​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#presence","content":" presence allows getting channel online presence information (all clients currently subscribed on this channel).  tip Presence in channels is not enabled by default. See how to enable it over channel options. Also check out dedicated chapter about it.  { &quot;method&quot;: &quot;presence&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot; } }   Example:  fz@centrifugo: echo '{&quot;method&quot;: &quot;presence&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 127 Content-Type: application/json Date: Thu, 17 May 2018 22:13:17 GMT { &quot;result&quot;: { &quot;presence&quot;: { &quot;c54313b2-0442-499a-a70c-051f8588020f&quot;: { &quot;client&quot;: &quot;c54313b2-0442-499a-a70c-051f8588020f&quot;, &quot;user&quot;: &quot;42&quot; }, &quot;adad13b1-0442-499a-a70c-051f858802da&quot;: { &quot;client&quot;: &quot;adad13b1-0442-499a-a70c-051f858802da&quot;, &quot;user&quot;: &quot;42&quot; } } } }   Presence params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call presence from  Presence result​  Field name\tField type\tOptional\tDescriptionpresence\tMap of client ID (string) to ClientInfo object\tno\tOffset of publication in history stream  ClientInfo​  Field name\tField type\tOptional\tDescriptionclient\tstring\tno\tClient ID user\tstring\tno\tUser ID conn_info\tJSON\tyes\tOptional connection info chan_info\tJSON\tyes\tOptional channel info  ","version":"v4","tagName":"h3"},{"title":"presence_stats​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#presence_stats","content":" presence_stats allows getting short channel presence information - number of clients and number of unique users (based on user ID).  { &quot;method&quot;: &quot;presence_stats&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot; } }   Example:  echo '{&quot;method&quot;: &quot;presence_stats&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;public:chat&quot;}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 43 Content-Type: application/json Date: Thu, 17 May 2018 22:09:44 GMT { &quot;result&quot;: { &quot;num_clients&quot;: 0, &quot;num_users&quot;: 0 } }   Presence stats params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call presence from  Presence stats result​  Field name\tField type\tOptional\tDescriptionnum_clients\tinteger\tno\tTotal number of clients in channel num_users\tinteger\tno\tTotal number of unique users in channel  ","version":"v4","tagName":"h3"},{"title":"history​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#history","content":" history allows getting channel history information (list of last messages published into the channel). By default if no limit parameter set in request history call will only return current stream position information - i.e. offset and epoch fields. To get publications you must explicitly provide limit parameter. See also history API description in special doc chapter.  tip History in channels is not enabled by default. See how to enable it over channel options.  { &quot;method&quot;: &quot;history&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot;, &quot;limit&quot;: 2 } }   Example:  echo '{&quot;method&quot;: &quot;history&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;, &quot;limit&quot;: 2}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 129 Content-Type: application/json Date: Wed, 21 Jul 2021 05:30:48 GMT { &quot;result&quot;: { &quot;epoch&quot;: &quot;qFhv&quot;, &quot;offset&quot;: 4, &quot;publications&quot;: [ { &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; }, &quot;offset&quot;: 2 }, { &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; }, &quot;offset&quot;: 3 } ] } }   History params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call history from limit\tint\tno\tLimit number of returned publications, if not set in request then only current stream position information will present in result (without any publications) since\tStreamPosition object\tno\tTo return publications after this position reverse\tbool\tno\tIterate in reversed order (from latest to earliest)  StreamPosition​  Field name\tField type\tRequired\tDescriptionoffset\tinteger\tyes\tOffset in a stream epoch\tstring\tyes\tStream epoch  History result​  Field name\tField type\tOptional\tDescriptionpublications\tArray of publication objects\tyes\tList of publications in channel offset\tinteger\tyes\tTop offset in history stream epoch\tstring\tyes\tEpoch of current stream  ","version":"v4","tagName":"h3"},{"title":"history_remove​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#history_remove","content":" history_remove allows removing publications in channel history. Current top stream position meta data kept untouched to avoid client disconnects due to insufficient state.  { &quot;method&quot;: &quot;history_remove&quot;, &quot;params&quot;: { &quot;channel&quot;: &quot;chat&quot; } }   Example:  echo '{&quot;method&quot;: &quot;history_remove&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;chat&quot;}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 43 Content-Type: application/json Date: Thu, 17 May 2018 22:09:44 GMT { &quot;result&quot;: {} }   History remove params​  Parameter name\tParameter type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to remove history  History remove result​  Empty object at the moment.  ","version":"v4","tagName":"h3"},{"title":"channels​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#channels","content":" channels return active channels (with one or more active subscribers in it).  { &quot;method&quot;: &quot;channels&quot;, &quot;params&quot;: {} }   Channels params​  Parameter name\tParameter type\tRequired\tDescriptionpattern\tstring\tno\tPattern to filter channels, we are using gobwas/glob library for matching  Channels result​  Field name\tField type\tOptional\tDescriptionchannels\tMap of string to ChannelInfo\tno\tMap where key is channel and value is ChannelInfo (see below)  ChannelInfo​  Field name\tField type\tOptional\tDescriptionnum_clients\tinteger\tno\tTotal number of connections currently subscribed to a channel  caution Keep in mind that since the channels method by default returns all active channels it can be really heavy for massive deployments. Centrifugo does not provide a way to paginate over channels list. At the moment we mostly suppose that channels API call will be used in the development process or for administrative/debug purposes, and in not very massive Centrifugo setups (with no more than 10k active channels). A better and scalable approach for huge setups could be a real-time analytics approach described here.  ","version":"v4","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#info","content":" info method allows getting information about running Centrifugo nodes.  Example:  echo '{&quot;method&quot;: &quot;info&quot;, &quot;params&quot;: {}}' | http &quot;localhost:8000/api&quot; Authorization:&quot;apikey KEY&quot; HTTP/1.1 200 OK Content-Length: 184 Content-Type: application/json Date: Thu, 17 May 2018 22:07:58 GMT { &quot;result&quot;: { &quot;nodes&quot;: [ { &quot;name&quot;: &quot;Alexanders-MacBook-Pro.local_8000&quot;, &quot;num_channels&quot;: 0, &quot;num_clients&quot;: 0, &quot;num_users&quot;: 0, &quot;uid&quot;: &quot;f844a2ed-5edf-4815-b83c-271974003db9&quot;, &quot;uptime&quot;: 0, &quot;version&quot;: &quot;&quot; } ] } }   Info params​  Empty object at the moment.  Info result​  Field name\tField type\tOptional\tDescriptionnodes\tArray of Node objects\tno\tInformation about all nodes in a cluster  ","version":"v4","tagName":"h3"},{"title":"Command pipelining​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#command-pipelining","content":" It's possible to combine several commands into one request to Centrifugo. To do this use JSON streaming format. This can improve server throughput and reduce traffic traveling around.  Example:  curl --header &quot;Authorization: apikey &lt;API_KEY&gt;&quot; \\ --request POST \\ --data $'{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;test1&quot;, &quot;data&quot;: {&quot;test&quot;: 1}}}\\n{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;test2&quot;, &quot;data&quot;: {&quot;test&quot;: 2}}}' \\ http://localhost:8000/api {&quot;result&quot;:{}} {&quot;result&quot;:{}}   Note that with CURL we had to use $ to properly send new line \\n character in data.  ","version":"v4","tagName":"h3"},{"title":"HTTP API libraries​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#http-api-libraries","content":" Sending an API request to Centrifugo is a simple task to do in any programming language - this is just a POST request with JSON payload in body and Authorization header.  But we have several official HTTP API libraries for different languages, to help developers to avoid constructing proper HTTP requests manually:  cent for Pythonphpcent for PHPgocent for Gorubycent for Ruby  Also, there are API libraries created by community:  crystalcent API client for Crystal languagecent.js API client for NodeJSCentrifugo.AspNetCore API client for ASP.NET Core  tip Also, keep in mind that Centrifugo has GRPC API so you can automatically generate client API code for your language.  ","version":"v4","tagName":"h3"},{"title":"GRPC API​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#grpc-api","content":" Centrifugo also supports GRPC API. With GRPC it's possible to communicate with Centrifugo using a more compact binary representation of commands and use the power of HTTP/2 which is the transport behind GRPC.  GRPC API is also useful if you want to publish binary data to Centrifugo channels.  tip GRPC API allows calling all commands described in HTTP API doc, actually both GRPC and HTTP API in Centrifugo based on the same Protobuf schema definition. So refer to the HTTP API description doc for the parameter and the result field description.  You can enable GRPC API in Centrifugo using grpc_api option:  config.json { ... &quot;grpc_api&quot;: true }   By default, GRPC will be served on port 10000 but you can change it using the grpc_api_port option.  Now, as soon as Centrifugo started – you can send GRPC commands to it. To do this get our API Protocol Buffer definitions from this file.  Then see GRPC docs specific to your language to find out how to generate client code from definitions and use generated code to communicate with Centrifugo.  ","version":"v4","tagName":"h2"},{"title":"GRPC example for Python​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#grpc-example-for-python","content":" For example for Python you need to run sth like this according to GRPC docs:  pip install grpcio-tools python -m grpc_tools.protoc -I ./ --python_out=. --grpc_python_out=. api.proto   As soon as you run the command you will have 2 generated files: api_pb2.py and api_pb2_grpc.py. Now all you need is to write a simple program that uses generated code and sends GRPC requests to Centrifugo:  import grpc import api_pb2_grpc as api_grpc import api_pb2 as api_pb channel = grpc.insecure_channel('localhost:10000') stub = api_grpc.CentrifugoApiStub(channel) try: resp = stub.Info(api_pb.InfoRequest()) except grpc.RpcError as err: # GRPC level error. print(err.code(), err.details()) else: if resp.error.code: # Centrifugo server level error. print(resp.error.code, resp.error.message) else: print(resp.result)   Note that you need to explicitly handle Centrifugo API level error which is not transformed automatically into GRPC protocol-level error.  ","version":"v4","tagName":"h3"},{"title":"GRPC example for Go​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#grpc-example-for-go","content":" Here is a simple example of how to run Centrifugo with the GRPC Go client.  You need protoc, protoc-gen-go and protoc-gen-go-grpc installed.  First start Centrifugo itself with GRPC API enabled:  CENTRIFUGO_GRPC_API=1 centrifugo --config config.json   In another terminal tab:  mkdir centrifugo_grpc_example cd centrifugo_grpc_example/ touch main.go go mod init centrifugo_example mkdir apiproto cd apiproto wget https://raw.githubusercontent.com/centrifugal/centrifugo/master/internal/apiproto/api.proto -O api.proto   Run protoc to generate code:  protoc -I ./ api.proto --go_out=. --go-grpc_out=.   Put the following code to main.go file (created on the last step above):  package main import ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;centrifugo_example/apiproto&quot; &quot;google.golang.org/grpc&quot; ) func main() { conn, err := grpc.Dial(&quot;localhost:10000&quot;, grpc.WithInsecure()) if err != nil { log.Fatalln(err) } defer conn.Close() client := apiproto.NewCentrifugoApiClient(conn) for { resp, err := client.Publish(context.Background(), &amp;apiproto.PublishRequest{ Channel: &quot;chat:index&quot;, Data: []byte(`{&quot;input&quot;: &quot;hello from GRPC&quot;}`), }) if err != nil { log.Printf(&quot;Transport level error: %v&quot;, err) } else { if resp.GetError() != nil { respError := resp.GetError() log.Printf(&quot;Error %d (%s)&quot;, respError.Code, respError.Message) } else { log.Println(&quot;Successfully published&quot;) } } time.Sleep(time.Second) } }   Then run:  go run main.go   The program starts and periodically publishes the same payload into chat:index channel.  ","version":"v4","tagName":"h3"},{"title":"GRPC API key authorization​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/4/server/server_api#grpc-api-key-authorization","content":" You can also set grpc_api_key (string) in Centrifugo configuration to protect GRPC API with key. In this case, you should set per RPC metadata with key authorization and value apikey &lt;KEY&gt;. For example in Go language:  package main import ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;centrifugo_example/apiproto&quot; &quot;google.golang.org/grpc&quot; ) type keyAuth struct { key string } func (t keyAuth) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) { return map[string]string{ &quot;authorization&quot;: &quot;apikey &quot; + t.key, }, nil } func (t keyAuth) RequireTransportSecurity() bool { return false } func main() { conn, err := grpc.Dial(&quot;localhost:10000&quot;, grpc.WithInsecure(), grpc.WithPerRPCCredentials(keyAuth{&quot;xxx&quot;})) if err != nil { log.Fatalln(err) } defer conn.Close() client := apiproto.NewCentrifugoClient(conn) for { resp, err := client.Publish(context.Background(), &amp;PublishRequest{ Channel: &quot;chat:index&quot;, Data: []byte(`{&quot;input&quot;: &quot;hello from GRPC&quot;}`), }) if err != nil { log.Printf(&quot;Transport level error: %v&quot;, err) } else { if resp.GetError() != nil { respError := resp.GetError() log.Printf(&quot;Error %d (%s)&quot;, respError.Code, respError.Message) } else { log.Println(&quot;Successfully published&quot;) } } time.Sleep(time.Second) } }   For other languages refer to GRPC docs. ","version":"v4","tagName":"h3"},{"title":"Server-side subscriptions","type":0,"sectionRef":"#","url":"/docs/4/server/server_subs","content":"","keywords":"","version":"v4"},{"title":"Dynamic server-side subscriptions​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/4/server/server_subs#dynamic-server-side-subscriptions","content":" See subscribe and unsubscribe server API  ","version":"v4","tagName":"h3"},{"title":"Automatic personal channel subscription​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/4/server/server_subs#automatic-personal-channel-subscription","content":" It's possible to automatically subscribe a user to a personal server-side channel.  To enable this you need to enable the user_subscribe_to_personal boolean option (by default false). As soon as you do this every connection with a non-empty user ID will be automatically subscribed to a personal user-limited channel. Anonymous users with empty user IDs won't be subscribed to any channel.  For example, if you set this option and the user with ID 87334 connects to Centrifugo it will be automatically subscribed to channel #87334 and you can process personal publications on the client-side in the same way as shown above.  As you can see by default generated personal channel name belongs to the default namespace (i.e. no explicit namespace used). To set custom namespace name use user_personal_channel_namespace option (string, default &quot;&quot;) – i.e. the name of namespace from configured configuration namespaces array. In this case, if you set user_personal_channel_namespace to personal for example – then the automatically generated personal channel will be personal:#87334 – user will be automatically subscribed to it on connect and you can use this channel name to publish personal notifications to the online user.  ","version":"v4","tagName":"h3"},{"title":"Maintain single user connection​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/4/server/server_subs#maintain-single-user-connection","content":" Usage of personal channel subscription also opens a road to enable one more feature: maintaining only a single connection for each user globally around all Centrifugo nodes.  user_personal_single_connection boolean option (default false) turns on a mode in which Centrifugo will try to maintain only a single connection for each user at the same moment. As soon as the user establishes a connection other connections from the same user will be closed with connection limit reason (client won't try to automatically reconnect).  This feature works with a help of presence information inside a personal channel. So presence should be turned on in a personal channel.  Example config:  config.json { &quot;user_subscribe_to_personal&quot;: true, &quot;user_personal_single_connection&quot;: true, &quot;user_personal_channel_namespace&quot;: &quot;personal&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot;, &quot;presence&quot;: true } ] }   note Centrifugo can't guarantee that other user connections will be closed – since Disconnect messages are distributed around Centrifugo nodes with at most once guarantee. So don't add critical business logic based on this feature to your application. Though this should work just fine most of the time if the connection between the Centrifugo node and PUB/SUB broker is OK. ","version":"v4","tagName":"h3"},{"title":"Configure TLS","type":0,"sectionRef":"#","url":"/docs/4/server/tls","content":"","keywords":"","version":"v4"},{"title":"Using crt and key files​","type":1,"pageTitle":"Configure TLS","url":"/docs/4/server/tls#using-crt-and-key-files","content":" In first way you already have cert and key files. For development you can create self-signed certificate - see this instruction as example.  config.json { ... &quot;tls&quot;: true, &quot;tls_key&quot;: &quot;server.key&quot;, &quot;tls_cert&quot;: &quot;server.crt&quot; }   And run:  ./centrifugo --config=config.json   ","version":"v4","tagName":"h3"},{"title":"Automatic certificates​","type":1,"pageTitle":"Configure TLS","url":"/docs/4/server/tls#automatic-certificates","content":" For automatic certificates from Let's Encrypt add into configuration file:  config.json { ... &quot;tls_autocert&quot;: true, &quot;tls_autocert_host_whitelist&quot;: &quot;www.example.com&quot;, &quot;tls_autocert_cache_dir&quot;: &quot;/tmp/certs&quot;, &quot;tls_autocert_email&quot;: &quot;user@example.com&quot;, &quot;tls_autocert_http&quot;: true, &quot;tls_autocert_http_addr&quot;: &quot;:80&quot; }   tls_autocert (boolean) says Centrifugo that you want automatic certificate handling using ACME provider.  tls_autocert_host_whitelist (string) is a string with your app domain address. This can be comma-separated list. It's optional but recommended for extra security.  tls_autocert_cache_dir (string) is a path to a folder to cache issued certificate files. This is optional but will increase performance.  tls_autocert_email (string) is optional - it's an email address ACME provider will send notifications about problems with your certificates.  tls_autocert_http (boolean) is an option to handle http_01 ACME challenge on non-TLS port.  tls_autocert_http_addr (string) can be used to set address for handling http_01 ACME challenge (default is :80)  When configured correctly and your domain is valid (localhost will not work) - certificates will be retrieved on first request to Centrifugo.  Also Let's Encrypt certificates will be automatically renewed.  There are two options that allow Centrifugo to support TLS client connections from older browsers such as Chrome 49 on Windows XP and IE8 on XP:  tls_autocert_force_rsa - this is a boolean option, by default false. When enabled it forces autocert manager generate certificates with 2048-bit RSA keys.tls_autocert_server_name - string option, allows to set server name for client handshake hello. This can be useful to deal with old browsers without SNI support - see comment  grpc_api_tls_disable boolean flag allows to disable TLS for GRPC API server but keep it on for HTTP endpoints.  uni_grpc_tls_disable boolean flag allows to disable TLS for GRPC uni stream server but keep it on for HTTP endpoints.  ","version":"v4","tagName":"h3"},{"title":"TLS for GRPC API​","type":1,"pageTitle":"Configure TLS","url":"/docs/4/server/tls#tls-for-grpc-api","content":" You can provide custom certificate files to configure TLS for GRPC API server.  grpc_api_tls boolean flag enables TLS for GRPC API server, requires an X509 certificate and a key filegrpc_api_tls_cert string provides a path to an X509 certificate file for GRPC API servergrpc_api_tls_key string provides a path to an X509 certificate key for GRPC API server  ","version":"v4","tagName":"h3"},{"title":"TLS for GRPC unidirectional stream​","type":1,"pageTitle":"Configure TLS","url":"/docs/4/server/tls#tls-for-grpc-unidirectional-stream","content":" You can provide custom certificate files to configure TLS for GRPC unidirectional stream endpoint.  uni_grpc_tls boolean flag enables TLS for GRPC server, requires an X509 certificate and a key fileuni_grpc_tls_cert string provides a path to an X509 certificate file for GRPC uni stream serveruni_grpc_tls_key string provides a path to an X509 certificate key for GRPC uni stream server ","version":"v4","tagName":"h3"},{"title":"Client protocol","type":0,"sectionRef":"#","url":"/docs/4/transports/client_protocol","content":"","keywords":"","version":"v4"},{"title":"Protobuf schema​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#protobuf-schema","content":" Centrifugo is built on top of Centrifuge library for Go. Centrifuge library uses its own framing for wrapping Centrifuge-specific messages – synchronous commands from a client to a server (which expect replies from a server) and asynchronous pushes.  Centrifuge client protocol is defined by a Protobuf schema. This is the source of truth.  tip At the moment Protobuf schema contains some fields which are only used in client protocol v1. This is for backwards compatibility – server supports clients connecting over both client protocol v2 and client protocol v1. Client protocol v1 is considered deprecated and will be removed at some point in the future (giving enough time to our users to migrate).  ","version":"v4","tagName":"h2"},{"title":"Command-Reply​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#command-reply","content":" In bidirectional case client sends Command to a server and server sends Reply to a client. I.e. all communication between client and server is a bidirectional exchange of Command and Reply messages.  Each Command has id field. This is an incremental uint32 field. This field will be echoed in a server replies to commands so client could match a certain Reply to Command sent before. This is important since Websocket is an asynchronous transport where server and client both send messages at any moment and there is no builtin request-response matching. Having id allows matching a reply with a command send before on SDK level.  In JSON case client can send command like this:  {&quot;id&quot;: 1, &quot;subscribe&quot;: {&quot;channel&quot;: &quot;example&quot;}}   And client can expect something like this in response:  {&quot;id&quot;: 1, &quot;subscribe&quot;: {}}   Reply for different commands has corresponding field with command result (&quot;subscribe&quot; in example above).  Reply can also contain error if Command processing resulted into an error on a server. error is optional and if Reply does not have error then it means that Command processed successfully and client can handle result object appropriately.  error looks like this in JSON case:  { &quot;code&quot;: 100, &quot;message&quot;: &quot;internal server error&quot;, &quot;temporary&quot;: true }   I.e. reply with error may look like this:  {&quot;id&quot;: 1, &quot;error&quot;: {&quot;code&quot;: 100, &quot;message&quot;: &quot;internal server error&quot;}}   We will talk more about error handling below.  Centrifuge library defines several command types client can issue. A well-written client must be aware of all those commands and client workflow.  Current commands:  connect – sent to authenticate connection, sth like hello from a client which can carry authentication token and arbitrary data.subscribe – sent to subscribe to a channelunsubscribe - sent to unsubscribe from a channelpublish - sent to publish data into a channelpresence - sent to request presence information from a channelpresence_stats - sent to request presence stats information from a channelhistory - sent to request history information for a channelsend - sent to send async message to a server (this command is a bit special since it must not contain id - as we don't wait for any response from a server in this case).rpc - sent to send RPC to a channel (execute arbitrary logic and wait for response)refresh - sent to refresh connection tokensub_refresh - sent to refresh channel subscription token  ","version":"v4","tagName":"h2"},{"title":"Asynchronous pushes​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#asynchronous-pushes","content":" The special type of Reply is asynchronous Reply. Such replies have no id field set (or id can be equal to zero). Async replies can come to a client at any moment - not as reaction to issued Command but as a message from a server to a client at arbitrary time. For example, this can be a message published into channel.  There are several types of asynchronous messages that can come from a server to a client.  pub is a message published into channeljoin messages sent when someone joined (subscribed on) channel.leave messages sent when someone left (unsubscribed from) channel.unsubscribe message sent when a server unsubscribed current client from a channel:subscribe may be sent when a server subscribes client to a channel.disconnect may be sent be a server before closing connection and contains disconnect code/reasonmessage may be sent when server sends asynchronous message to a clientconnect push can be sent in unidirectional transport caserefresh may be sent when a server refreshes client credentials (useful in unidirectional transports)  ","version":"v4","tagName":"h2"},{"title":"Top level batching​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#top-level-batching","content":" To reduce number of system calls one request from a client to a server and one response from a server to a client can have more than one Command or Reply. This allows reducing number of system calls for writing and reading data.  When JSON format used then many Command can be sent from client to server in JSON streaming line-delimited format. I.e. each individual Command encoded to JSON and then commands joined together using new line symbol \\n:  {&quot;id&quot;: 1, &quot;subscribe&quot;: {&quot;channel&quot;: &quot;ch1&quot;}} {&quot;id&quot;: 2, &quot;subscribe&quot;: {&quot;channel&quot;: &quot;ch2&quot;}}   Here is an example how we do this in Javascript client when JSON format used:  function encodeCommands(commands) { const encodedCommands = []; for (const i in commands) { if (commands.hasOwnProperty(i)) { encodedCommands.push(JSON.stringify(commands[i])); } } return encodedCommands.join('\\n'); }   info This doc uses JSON format for examples because it's human-readable. Everything said here for JSON is also true for Protobuf encoded case. There is a difference how several individual Command or server Reply joined into one request – see details below. Also, in JSON format bytes fields transformed into embedded JSON by Centrifugo.  When Protobuf format used then many Command can be sent from a client to a server in a length-delimited format where each individual Command marshaled to bytes prepended by varint length. See existing client implementations for encoding example.  The same rules relate to many Reply in one response from server to client. Line-delimited JSON and varint-length prefixed Protobuf also used there.  tip Server can even send reply to a command and asynchronous message batched together in a one frame.  For example here is how we read server response and extracting individual replies in Javascript client when JSON format used:  function decodeReplies(data) { const replies = []; const encodedReplies = data.split('\\n'); for (const i in encodedReplies) { if (encodedReplies.hasOwnProperty(i)) { if (!encodedReplies[i]) { continue; } const reply = JSON.parse(encodedReplies[i]); replies.push(reply); } } return replies; }   For Protobuf case see existing client implementations for decoding example.  ","version":"v4","tagName":"h2"},{"title":"Ping Pong​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#ping-pong","content":" To maintain connection alive and detect broken connections server periodically sends empty commands to clients and expects empty replies from them.  When client does not receive ping from a server for some time it can consider connection broken and try to reconnect. Usually a server sends pings every 25 seconds.  ","version":"v4","tagName":"h2"},{"title":"Handle disconnects​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#handle-disconnects","content":" Client should handle disconnect advices from server. In websocket case disconnect advice is sent in CLOSE Websocket frame. Disconnect advice contains uint32 code and human-readable string reason.  ","version":"v4","tagName":"h2"},{"title":"Handle errors​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#handle-errors","content":" This section contains advices to error handling in client implementations.  Errors can happen during various operations and can be handled in special way in context of some commands to tolerate network and server problems.  Errors during connect must result in full client reconnect with exponential backoff strategy. The special case is error with code 110 which signals that connection token already expired. As we said above client should update its connection JWT before connecting to server again.  Errors during subscribe must result in full client reconnect in case of internal error (code 100). And be sent to subscribe error event handler of subscription if received error is persistent. Persistent errors are errors like permission denied, bad request, namespace not found etc. Persistent errors in most situation mean a mistake from developers side.  The special corner case is client-side timeout during subscribe operation. As protocol is asynchronous it's possible in this case that server will eventually subscribe client on channel but client will think that it's not subscribed. It's possible to retry subscription request and tolerate already subscribed (code 105) error as expected. But the simplest solution is to reconnect entirely as this is simpler and gives client a chance to connect to working server instance.  Errors during rpc-like operations can be just returned to caller - i.e. user javascript code. Calls like history and presence are idempotent. You should be accurate with non-idempotent operations like publish - in case of client timeout it's possible to send the same message into channel twice if retry publish after timeout - so users of libraries must care about this case – making sure they have some protection from displaying message twice on client side (maybe some sort of unique key in payload).  ","version":"v4","tagName":"h2"},{"title":"Additional notes​","type":1,"pageTitle":"Client protocol","url":"/docs/4/transports/client_protocol#additional-notes","content":" Client protocol does not allow one client connection to subscribe to the same channel twice. In this case client will receive already subscribed error in a reply to a subscribe command. ","version":"v4","tagName":"h2"},{"title":"Client real-time SDKs","type":0,"sectionRef":"#","url":"/docs/4/transports/client_sdk","content":"","keywords":"","version":"v4"},{"title":"List of client SDKs​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/4/transports/client_sdk#list-of-client-sdks","content":" centrifuge-js – for browser, NodeJS and React Nativecentrifuge-go - for Go languagecentrifuge-dart - for Dart and Fluttercentrifuge-swift – for native iOS developmentcentrifuge-java – for native Android development and general Java  See a description of client protocol if you want to write a custom bidirectional connector or eager to learn how Centrifugo protocol internals are structured.  ","version":"v4","tagName":"h2"},{"title":"Protobuf and JSON formats in SDKs​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/4/transports/client_sdk#protobuf-and-json-formats-in-sdks","content":" Centrifugo real-time SDKs work using two possible serialization formats: JSON and Protobuf. The entire bidirectional client protocol is described by the Protobuf schema. But those Protobuf messages may be also encoded as JSON objects (in JSON representation bytes fields in the Protobuf schema is replaced by the embedded JSON object in Centrifugo case).  Our Javascript SDK - centrifuge-js - uses JSON serialization for protocol frames by default. This makes communication with Centrifugo server convenient as we are exchanging human-readable JSON frames between client and server. And it makes it possible to use centrifuge-js without extra dependency to protobuf.js library. It's possible to switch to Protobuf protocol with centrifuge-js SDK though, in case you want more compact Centrifuge protocol representation, faster decode/encode speeds on Centrifugo server side, or payloads you need to pass are custom binary. See more details on how to use centrifuge-js with Protobuf serialization in README.  centrifuge-go real-time SDK for Go language also supports both JSON and Protobuf formats when communicating with Centrifugo server.  Other SDKs, like centrifuge-dart, centrifuge-swift, centrifuge-java work using only Protobuf serialization for Centrifuge protocol internally. So they utilize the fastest and the most compact wire representation by default. Note, that while internally in those SDKs the serialization format is Protobuf, you can still send JSON towards these clients as JSON objects may be encoded as UTF-8 bytes. So these SDKs may work with both custom binary and JSON payloads.  There are some important notes about JSON and Protobuf interoperability mentioned in our FAQ.  ","version":"v4","tagName":"h2"},{"title":"SDK feature matrix​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/4/transports/client_sdk#sdk-feature-matrix","content":" Below you can find an information regarding support of different features in our official client SDKs  ","version":"v4","tagName":"h2"},{"title":"Connection related features​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/4/transports/client_sdk#connection-related-features","content":" Client feature\tjs\tdart\tswift\tgo\tjavaconnect to a server\t✅\t✅\t✅\t✅\t✅ setting client options\t✅\t✅\t✅\t✅\t✅ automatic reconnect with backoff algorithm\t✅\t✅\t✅\t✅\t✅ client state changes\t✅\t✅\t✅\t✅\t✅ command-reply\t✅\t✅\t✅\t✅\t✅ command timeouts\t✅\t✅\t✅\t✅\t✅ async pushes\t✅\t✅\t✅\t✅\t✅ ping-pong\t✅\t✅\t✅\t✅\t✅ connection token refresh\t✅\t✅\t✅\t✅\t✅ handle disconnect advice from server\t✅\t✅\t✅\t✅\t✅ server-side subscriptions\t✅\t✅\t✅\t✅\t✅ batching API\t✅ bidirectional WebSocket emulation\t✅   ","version":"v4","tagName":"h3"},{"title":"Client-side subscription related features​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/4/transports/client_sdk#client-side-subscription-related-features","content":" Client feature\tjs\tdart\tswift\tgo\tjavasubscrbe to a channel\t✅\t✅\t✅\t✅\t✅ setting subscription options\t✅\t✅\t✅\t✅\t✅ automatic resubscribe with backoff algorithm\t✅\t✅\t✅\t✅\t✅ subscription state changes\t✅\t✅\t✅\t✅\t✅ subscription command-reply\t✅\t✅\t✅\t✅\t✅ subscription async pushes\t✅\t✅\t✅\t✅\t✅ subscription token refresh\t✅\t✅\t✅\t✅\t✅ handle unsubscribe advice from server\t✅\t✅\t✅\t✅\t✅ manage subscription registry\t✅\t✅\t✅\t✅\t✅ optimistic subscriptions\t✅  ","version":"v4","tagName":"h3"},{"title":"HTTP streaming, with bidirectional emulation","type":0,"sectionRef":"#","url":"/docs/4/transports/http_stream","content":"","keywords":"","version":"v4"},{"title":"Options​","type":1,"pageTitle":"HTTP streaming, with bidirectional emulation","url":"/docs/4/transports/http_stream#options","content":" ","version":"v4","tagName":"h2"},{"title":"http_stream​","type":1,"pageTitle":"HTTP streaming, with bidirectional emulation","url":"/docs/4/transports/http_stream#http_stream","content":" Boolean, default: false.  Enables HTTP streaming endpoint. And enables emulation endpoint (/emulation by default) to accept emulation HTTP requests from clients.  config.json { ... &quot;http_stream&quot;: true }   ","version":"v4","tagName":"h3"},{"title":"http_stream_max_request_body_size​","type":1,"pageTitle":"HTTP streaming, with bidirectional emulation","url":"/docs/4/transports/http_stream#http_stream_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes. ","version":"v4","tagName":"h3"},{"title":"Real-time transports","type":0,"sectionRef":"#","url":"/docs/4/transports/overview","content":"","keywords":"","version":"v4"},{"title":"Bidirectional​","type":1,"pageTitle":"Real-time transports","url":"/docs/4/transports/overview#bidirectional","content":" Bidirectional transports are capable to serve all Centrifugo features. These transports are the main Centrifugo focus.  Bidirectional transports come with a cost that developers need to use a special client connector library (SDK) which speaks Centrifugo client protocol. The reason why we need a special client connector library is that a bidirectional connection is asynchronous – it's required to match requests to responses, properly manage connection state, handle request queueing/timeouts/errors, etc.  Centrifugo has several official client SDKs for popular environments. All of them work over WebSocket transport. Our Javascript SDK also offers bidirectional fallbacks over HTTP-Streaming, Server-Sent Events (SSE) or SockJS.  ","version":"v4","tagName":"h2"},{"title":"Unidirectional​","type":1,"pageTitle":"Real-time transports","url":"/docs/4/transports/overview#unidirectional","content":" Unidirectional transports suit well for simple use-cases with stable subscriptions, usually known at connection time.  The advantage is that unidirectional transports do not require special client connectors - developers can use native browser APIs (like WebSocket, EventSource/SSE, HTTP-streaming), or GRPC generated code to receive real-time updates from Centrifugo. Thus avoiding dependency to a client connector that abstracts bidirectional communication.  The drawback is that with unidirectional transports you are not inheriting all Centrifugo features out of the box (like dynamic subscriptions/unsubscriptions, automatic message recovery on reconnect, possibility to send RPC calls over persistent connection). But some of the missing client APIs can be mimicked by using calls to Centrifugo server API (i.e. over client -&gt; application backend -&gt; Centrifugo).  ","version":"v4","tagName":"h2"},{"title":"Unidirectional message types​","type":1,"pageTitle":"Real-time transports","url":"/docs/4/transports/overview#unidirectional-message-types","content":" In case of unidirectional transports Centrifugo will send Push frames to the connection. Push frames defined by client protocol schema. I.e. Centrifugo reuses a part of its bidirectional protocol for unidirectional communication. Push message defined as:  message Push { string channel = 2; Publication pub = 4; Join join = 5; Leave leave = 6; Unsubscribe unsubscribe = 7; Message message = 8; Subscribe subscribe = 9; Connect connect = 10; Disconnect disconnect = 11; Refresh refresh = 12; }   tip Some numbers in Protobuf definitions skipped for backwards compatibility with previous client protocol version.  So unidirectional connection will receive various pushes. Every push contains one of the following objects:  PublicationJoinLeaveUnsubscribeMessageSubscribeConnectDisconnectRefresh  Some pushes belong to a channel which may be set on Push top level.  All you need to do is look at Push, process messages you are interested in and ignore others. In most cases you will be most interested in pushes which contain Connect or Publication messages.  For example, according to protocol schema Publication message type looks like this:  message Publication { bytes data = 4; ClientInfo info = 5; uint64 offset = 6; map&lt;string, string&gt; tags = 7; }   tip In JSON protocol case Centrifugo replaces bytes type with embedded JSON.  Just try using any unidirectional transport and you will quickly get the idea.  ","version":"v4","tagName":"h3"},{"title":"PING/PONG behavior​","type":1,"pageTitle":"Real-time transports","url":"/docs/4/transports/overview#pingpong-behavior","content":" Centrifugo server periodically sends pings to clients and expects pong from clients that works over bidirectional transports. Sending ping and receiving pong allows to find broken connections faster. Centrifugo sends pings on the Centrifugo client protocol level, thus it's possible for clients to handle ping messages on the client side to make sure connection is not broken (our bidirectional SDKs do this automatically).  By default Centrifugo sends pings every 25 seconds. This may be changed using ping_interval option (duration, default &quot;25s&quot;).  Centrifugo expects pong message from bidirectional client SDK after sending ping to it. By default, it waits no more than 8 seconds before closing a connection. This may be changed using pong_timeout option (duration, default &quot;8s&quot;).  In most cases default ping/pong intervals are fine so you don't really need to tweak them. Reducing timeouts may help you to find non-gracefully closed connections faster, but will increase network traffic and CPU resource usage since ping/pongs are sent faster.  caution ping_interval must be greater than pong_timeout in the current implementation.  Here is a scheme how ping/pong works in bidirectional and unidirectional client scenarios:   ","version":"v4","tagName":"h2"},{"title":"SockJS","type":0,"sectionRef":"#","url":"/docs/4/transports/sockjs","content":"","keywords":"","version":"v4"},{"title":"SockJS caveats​","type":1,"pageTitle":"SockJS","url":"/docs/4/transports/sockjs#sockjs-caveats","content":" caution There are several important caveats to know when using SockJS – see below.  ","version":"v4","tagName":"h2"},{"title":"Sticky sessions​","type":1,"pageTitle":"SockJS","url":"/docs/4/transports/sockjs#sticky-sessions","content":" First is that you need to use sticky sessions mechanism if you have more than one Centrifugo nodes running behind a load balancer. This mechanism usually supported by load balancers (for example Nginx). Sticky sessions mean that all requests from the same client will come to the same Centrifugo node. This is necessary because SockJS maintains connection session in process memory thus allowing bidirectional communication between a client and a server. Sticky mechanism not required if you only use one Centrifugo node on a backend.  For example, with Nginx sticky support can be enabled with ip_hash directive for upstream:  upstream centrifugo { ip_hash; server 127.0.0.1:8000; server 127.0.0.2:8000; }   With this configuration Nginx will proxy connections with the same ip address to the same upstream backend.  But ip_hash; is not the best choice in this case, because there could be situations where a lot of different connections are coming with the same IP address (behind proxies) and the load balancing system won't be fair.  So the best solution would be using something like nginx-sticky-module which uses setting a special cookie to track the upstream server for a client.  ","version":"v4","tagName":"h3"},{"title":"Browser only​","type":1,"pageTitle":"SockJS","url":"/docs/4/transports/sockjs#browser-only","content":" SockJS is only supported by centrifuge-js – i.e. our browser client. There is no much sense to use SockJS outside of a browser these days.  ","version":"v4","tagName":"h3"},{"title":"JSON only​","type":1,"pageTitle":"SockJS","url":"/docs/4/transports/sockjs#json-only","content":" One more thing to be aware of is that SockJS does not support binary data, so there is no option to use Centrifugo Protobuf protocol on top of SockJS (unlike WebSocket). Only JSON payloads can be transferred.  ","version":"v4","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"SockJS","url":"/docs/4/transports/sockjs#options","content":" ","version":"v4","tagName":"h2"},{"title":"sockjs​","type":1,"pageTitle":"SockJS","url":"/docs/4/transports/sockjs#sockjs","content":" Boolean, default: false.  Enables SockJS transport.  ","version":"v4","tagName":"h3"},{"title":"sockjs_url​","type":1,"pageTitle":"SockJS","url":"/docs/4/transports/sockjs#sockjs_url","content":" Default: https://cdn.jsdelivr.net/npm/sockjs-client@1/dist/sockjs.min.js  Link to SockJS url which is required when iframe-based HTTP fallbacks are in use. ","version":"v4","tagName":"h3"},{"title":"SSE (EventSource), with bidirectional emulation","type":0,"sectionRef":"#","url":"/docs/4/transports/sse","content":"","keywords":"","version":"v4"},{"title":"Options​","type":1,"pageTitle":"SSE (EventSource), with bidirectional emulation","url":"/docs/4/transports/sse#options","content":" ","version":"v4","tagName":"h2"},{"title":"sse​","type":1,"pageTitle":"SSE (EventSource), with bidirectional emulation","url":"/docs/4/transports/sse#sse","content":" Boolean, default: false.  Enables SSE (EventSource) endpoint. And enables emulation endpoint (/emulation by default) to accept emulation HTTP requests from clients.  config.json { ... &quot;sse&quot;: true }   ","version":"v4","tagName":"h3"},{"title":"sse_max_request_body_size​","type":1,"pageTitle":"SSE (EventSource), with bidirectional emulation","url":"/docs/4/transports/sse#sse_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes when using HTTP POST requests to connect (browsers are using GET so it's not applied). ","version":"v4","tagName":"h3"},{"title":"Unidirectional GRPC","type":0,"sectionRef":"#","url":"/docs/4/transports/uni_grpc","content":"","keywords":"","version":"v4"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#supported-data-formats","content":" JSON and binary.  ","version":"v4","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#options","content":" ","version":"v4","tagName":"h2"},{"title":"uni_grpc​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#uni_grpc","content":" Boolean, default: false.  Enables unidirectional GRPC endpoint.  config.json { ... &quot;uni_grpc&quot;: true }   ","version":"v4","tagName":"h3"},{"title":"uni_grpc_port​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#uni_grpc_port","content":" String, default &quot;11000&quot;.  Port to listen on.  ","version":"v4","tagName":"h3"},{"title":"uni_grpc_address​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#uni_grpc_address","content":" String, default &quot;&quot; (listen on all interfaces)  Address to bind uni GRPC to.  ","version":"v4","tagName":"h3"},{"title":"uni_grpc_max_receive_message_size​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#uni_grpc_max_receive_message_size","content":" Default: 65536 (64KB)  Maximum allowed size of a first connect message received from GRPC connection in bytes.  ","version":"v4","tagName":"h3"},{"title":"uni_grpc_tls​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#uni_grpc_tls","content":" Boolean, default: false  Enable custom TLS for unidirectional GRPC server.  ","version":"v4","tagName":"h3"},{"title":"uni_grpc_tls_cert​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#uni_grpc_tls_cert","content":" String, default: &quot;&quot;.  Path to cert file.  ","version":"v4","tagName":"h3"},{"title":"uni_grpc_tls_key​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#uni_grpc_tls_key","content":" String, default: &quot;&quot;.  Path to key file.  ","version":"v4","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/4/transports/uni_grpc#example","content":" A basic example will come soon as we update docs for v4. In general, algorithm is like this:  Copy Protobuf definitionsGenerate GRPC client codeUse generated code to connect to CentrifugoProcess Push messages, drop unknown Push types, handle those necessary for the application. ","version":"v4","tagName":"h2"},{"title":"Unidirectional HTTP streaming","type":0,"sectionRef":"#","url":"/docs/4/transports/uni_http_stream","content":"","keywords":"","version":"v4"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#connect-command","content":" It's possible to pass initial connect command by posting a JSON body to a streaming endpoint.  Refer to the full Connect command description – it's the same as for unidirectional WebSocket.  ","version":"v4","tagName":"h2"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#supported-data-formats","content":" JSON  ","version":"v4","tagName":"h2"},{"title":"Pings​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#pings","content":" Centrifugo will send different message types to a connection. Every message is JSON encoded. A special JSON value null used as a PING message. You can simply ignore it on a client side upon receiving. You can ignore such messages or use them to detect broken connections (nothing received from a server for a long time).  ","version":"v4","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#options","content":" ","version":"v4","tagName":"h2"},{"title":"uni_http_stream​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#uni_http_stream","content":" Boolean, default: false.  Enables unidirectional HTTP streaming endpoint.  config.json { ... &quot;uni_http_stream&quot;: true }   ","version":"v4","tagName":"h3"},{"title":"uni_http_stream_max_request_body_size​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#uni_http_stream_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes.  ","version":"v4","tagName":"h3"},{"title":"Connecting using CURL​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#connecting-using-curl","content":" Let's look how simple it is to connect to Centrifugo using HTTP streaming.  We will start from scratch, generate new configuration file:  centrifugo genconfig   Turn on uni HTTP stream and automatically subscribe users to personal channel upon connect:  config.json { ... &quot;uni_http_stream&quot;: true, &quot;user_subscribe_to_personal&quot;: true }   Run Centrifugo:  centrifugo -c config.json   In separate terminal window create token for a user:  ❯ go run main.go gentoken -u user12 HMAC SHA-256 JWT for user user12 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMTIiLCJleHAiOjE2MjUwNzMyODh9.BxmS4R-X6YXMxLfXNhYRzeHvtu_M2NCaXF6HNu7VnDM   Then connect to Centrifugo uni HTTP stream endpoint with simple CURL POST request:  curl -X POST http://localhost:8000/connection/uni_http_stream \\ -d '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMTIiLCJleHAiOjE2MjUwNzMyODh9.BxmS4R-X6YXMxLfXNhYRzeHvtu_M2NCaXF6HNu7VnDM&quot;}'   Open one more terminal window and publish message to a personal user channel:  curl -X POST http://localhost:8000/api \\ -d '{&quot;method&quot;: &quot;publish&quot;, &quot;params&quot;: {&quot;channel&quot;: &quot;#user12&quot;, &quot;data&quot;: {&quot;input&quot;: &quot;hello&quot;}}}' \\ -H &quot;Authorization: apikey 9230f514-34d2-4971-ace2-851c656e81dc&quot;   You should see this messages coming from server.  {} messages are pings from a server.  That's all, happy streaming!  ","version":"v4","tagName":"h2"},{"title":"Browser example​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/4/transports/uni_http_stream#browser-example","content":" A basic browser will come soon as we update docs for v4. ","version":"v4","tagName":"h2"},{"title":"Unidirectional SSE (EventSource)","type":0,"sectionRef":"#","url":"/docs/4/transports/uni_sse","content":"","keywords":"","version":"v4"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/4/transports/uni_sse#connect-command","content":" Unfortunately SSE specification does not allow POST requests from a web browser, so the only way to pass initial connect command is over URL params. Centrifugo is looking for cf_connect URL param for connect command. Connect command value expected to be a JSON-encoded string, properly encoded into URL. For example:  const url = new URL('http://localhost:8000/connection/uni_sse'); url.searchParams.append(&quot;cf_connect&quot;, JSON.stringify({ 'token': '&lt;JWT&gt;' })); const eventSource = new EventSource(url);   Refer to the full Connect command description – it's the same as for unidirectional WebSocket.  The length of URL query should be kept less than 2048 characters to work throughout browsers. This should be more than enough for most use cases.  tip Centrifugo unidirectional SSE endpoint also supports POST requests. While it's not very useful for browsers which only allow GET requests for EventSource this can be useful when connecting from a mobile device. In this case you must send the connect object as a JSON body of a POST request (instead of using cf_connect URL parameter), similar to what we have in unidirectional HTTP streaming transport case.  ","version":"v4","tagName":"h2"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/4/transports/uni_sse#supported-data-formats","content":" JSON  ","version":"v4","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/4/transports/uni_sse#options","content":" ","version":"v4","tagName":"h2"},{"title":"uni_sse​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/4/transports/uni_sse#uni_sse","content":" Boolean, default: false.  Enables unidirectional SSE (EventSource) endpoint.  config.json { ... &quot;uni_sse&quot;: true }   ","version":"v4","tagName":"h3"},{"title":"uni_sse_max_request_body_size​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/4/transports/uni_sse#uni_sse_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes when using HTTP POST requests to connect (browsers are using GET so it's not applied).  ","version":"v4","tagName":"h3"},{"title":"Browser example​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/4/transports/uni_sse#browser-example","content":" A basic browser will come soon as we update docs for v4. ","version":"v4","tagName":"h2"},{"title":"Unidirectional WebSocket","type":0,"sectionRef":"#","url":"/docs/4/transports/uni_websocket","content":"","keywords":"","version":"v4"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#connect-command","content":" It's possible to send connect command as first WebSocket message (as JSON).  Field name\tField type\tRequired\tDescriptiontoken\tstring\tno\tConnection JWT, not required when using the connect proxy feature. data\tany JSON\tno\tCustom JSON connection data name\tstring\tno\tApplication name version\tstring\tno\tApplication version subs\tmap of channel to SubscribeRequest\tno\tPass an information about desired subscriptions to a server  ","version":"v4","tagName":"h2"},{"title":"SubscribeRequest​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#subscriberequest","content":" Field name\tField type\tRequired\tDescriptionrecover\tboolean\tno\tWhether a client wants to recover from a certain position offset\tinteger\tno\tKnown stream position offset when recover is used epoch\tstring\tno\tKnown stream position epoch when recover is used  ","version":"v4","tagName":"h3"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#supported-data-formats","content":" JSON  ","version":"v4","tagName":"h2"},{"title":"Pings​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#pings","content":" Centrifugo uses empty commands ({} in JSON case) as pings for unidirectional WS. You can ignore such messages or use them to detect broken connections (nothing received from a server for a long time).  ","version":"v4","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#options","content":" ","version":"v4","tagName":"h2"},{"title":"uni_websocket​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#uni_websocket","content":" Boolean, default: false.  Enables unidirectional WebSocket endpoint.  config.json { ... &quot;uni_websocket&quot;: true }   ","version":"v4","tagName":"h3"},{"title":"uni_websocket_message_size_limit​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#uni_websocket_message_size_limit","content":" Default: 65536 (64KB)  Maximum allowed size of a first connect message received from WebSocket connection in bytes.  ","version":"v4","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/4/transports/uni_websocket#example","content":" Let's connect to a unidirectional WebSocket endpoint using wscat tool – it allows connecting to WebSocket servers interactively from a terminal.  First, run Centrifugo with uni_websocket enabled. Also let's enable automatic personal channel subscriptions for users. Configuration example:  config.json { &quot;token_hmac_secret_key&quot;: &quot;secret&quot;, &quot;uni_websocket&quot;:true, &quot;user_subscribe_to_personal&quot;: true }   Run Centrifugo:  ./centrifugo -c config.json   In another terminal:  ❯ ./centrifugo gentoken -c config.json -u test_user HMAC SHA-256 JWT for user test_user with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ0ZXN0X3VzZXIiLCJleHAiOjE2MzAxMzAxNzB9.u7anX-VYXywX1p1lv9UC9CAu04vpA6LgG5gsw5lz1Iw   Install wscat and run:  wscat -c &quot;ws://localhost:8000/connection/uni_websocket&quot;   This will establish a connection with a server and you then can send connect command to a server:  ❯ wscat -c &quot;ws://localhost:8000/connection/uni_websocket&quot; Connected (press CTRL+C to quit) &gt; {&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ0ZXN0X3VzZXIiLCJleHAiOjE2NTY1MDMwNDV9.3UYL-UCUBp27TybeBK7Z0OenwdsKwCMRe46fuEjJnzI&quot;, &quot;subs&quot;: {&quot;abc&quot;: {}}} &lt; {&quot;connect&quot;:{&quot;client&quot;:&quot;bfd28799-b958-4791-b9e9-b011eaef68c1&quot;,&quot;version&quot;:&quot;0.0.0&quot;,&quot;subs&quot;:{&quot;#test_user&quot;:{}},&quot;expires&quot;:true,&quot;ttl&quot;:604407,&quot;ping&quot;:25,&quot;session&quot;:&quot;57b1287b-44ec-45c8-93fc-696c5294af25&quot;}}   The connection will receive server pings (empty commands {}) periodically. You can try to publish something to #test_user or abc channels (using Centrifugo server API or using admin UI) – and the message should come to the connection we just established. ","version":"v4","tagName":"h2"},{"title":"WebSocket","type":0,"sectionRef":"#","url":"/docs/4/transports/websocket","content":"","keywords":"","version":"v4"},{"title":"Options​","type":1,"pageTitle":"WebSocket","url":"/docs/4/transports/websocket#options","content":" ","version":"v4","tagName":"h2"},{"title":"websocket_message_size_limit​","type":1,"pageTitle":"WebSocket","url":"/docs/4/transports/websocket#websocket_message_size_limit","content":" Default: 65536 (64KB)  Maximum allowed size of a message received from WebSocket connection in bytes.  ","version":"v4","tagName":"h3"},{"title":"websocket_read_buffer_size​","type":1,"pageTitle":"WebSocket","url":"/docs/4/transports/websocket#websocket_read_buffer_size","content":" In bytes, by default 0 which tells Centrifugo to reuse read buffer from HTTP server for WebSocket connection (usually 4096 bytes in size). If set to a lower value can reduce memory usage per WebSocket connection (but can increase number of system calls depending on average message size).  config.json { ... &quot;websocket_read_buffer_size&quot;: 512 }   ","version":"v4","tagName":"h3"},{"title":"websocket_write_buffer_size​","type":1,"pageTitle":"WebSocket","url":"/docs/4/transports/websocket#websocket_write_buffer_size","content":" In bytes, by default 0 which tells Centrifugo to reuse write buffer from HTTP server for WebSocket connection (usually 4096 bytes in size). If set to a lower value can reduce memory usage per WebSocket connection (but HTTP buffer won't be reused):  config.json { ... &quot;websocket_write_buffer_size&quot;: 512 }   ","version":"v4","tagName":"h3"},{"title":"websocket_use_write_buffer_pool​","type":1,"pageTitle":"WebSocket","url":"/docs/4/transports/websocket#websocket_use_write_buffer_pool","content":" If you have a few writes then websocket_use_write_buffer_pool (boolean, default false) option can reduce memory usage of Centrifugo a bit as there won't be separate write buffer binded to each WebSocket connection.  ","version":"v4","tagName":"h3"},{"title":"websocket_compression​","type":1,"pageTitle":"WebSocket","url":"/docs/4/transports/websocket#websocket_compression","content":" An experimental feature for raw WebSocket endpoint - permessage-deflate compression for websocket messages. Btw look at great article about websocket compression. WebSocket compression can reduce an amount of traffic travelling over the wire.  We consider this experimental because this websocket compression is experimental in Gorilla Websocket library that Centrifugo uses internally.  caution Enabling WebSocket compression will result in much slower Centrifugo performance and more memory usage – depending on your message rate this can be very noticeable.  To enable WebSocket compression for raw WebSocket endpoint set websocket_compression to true in a configuration file. After this clients that support permessage-deflate will negotiate compression with server automatically. Note that enabling compression does not mean that every connection will use it - this depends on client support for this feature.  Another option is websocket_compression_min_size. Default 0. This is a minimal size of message in bytes for which we use deflate compression when writing it to client's connection. Default value 0 means that we will compress all messages when websocket_compression enabled and compression support negotiated with client.  It's also possible to control websocket compression level defined at compress/flate By default when compression with a client negotiated Centrifugo uses compression level 1 (BestSpeed). If you want to set custom compression level use websocket_compression_level configuration option.  ","version":"v4","tagName":"h3"},{"title":"Protobuf binary protocol​","type":1,"pageTitle":"WebSocket","url":"/docs/4/transports/websocket#protobuf-binary-protocol","content":" In most cases you will use Centrifugo with JSON protocol which is used by default. It consists of simple human-readable frames that can be easily inspected. Also it's a very simple task to publish JSON encoded data to HTTP API endpoint. You may want to use binary Protobuf client protocol if:  you want less traffic on wire as Protobuf is very compactyou want maximum performance on server-side as Protobuf encoding/decoding is very efficientyou can sacrifice human-readable JSON for your application  Binary protobuf protocol only works for raw Websocket connections (as SockJS can't deal with binary). With most clients to use binary you just need to provide query parameter format to Websocket URL, so final URL look like:  wss://centrifugo.example.com/connection/websocket?format=protobuf   After doing this Centrifugo will use binary frames to pass data between client and server. Your application specific payload can be random bytes.  tip You still can continue to encode your application specific data as JSON when using Protobuf protocol thus have a possibility to co-exist with clients that use JSON protocol on the same Centrifugo installation inside the same channels. ","version":"v4","tagName":"h2"},{"title":"Client SDK API","type":0,"sectionRef":"#","url":"/docs/4/transports/client_api","content":"","keywords":"","version":"v4"},{"title":"Client connection states​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#client-connection-states","content":" Client connection has 4 states:  disconnectedconnectingconnectedclosed  note closed state is only implemented by SDKs where it makes sense (need to clean up allocated resources when app gracefully shuts down – for example in Java SDK we close thread executors used internally).  When a new Client is created it has a disconnected state. To connect to a server connect() method must be called. After calling connect Client moves to the connecting state. If a Client can't connect to a server it attempts to create a connection with an exponential backoff algorithm (with full jitter). If a connection to a server is successful then the state becomes connected.  If a connection is lost (due to a missing network for example, or due to reconnect advice received from a server, or due to some client-side error that can't be recovered without reconnecting) Client goes to the connecting state again. In this state Client tries to reconnect (again, with an exponential backoff algorithm).  The Client's state can become disconnected. This happens when Client's disconnect() method was called by a developer. Also, this can happen due to server advice from a server, or due to a terminal problem that happened on the client-side.  Here is a program where we create a Client instance, set callbacks to listen to state updates and establish a connection with a server:      JavascriptDartSwiftJavaGo const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); client.on('connecting', function(ctx) { console.log('connecting', ctx); }); client.on('connected', function(ctx) { console.log('connected', ctx); }); client.on('disconnected', function(ctx) { console.log('disconnected', ctx); }); client.connect();   In case of successful connection Client states will transition like this:  disconnected (initial) -&gt; connecting (on('connecting') called) -&gt; connected (on('connected') called).  In case of already connected Client temporary lost a connection with a server and then successfully reconnected:  connected -&gt; connecting (on('connecting') called) -&gt; connected (on('connected') called).  In case of already connected Client temporary lost a connection with a server, but got a terminal error upon reconnection:  connected -&gt; connecting (on('connecting') called) -&gt; disconnected (on('disconnected') called).  In case of already connected Client came across terminal condition (for example, if during a connection token refresh application found that user has no permission to connect anymore):  connected -&gt; disconnected (on('disconnected') called).  Both connecting and disconnected events have numeric code and human-readable string reason in their context, so you can look at them and find the exact reason why the Client went to the connecting state or to the disconnected state.  This diagram demonstrates possible Client state transitions:    You can also listen for all errors happening internally (which are not reflected by state changes, for example, transport errors happening on initial connect, transport during reconnect, connection token refresh related errors, etc) while the client works by using error event:  client.on('error', function(ctx) { console.log('client error', ctx); });   If you want to disconnect from a server call .disconnect() method:  client.disconnect();   In this case on('disconnected') will be called. You can call connect() again when you need to establish a connection.  closed state implemented in SDKs where resources like internal queues, thread executors, etc must be cleaned up when the Client is not needed anymore. All subscriptions should automatically go to the unsubscribed state upon closing. The client is not usable after going to a closed state.  ","version":"v4","tagName":"h2"},{"title":"Client common options​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#client-common-options","content":" There are several common options available when creating Client instance.  option to set connection token and callback to get connection token upon expiration (see below mode details)option to set connect dataoption to configure operation timeouttweaks for reconnect backoff algorithm (min delay, max delay)configure max delay of server pings (to detect broken connection)configure headers to send in WebSocket upgrade request (except centrifuge-js)configure client name and version for analytics purpose  ","version":"v4","tagName":"h2"},{"title":"Client methods​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#client-methods","content":" connect() – connect to a serverdisconnect() - disconnect from a serverclose() - close Client if not needed anymoresend(data) - send asynchronous message to a serverrpc(method, data) - send arbitrary RPC and wait for response  ","version":"v4","tagName":"h2"},{"title":"Client connection token​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#client-connection-token","content":" All SDKs support connecting to Centrifugo with JWT. Initial connection token can be set in Client option upon initialization. Example:  const client = new Centrifuge('ws://localhost:8000/connection/websocket', { token: 'JWT-GENERATED-ON-BACKEND-SIDE' });   If the token sets connection expiration then the client SDK will keep the token refreshed. It does this by calling a special callback function. This callback must return a new token. If a new token with updated connection expiration is returned from callback then it's sent to Centrifugo. If your callback returns an empty string – this means the user has no permission to connect to Centrifugo and the Client will move to a disconnected state. In case of error returned by your callback SDK will retry the operation after some jittered time.  An example:  function getToken(url, ctx) { return new Promise((resolve, reject) =&gt; { fetch(url, { method: 'POST', headers: new Headers({ 'Content-Type': 'application/json' }), body: JSON.stringify(ctx) }) .then(res =&gt; { if (!res.ok) { throw new Error(`Unexpected status code ${res.status}`); } return res.json(); }) .then(data =&gt; { resolve(data.token); }) .catch(err =&gt; { reject(err); }); }); } const client = new Centrifuge( 'ws://localhost:8000/connection/websocket', { token: 'JWT-GENERATED-ON-BACKEND-SIDE', getToken: function (ctx) { return getToken('/centrifuge/connection_token', ctx); } } );   tip If initial token is not provided, but getToken is specified – then SDK should assume that developer wants to use token authentication. In this case SDK should attempt to get a connection token before establishing an initial connection.  ","version":"v4","tagName":"h2"},{"title":"Connection PING/PONG​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#connection-pingpong","content":" PINGs sent by a server, a client should answer with PONGs upon receiving PING. If a client does not receive PING from a server for a long time (ping interval + configured delay) – the connection is considered broken and will be re-established.  ","version":"v4","tagName":"h2"},{"title":"Subscription states​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#subscription-states","content":" Client allows subscribing on channels. This can be done by creating Subscription object.  const sub = centrifuge.newSubscription(channel); sub.subscribe();   When anewSubscription method is called Client allocates a new Subscription instance and saves it in the internal subscription registry. Having a registry of allocated subscriptions allows SDK to manage resubscribes upon reconnecting to a server. Centrifugo connectors do not allow creating two subscriptions to the same channel – in this case, newSubscription can throw an exception.  Subscription has 3 states:  unsubscribedsubscribingsubscribed  When a new Subscription is created it has an unsubscribed state.  To initiate the actual process of subscribing to a channel subscribe() method of Subscription instance should be called. After calling subscribe() Subscription moves to subscribing state.  If subscription to a channel is not successful then depending on error type subscription can automatically resubscribe (with exponential backoff) or go to an unsubscribed state (upon non-temporary error). If subscription to a channel is successful then the state becomes subscribed.  JavascriptDartSwiftJavaGo const sub = client.newSubscription(channel); sub.on('subscribing', function(ctx) { console.log('subscribing'); }); sub.on('subscribed', function(ctx) { console.log('subscribed'); }); sub.on('unsubscribed', function(ctx) { console.log('unsubscribed'); }); sub.subscribe();   Subscriptions also go to subscribing state when Client connection (i.e. transport) becomes unavailable. Upon connection re-establishement all subscriptions which are not in unsubscribed state will resubscribe automatically.  In case of successful subscription states will transition like this:  unsubscribed (initial) -&gt; subscribing (on('subscribing') called) -&gt; subscribed (on('subscribed') called).  In case of connected and subscribed Client temporary lost a connection with a server and then succesfully reconnected and resubscribed:  subscribed -&gt; subscribing (on('subscribing') called) -&gt; subscribed (on('subscribed') called).  Both subscribing and unsubscribed events have numeric code and human-readable string reason in their context, so you can look at them and find the exact reason why Subscription went to subscribing state or to unsubscribed state.  This diagram demonstrates possible Subscription state transitions:    You can listen for all errors happening internally in Subscription (which are not reflected by state changes, for example, temporary subscribe errors, subscription token related errors, etc) by using error event:  sub.on('error', function(ctx) { console.log(&quot;subscription error&quot;, ctx); });   If you want to unsubscribe from a channel call .unsubscribe() method:  sub.unsubscribe();   In this case on('unsubscribed') will be called. Subscription still kept in Client's registry, but no resubscription attempts will be made. You can call subscribe() again when you need Subscription again. Or you can remove Subscription from Client's registry (see below).  ","version":"v4","tagName":"h2"},{"title":"Subscription management​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#subscription-management","content":" The client SDK provides several methods to manage the internal registry of client-side subscriptions.  newSubscription(channel, options) allocates a new Subscription in the registry or throws an exception if the Subscription is already there. We will discuss common Subscription options below.  getSubscription(channel) returns the existing Subscription by a channel from the registry (or null if it does not exist).  removeSubscription(sub) removes Subscription from Client's registry. Subscription is automatically unsubscribed before being removed. Use this to free resources if you don't need a Subscription to a channel anymore.  subscriptions() returns all registered subscriptions, so you can iterate over all and do some action if required (for example, you want to unsubscribe/remove all subscriptions).  ","version":"v4","tagName":"h2"},{"title":"Listen to channel publications​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#listen-to-channel-publications","content":" Of course the main point of having Subscriptions is the ability to listen for publications (i.e. messages published to a channel).  sub.on('publication', function(ctx) { console.log(&quot;received publication&quot;, ctx); });   Publication context has several fields:  data - publication payload, this can be JSON or binary dataoffset - optional offset inside history stream, this is an incremental numbertags - optional tags, this is a map with string keys and string valuesinfo - optional information about client connection who published this (only exists if publication comes from client-side publish() API).  So minimal code where we connect to a server and listen for messages published into example channel may look like:  JavascriptDartSwiftJavaGo const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); const sub = client.newSubscription('example').on('publication', function(ctx) { console.log(&quot;received publication from a channel&quot;, ctx.data); }); sub.subscribe(); client.connect();   Note, that we can call subscribe() before making a connection to a server – and this will work just fine, subscription goes to subscribing state and will be subscribed upon succesfull connection. And of course, it's possible to call .subscribe() after .connect().  ","version":"v4","tagName":"h2"},{"title":"Subscription recovery state​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#subscription-recovery-state","content":" Subscriptions to channels with recovery option enabled maintain stream position information internally. On every publication received this information updated and used to recover missed publications upon resubscribe (caused by reconnect for example).  When you call unsubscribe() Subscription position state is not cleared. So it's possible to call subscribe() later and catch up a state.  The recovery process result – i.e. whether all missed publications recovered or not – can be found in on('subscribed') event context. Centrifuge protocol provides two fields:  wasRecovering - boolean flag that tells whether recovery was used during subscription process resulted into subscribed state. Can be useful if you want to distinguish first subscribe attempt (when subscription does not have any position information yet)recovered - boolean flag that tells whether Centrifugo thinks that all missed publications can be successfully recovered and there is no need to load state from the main application database. It's always false when wasRecovering is false.  ","version":"v4","tagName":"h2"},{"title":"Subscription common options​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#subscription-common-options","content":" There are several common options available when creating Subscription instance.  option to set subscription token and callback to get subscription token upon expiration (see below more details)option to set subscription data (attached to every subscribe/resubscribe request)options to tweak resubscribe backoff algorithmoption to start Subscription since known Stream Position (i.e. attempt recovery on first subscribe)option to ask server to make subscription positioned (if not forced by a server)option to ask server to make subscription recoverable (if not forced by a server)option to ask server to push Join/Leave messages (if not forced by a server)  ","version":"v4","tagName":"h2"},{"title":"Subscription methods​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#subscription-methods","content":" subscribe() – start subscribing to a channelunsubscribe() - unsubscribe from a channelpublish(data) - publish data to Subscription channelhistory(options) - request Subscription channel historypresence() - request Subscription channel online presence informationpresenceStats() - request Subscription channel online presence stats information (number of client connections and unique users in a channel).  ","version":"v4","tagName":"h2"},{"title":"Subscription token​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#subscription-token","content":" All SDKs support subscribing to Centrifugo channels with JWT. Channel subscription token can be set as a Subscription option upon initialization. Example:  const sub = centrifuge.newSubscription(channel, { token: 'JWT-GENERATED-ON-BACKEND-SIDE' }); sub.subscribe();   If token sets subscription expiration client SDK will keep token refreshed. It does this by calling special callback function. This callback must return a new token. If new token with updated subscription expiration returned from a calbback then it's sent to Centrifugo. If your callback returns an empty string – this means user has no permission to subscribe to a channel anymore and subscription will be unsubscribed. In case of error returned by your callback SDK will retry operation after some jittered time.  An example:  function getToken(url, ctx) { return new Promise((resolve, reject) =&gt; { fetch(url, { method: 'POST', headers: new Headers({ 'Content-Type': 'application/json' }), body: JSON.stringify(ctx) }) .then(res =&gt; { if (!res.ok) { throw new Error(`Unexpected status code ${res.status}`); } return res.json(); }) .then(data =&gt; { resolve(data.token); }) .catch(err =&gt; { reject(err); }); }); } const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); const sub = centrifuge.newSubscription(channel, { token: 'JWT-GENERATED-ON-BACKEND-SIDE', getToken: function (ctx) { // ctx has channel in the Subscription token case. return getToken('/centrifuge/subscription_token', ctx); }, }); sub.subscribe();   tip If initial token is not provided, but getToken is specified – then SDK should assume that developer wants to use token authorization for a channel subscription. In this case SDK should attempt to get a subscription token before initial subscribe.  ","version":"v4","tagName":"h2"},{"title":"Server-side subscriptions​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#server-side-subscriptions","content":" We encourage using client-side subscriptions where possible as they provide a better control and isolation from connection. But in some cases you may want to use server-side subscriptions (i.e. subscriptions created by server upon connection establishment).  Technically, client SDK keeps server-side subscriptions in internal registry (similar to client-side subscriptions but without possibility to control them).  To listen for server-side subscription events use callbacks as shown in example below:  const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); client.on('subscribed', function(ctx) { // Called when subscribed to a server-side channel upon Client moving to // connected state or during connection lifetime if server sends Subscribe // push message. console.log('subscribed to server-side channel', ctx.channel); }); client.on('subscribing', function(ctx) { // Called when existing connection lost (Client reconnects) or Client // explicitly disconnected. Client continue keeping server-side subscription // registry with stream position information where applicable. console.log('subscribing to server-side channel', ctx.channel); }); client.on('unsubscribed', function(ctx) { // Called when server sent unsubscribe push or server-side subscription // previously existed in SDK registry disappeared upon Client reconnect. console.log('unsubscribed from server-side channel', ctx.channel); }); client.on('publication', function(ctx) { // Called when server sends Publication over server-side subscription. console.log('publication receive from server-side channel', ctx.channel, ctx.data); }); client.connect();   Server-side subscription events mostly mimic events of client-side subscriptions. But again – they do not provide control to the client and managed entirely by a server side.  Additionally, Client has several top-level methods to call with server-side subscription related operations:  publish(channel, data)history(channel, options)presence(channel)presenceStats(channel)  ","version":"v4","tagName":"h2"},{"title":"Error codes​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#error-codes","content":" Server can return error codes in range 100-1999. Error codes in interval 0-399 reserved by Centrifuge/Centrifugo server. Codes in range [400, 1999] may be returned by application code built on top of Centrifuge/Centrifugo.  Server errors contain a temporary boolean flag which works as a signal that error may be fixed by a later retry.  Errors with codes 0-100 can be used by client-side implementation. Client-side errors may not have code attached at all since in many languages error can be distinguished by its type.  ","version":"v4","tagName":"h2"},{"title":"Unsubscribe codes​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#unsubscribe-codes","content":" Server may return unsubscribe codes. Server unsubscribe codes must be in range [2000, 2999].  Unsubscribe codes &gt;= 2500 coming from server to client result into automatic resubscribe attempt (i.e. client goes to subscribing state). Codes &lt; 2500 result into going to unsubscribed state.  Client implementation can use codes &lt; 2000 for client-side specific unsubscribe reasons.  ","version":"v4","tagName":"h2"},{"title":"Disconnect codes​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#disconnect-codes","content":" Server may send custom disconnect codes to a client. Custom disconnect codes must be in range [3000, 4999].  Client automatically reconnects upon receiving code in range 3000-3499, 4000-4499 (i.e. Client goes to connecting state). Other codes result into going to disconnected state.  Client implementation can use codes &lt; 3000 for client-side specific disconnect reasons.  ","version":"v4","tagName":"h2"},{"title":"RPC​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#rpc","content":" An SDK provides a way to send RPC to a server. RPC is a call that is not related to channels at all. It's just a way to call the server method from the client-side over the real-time connection. RPC is only available when RPC proxy configured (so Centrifugo proxies the RPC to your application backend).  const rpcRequest = {'key': 'value'}; const data = await centrifuge.namedRPC('example_method', rpcRequest);   ","version":"v4","tagName":"h2"},{"title":"Channel history API​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#channel-history-api","content":" SDK provides a method to call publication history inside a channel (only for channels where history is enabled) to get last publications in a channel.  Get stream current top position:  const resp = await subscription.history(); console.log(resp.offset); console.log(resp.epoch);   Get up to 10 publications from history since known stream position:  const resp = await subscription.history({limit: 10, since: {offset: 0, epoch: '...'}}); console.log(resp.publications);   Get up to 10 publications from history since current stream beginning:  const resp = await subscription.history({limit: 10}); console.log(resp.publications);   Get up to 10 publications from history since current stream end in reversed order (last to first):  const resp = await subscription.history({limit: 10, reverse: true}); console.log(resp.publications);   ","version":"v4","tagName":"h2"},{"title":"Presence and presence stats API​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#presence-and-presence-stats-api","content":" Once subscribed client can call presence and presence stats information inside channel (only for channels where presence configured):  For presence (full information about active subscribers in channel):  const resp = await subscription.presence(); // resp contains presence information - a map client IDs as keys // and client information as values.   For presence stats (just a number of clients and unique users in a channel):  const resp = await subscription.presenceStats(); // resp contains a number of clients and a number of unique users.   ","version":"v4","tagName":"h2"},{"title":"SDK common best practices​","type":1,"pageTitle":"Client SDK API","url":"/docs/4/transports/client_api#sdk-common-best-practices","content":" Callbacks must be fast. Avoid blocking operations inside event handlers. Callbacks caused by protocol messages received from a server are called synchronously and connection read loop is blocked while such callbacks are being executed. Consider doing heavy work asynchronously.Do not blindly rely on the current Client or Subscription state when making client API calls – state can change at any moment, so don't forget to handle errors.Disconnect from a server when a mobile application goes to the background since a mobile OS can kill the connection at some point without any callbacks called. ","version":"v4","tagName":"h2"},{"title":"WebTransport","type":0,"sectionRef":"#","url":"/docs/4/transports/webtransport","content":"WebTransport WebTransport is an API offering low-latency, bidirectional, client-server messaging on top of HTTP/3. See Using WebTransport article that gives a good overview of it. danger WebTransport support in Centrifugo is EXPERIMENTAL and not recommended for production usage. WebTransport IETF specification is not finished yet and may have breaking changes. To use WebTransport you first need to run HTTP/3 experimental server and enable webtransport endpoint: config.json { &quot;http3&quot;: true, &quot;tls&quot;: true, &quot;tls_cert&quot;: &quot;path/to/crt&quot;, &quot;tls_key&quot;: &quot;path/to/key&quot;, &quot;webtransport&quot;: true } In HTTP/3 and WebTransport case TLS is required. tip At the time of writing only Chrome (since v97) supports WebTransport API. If you are experimenting with self-signed certificates you may need to run Chrome with flags to force HTTP/3 on origin and ignore certificate errors: /path/to/your/Chrome --origin-to-force-quic-on=localhost:8000 --ignore-certificate-errors-spki-list=TSZTiMjLG+DNjESXdJh3f+S8C+RhsFCav7T24VNuCPQ= Where the value of --ignore-certificate-errors-spki-list is a certificate fingerprint obtained this way: openssl x509 -in server.crt -pubkey -noout | openssl pkey -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64 With not self-signed certs things should work just fine in Chrome. Here is a video tutorial that shows this in action: After starting Centrifugo with HTTP/3 and WebTransport endpoint you can connect to that endpoint (by default – /connection/webtransport) using centrifuge-js. For example, let's enable WebTransport and will use WebSocket as a fallback option: const transports = [ { transport: 'webtransport', endpoint: 'https://localhost:8000/connection/webtransport' }, { transport: 'websocket', endpoint: 'wss://localhost:8000/connection/websocket' } ]; const centrifuge = new Centrifuge(transports); centrifuge.connect() Note, that we are using secure schemes here – https:// and wss://. While in WebSocket case you could opt for non-TLS communication, in WebTransport case non-TLS http:// scheme is simply not supported by the specification. tip Make sure you run Centrifugo without load balancer or reverse proxy in front, or make sure your proxy can proxy HTTP/3 traffic to Centrifugo. In Centrifugo case, we utilize a single bidirectional stream of WebTransport to pass our protocol between client and server. Both JSON and Protobuf communication are supported. There are some issues with the proper passing of the disconnect advice in some cases, otherwise it's fully functional. Obviously, due to the limited WebTransport support in browsers at the moment, possible breaking changes in the WebTransport specification it's an experimental feature. And it's not recommended for production usage for now. At some point in the future, it may become a reasonable alternative to WebSocket.","keywords":"","version":"v4"},{"title":"Attributions","type":0,"sectionRef":"#","url":"/docs/attributions","content":"","keywords":"","version":"v5"},{"title":"Landing Page Images​","type":1,"pageTitle":"Attributions","url":"/docs/attributions#landing-page-images","content":" The following images have been used in the landing page.  Icons made by kerismaker:  https://www.flaticon.com/packs/web-maintenance-35 ","version":"v5","tagName":"h2"},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/faq","content":"","keywords":"","version":"v5"},{"title":"How many connections can one Centrifugo instance handle?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#how-many-connections-can-one-centrifugo-instance-handle","content":" This depends on many factors. Real-time transport choice, hardware, message rate, size of messages, Centrifugo features enabled, client distribution over channels, compression on/off, etc. So no certain answer to this question exists. Common sense, performance measurements, and monitoring can help here.  Generally, we suggest not put more than 50-100k clients on one node - but you should measure for your use case.  You can find a description of a test stand with million WebSocket connections in this blog post. Though the point above is still valid – measure and monitor your setup.  ","version":"v5","tagName":"h3"},{"title":"Memory usage per connection?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#memory-usage-per-connection","content":" Depending on transport used and features enabled the amount of RAM required per each connection can vary.  For example, you can expect that each WebSocket connection will cost about 30-50 KB of RAM, thus a server with 1 GB of RAM can handle about 20-30k connections.  For other real-time transports, the memory usage per connection can differ (for example, SockJS connections will cost ~ 2 times more RAM than pure WebSocket connections). So the best way is again – measure for your custom case since depending on Centrifugo transport/features memory usage can vary.  ","version":"v5","tagName":"h3"},{"title":"Can Centrifugo scale horizontally?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#can-centrifugo-scale-horizontally","content":" Yes, it can do this using built-in engines: Redis, KeyDB, Tarantool, or Nats broker.  See engines and scalability considerations.  ","version":"v5","tagName":"h3"},{"title":"Message delivery model​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#message-delivery-model","content":" See design overview  ","version":"v5","tagName":"h3"},{"title":"Message order guarantees​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#message-order-guarantees","content":" See design overview.  ","version":"v5","tagName":"h3"},{"title":"Should I create channels explicitly?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#should-i-create-channels-explicitly","content":" No. By default, channels are created automatically as soon as the first client subscribed to it. And destroyed automatically when the last client unsubscribes from a channel.  When history inside the channel is on then a window of last messages is kept automatically during the retention period. So a client that comes later and subscribes to a channel can retrieve those messages using the call to the history API (or maybe by using the automatic recovery feature which also uses a history internally).  ","version":"v5","tagName":"h3"},{"title":"What about best practices with the number of channels?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#what-about-best-practices-with-the-number-of-channels","content":" Channel is a very lightweight ephemeral entity - Centrifugo can deal with lots of channels, don't be afraid to have many channels in an application.  But keep in mind that one client should be subscribed to a reasonable number of channels at one moment. Client-side subscription to a channel requires a separate frame from client to server – more frames mean more heavy initial connection, more heavy reconnect, etc.  One example which may lead to channel misusing is a messenger app where user can be part of many groups. In this case, using a separate channel for each group/chat in a messenger may be a bad approach. The problem is that messenger app may have chat list screen – a view that displays all user groups (probably with pagination). If you are using separate channel for each group then this may lead to lots of subscriptions. Also, with pagination, to receive updates from older chats (not visible on a screen due to pagination) – user may need to subscribe on their channels too. In this case, using a single personal channel for each user is a preferred approach. As soon as you need to deliver a message to a group you can use Centrifugo broadcast API to send it to many users. If your chat groups are huge in size then you may also need additional queuing system between your application backend and Centrifugo to broadcast a message to many personal channels.  ","version":"v5","tagName":"h3"},{"title":"Any way to exclude message publisher from receiving a message from a channel?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#any-way-to-exclude-message-publisher-from-receiving-a-message-from-a-channel","content":" Currently, no.  We know that services like Pusher provide a way to exclude current client by providing a client ID (socket ID) in publish request. A couple of problems with this:  Client can reconnect while message travels over wire/Backend/Centrifugo – in this case client has a chance to receive a message unexpectedly since it will have another client ID (socket ID)Client can call a history manually or message recovery process can run upon reconnect – in this case a message will present in a history  Both cases may result in duplicate messages. These reasons prevent us adding such functionality into Centrifugo, the correct application architecture requires having some sort of idempotent identifier which allow dealing with message duplicates.  Once added nobody will think about idempotency and this can lead to hard to catch/fix problems in an application. This can also make enabling channel history harder at some point.  Centrifugo behaves similar to Kafka here – i.e. channel should be considered as immutable stream of events where each channel subscriber simply receives all messages published to a channel.  In the future releases Centrifugo may have some sort of server-side message filtering, but we are searching for a proper and safe way of adding it.  ","version":"v5","tagName":"h3"},{"title":"Can I have both binary and JSON clients in one channel?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#can-i-have-both-binary-and-json-clients-in-one-channel","content":" No. It's not possible to transparently encode binary data into JSON protocol (without converting binary to base64 for example which we don't want to do due to increased complexity and performance penalties). So if you have clients in a channel which work with JSON – you need to use JSON payloads everywhere.  Most Centrifugo bidirectional connectors are using binary Protobuf protocol between a client and Centrifugo. But you can send JSON over Protobuf protocol just fine (since JSON is a UTF-8 encoded sequence of bytes in the end).  To summarize:  if you are using binary Protobuf clients and binary payloads everywhere – you are fine.if you are using binary or JSON clients and valid JSON payloads everywhere – you are fine.if you try to send binary data to JSON protocol based clients – you will get errors from Centrifugo.  ","version":"v5","tagName":"h3"},{"title":"Online presence for chat apps - online status of your contacts​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#online-presence-for-chat-apps---online-status-of-your-contacts","content":" While online presence is a good feature it does not fit well for some apps. For example, if you make a chat app - you may probably use a single personal channel for each user. In this case, you cannot find who is online at moment using the built-in Centrifugo presence feature as users do not share a common channel.  You can solve this using a separate service that tracks the online status of your users (for example in Redis) and has a bulk API that returns online status approximation for a list of users. This way you will have an efficient scalable way to deal with online statuses. This is also available as Centrifugo PRO feature.  ","version":"v5","tagName":"h3"},{"title":"Centrifugo stops accepting new connections, why?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#centrifugo-stops-accepting-new-connections-why","content":" The most popular reason behind this is reaching the open file limit. You can make it higher, we described how to do this nearby in this doc. Also, check out an article in our blog which mentions possible problems when dealing with many persistent connections like WebSocket.  ","version":"v5","tagName":"h3"},{"title":"Can I use Centrifugo without reverse-proxy like Nginx before it?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#can-i-use-centrifugo-without-reverse-proxy-like-nginx-before-it","content":" Yes, you can - Go standard library designed to allow this. Though proxy before Centrifugo can be very useful for load balancing clients.  ","version":"v5","tagName":"h3"},{"title":"Does Centrifugo work with HTTP/2?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#does-centrifugo-work-with-http2","content":" Yes, Centrifugo works with HTTP/2. This is provided by built-in Go http server implementation.  You can disable HTTP/2 running Centrifugo server with GODEBUG environment variable:  GODEBUG=&quot;http2server=0&quot; centrifugo -c config.json   Keep in mind that when using WebSocket you are working only over HTTP/1.1, so HTTP/2 support mostly makes sense for SockJS HTTP transports and unidirectional transports: like EventSource (SSE) and HTTP-streaming.  ","version":"v5","tagName":"h3"},{"title":"Does Centrifugo work with HTTP/3?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#does-centrifugo-work-with-http3","content":" Centrifugo v4 added an experimental HTTP/3 support. As soon as you enabled TLS and provided &quot;http3&quot;: true option all endpoints on external port will be served by HTTP/3 server based on github.com/quic-go/quic-go implementation. This (among other benefits which HTTP/3 can provide) is a step towards a proper WebTransport support. For now we support WebTransport experimentally.  It's worth noting that WebSocket transport does not work over HTTP/3, it still starts with HTTP/1.1 Upgrade request (there is an interesting IETF draft BTW about Bootstrapping WebSockets with HTTP/3). But HTTP-streaming and Eventsource should work just fine with HTTP/3.  HTTP/3 does not work with ACME autocert TLS at the moment - i.e. you need to explicitly provide paths to cert and key files as described here.  ","version":"v5","tagName":"h3"},{"title":"Is there a way to use a single connection to Centrifugo from different browser tabs?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-there-a-way-to-use-a-single-connection-to-centrifugo-from-different-browser-tabs","content":" If the underlying transport is HTTP-based, and you use HTTP/2 then this will work automatically. For WebSocket, each browser tab creates a new connection.  ","version":"v5","tagName":"h3"},{"title":"What if I need to send push notifications to mobile or web applications?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#what-if-i-need-to-send-push-notifications-to-mobile-or-web-applications","content":" We provide push notifications API implementation as part of Centrifugo PRO. It allows sending push notifications to devices - to Apple iOS devices via APNS, Android devices via FCM, or browsers over Web Push API. Also, we cover HMS (Huawei Mobile Services). But in general the task of push notification delivery may be done using another open-source solution, or with Firebase directly.  The reasonable question here is how can you know when you need to send a real-time message to an online client or push notification to its device for an offline client. The solution is pretty simple. You can keep critical notifications for a client in the database. And when a client reads a message you should send an ack to your backend marking the notification as read by the client. Periodically you can check which notifications were sent to clients but have not been read (no read ack received). For such notifications, you can send push notification to the device.  ","version":"v5","tagName":"h3"},{"title":"How can I know a message is delivered to a client?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#how-can-i-know-a-message-is-delivered-to-a-client","content":" You can, but Centrifugo does not have such an API. What you have to do to ensure your client has received a message is sending confirmation ack from your client to your application backend as soon as the client processed the message coming from a Centrifugo channel.  ","version":"v5","tagName":"h3"},{"title":"Can I publish new messages over a WebSocket connection from a client?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#can-i-publish-new-messages-over-a-websocket-connection-from-a-client","content":" It's possible to publish messages into channels directly from a client (when publish channel option is enabled). But we strongly discourage this in production usage as those messages just go through Centrifugo without any additional control and validation from the application backend.  We suggest using one of the available approaches:  When a user generates an event it must be first delivered to your app backend using a convenient way (for example AJAX POST request for a web application), processed on the backend (validated, saved into the main application database), and then published to Centrifugo using Centrifugo HTTP or GRPC API.Utilize the RPC proxy feature – in this case, you can call RPC over Centrifugo WebSocket which will be translated to an HTTP request to your backend. After receiving this request on the backend you can publish a message to Centrifugo server API. This way you can utilize WebSocket transport between the client and your server in a bidirectional way. HTTP traffic will be concentrated inside your private network.Utilize the publish proxy feature – in this case client can call publish on the frontend, this publication request will be transformed into HTTP or GRPC call to the application backend. If your backend allows publishing - Centrifugo will pass the payload to the channel (i.e. will publish message to the channel itself).  Sometimes publishing from a client directly into a channel (without any backend involved) can be useful though - for personal projects, for demonstrations (like we do in our examples) or if you trust your users and want to build an application without backend. In all cases when you don't need any message control on your backend.  ","version":"v5","tagName":"h3"},{"title":"How to create a secure channel for two users only (private chat case)?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#how-to-create-a-secure-channel-for-two-users-only-private-chat-case","content":" There are several ways to achieve it:  use a private channel (starting with $) - every time a user subscribes to it your backend should provide a sign to confirm that subscription request. Read more in channels chapternext is user limited channels (with #) - you can create a channel with a name like dialog#42,567 to limit subscribers only to the user with id 42 and user with ID 567, this does not fit well for channels with many or dynamic possible subscribersyou can use subscribe proxy feature to validate subscriptions, see chapter about proxyfinally, you can create a hard-to-guess channel name (based on some secret key and user IDs or just generate and save this long unique name into your main app database) so other users won't know this channel to subscribe on it. This is the simplest but not the safest way - but can be reasonable to consider in many situations  ","version":"v5","tagName":"h3"},{"title":"What's the best way to organize channel configuration?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#whats-the-best-way-to-organize-channel-configuration","content":" In most situations, your application needs several different real-time features. We suggest using namespaces for every real-time feature if it requires some option enabled.  For example, if you need join/leave messages for a chat app - create a special channel namespace with this join_leave option enabled. Otherwise, your other channels will receive join/leave messages too - increasing load and traffic in the system but not used by clients.  The same relates to other channel options.  ","version":"v5","tagName":"h3"},{"title":"Does Centrifugo support webhooks?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#does-centrifugo-support-webhooks","content":" Proxy feature allows integrating Centrifugo with your session mechanism (via connect proxy) and provides a way to react to connection events (rpc, subscribe, publish). Also, it opens a road for bidirectional communication with RPC calls. And periodic connection refresh hooks are also there.  Centrifugo does not support unsubscribe/disconnect hooks – see the reasoning below.  ","version":"v5","tagName":"h3"},{"title":"Why Centrifugo does not have disconnect hooks?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#why-centrifugo-does-not-have-disconnect-hooks","content":" UPDATE Centrifugo PRO now solves the pitfalls mentioned here with its Channel State Events feature.  Centrifugo does not support disconnect hooks at this point. We understand that this may be useful for some use cases but there are some pitfalls which prevent us adding such hooks to Centrifugo.  Let's consider a case when Centrifugo node is unexpectedly killed. In this case there is no chance for Centrifugo to emit disconnect events for connections on that node. While this may be rare thing in practice – it may lead to inconsistent state in your app if you'd rely on disconnect hooks.  Another reason is that Centrifugo designed to scale to many concurrent connections. Think millions of them. As we mentioned in our blog there are cases when all connections start reconnecting at the same time. In this case Centrifugo could potentially generate lots of disconnect events. Even if disconnect events were queued, rate-limited, or suppressed for quickly reconnected clients there could be situations when your app processes disconnect hook after user already reconnected. This is a racy situation which also can lead to the inconsistency if not properly addressed.  Is there a workaround though? If you need to know that client disconnected and program your business logic around this fact then the reasonable approach could be periodically call your backend while client connection is active and update status somewhere on the backend (possibly using Redis for this). Then periodically do clealup logic for connections/users not updated for a configured interval. This is a robust solution where you can't occasionally miss disconnect events. You can also utilize Centrifugo connect proxy + refresh proxy for getting notified about initial connection and get periodic refresh requests while connection is alive.  The trade-off of the described workaround scenario is that you will notice disconnection only with some delay – this may be a acceptable in many cases though.  Having said that, processing disconnect events may be reasonable – as a best-effort solution while taking into account everything said above. Centrifuge library for Go language (which is the core of Centrifugo) supports client disconnect callbacks on a server-side – so technically the possibility exists. If someone comes with a use case which definitely wins from having disconnect hooks in Centrifugo we are ready to discuss this and try to design a proper solution together.  All the pitfalls and workarounds here may be also applied to unsubscribe event hooks.  ","version":"v5","tagName":"h3"},{"title":"Is it possible to listen to join/leave events on the app backend side?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-it-possible-to-listen-to-joinleave-events-on-the-app-backend-side","content":" No, join/leave events are only available in the client protocol. In most cases join event can be handled by using subscribe proxy. Leave events are harder – there is no unsubscribe hook available (mostly the same reasons as for disconnect hook described above). So the workaround here can be similar to one for disconnect – ping an app backend periodically while client is subscribed and thus know that client is currently in a channel with some approximation in time.  ","version":"v5","tagName":"h3"},{"title":"How scalable is the online presence and join/leave features?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#how-scalable-is-the-online-presence-and-joinleave-features","content":" Online presence is good for channels with a reasonably small number of active subscribers. As soon as there are tons of active subscribers, presence information becomes very expensive in terms of bandwidth (as it contains full information about all clients in a channel).  There is presence_stats API method that can be helpful if you only need to know the number of clients (or unique users) in a channel. But in the case of the Redis engine even presence_stats call is not optimized for channels with more than several thousand active subscribers.  You may consider using a separate service to deal with presence status information that provides information in near real-time maybe with some reasonable approximation. Centrifugo PRO provides a user status feature which may fit your needs.  The same is true for join/leave messages - as soon as you turn on join/leave events for a channel with many active subscribers each subscriber starts generating indiviaual join/leave events. This may result in many messages sent to each subscriber in a channel, drastically multiplying amount of messages traveling through the system. Especially when all clients reconnect simulteniously. So be careful and estimate the possible load. There is no magic, unfortunately.  ","version":"v5","tagName":"h3"},{"title":"How to send initial data to channel subscriber?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#how-to-send-initial-data-to-channel-subscriber","content":" Sometimes you need to send some initial state towards channel subscriber. Centrifugo provides a way to attach any data to a successful subscribe reply when using subscribe proxy feature. See data and b64data fields. This data will be part of subscribed event context. And of course, you can always simply send request to get initial data from the application backend before or after subscribing to a channel without Centrifugo connection involved (i.e. using sth like general AJAX/HTTP call or passing data to the template when rendering an application page).  ","version":"v5","tagName":"h3"},{"title":"Does Centrifugo support multitenancy?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#does-centrifugo-support-multitenancy","content":" If you want to use Centrifugo with different projects the recommended approach is to have different Centrifugo installations for each project. Multitenancy is better to solve on infrastructure level in case of Centrifugo.  It's possible to share one Redis setup though by setting unique redis_prefix. But we recommend having completely isolated setups.  ","version":"v5","tagName":"h3"},{"title":"I have not found an answer to my question here​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#i-have-not-found-an-answer-to-my-question-here","content":" Ask in our community rooms:    ","version":"v5","tagName":"h3"},{"title":"flow_diagrams","type":0,"sectionRef":"#","url":"/docs/flow_diagrams","content":"flow_diagrams For swimlanes.io: Client &lt;- App Backend: JWT note: The backend generates JWT for a user and passes it to the client side. Client -&gt; Centrifugo: Client connects to Centrifugo with JWT ...: {fas-spinner} Persistent connection established Client -&gt; Centrifugo: Client issues channel subscribe requests Centrifugo --&gt;&gt; Client: Client receives real-time updates from channels Client -&gt; Centrifugo: Connect request note: Client connects to Centrifugo without JWT. Centrifugo -&gt; App backend: Sends request further (via HTTP or GRPC) note: The application backend validates client connection and tells Centrifugo user credentials in Connect reply. App backend -&gt; Centrifugo: Connect reply Centrifugo -&gt; Client: Connect Reply ...: {fas-spinner} Persistent connection established Client -&gt; App Backend: Publish request note: Client sends data to publish to the application backend. Backend validates it, maybe modifies, optionally saves to the main database, constructs real-time update and publishes it to the Centrifugo server API. App Backend -&gt; Centrifugo: Publish over Centrifugo API Centrifugo --&gt;&gt; Client: {far-bolt fa-lg} Real-time notification note: Centrifugo delivers real-time message to active channel subscribers. Client -&gt; App Backend: Publish request note: Client sends data to publish to the application backend. Backend validates it, maybe modifies, optionally saves to the main database, constructs real-time update and publishes it to the Centrifugo server API. App Backend -&gt; Centrifugo: Publish over Centrifugo API Centrifugo --&gt;&gt; Client: {far-bolt fa-lg} Real-time notification note: Centrifugo delivers real-time message to active channel subscribers. ","keywords":"","version":"v5"},{"title":"Client API showcase","type":0,"sectionRef":"#","url":"/docs/getting-started/client_api","content":"","keywords":"","version":"v5"},{"title":"Connecting to a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#connecting-to-a-server","content":" Each Centrifugo client allows connecting to a server.  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket'); centrifuge.connect();   In most cases you will need to pass JWT (JSON Web Token) for authentication, so the example above transforms to:  const centrifuge = new Centrifuge('ws://localhost:8000/connection/websocket'); centrifuge.setToken('&lt;USER-JWT&gt;') centrifuge.connect();   See authentication chapter for more information on how to generate connection JWT.  If you are using connect proxy then you may go without setting JWT.  ","version":"v5","tagName":"h2"},{"title":"Disconnecting from a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#disconnecting-from-a-server","content":" After connecting you can disconnect from a server at any moment.  centrifuge.disconnect();   ","version":"v5","tagName":"h2"},{"title":"Reconnecting to a server​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#reconnecting-to-a-server","content":" Centrifugo clients automatically reconnect to a server in case of temporary connection loss, also clients periodically ping the server to detect broken connections.  ","version":"v5","tagName":"h2"},{"title":"Connection lifecycle events​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#connection-lifecycle-events","content":" All client implementations allow setting handlers on connect and disconnect events.  For example:  centrifuge.on('connect', function(connectCtx){ console.log('connected', connectCtx) }); centrifuge.on('disconnect', function(disconnectCtx){ console.log('disconnected', disconnectCtx) });   ","version":"v5","tagName":"h2"},{"title":"Subscribe to a channel​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#subscribe-to-a-channel","content":" Another core functionality of client API is the possibility to subscribe to a channel to receive all messages published to that channel.  centrifuge.subscribe('channel', function(messageCtx) { console.log(messageCtx); })   Clients can subscribe to different channels. The subscribe method returns the Subscription object. It's also possible to react to different Subscription events: join and leave events, subscribe success and subscribe error events, unsubscribe events.  In the idiomatic case, messages are published to channels from the application backend over the Centrifugo server API. However, this is not always the case.  Centrifugo also provides a message recovery feature to restore missed publications in channels. Publications can be missed due to temporary disconnects (bad network) or server reloads. Recovery happens automatically on reconnect (due to bad network or server reloads) as soon as recovery in the channel is properly configured. The client keeps the last seen Publication offset and restores missed publications since the known offset upon reconnecting. If recovery fails, then the client implementation provides a flag inside the subscribe event to let the application know that some publications were missed – so you may need to load the state from scratch from the application backend. Not all Centrifugo clients implement a recovery feature – refer to specific client implementation docs. More details about recovery can be found in a dedicated chapter.  ","version":"v5","tagName":"h2"},{"title":"Server-side subscriptions​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#server-side-subscriptions","content":" To handle publications coming from server-side subscriptions client API allows listening publications simply on Centrifuge client instance:  centrifuge.on('publish', function(messageCtx) { console.log(messageCtx); });   It's also possible to react on different server-side Subscription events: join and leave events, subscribe success, unsubscribe event. There is no subscribe error event here since the subscription was initiated on the server-side.  ","version":"v5","tagName":"h2"},{"title":"Send RPC​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#send-rpc","content":" A client can send an RPC to the server. RPC is a call that is not related to channels at all. It's just a way to call a server method from the client side over the WebSocket or SockJS connection. RPC is only available when an RPC proxy is configured.  const rpcRequest = {'key': 'value'}; const data = await centrifuge.namedRPC('example_method', rpcRequest);   ","version":"v5","tagName":"h2"},{"title":"Call channel history​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#call-channel-history","content":" Once subscribed client can call publication history inside a channel (only for channels where history configured) to get last publications in channel:  Get stream current top position:  const resp = await subscription.history(); console.log(resp.offset); console.log(resp.epoch);   Get up to 10 publications from history since known stream position:  const resp = await subscription.history({limit: 10, since: {offset: 0, epoch: '...'}}); console.log(resp.publications);   Get up to 10 publications from history since current stream beginning:  const resp = await subscription.history({limit: 10}); console.log(resp.publications);   Get up to 10 publications from history since current stream end in reversed order (last to first):  const resp = await subscription.history({limit: 10, reverse: true}); console.log(resp.publications);   ","version":"v5","tagName":"h2"},{"title":"Presence and presence stats​","type":1,"pageTitle":"Client API showcase","url":"/docs/getting-started/client_api#presence-and-presence-stats","content":" Once subscribed client can call presence and presence stats information inside channel (only for channels where presence configured):  For presence (full information about active subscribers in channel):  const resp = await subscription.presence(); // resp contains presence information - a map client IDs as keys // and client information as values.   For presence stats (just a number of clients and unique users in a channel):  const resp = await subscription.presenceStats(); // resp contains a number of clients and a number of unique users.  ","version":"v5","tagName":"h2"},{"title":"Join community","type":0,"sectionRef":"#","url":"/docs/getting-started/community","content":"Join community If you find Centrifugo interesting, you are welcome to join our community rooms on Telegram (the most active) and Discord: We strive to create a respectful and curious community. In these rooms, you may find answers to questions not fully covered by the documentation, share your thoughts and ideas on real-time messaging topics and Centrifugo in particular. We also have a Twitter account and a YouTube channel. See you there!","keywords":"","version":"v5"},{"title":"Comparing with others","type":0,"sectionRef":"#","url":"/docs/getting-started/comparisons","content":"","keywords":"","version":"v5"},{"title":"Centrifugo vs Socket.io​","type":1,"pageTitle":"Comparing with others","url":"/docs/getting-started/comparisons#centrifugo-vs-socketio","content":" Socket.io is a library; you need to write your own server on top of it. Centrifugo is a ready-to-use standalone server.  Because of this, Socket.io may give you more flexibility, but you are mostly limited to writing code in JavaScript on the backend to get the most out of it.  Since Centrifugo is a standalone server, it's a universal, language-agnostic element that integrates well with a backend written in any language. But because it's universal, it comes with integration rules, its own mechanics, and limitations of such a design.  ","version":"v5","tagName":"h2"},{"title":"Centrifugo vs Pusher, Ably, Pubnub​","type":1,"pageTitle":"Comparing with others","url":"/docs/getting-started/comparisons#centrifugo-vs-pusher-ably-pubnub","content":" The main difference is that Centrifugo is a self-hosted solution, while the mentioned technologies are all cloud SaaS platforms.  So, when using Centrifugo, you need to configure and run it on your own; you need to be skilled in engineering. The benefit is obvious – it's much cheaper once the integration is done. And all the data stays within your organization.  With cloud services, all the hard work of setting up an infrastructure for a WebSocket server and its maintenance is done for you. But it's more expensive, and the data flows through an external network.  ","version":"v5","tagName":"h2"},{"title":"Centrifugo vs Redis​","type":1,"pageTitle":"Comparing with others","url":"/docs/getting-started/comparisons#centrifugo-vs-redis","content":" A popular question from newcomers – does Centrifugo provide the same as Redis PUB/SUB? The answer is that Centrifugo and Redis can't be directly compared at all. Centrifugo uses Redis internally for PUB/SUB scalability, keeping channel message history, and online presence.  You can build a system similar to Centrifugo on top of Redis – but you would need to write a lot of code, i.e., replicate everything that Centrifugo provides out of the box – like real-time client SDKs, client protocol, re-implement all the transport endpoints, write efficient Redis integration, etc.  ","version":"v5","tagName":"h2"},{"title":"Centrifugo vs Kafka​","type":1,"pageTitle":"Comparing with others","url":"/docs/getting-started/comparisons#centrifugo-vs-kafka","content":" At first glance, Centrifugo provides concepts similar to Apache Kafka - it has channels which seem similar to Kafka's topics, channel history with time and size retention policies. So sometimes people ask whether Centrifugo may be used as a lightweight Kafka replacement.  But Centrifugo and Kafka were designed for different purposes.  Centrifugo is a real-time messaging system with push semantics. It provides a lightweight PUB/SUB with ephemeral in-memory channels, designed to be exposed to frontend real-time apps. It has limited guarantees about message persistence in channel history, though it provides client hooks to be notified about message loss and recover the state from the main application database.  Kafka is a persistent disk-based message bus that you can't easily expose to frontend users. It won't work well with millions of topics where users connect and disconnect constantly – this would cause constant repartitioning and eventually require much more resources. Kafka fits well for service-to-service communication where topics may be pre-created and under control; Centrifugo fits well for frontend-to-backend real-time communication.  ","version":"v5","tagName":"h2"},{"title":"Centrifugo vs Nats​","type":1,"pageTitle":"Comparing with others","url":"/docs/getting-started/comparisons#centrifugo-vs-nats","content":" This is also a popular comparison request. Especially since both Centrifugo and Nats are written in the Go language, and Nats also supports connections from the application client side over the WebSocket protocol.  Nats is a very powerful messaging system, and it also has a built-in Jetstream system to provide at least once delivery guarantees. It has a larger community, more SDKs for various languages, incredible performance.  How Centrifugo is special:  Centrifugo actually uses Nats as one of the options for a PUB/SUB broker, i.e., as a PUB/SUB scalability backend.Centrifugo was originally designed to be exposed to application frontend clients, providing various convenient authentication and channel authorization mechanisms for client-side integration.It supports more transports, including WebSocket fallbacks.It provides many unique features that are out of scope for Nats – like online presence, individual GRPC subscription streams, or some of Centrifugo PRO features like push notifications support, real-time analytics with ClickHouse, etc.Topics in Jetstream still should be pre-created, while Centrifugo has ephemeral channels – created on the fly, even when using a channel history cache.Centrifugo provides client connection event proxy features - it's possible to delegate authentication, channel authorization to the application backend, and provides channel state events (when a channel is occupied or vacated) in the PRO version.  Nats is great, and we are constantly looking for tighter integration with Nats. But both systems have unique sets of features and may be better or worse for various tasks. ","version":"v5","tagName":"h2"},{"title":"Design overview","type":0,"sectionRef":"#","url":"/docs/getting-started/design","content":"","keywords":"","version":"v5"},{"title":"Idiomatic usage​","type":1,"pageTitle":"Design overview","url":"/docs/getting-started/design#idiomatic-usage","content":" Centrifugo is a standalone server that abstracts away the complexity of working with many persistent connections and efficiently broadcasting messages from the application backend. The fact that Centrifugo acts as a separate service dictates some idiomatic patterns for how to integrate with Centrifugo for real-time message delivery.  Usually, you want to deliver content created by a user in your app to other users in real time. Each user may have several real-time connections with Centrifugo. For example, a user opened several browser tabs, with each tab creating a separate connection. Or a user has two mobile devices and created a separate connection to your app from each of them. We call a connection a client in Centrifugo. Thus, the words connection and client are synonyms for us.  All requests from users that generate new data should first go to the application backend – i.e., calling the app backend API from the client side. The backend can validate the message, process it, save it into a database for long-term persistence – and then publish an event to a channel using Centrifugo server API. This event is then efficiently broadcast by Centrifugo to all active channel subscribers.  The following diagram shows the process (assuming the client that generates new content is also a channel subscriber and thus also receives the real-time message):    This is usually a natural workflow for applications since this is how applications traditionally work (without real-time features) and Centrifugo is fully decoupled from the application in this case.  Centrifugo serves the role of a real-time transport layer in this case, and you may design the app with graceful degradation in mind – so that removing Centrifugo won't be a fatal problem for the application – it will continue working, just without the real-time features.  If the original source of events is your app backend (without any user involvement) – then the above diagram simplifies to:    So the backend publishes data to channels and if there are active subscribers – events are delivered. If there are no active subscribers then events are dropped by Centrifugo (or, in case of using history features in channels, events may be temporarily kept in the Centrifugo history stream).  It's also possible to utilize Centrifugo's bidirectional connection for sending requests to the backend. To achieve this, Centrifugo provides event proxy features. It's possible to send RPC (with a custom request-response) requests from the client to Centrifugo and the request will then be proxied to the application backend (see RPC proxy). Moreover, the proxy provides a way to utilize the bidirectional connection for publishing into channels (using publish proxy). But again – in most real scenarios, your backend must validate the publication attempt, so the scheme will look like this:    ","version":"v5","tagName":"h2"},{"title":"Message history considerations​","type":1,"pageTitle":"Design overview","url":"/docs/getting-started/design#message-history-considerations","content":" Idiomatic Centrifugo usage requires having the main application database from which the initial and actual state can be loaded at any point in time.  While Centrifugo has channel history, it has been mostly designed to be a hot cache to reduce the load on the main application database when all users reconnect at once (in case of a load balancer configuration reload, Centrifugo restart, temporary network problems, etc.). This allows for a radical reduction in the load on the application's main database during a reconnect storm. Since such disconnects are usually pretty short in time, having a reasonably small number of messages cached in history is sufficient.  The addition of the history iteration API shifts possible use cases a bit. Manually calling history chunk by chunk allows for keeping a larger number of publications per channel.  Depending on the Engine used and the configuration of the underlying storage, history stream persistence characteristics can vary. For example, with the Memory Engine, history will be lost upon Centrifugo restart. With Redis or Tarantool engines, history will survive Centrifugo restarts, but depending on the storage configuration, it can be lost upon storage restart – so you should take into account storage configuration and persistence properties as well. For example, consider enabling Redis AOF with fsync for maximum durability, or configure replication for high availability, use Redis Cluster, or maybe synchronous replication with Tarantool.  When using history with automatic recovery, Centrifugo provides clients with a flag to distinguish whether the missed messages were all successfully restored from Centrifugo history upon recovery or not. If not – the client may restore the state from the main application database. Centrifugo message history can be used as a complementary way to restore messages and thus reduce the load on the main application database most of the time.  ","version":"v5","tagName":"h2"},{"title":"Message delivery model​","type":1,"pageTitle":"Design overview","url":"/docs/getting-started/design#message-delivery-model","content":" By default, the message delivery model of Centrifugo is 'at most once'. With history and the positioning/recovery features enabled, it's possible to achieve 'at least once' guarantee within the history retention time and size. After an abnormal disconnect, clients have an option to recover missed messages from the publication channel stream history that Centrifugo maintains.  Without the positioning or recovery features enabled, a message sent to Centrifugo could theoretically be lost while moving towards clients. Centrifugo makes its best effort only to prevent message loss on the way to online clients, but the application should tolerate the loss.  As noted, Centrifugo has a feature called message recovery to automatically recover messages missed due to short network disconnections. It also compensates for the 'at most once' delivery of a broker PUB/SUB system (Redis, Tarantool) by using additional publication offset checks and periodic offset synchronization. So publication loss missed in the PUB/SUB layer will be detected eventually, and the client may catch up on the state by loading it from history.  ","version":"v5","tagName":"h2"},{"title":"Message order guarantees​","type":1,"pageTitle":"Design overview","url":"/docs/getting-started/design#message-order-guarantees","content":" Message order in channels is guaranteed to be the same when you publish messages into a channel one after another or publish them in one request. If you do parallel publications into the same channel, then Centrifugo can't guarantee message order since those are processed in parallel.  ","version":"v5","tagName":"h2"},{"title":"Graceful degradation​","type":1,"pageTitle":"Design overview","url":"/docs/getting-started/design#graceful-degradation","content":" It is recommended to design an application in a way that users don't even notice when Centrifugo does not work. Use graceful degradation. For example, if a user posts a new comment over AJAX to your application backend - you should not rely solely on Centrifugo to receive a new comment from a channel and display it. You should return new comment data in the AJAX call response and render it. This way, the user that posts a comment will think that everything works just fine. Be careful not to draw comments twice in this case because you may also receive the same data from a channel - think about idempotent identifiers for your entities.  ","version":"v5","tagName":"h2"},{"title":"Online presence considerations​","type":1,"pageTitle":"Design overview","url":"/docs/getting-started/design#online-presence-considerations","content":" Online presence in a channel is designed to be eventually consistent. It will return the correct state most of the time. But when using Redis or Tarantool engines, due to network failures and the unexpected shutdown of a Centrifugo node, there are chances that clients can be present in a presence for up to one minute more (until the presence entry expires).  Also, channel presence does not scale well for channels with a lot of active subscribers. This is due to the fact that presence returns the entire snapshot of all clients in a channel – as soon as the number of active subscribers grows, the response size becomes larger. In some cases, the presence_stats API call can be sufficient to avoid receiving the entire presence state.  ","version":"v5","tagName":"h2"},{"title":"Scalability considerations​","type":1,"pageTitle":"Design overview","url":"/docs/getting-started/design#scalability-considerations","content":" Centrifugo can scale horizontally with built-in engines (Redis, Tarantool, KeyDB) or with the Nats broker. See engines.  All supported brokers are fast – they can handle hundreds of thousands of requests per second. This should be enough for most applications.  But if you approach broker resource limits (CPU or memory), then it's possible:  Use Centrifugo's consistent sharding support to balance queries between different broker instances (supported for Redis, KeyDB, Tarantool).Use Redis Cluster (it's also possible to consistently shard data between different Redis Clusters).Nats broker should scale well itself in a cluster setup.  All brokers can be set up in a highly available way so there won't be a single point of failure.  All Centrifugo data (history, online presence) is designed to be ephemeral and have an expiration time. Due to this fact, and the fact that Centrifugo provides hooks for the application to understand history loss, makes the process of resharding mostly automatic. As soon as you need to add an additional broker shard (when using client-side sharding), you can just add it to the configuration and restart Centrifugo. Since data is sharded consistently, part of the data will stay on the same broker nodes. Applications should handle cases where channel data moved to another shard and restore the state from the main application database when needed (i.e., when the recovered flag provided by the SDK is false). ","version":"v5","tagName":"h2"},{"title":"Ecosystem notes","type":0,"sectionRef":"#","url":"/docs/getting-started/ecosystem","content":"","keywords":"","version":"v5"},{"title":"Centrifuge library for Go​","type":1,"pageTitle":"Ecosystem notes","url":"/docs/getting-started/ecosystem#centrifuge-library-for-go","content":" Centrifugo is built on top of the Centrifuge library for the Go language.  Due to its standalone language-agnostic nature, Centrifugo dictates some rules developers should follow when integrating. If you need more freedom and a tighter integration of a real-time server with application business logic, you may consider using the Centrifuge library to build something similar to Centrifugo but with customized behavior. The Centrifuge library can be considered as an analogue of Socket.IO in the Go language ecosystem.  The library's README has a detailed description, a link to examples, and an introduction post.  Many Centrifugo features should be re-implemented when using Centrifuge - like the API layer, admin web UI, proxy, etc. (if you need those of course). And you need to write in the Go language. But the core functionality like a client-server protocol (all Centrifugo client SDKs work with a Centrifuge library-based server) and Redis engine to scale come out of the box – in most cases, this is enough to start building an app.  tip Many things said in the Centrifugo doc can be considered as extra documentation for the Centrifuge library (for example, parts about infrastructure tuning or transport description). But not all of them.  ","version":"v5","tagName":"h2"},{"title":"Framework integrations​","type":1,"pageTitle":"Ecosystem notes","url":"/docs/getting-started/ecosystem#framework-integrations","content":" There are some community-driven projects that provide integration with frameworks for more native experience.  tip In general, integrating Centrifugo can be done in several steps even without third-party libraries – see our integration guide. Integrating directly may allow using all Centrifugo features without limitations which can be introduced by third-party wrapper.  laravel-centrifugo integration with Laravel frameworklaravel-centrifugo-broadcaster one more integration with Laravel framework to considerCentrifugoBundle integration with Symfony frameworkDjango-instant integration with Django frameworkroadrunner-php/centrifugo integration with RoadRunnerspiral/roadrunner-bridge integration with Spiral Framework ","version":"v5","tagName":"h2"},{"title":"Main highlights","type":0,"sectionRef":"#","url":"/docs/getting-started/highlights","content":"","keywords":"","version":"v5"},{"title":"Simple integration​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#simple-integration","content":" Centrifugo was originally designed to be used in conjunction with frameworks without built-in concurrency support (like Django, Laravel, etc.).  It works as a standalone service with well-defined communication contracts. It nicely fits both monolithic and microservice architectures. Application developers should not change the backend philosophy and technology stack at all – just integrate with Centrifugo HTTP or GRPC API and let users enjoy real-time updates.  ","version":"v5","tagName":"h3"},{"title":"Great performance​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#great-performance","content":" Centrifugo is fast. It's written in the Go language, built on top of fast and battle-tested open-source libraries, has some smart internal optimizations like message queuing on broadcasts, smart batching to reduce the number of RTTs with the broker, connection hub sharding to avoid lock contention, JSON and Protobuf encoding speedups through code generation, and others.  See the Million WebSocket with Centrifugo post on our blog to see some real-world numbers.  ","version":"v5","tagName":"h3"},{"title":"Built-in scalability​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#built-in-scalability","content":" Centrifugo scales well to many machines with the help of PUB/SUB brokers. So as soon as you have more client connections in the application – you can spread them over different Centrifugo nodes which will be connected together into a cluster.  The main PUB/SUB engine that Centrifugo integrates with is Redis. It supports client-side consistent sharding and Redis Cluster – so a single Redis instance won't be a bottleneck either.  There are other options to scale: KeyDB, Nats, Tarantool. See docs about available engines.  ","version":"v5","tagName":"h3"},{"title":"Strict client protocol​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#strict-client-protocol","content":" Centrifugo supports JSON and binary Protobuf protocols for client-server communication. The bidirectional protocol is defined by a strict schema and several ready-to-use SDKs wrap this protocol, handle asynchronous message passing, timeouts, reconnects, and various Centrifugo client API features. See detailed information about client real-time transports in a dedicated section.  ","version":"v5","tagName":"h3"},{"title":"Variety of real-time transports​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#variety-of-real-time-transports","content":" The main transport in Centrifugo is WebSocket. For browsers that do not support WebSocket, Centrifugo provides its own bidirectional WebSocket emulation layer based on HTTP-streaming (using Fetch and Readable streams browser APIs) and SSE (EventSource). It also supports SockJS as an older but battle-tested WebSocket polyfill option. Additionally, WebTransport is supported in an experimental form.  In addition to bidirectional transports, Centrifugo also supports a unidirectional approach for real-time updates: using SSE (EventSource), HTTP-streaming, and GRPC unidirectional stream. Utilizing a unidirectional transport is sufficient for many real-time applications and does not require using our client SDKs – just native standards or GRPC-generated code.  See detailed information about client real-time transports in a dedicated section.  ","version":"v5","tagName":"h3"},{"title":"Flexible authentication​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#flexible-authentication","content":" Centrifugo can authenticate connections by checking JWT (JSON Web Token) or by issuing an HTTP/GRPC request to your application backend upon a client connection to Centrifugo. It's possible to proxy original request headers or request metadata (in the case of a GRPC connection).  It supports the JWK specification.  ","version":"v5","tagName":"h3"},{"title":"Connection management​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#connection-management","content":" Connections can expire; developers can choose a way to handle connection refresh – using a client-side refresh workflow or a server-side call from Centrifugo to the application backend. Centrifugo provides APIs to disconnect users, unsubscribe users from channels, and inspect active channels.  ","version":"v5","tagName":"h3"},{"title":"Channel (room) concept​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#channel-room-concept","content":" Centrifugo is a PUB/SUB server – users subscribe to channels to receive real-time updates. A message sent to a channel is delivered to all online channel subscribers.  ","version":"v5","tagName":"h3"},{"title":"Different types of subscriptions​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#different-types-of-subscriptions","content":" Centrifugo supports client-side (initiated by the client) and server-side (forced by the server) channel subscriptions.  ","version":"v5","tagName":"h3"},{"title":"RPC over bidirectional connection​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#rpc-over-bidirectional-connection","content":" You can fully utilize bidirectional connections by sending RPC calls from the client-side to a configured endpoint on your backend. Calling RPC over WebSocket avoids sending headers on each request – thus reducing incoming traffic.  ","version":"v5","tagName":"h3"},{"title":"Online presence information​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#online-presence-information","content":" The online presence feature for channels provides information about active channel subscribers. Also, channel join and leave events (when someone subscribes/unsubscribes) can be received on the client side.  ","version":"v5","tagName":"h3"},{"title":"Message history in channels​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#message-history-in-channels","content":" Optionally, Centrifugo allows turning on history for publications in channels. This publication history has a limited size and retention period (TTL). With channel history, Centrifugo can help to survive the mass reconnect scenario – clients can automatically catch up on missed state from a fast cache thus reducing the load on your primary database. It's also possible to manually iterate over a history stream from the client or from the application backend side.  ","version":"v5","tagName":"h3"},{"title":"Embedded admin web UI​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#embedded-admin-web-ui","content":" The built-in admin UI allows publishing messages to channels, looking at Centrifugo cluster information, and more.  ","version":"v5","tagName":"h3"},{"title":"Cross-platform​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#cross-platform","content":" Centrifugo works on Linux, macOS, and Windows.  ","version":"v5","tagName":"h3"},{"title":"Ready to deploy​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#ready-to-deploy","content":" Centrifugo supports various deployment methods: in Docker, using prepared RPM or DEB packages, via a Kubernetes Helm chart. It supports automatic TLS with Let's Encrypt TLS, outputs Prometheus/Graphite metrics, and has an official Grafana dashboard for the Prometheus data source.  ","version":"v5","tagName":"h3"},{"title":"Open-source​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#open-source","content":" Centrifugo stands on top of the open-source library Centrifuge (MIT license). The OSS version of Centrifugo is based on the permissive open-source license (Apache 2.0). All our official client SDKs and API libraries are MIT-licensed.  ","version":"v5","tagName":"h3"},{"title":"PRO features​","type":1,"pageTitle":"Main highlights","url":"/docs/getting-started/highlights#pro-features","content":" Centrifugo PRO extends Centrifugo with several unique features which can provide interesting advantages for business adopters. For additional details, refer to the Centrifugo PRO documentation. ","version":"v5","tagName":"h3"},{"title":"Install Centrifugo","type":0,"sectionRef":"#","url":"/docs/getting-started/installation","content":"","keywords":"","version":"v5"},{"title":"Install from the binary release​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/getting-started/installation#install-from-the-binary-release","content":" For local development, you can download the prebuilt Centrifugo binary release (i.e., a single all-contained executable file) for your system.  Binary releases are available on Github. Download the latest release for your operating system, unpack it, and you are done. Centrifugo is pre-built for:  Linux 64-bit (linux_amd64)Linux 32-bit (linux_386)Linux ARM 64-bit (linux_arm64)MacOS (darwin_amd64)MacOS on Apple Silicon (darwin_arm64)Windows (windows_amd64)FreeBSD (freebsd_amd64)ARM v6 (linux_armv6)  Archives contain a single statically compiled binary centrifugo file that is ready to run.  ./centrifugo   If you are unsure which distribution you need, then on Linux or MacOS you can use the following command to download and unpack the centrifugo binary to your current working directory:  curl -sSLf https://centrifugal.dev/install.sh | sh   See the version of Centrifugo:  ./centrifugo version   Centrifugo requires a configuration file with several secret keys. If you are new to Centrifugo, then there is a genconfig command which generates a minimal configuration file to get started:  ./centrifugo genconfig   This creates a configuration file config.json with some auto-generated option values in the current directory (by default).  tip It's possible to generate a file in YAML or TOML format, for example: ./centrifugo genconfig -c config.toml  With a configuration file, you can finally run a Centrifugo instance:  ./centrifugo --config=config.json   We will talk about configuration in detail in the next sections.  You can also put or symlink centrifugo into your bin OS directory and run it from anywhere:  centrifugo --config=config.json   ","version":"v5","tagName":"h2"},{"title":"Docker image​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/getting-started/installation#docker-image","content":" Centrifugo server has a docker image available on Docker Hub.  docker pull centrifugo/centrifugo   Run:  docker run --ulimit nofile=262144:262144 -v /host/dir/with/config/file:/centrifugo -p 8000:8000 centrifugo/centrifugo:v5 centrifugo -c config.json   Note that docker allows setting nofile limits in command-line arguments, which is quite important to handle many simultaneous persistent connections and not run out of the open file limit (each connection requires one file descriptor). See also infrastructure tuning chapter.  caution Pin to the exact Docker Image tag in production, for instance: centrifugo/centrifugo:v5.0.0. This will help to avoid unexpected problems during redeployment process.  ","version":"v5","tagName":"h2"},{"title":"Docker-compose example​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/getting-started/installation#docker-compose-example","content":" Create configuration file config.json:  { &quot;token_hmac_secret_key&quot;: &quot;my_secret&quot;, &quot;api_key&quot;: &quot;my_api_key&quot;, &quot;admin_password&quot;: &quot;password&quot;, &quot;admin_secret&quot;: &quot;secret&quot;, &quot;admin&quot;: true }   Create docker-compose.yml:  version: &quot;3.9&quot; services: centrifugo: container_name: centrifugo image: centrifugo/centrifugo:v5 volumes: - ./config.json:/centrifugo/config.json command: centrifugo -c config.json ports: - 8000:8000 ulimits: nofile: soft: 65535 hard: 65535   Run with:  docker-compose up   ","version":"v5","tagName":"h2"},{"title":"Kubernetes Helm chart​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/getting-started/installation#kubernetes-helm-chart","content":" See our official Kubernetes Helm chart. Follow instructions in a Centrifugo chart README to bootstrap Centrifugo inside your Kubernetes cluster.  ","version":"v5","tagName":"h2"},{"title":"RPM and DEB packages for Linux​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/getting-started/installation#rpm-and-deb-packages-for-linux","content":" Every time we make a new Centrifugo release we upload rpm and deb packages for popular Linux distributions on packagecloud.io.  At moment, we support versions of the following distributions:  64-bit Debian 10 Buster64-bit Debian 11 Bullseye64-bit Ubuntu 18.04 Bionic64-bit Ubuntu 20.04 Focal Fossa64-bit Ubuntu 20.04 Jammy64-bit Centos 7  See full list of available packages and installation instructions.  Centrifugo also works on 32-bit architecture, but we don't support packaging for it since 64-bit is more convenient for servers today.  ","version":"v5","tagName":"h2"},{"title":"With brew on macOS​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/getting-started/installation#with-brew-on-macos","content":" If you are developing on macOS then you can install Centrifugo over brew:  brew tap centrifugal/centrifugo brew install centrifugo   ","version":"v5","tagName":"h2"},{"title":"Build from source​","type":1,"pageTitle":"Install Centrifugo","url":"/docs/getting-started/installation#build-from-source","content":" You need Go language installed. Then:  git clone https://github.com/centrifugal/centrifugo.git cd centrifugo go build ./centrifugo  ","version":"v5","tagName":"h2"},{"title":"Integration guide","type":0,"sectionRef":"#","url":"/docs/getting-started/integration","content":"","keywords":"","version":"v5"},{"title":"0. Install​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#0-install","content":" The first thing you need to do is download/install the Centrifugo server. See the install chapter for details.  ","version":"v5","tagName":"h2"},{"title":"1. Configure Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#1-configure-centrifugo","content":" Create a basic configuration file with token_hmac_secret_key (or token_rsa_public_key) and api_key set, and then run Centrifugo. See this chapter for details about token_hmac_secret_key/token_rsa_public_key and the chapter about server API for the API description. The simplest way to do this automatically is by using the genconfig command:  ./centrifugo genconfig   – which will generate config.json file for you with a minimal set of fields to start from.  Properly configure allowed_origins option.  ","version":"v5","tagName":"h2"},{"title":"2. Configure your backend​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#2-configure-your-backend","content":" In the configuration file of your application backend, register several variables: the Centrifugo token secret (if you decided to stick with JWT authentication) and the Centrifugo API key you set in the previous step, as well as the Centrifugo API endpoint address. By default, the API address is http://localhost:8000/api. You must never reveal the token secret and API key to your users.  ","version":"v5","tagName":"h2"},{"title":"3. Connect to Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#3-connect-to-centrifugo","content":" Now your users can start connecting to Centrifugo. You should get a client library (see the list of available client SDKs) for your application frontend. Every library has a method to connect to Centrifugo. See information about Centrifugo connection endpoints here.  Every client should provide a connection token (JWT) upon connecting. You must generate this token on your backend side using the Centrifugo secret key you set in the backend configuration (note that in the case of RSA tokens, you are generating JWT with a private key). See how to generate this JWT in the special chapter.  You pass this token from the backend to your frontend app (pass it in the template context or use a separate request from the client-side to get a user-specific JWT from the backend side). And use this token when connecting to Centrifugo (for example, the browser client has a special method setToken).  There is also a way to authenticate connections without using JWT - see the chapter about proxying to the backend.  You connect to Centrifugo using one of the available transports.  ","version":"v5","tagName":"h2"},{"title":"4. Subscribe to channels​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#4-subscribe-to-channels","content":" After connecting to Centrifugo, subscribe clients to channels they are interested in. See more about channels in the special chapter. All bidirectional client SDKs provide a way to handle messages coming to a client from a channel after subscribing to it. Learn more about client SDK possibilities from the client SDK API spec.  There is also a way to subscribe a connection to a list of channels on the server side at the moment of connection establishment. See the chapter about server-side subscriptions.  ","version":"v5","tagName":"h2"},{"title":"5. Publish to a channel​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#5-publish-to-a-channel","content":" Everything should work now – as soon as a user opens some page of your application, they must successfully connect to Centrifugo and subscribe to a channel (or channels).  Now let's imagine you want to send a real-time message to users subscribed to a specific channel. This message can be a reaction to some event that happened in your app: someone posted a new comment, an administrator just created a new post, a user pressed the &quot;like&quot; button, etc. Anyway, this is an event your backend has just received, and you want to immediately send it to interested users.  You can do this using the Centrifugo HTTP API. To simplify your life, we have several API libraries for different languages. You can publish messages into a channel using one of those libraries, or you can simply follow the API description to construct API requests yourself - this is very simple. Centrifugo also supports a GRPC API. As soon as you have published a message to the channel, it must be delivered to your online clients subscribed to that channel.  ","version":"v5","tagName":"h2"},{"title":"6. Deploy to production​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#6-deploy-to-production","content":" To put all this into production, you need to deploy Centrifugo on your production server. To help you with this, we have many things like a Docker image, rpm and deb packages, Nginx configuration. See the Infrastructure tuning chapter for some actions you have to do to prepare your server infrastructure for handling many persistent connections.  ","version":"v5","tagName":"h2"},{"title":"7. Monitor Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#7-monitor-centrifugo","content":" Don't forget to configure metrics monitoring for your production Centrifugo setup. This may help you to understand what's going on with your Centrifugo setup, understand the number of connections, operation count, and latency distributions, etc.  ","version":"v5","tagName":"h2"},{"title":"8. Scale Centrifugo​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#8-scale-centrifugo","content":" As soon as you are close to machine resource limits, you may want to scale Centrifugo – you can run many Centrifugo instances and load-balance clients between them using the Redis engine, or with KeyDB, or with Tarantool, or with a Nats broker. The Engines and scalability chapter describes available options in detail.  ","version":"v5","tagName":"h2"},{"title":"9. Read FAQ​","type":1,"pageTitle":"Integration guide","url":"/docs/getting-started/integration#9-read-faq","content":" That's all for the basics. The documentation actually covers a lot of other concepts Centrifugo server has: scalability, private channels, the admin web interface, SockJS fallback, Protobuf support, and more. And don't forget to read our FAQ – it contains a lot of useful information. ","version":"v5","tagName":"h2"},{"title":"Centrifugo introduction","type":0,"sectionRef":"#","url":"/docs/getting-started/introduction","content":"","keywords":"","version":"v5"},{"title":"Background and motivation​","type":1,"pageTitle":"Centrifugo introduction","url":"/docs/getting-started/introduction#background-and-motivation","content":"   Centrifugo was born more than a decade ago to help applications whose server-side code was written in a language or framework lacking built-in concurrency support. In such cases, managing persistent connections can be a real headache, usually resolvable only by altering the technology stack and investing time in developing a production-ready solution.  For instance, frameworks like Django, Flask, Yii, Laravel, Ruby on Rails, and others offer limited or suboptimal support for handling numerous persistent connections for real-time messaging tasks.  Here, Centrifugo provides a straightforward and non-obtrusive way to introduce real-time updates and manage many persistent connections without radical changes in the application backend architecture. Developers can continue to work on the application's backend using their preferred language or framework, and keep the existing architecture. Just let Centrifugo deal with persistent connections and be the real-time messaging transport layer.  These days, Centrifugo offers advanced and unique features that can significantly simplify a developer's workload and save months (if not years) of development time, even if the application's backend is built with an asynchronous concurrent language or framework. One example is Centrifugo's built-in support for scaling across numerous machines to accommodate more connections while ensuring that channel subscribers on different Centrifugo nodes receive all publications. Or the fact that Centrifugo has a bunch of real-time SDKs which provide subscription multiplexing over a WebSocket connection, robust reconnect logic, built-in ping-pong, etc. And there are more things to mention: the documentation uncovers features step by step.  Centrifugo fits well with modern architectures and can serve as a universal real-time component, regardless of the application's technology stack. It stands as a viable self-hosted alternative to cloud solutions like Pusher, Ably, or PubNub. ","version":"v5","tagName":"h2"},{"title":"Migrating to v4","type":0,"sectionRef":"#","url":"/docs/getting-started/migration_v4","content":"","keywords":"","version":"v5"},{"title":"Client SDK migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#client-sdk-migration","content":" New generation of client protocol requires using the latest versions of client SDKs. During the next several days we will release the following SDK versions which are compatible with Centrifugo v4:  centrifuge-js &gt;= v3.0.0centrifuge-go &gt;= v0.9.0centrifuge-dart &gt;= v0.9.0centrifuge-swift &gt;= v0.5.0centrifuge-java &gt;= v0.2.0  New client SDKs support only new client protocol – you can not connect to Centrifugo v3 with them.  If you have a production system where you want to upgrade Centrifugo from v3 to v4 then the plan is:  danger If you are using private channels (starting with $) or user-limited channels (containing #) then carefully read about subscription token migration and user-limited channels migration below.  Upgrade Centrifugo and its configuration to adopt changes in v4.In Centrifugo v4 config turn on use_client_protocol_v1_by_default.Run Centrifugo v4 – all current clients should continue working with it.Then on the client-side uprade client SDK version to the one which works with Centrifugo v4, adopt changes in SDK API dictated by our new client SDK API spec. Important thing – add ?cf_protocol_version=v2 URL param to the connection endpoint to tell Centrifugo that modern generation of protocol is being used by the connection (otherwise, it assumes old protocol since we have use_client_protocol_v1_by_default option enabled).As soon as all your clients migrated to use new protocol generation you can remove use_client_protocol_v1_by_default option from the server configuration.After that you can remove ?cf_protocol_version=v2 from connection endpoint on the client-side.  tip If you are using mobile client SDKs then most probably some time must pass while clients update their apps to use an updated Centrifugo SDK version.  tip Starting from Centrifugo v4.1.1 it's possible to completely turn off client protocol v1 by setting disable_client_protocol_v1 boolean option to true.  ","version":"v5","tagName":"h2"},{"title":"Unidirectional transport migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#unidirectional-transport-migration","content":" Client protocol framing also changed in unidirectional transports. The good news is that Centrifugo v4 still supports previous format for unidirectional transports.  When you are enabling use_client_protocol_v1_by_default option described above you also make unidirectional transports to work over old protocol format. So your existing clients will continue working just fine with Centrifugo v4. Then the same steps to migrate described above can be applied to unidirectional transport case. The only difference that in unidirectional approach you are not using Centrifugo SDKs.  ","version":"v5","tagName":"h2"},{"title":"SockJS migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#sockjs-migration","content":" SockJS is now DEPRECATED in Centrifugo. Centrifugo v4 may be the last release which supports it. We now offer our own bidirectional emulation layer on top of HTTP-streaming and EventSource. See additional information in Centrifugo v4 introduction post.  ","version":"v5","tagName":"h2"},{"title":"Channel ASCII enforced​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#channel-ascii-enforced","content":" Centrifugo v2 and v3 docs mentioned the fact that channels must contain only ASCII characters. But it was not actually enforced by a server. Now Centrifugo is more strict. If a channel has non-ASCII characters then the 107: bad request error will be returned to the client. Please reach us out if this behavior is not suitable for your use case – we can discuss the use case and think on a proper solution together.  ","version":"v5","tagName":"h2"},{"title":"Subscription token migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#subscription-token-migration","content":" Subscription token now requires sub claim (current user ID) to be set.  In most cases the only change which is required to smoothly migrate to v4 without breaking things is to add a boolean option &quot;skip_user_check_in_subscription_token&quot;: true to a Centrifugo v4 configuration. This skips the check of sub claim to contain the current user ID set to a connection during authentication.  After that start adding sub claim (with current user ID) to subscription tokens. As soon as all subscription tokens in your system contain user ID in sub claim you can remove the skip_user_check_in_subscription_token from a server configuration.  One more important note is that client claim in subscription token in Centrifugo v4 only supported for backwards compatibility. It must not be included into new subscription tokens.  It's worth mentioning that Centrifugo v4 does not allow subscribing on channels starting with $ without token even if namespace marked as available for subscribing using sth like allow_subscribe_for_client option. This is done to prevent potential security risk during v3 -&gt; v4 migration when client previously not available to subscribe to channels starting with $ in any case may get permissions to do so.  ","version":"v5","tagName":"h2"},{"title":"User-limited channel migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#user-limited-channel-migration","content":" User-limited channel support should now be allowed over a separate channel namespace option allow_user_limited_channels. See below the namespace option converter which takes this change into account.  ","version":"v5","tagName":"h2"},{"title":"Namespace configuration migration​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#namespace-configuration-migration","content":" In Centrifugo v4 namespace configuration options have been changed. Centrifugo now has secure by default namespaces. First thing to do is to read the new docs about channels and namespaces.  Then you can use the following converter which will transform your old namespace configuration to a new one. This converter tries to keep backwards compatibility – i.e. it should be possible to deploy Centrifugo with namespace configuration from converter output and have the same behaviour as before regarding channel permissions. We believe that new option names should provide a more readable configuration and may help to reveal some potential security improvements in your namespace configuration – i.e. making it more strict and protective.  caution Do not blindly deploy things to production – test your system first, go through the possible usage scenarios and/or test cases.  tip It's fully client-side: your data won't be sent anywhere.      Convert Here will be configuration for v4 Here will be log of changes made in your config  ","version":"v5","tagName":"h2"},{"title":"Proxy disconnect code changes​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#proxy-disconnect-code-changes","content":" reconnect flag from custom disconnect code is removed. Reconnect advice is now determined by disconnect code value. This allowed us avoiding using JSON in WebSocket CLOSE frame reason. See proxy docs docs for more details.  ","version":"v5","tagName":"h2"},{"title":"Other configuration option changes​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#other-configuration-option-changes","content":" Several other non-namespace related options have been renamed or removed:  client_anonymous option renamed to allow_anonymous_connect_without_token – new name better describes the purpose of this option which was previously not clear. Converter above takes this into account.use_unlimited_history_by_default option was removed. It was used to help migrating from Centrifugo v2 to v3.  ","version":"v5","tagName":"h2"},{"title":"Server API changes​","type":1,"pageTitle":"Migrating to v4","url":"/docs/getting-started/migration_v4#server-api-changes","content":" The only breaking change is that user_connections API method (which is available in Centrifugo PRO only) was renamed to connections. The method is more generic now with a broader possibilities – so previous name does not match the current behavior. ","version":"v5","tagName":"h2"},{"title":"Migrating to v5","type":0,"sectionRef":"#","url":"/docs/getting-started/migration_v5","content":"","keywords":"","version":"v5"},{"title":"Client SDK token behaviour adjustments​","type":1,"pageTitle":"Migrating to v5","url":"/docs/getting-started/migration_v5#client-sdk-token-behaviour-adjustments","content":" In Centrifugo v5 client SDK spec was adjusted in regards how SDKs work with tokens.  Returning an empty string from getToken function (for Javascript, and the same for analogous functions in other SDKs) is a valid scenario which won't result into disconnect on the client side. It's still possible to disconnect client by returning a special error from getToken function. We updated all our SDKs to inherit this behaviour. Specifically, here is a list of SDK versions which work according to adjusted spec:  centrifuge-js &gt;= v4.0.0centrifuge-go &gt;= v0.10.0centrifuge-dart &gt;= v0.10.0centrifuge-swift &gt;= v0.6.0centrifuge-java &gt;= v0.3.0  Nothing prevents you from updating Centrifugo v4 to v5 first and then migrate to new client versions or doing vice versa. This change is client-side only. But we bind it to major server release to make it more notable – as it changes the core SDK behavior.  ","version":"v5","tagName":"h2"},{"title":"Node communication format changed​","type":1,"pageTitle":"Migrating to v5","url":"/docs/getting-started/migration_v5#node-communication-format-changed","content":" Avoid running Centrifugo v5 in the same cluster with Centrifugo v4 nodes – v4 and v5 have backwards incompatible node communication protocols.  ","version":"v5","tagName":"h2"},{"title":"Old HTTP API format is DEPRECATED​","type":1,"pageTitle":"Migrating to v5","url":"/docs/getting-started/migration_v5#old-http-api-format-is-deprecated","content":" Prefer using new HTTP API format instead of old one where possible. The old format still works and enabled by default. But we are planning to migrate our API libraries to the new format eventually – and then remove the old format. If you are using one of our HTTP API libs - at some point a new version will be released which will seamlessly migrate you to the modern HTTP API format.  If you are using hand-written requests – then some refactoring is required. It should be rather straighforward:  replace request path from /api to /api/&lt;method&gt;replace payload from having method and params on top level. Payload does not include method and params keys anymore. Please refer to: https://centrifugal.dev/blog/2023/06/29/centrifugo-v5-released#new-http-api-formatprefer using X-API-Key: &lt;YOUR_API_KEY header format instead of Authorization: apikey &lt;YOUR_API_KEY.  Don't forget that you can now generate HTTP clients from OpenAPI spec we now maintain for the new HTTP API format.  ","version":"v5","tagName":"h2"},{"title":"Other changes​","type":1,"pageTitle":"Migrating to v5","url":"/docs/getting-started/migration_v5#other-changes","content":" skip_user_check_in_subscription_token removedreconnect flag removed from disconnect API call and proxy structures for custom disconnectionuse_client_protocol_v1_by_default option removeddisable_client_protocol_v1 option removedredis_tls_skip_verify option removedpresence_ttl renamed to global_presence_ttlsockjs_heartbeat_delay option removeduni_websocket_ping_interval option removedwebsocket_ping_interval option removed  ","version":"v5","tagName":"h2"},{"title":"Shutting down Centrifugo v2 doc site​","type":1,"pageTitle":"Migrating to v5","url":"/docs/getting-started/migration_v5#shutting-down-centrifugo-v2-doc-site","content":" With Centrifugo v5 release we are shutting down Centrifugo v2 documentation site - this means https://centrifugal.github.io/centrifugo/ won't be available anymore. Documents may still be found on Github. Documentation for v3, v4 and v5 is hosted here. ","version":"v5","tagName":"h2"},{"title":"Quickstart tutorial ⏱️","type":0,"sectionRef":"#","url":"/docs/getting-started/quickstart","content":"Quickstart tutorial ⏱️ In this tutorial, we will build a very simple browser application with Centrifugo. Users will connect to Centrifugo over WebSocket, subscribe to a channel, and start receiving all channel publications (messages published to that channel). In our case, we will send a counter value to all channel subscribers to update the counter widget in all open browser tabs in real-time. First, you need to install Centrifugo. In this example, we are using a binary file release which is fine for development. Once you have the Centrifugo binary available on your machine, you can generate the minimal required configuration file with the following command: ./centrifugo genconfig This helper command will generate config.json file in the working directory with a content like this: config.json { &quot;token_hmac_secret_key&quot;: &quot;bbe7d157-a253-4094-9759-06a8236543f9&quot;, &quot;admin_password&quot;: &quot;d0683813-0916-4c49-979f-0e08a686b727&quot;, &quot;admin_secret&quot;: &quot;4e9eafcf-0120-4ddd-b668-8dc40072c78e&quot;, &quot;api_key&quot;: &quot;d7627bb6-2292-4911-82e1-615c0ed3eebb&quot;, &quot;allowed_origins&quot;: [] } Now we can start a server. Let's start Centrifugo with a built-in admin web interface: ./centrifugo --config=config.json --admin We could also enable the admin web interface by not using --admin flag but by adding &quot;admin&quot;: true option to the JSON configuration file: config.json { &quot;token_hmac_secret_key&quot;: &quot;bbe7d157-a253-4094-9759-06a8236543f9&quot;, &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;d0683813-0916-4c49-979f-0e08a686b727&quot;, &quot;admin_secret&quot;: &quot;4e9eafcf-0120-4ddd-b668-8dc40072c78e&quot;, &quot;api_key&quot;: &quot;d7627bb6-2292-4911-82e1-615c0ed3eebb&quot;, &quot;allowed_origins&quot;: [] } And then running Centrifugo only with a path to a configuration file: ./centrifugo --config=config.json Now open http://localhost:8000. You should see Centrifugo admin web panel. Enter admin_password value from the configuration file to log in (in our case it's d0683813-0916-4c49-979f-0e08a686b727, but you will have a different value). Inside the admin panel, you should see that one Centrifugo node is running, and it does not have connected clients: Now let's create index.html file with our simple app: index.html &lt;html&gt; &lt;head&gt; &lt;title&gt;Centrifugo quick start&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;counter&quot;&gt;-&lt;/div&gt; &lt;script src=&quot;https://unpkg.com/centrifuge@5.0.1/dist/centrifuge.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; const container = document.getElementById('counter'); const centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, { token: &quot;&lt;TOKEN&gt;&quot; }); centrifuge.on('connecting', function (ctx) { console.log(`connecting: ${ctx.code}, ${ctx.reason}`); }).on('connected', function (ctx) { console.log(`connected over ${ctx.transport}`); }).on('disconnected', function (ctx) { console.log(`disconnected: ${ctx.code}, ${ctx.reason}`); }).connect(); const sub = centrifuge.newSubscription(&quot;channel&quot;); sub.on('publication', function (ctx) { container.innerHTML = ctx.data.value; document.title = ctx.data.value; }).on('subscribing', function (ctx) { console.log(`subscribing: ${ctx.code}, ${ctx.reason}`); }).on('subscribed', function (ctx) { console.log('subscribed', ctx); }).on('unsubscribed', function (ctx) { console.log(`unsubscribed: ${ctx.code}, ${ctx.reason}`); }).subscribe(); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; Note that we are using centrifuge-js 5.0.1 in this example, getting it from a CDN. You should use its latest version at the moment of reading this tutorial. In a real Javascript app, you would most likely load centrifuge from NPM. In the index.html above, we created an instance of a Centrifuge client by passing the Centrifugo server's default WebSocket endpoint address to it. Then we subscribed to a channel called channel and provided a callback function to process incoming real-time messages (publications). Upon receiving a new publication, we update the page's HTML by setting the counter value to the page title. We call .subscribe() to initiate the subscription and the .connect() method of the Client to start a WebSocket connection. We also handle Client state transitions (disconnected, connecting, connected) and Subscription state transitions (unsubscribed, subscribing, subscribed) – see a detailed description in client SDK spec. Now you need to serve this file with an HTTP server. In a real-world Javascript application, you would serve your HTML files with a web server of your choice – but for this simple example, we can use a simple built-in Centrifugo static file server: ./centrifugo serve --port 3000 Alternatively, if you have Python 3 installed: python3 -m http.server 3000 These commands start a simple static file web server that serves the current directory on port 3000. Make sure you still have Centrifugo server running. Open http://localhost:3000/. Now if you look at browser developer tools or in Centrifugo logs you will notice that a connection can not be successfully established: 2021-09-01 10:17:33 [INF] request Origin is not authorized due to empty allowed_origins origin=http://localhost:3000 That's because we have not set allowed_origins in the configuration. Modify allowed_origins like this: config.json { ... &quot;allowed_origins&quot;: [&quot;http://localhost:3000&quot;] } Allowed origins is a security option for request originating from web browsers – see more details in server configuration docs. Restart Centrifugo after modifying allowed_origins in a configuration file. Now if you reload a browser window with an application you should see new information logs in server output: 2022-06-10 09:44:21 [INF] invalid connection token error=&quot;invalid token: token format is not valid&quot; client=a65a8463-6a36-421d-814a-0083c8836529 2022-06-10 09:44:21 [INF] disconnect after handling command client=a65a8463-6a36-421d-814a-0083c8836529 command=&quot;id:1 connect:{token:\\&quot;&lt;TOKEN&gt;\\&quot; name:\\&quot;js\\&quot;}&quot; reason=&quot;invalid token&quot; user= We still cannot connect. That's because the client must provide a valid JWT (JSON Web Token) to authenticate itself. This token must be generated on your backend and passed to the client-side (over template variables or using a separate AJAX call – whichever way you prefer). Since in our simple example we don't have an application backend, we can quickly generate an example token for a user using the centrifugo sub-command gentoken, like this: ./centrifugo gentoken -u 123722 – where -u flag sets user ID. The output should be like this: HMAC SHA-256 JWT for user &quot;123722&quot; with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE2NTU0NDgyOTl9.mUU9s5kj3yqp-SAEqloGy8QBgsLg0llA7lKUNwtHRnw – you will have a different token value since this one is based on the randomly generated token_hmac_secret_key from the configuration file we created at the beginning of this tutorial. See the token authentication docs for information about proper token generation in a real application. Now we can copy generated HMAC SHA-256 JWT and paste it into Centrifugo constructor instead of &lt;TOKEN&gt; placeholder in index.html file. I.e.: const centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, { token: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE2NTU0NDgyOTl9.mUU9s5kj3yqp-SAEqloGy8QBgsLg0llA7lKUNwtHRnw&quot; }); If you reload your browser tab – the connection will be successfully established, but the client still cannot subscribe to a channel: 2022-06-10 09:45:49 [INF] client command error error=&quot;permission denied&quot; client=88116489-350f-447f-9ff3-ab61c9341efe code=103 command=&quot;id:2 subscribe:{channel:\\&quot;channel\\&quot;}&quot; reply=&quot;id:2 error:{code:103 message:\\&quot;permission denied\\&quot;}&quot; user=123722 We need to give the client permission to subscribe to the channel channel. There are several ways to do this. For example, the client can provide a subscription JWT for a channel. But here we will use an option to allow all authenticated clients to subscribe to any channel. To do this let's extend the server configuration with the allow_subscribe_for_client option: config.json { &quot;token_hmac_secret_key&quot;: &quot;bbe7d157-a253-4094-9759-06a8236543f9&quot;, &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;d0683813-0916-4c49-979f-0e08a686b727&quot;, &quot;admin_secret&quot;: &quot;4e9eafcf-0120-4ddd-b668-8dc40072c78e&quot;, &quot;api_key&quot;: &quot;d7627bb6-2292-4911-82e1-615c0ed3eebb&quot;, &quot;allowed_origins&quot;: [&quot;http://localhost:3000&quot;], &quot;allow_subscribe_for_client&quot;: true } tip A good practice with Centrifugo is to configure channel namespaces for the different types of real-time features you have in the application. By defining namespaces, you can achieve granular control over channel behavior and permissions. Restart Centrifugo – and after doing this, everything should start working. The client can now successfully connect and subscribe to a channel. Open the developer tools and look at the WebSocket frames panel; you should see something like this: Note that in this example, we generated a connection JWT – but it has an expiration time, so after some time, Centrifugo will stop accepting those tokens. In real life, you need to add a token refresh function to the client to rotate tokens. See our client API SDK spec. OK, the last thing we need to do here is to publish a new counter value to a channel and make sure our app works properly. We can do this over the Centrifugo API by sending an HTTP request to the default API endpoint http://localhost:8000/api, but let's do this over the admin web panel first. Open the Centrifugo admin web panel in another browser tab (http://localhost:8000/) and go to the Actions section. Select the publish action, insert the channel name that you want to publish to – in our case, this is the string channel – and insert into the data area JSON like this: { &quot;value&quot;: 1 } Click PUBLISH button and check out the application browser tab – counter value must be immediately received and displayed. Open several browser tabs with our app and make sure all tabs receive a message as soon as you publish it. BTW, let's also look at how you can publish data to a channel over Centrifugo server API from a terminal using curl tool: curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: d7627bb6-2292-4911-82e1-615c0ed3eebb&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;channel&quot;, &quot;data&quot;: {&quot;value&quot;: 2}}' \\ http://localhost:8000/api/publish – where for Authorization header we set api_key value from Centrifugo config file generated above. We did it! We built the simplest browser real-time app with Centrifugo and its JavaScript client. It doesn't have a backend; it's not very useful, to be honest, but it should give you an insight into how to start working with the Centrifugo server. Read more about the Centrifugo server in the next documentation chapters – it can do much more than we just showed here. The Integration guide describes the process of idiomatic Centrifugo integration with your application backend. And chat/messenger app tutorial provides a comprehensive guide how to build real-time app with Centrifugo from scratch.","keywords":"","version":"v5"},{"title":"Channel capabilities","type":0,"sectionRef":"#","url":"/docs/pro/capabilities","content":"","keywords":"","version":"v5"},{"title":"Connection capabilities​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#connection-capabilities","content":" Connection capabilities can be set:  in connection JWT (in caps claim)in connect proxy result (caps field)  For example, here we are issuing permissions to subscribe on channel news and channel user_42 to a client:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;, &quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;] } ] }   Known capabilities:  sub - subscribe to a channel to receive publications from itpub - publish into a channel (your backend won't be able to process the publication in this case)prs - call presence and presence stats API, also consume join/leave events upon subscribinghst - call history API, also make Subscription positioned or recoverable upon subscribing  ","version":"v5","tagName":"h2"},{"title":"Caps processing behavior​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#caps-processing-behavior","content":" Centrifugo processes caps objects till it finds a match to a channel. At this point it applies permissions in the matched object and stops processing remaining caps. If no match found – then 103 permission denied returned to a client (of course if namespace does not have other permission-related options enabled). Let's consider example like this:  WRONG! { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;], &quot;allow&quot;: [&quot;pub&quot;] }, { &quot;channels&quot;: [&quot;news&quot;], &quot;allow&quot;: [&quot;sub&quot;] }, ] }   Here we have two entries for channel news, but when client subscribes on news only the first entry will be taken into considiration by Centrifugo – so Subscription attempt will be rejected (since first cap object does not have sub capability). In real life you don't really want to have cap objects with identical channels – but below we will introduce wildcard matching where understanding how caps processed becomes important.  Another example:  WRONG! { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;, &quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;] }, { &quot;channels&quot;: [&quot;user_42&quot;], &quot;allow&quot;: [&quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] }, ] }   One could expect that client will have [&quot;sub&quot;, &quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] capabilities for a channel user_42. But it's not true since Centrifugo processes caps objects and channels inside caps object in order – it finds a match to user_42 in first caps object, it contains only &quot;sub&quot; capability, processing stops. So user can subscribe to a channel, but can not publish, can not call history and presence APIs even though those capabilities are mentioned in caps object. The correct way to give all caps to the channel user_42 would be to split channels into different caps objects:  CORRECT { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news&quot;], &quot;allow&quot;: [&quot;sub&quot;] }, { &quot;channels&quot;: [&quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;, &quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] }, ] }   The processing behaves like this to avoid potential problems with possibly conflicting matches (mostly when using wildcard and regex matching – see below) and still allow overriding capabilities for specific channels.  ","version":"v5","tagName":"h3"},{"title":"Expiration considirations​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#expiration-considirations","content":" In JWT auth case – capabilities in JWT will work till token expiration, that's why it's important to keep reasonably small token expiration times. We can recommend using sth like 5-10 mins as a good expiration value, but of course this is application specific.In connect proxy case – capabilities will work until client connection close (disconnect) or connection refresh triggered (with refresh proxy you can provide an updated set of capabilities).  ","version":"v5","tagName":"h3"},{"title":"Revoking connection caps​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#revoking-connection-caps","content":" If at some point you need to revoke some capability from a client:  Simplest way is to wait for a connection expiration, then upon refresh: if using proxy – provide new caps in refresh proxy result, Centrifugo will update caps and unsubscribe a client from channels it does not have permissions anymore (only those obtained due to previous connection-wide capabilities).if JWT auth - provide new caps in connection token, Centrifugo will update caps and unsubscribe a client from channels it does not have permissions anymore (only those obtained due to previous connection-wide capabilities). In case of using connect proxy – you can disconnect a user (or client) with a reconnect code. New capabilities will be asked upon reconnection.In case of using token auth – revoke token (Centrifugo PRO feature) and disconnect user (or client) with reconnect code. Upon reconnection user will receive an error that token revoked and will try to load a new one.  ","version":"v5","tagName":"h3"},{"title":"Example: wildcard match​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#example-wildcard-match","content":" It's possible to use wildcards in channel resource names. For example, let's give a permission to subscribe on all channels in news namespace.  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;news:*&quot;], &quot;match&quot;: &quot;wildcard&quot;, &quot;allow&quot;: [&quot;sub&quot;] } ] }   note Match type is used for all channels in caps object. If you need different matching behavior for different channels then split them on different caps objects.  ","version":"v5","tagName":"h3"},{"title":"Example: regex match​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#example-regex-match","content":" Or regex:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;^posts_[\\d]+$&quot;], &quot;match&quot;: &quot;regex&quot;, &quot;allow&quot;: [&quot;sub&quot;] } ] }   ","version":"v5","tagName":"h3"},{"title":"Example: different types of match​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#example-different-types-of-match","content":" Of course it's possible to combine different types of match inside one caps array:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;^posts_[\\d]+$&quot;], &quot;match&quot;: &quot;regex&quot;, &quot;allow&quot;: [&quot;sub&quot;] } { &quot;channels&quot;: [&quot;user_42&quot;], &quot;allow&quot;: [&quot;sub&quot;] } ] }   ","version":"v5","tagName":"h3"},{"title":"Example: full access to all channels​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#example-full-access-to-all-channels","content":" Let's look how to allow all permissions to a client:  { &quot;caps&quot;: [ { &quot;channels&quot;: [&quot;*&quot;], &quot;match&quot;: &quot;wildcard&quot;, &quot;allow&quot;: [&quot;sub&quot;, &quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] } ] }   Full access warn Should we mention that giving full access to a client is something to wisely consider? 🤔  ","version":"v5","tagName":"h3"},{"title":"Subscription capabilities​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#subscription-capabilities","content":" Subscription capabilities can be set:  in subscription JWT (in allow claim)in subscribe proxy result (allow field)  Subscription token already belongs to a channel (it has a channel claim). So users with a valid subscription token can subscribe to a channel. But it's possible to additionally grant channel permissions to a user for publishing and calling presence and history using allow claim:  { &quot;allow&quot;: [&quot;pub&quot;, &quot;hst&quot;, &quot;prs&quot;] }   Putting sub permission to the Subscription token does not make much sense – Centrifugo only expects valid token for a subscription permission check.  ","version":"v5","tagName":"h2"},{"title":"Expiration considirations​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#expiration-considirations-1","content":" In JWT auth case – capabilities in subscription JWT will work till token expiration, that's why it's important to keep reasonably small token expiration times. We can recommend using sth like 5-10 mins as a good expiration value, but of course this is application specific.In subscribe proxy case – capabilities will work until client unsubscribe (or connection close).  ","version":"v5","tagName":"h3"},{"title":"Revoking subscription permissions​","type":1,"pageTitle":"Channel capabilities","url":"/docs/pro/capabilities#revoking-subscription-permissions","content":" If at some point you need to revoke some capability from a client:  Simplest way is to wait for a subscription expiration, then upon refresh: provide new caps in subscription token, Centrifugo will update channel caps. In case of using subscribe proxy – you can unsubscribe a user (or client) with a resubscribe code. Or disconnect with reconnect code. New capabilities will be set up upon resubscription/reconnection.In case of using JWT auth – revoke token (Centrifugo PRO feature) and unsubscribe/disconnect user (or client) with resubscribe/reconnect code. Upon resubscription/reconnection user will receive an error that token revoked and will try to load a new one. ","version":"v5","tagName":"h3"},{"title":"CEL expressions","type":0,"sectionRef":"#","url":"/docs/pro/cel_expressions","content":"","keywords":"","version":"v5"},{"title":"subscribe_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/pro/cel_expressions#subscribe_cel","content":" We suppose that the main operation for which developers may use CEL expressions in Centrifugo is a subscribe operation. Let's look at it in detail.  It's possible to configure subscribe_cel for a channel namespace (subscribe_cel is just an additional namespace channel option, with same rules applied). This expression should be a valid CEL expression.  config.json { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;admin&quot;, &quot;subscribe_cel&quot;: &quot;'admin' in meta.roles&quot; } ] }   In the example we are using custom meta information (must be an object) attached to the connection. As mentioned before in the doc this meta may be attached to the connection:  when set in the connect proxy resultor provided in JWT as meta claim  An expression is evaluated for every subscription attempt to a channel in a namespace. So if meta attached to the connection is sth like this:  { &quot;roles&quot;: [&quot;admin&quot;] }   – then for every channel in the admin namespace defined above expression will be evaluated to True and subscription will be accepted by Centrifugo.  tip meta must be JSON object (any {}) for CEL expressions to work.  ","version":"v5","tagName":"h2"},{"title":"Expression variables​","type":1,"pageTitle":"CEL expressions","url":"/docs/pro/cel_expressions#expression-variables","content":" Inside the expression developers can use some variables which are injected by Centrifugo to the CEL runtime.  Information about current user ID, meta information attached to the connection, all the variables defined in matched channel pattern will be available for CEL expression evaluation.  Say client with user ID 123 subscribes to a channel /users/4 which matched the channel pattern /users/:user:  Variable\tType\tExample\tDescriptionsubscribed\tbool\tfalse\tWhether client is subscribed to channel, always false for subscribe operation user\tstring\t&quot;123&quot;\tCurrent authenticated user ID (known from from JWT or connect proxy result) meta\tmap[string]any\t{&quot;roles&quot;: [&quot;admin&quot;]}\tMeta information attached to the connection by the apllication backend (in JWT or over connect proxy result) channel\tstring\t&quot;/users/4&quot;\tChannel client tries to subscribe vars\tmap[string]string\t{&quot;user&quot;: &quot;4&quot;}\tExtracted variables from the matched channel pattern. It's empty in case of using channels without variables.  In this case, to allow admin to subscribe on any user's channel or allow non-admin user to subscribe only on its own channel, you may construct an expression like this:  { ... &quot;subscribe_cel&quot;: &quot;vars.user == user or 'admin' in meta.roles&quot; }   Let's look at one more example. Say client with user ID 123 subscribes to a channel /example.com/users/4 which matched the channel pattern /:tenant/users/:user. The permission check may be transformed into sth like this (assuming meta information has information about current connection tenant):  { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;/:tenant/users/:user&quot;, &quot;subscribe_cel&quot;: &quot;vars.tenant == meta.tenant &amp;&amp; (vars.user == user or 'admin' in meta.roles)&quot; } ] }   ","version":"v5","tagName":"h3"},{"title":"publish_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/pro/cel_expressions#publish_cel","content":" CEL expression to check permissions to publish into a channel. Same expression variables are available.  ","version":"v5","tagName":"h2"},{"title":"history_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/pro/cel_expressions#history_cel","content":" CEL expression to check permissions for channel history. Same expression variables are available.  ","version":"v5","tagName":"h2"},{"title":"presence_cel​","type":1,"pageTitle":"CEL expressions","url":"/docs/pro/cel_expressions#presence_cel","content":" CEL expression to check permissions for channel presence. Same expression variables are available. ","version":"v5","tagName":"h2"},{"title":"Channel patterns","type":0,"sectionRef":"#","url":"/docs/pro/channel_patterns","content":"","keywords":"","version":"v5"},{"title":"Configuration​","type":1,"pageTitle":"Channel patterns","url":"/docs/pro/channel_patterns#configuration","content":" Let's look at the example:  { // rest of the config ... &quot;channel_patterns&quot;: true, // required to turn on the feature. &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;/users/:name&quot; // namespace options may go here ... }, { &quot;name&quot;: &quot;/events/:project/:type&quot; // namespace options may go here ... } ] }   As soon as namespace name starts with / - it's considered a channel pattern. Just like an HTTP path it consists of segments delimited by /. The : symbol in the segment beginning defines a variable part – more information below.  In this case a channel to be used must be sth like /users/mario - i.e. start with / and match one of the patterns defined in the configuration. So this channel pattern matching mechanics behaves mostly like HTTP route matching in many frameworks.  Given the configuration example above:  if channel is /users/mario, then the namespace with the name /users/:name will match and we apply all the options defined for it to the channel.if channel is /events/42/news, then the namespace with the name /events/:project/:type will match.if channel is /events/42, then no namespace will match and the unknown channel error will be returned.  Basic example demonstrating use of pattern channels in JS const client := new Centrifuge(&quot;ws://...&quot;, {}); const sub = client.newSubscription('/users/mario'); sub.subscribe(); client.connect();   ","version":"v5","tagName":"h3"},{"title":"Implementation details​","type":1,"pageTitle":"Channel patterns","url":"/docs/pro/channel_patterns#implementation-details","content":" Some implementation restrictions and details to know about:  When using channel patterns feature : symbol in a namespace name defines a variable part. It's not related to a namespace separator anymore – the entire channel is matched over the channel pattern. Similar to the HTTP routes semantics. So namespace separator is not needed at all when using channel patterns.Centrifugo only allows explicit channel pattern matching which do not result into channel pattern conflicts in runtime, this is checked during configuration validation on server start. Explicitly defined static patterns (without variables) have precedence over patterns with variables.There is no analogue of top-level namespace (like we have for standard namespace configuration) for channels starting with /. If a channel does not match any explicitly defined pattern then Centrifugo returns the 102: unknown channel error.If you define channel_regex inside channel pattern options – then regex matches over the entire channel (since variable parts are located in the namespace name in this case).Channel pattern must only contain ASCII characters.Duplicate variable names are not allowed inside an individual pattern, i.e. defining /users/:user/:user will result into validation error on start.  ","version":"v5","tagName":"h3"},{"title":"Variables​","type":1,"pageTitle":"Channel patterns","url":"/docs/pro/channel_patterns#variables","content":" : in the channel pattern name helps to define a variable to match against. Named parameters only match a single segment of the channel:  Channel pattern &quot;/users/:name&quot;: /users/mary ✅ match /users/john ✅ match /users/mary/info ❌ no match /users ❌ no match   Another example for channel pattern /news/:type/:subtype, i.e. with multiple variables:  Channel pattern &quot;/news/:type/:subtype&quot;: /news/sport/football ✅ match /news/sport/volleyball ✅ match /news/sport ❌ no match /news ❌ no match   Channel patterns support mid-segment variables, so the following is possible:  Channel pattern &quot;/personal/user_:user&quot;: /personal/user_mary ✅ match /personal/user_john ✅ match /personal/user_ ❌ no match   ","version":"v5","tagName":"h3"},{"title":"Using varibles​","type":1,"pageTitle":"Channel patterns","url":"/docs/pro/channel_patterns#using-varibles","content":" Additional benefits of using channel patterns may be achieved together with Centrifugo PRO CEL expressions. Channel pattern variables are available inside CEL expressions for evaluation in a custom way. ","version":"v5","tagName":"h3"},{"title":"Channel state events","type":0,"sectionRef":"#","url":"/docs/pro/channel_state_events","content":"Channel state events Centrifugo PRO has a feature to enable channel state event webhooks to be sent to your configured backend endpoint: channel occupied event - called whenever first subscriber occipies the channelchannel vacated event - called whenever last subscriber leaves the channel Preview state This feature is in the preview state now. We still need some time before it will be ready for usage in production. But starting from Centrifugo PRO v5.1.1 the feature is avalable for evaluation. To enable the feature you must use redis engine. Also, only channels with presence enabled and channel_state_events explicitly configured may deliver channel state notifications. When enabling channel state proxy Centrifugo PRO starts using another approach to create Redis keys for presence for namespaces where channel state events enabled, this is an important implementation detail. So the minimal config can look like this (occupied and vacated events for channels in chat namespace will be sent to proxy_channel_state_endpoint): { ... &quot;engine&quot;: &quot;redis&quot;, &quot;proxy_channel_state_endpoint&quot;: &quot;http://localhost:3000/centrifugo/channel_events&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;chat&quot;, &quot;presence&quot;: true, &quot;channel_state_events&quot;: [&quot;occupied&quot;, &quot;vacated&quot;] } ] } The proxy endpoint is an extension of Centrifugo OSS proxy and proto definitions may be found in the same proxy.proto file - see NotifyChannelState rpc. Example of the payload your backend request handler will receive: { &quot;events&quot;: [ {&quot;channel&quot;: &quot;chat:index&quot;, &quot;type&quot;: &quot;occupied&quot;, &quot;time_ms&quot;: 1697206286533}, ] } Payload may contain a batch of events, that's why events is an array – this is important for achieving a high event throughput. Your backend must be fast enough to keep up with the events rate and volume, otherwise event queues will grow and eventually new events will be dropped by Centrifugo PRO. Respond with empty object without error set to let Centrifugo PRO know that events were processed successfully. If last channel client resubscribes to a channel fast (during 5 secs) – then Centrifugo PRO won't send vacated events. If client does not resubscribe during 5 secs - event will be sent. So vacated events always delivered with a delay. This is implemented in such way to avoid unnecessary webhooks for quick reconnect scenarios. Centrifugo PRO does the best effort delivering channel state events, making retries when the backend endpoint is unavailable (with exponential backoff), also survives cases when Centrifugo node dies unexpectedly. But there are rare scenarios, when notifications may be lost – like when data lost in Redis. For such cases we recommend syncing the state periodically looking at channel presence information using server API – this depends on the use case though.","keywords":"","version":"v5"},{"title":"Message batching control","type":0,"sectionRef":"#","url":"/docs/pro/client_message_batching","content":"","keywords":"","version":"v5"},{"title":"client_write_delay​","type":1,"pageTitle":"Message batching control","url":"/docs/pro/client_message_batching#client_write_delay","content":" The client_write_delay is a duration option, it is a time Centrifugo will try to collect messages inside each connection message write loop before sending them towards the connection.  Enabling client_write_delay may reduce CPU usage of both server and client in case of high message rate inside individual connections. The reduction happens due to the lesser number of system calls to execute. Enabling client_write_delay limits the maximum throughput of messages towards the connection which may be achieved. For example, if client_write_delay is 100ms then the max throughput per second will be (1000 / 100) * client_max_messages_in_frame (16 by default), i.e. 160 messages per second. Though this should be more than enough for target Centrifugo use cases (frontend apps).  Example:  config.json { // Rest of config here ... &quot;client_write_delay&quot;: &quot;100ms&quot; }   ","version":"v5","tagName":"h2"},{"title":"client_reply_without_queue​","type":1,"pageTitle":"Message batching control","url":"/docs/pro/client_message_batching#client_reply_without_queue","content":" The client_reply_without_queue is a boolean option to not use client queue for replies to commands. When true replies are written to the transport without going through the connection message queue.  ","version":"v5","tagName":"h2"},{"title":"client_max_messages_in_frame​","type":1,"pageTitle":"Message batching control","url":"/docs/pro/client_message_batching#client_max_messages_in_frame","content":" The client_max_messages_in_frame is an integer option which controls the maximum number of messages which may be joined by Centrifugo into one transport frame. By default, 16. Use -1 for unlimited number. ","version":"v5","tagName":"h2"},{"title":"Connections API","type":0,"sectionRef":"#","url":"/docs/pro/connections","content":"","keywords":"","version":"v5"},{"title":"Example​","type":1,"pageTitle":"Connections API","url":"/docs/pro/connections#example","content":" Let's look at the quick example. First, generate a JWT for user 42:  $ centrifugo genconfig   Generate token for some user to be used in the example connections:  $ centrifugo gentoken -u 42 HMAC SHA-256 JWT for user 42 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y   Run Centrifugo with uni_http_stream transport enabled (it will allow us connecting from the terminal with curl):  CENTRIFUGO_UNI_HTTP_STREAM=1 centrifugo -c config.json   Create new terminal window and run:  curl -X POST http://localhost:8000/connection/uni_http_stream --data '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y&quot;, &quot;name&quot;: &quot;terminal&quot;}'   In another terminal create one more connection:  curl -X POST http://localhost:8000/connection/uni_http_stream --data '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI0MiIsImV4cCI6MTYyNzcxMzMzNX0.s3eOhujiyBjc4u21nuHkbcWJll4Um0QqGU3PF-6Mf7Y&quot;, &quot;name&quot;: &quot;terminal&quot;}'   Now let's call connections over HTTP API:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;user&quot;: &quot;42&quot;}' \\ http://localhost:8000/api/connections   The result:  { &quot;result&quot;: { &quot;connections&quot;: { &quot;db8bc772-2654-4283-851a-f29b888ace74&quot;: { &quot;app_name&quot;: &quot;terminal&quot;, &quot;transport&quot;: &quot;uni_http_stream&quot;, &quot;protocol&quot;: &quot;json&quot; }, &quot;4bc3ca70-ecc5-439d-af14-a78ae18e31c7&quot;: { &quot;app_name&quot;: &quot;terminal&quot;, &quot;transport&quot;: &quot;uni_http_stream&quot;, &quot;protocol&quot;: &quot;json&quot; } } } }   Here we can see that user has 2 connections from terminal app.  Each connection can be annotated with meta JSON information which is set during connection establishment (over meta claim of JWT or by returning meta in the connect proxy result).  ","version":"v5","tagName":"h3"},{"title":"connections​","type":1,"pageTitle":"Connections API","url":"/docs/pro/connections#connections","content":" Returns information about active connections according to the request.  connections params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tno\tfast filter by User ID expression\tstring\tno\tCEL expression to filter users  connections result​  Field name\tField type\tOptional\tDescriptionconnections\tmap[string]ConnectionInfo\tno\tactive user connections map where key is client ID and value is ConnectionInfo  ConnectionInfo​  Field name\tField type\tOptional\tDescriptionapp_name\tstring\tyes\tclient app name (if provided by client) app_version\tstring\tyes\tclient app version (if provided by client) transport\tstring\tno\tclient connection transport protocol\tstring\tno\tclient connection protocol (json or protobuf) user\tstring\tyes\tclient user ID state\tConnectionState\tyes\tconnection state  ConnectionState object​  Field name\tField type\tOptional\tDescriptionchannels\tmap[string]ChannelContext\tyes\tChannels client subscribed to connection_token\tConnectionTokenInfo\tyes\tinformation about connection token subscription_tokens\tmap&lt;string, SubscriptionTokenInfo&gt;\tyes\tinformation about channel tokens used to subscribe meta\tJSON object\tyes\tmeta information attached to a connection  ChannelContext object​  Field name\tField type\tOptional\tDescriptionsource\tint\tyes\tThe source of channel subscription  ConnectionTokenInfo object​  Field name\tField type\tOptional\tDescriptionuid\tstring\tyes\tunique token ID (jti) issued_at\tint\tyes\ttime (Unix seconds) when token was issued  SubscriptionTokenInfo object​  Field name\tField type\tOptional\tDescriptionuid\tstring\tyes\tunique token ID (jti) issued_at\tint\tyes\ttime (Unix seconds) when token was issued ","version":"v5","tagName":"h3"},{"title":"Distributed rate limit API","type":0,"sectionRef":"#","url":"/docs/pro/distributed_rate_limit","content":"","keywords":"","version":"v5"},{"title":"Overview​","type":1,"pageTitle":"Distributed rate limit API","url":"/docs/pro/distributed_rate_limit#overview","content":" Centrifugo distributed rate limiting is a high performance zero-configuration Redis-based token bucket with milliseconds precision. Zero configuration in this case means that you don't have to preconfigure buckets in Centrifugo – bucket configuration is a part of request to check allowed limits.  curl -X POST http://localhost:8000/api/rate_limit \\ -H &quot;Authorization: apikey &lt;KEY&gt;&quot; \\ -d @- &lt;&lt;'EOF' { &quot;key&quot;: &quot;rate_limit_test&quot;, &quot;interval&quot;: 60000, &quot;rate&quot;: 10 } EOF   Example result:  { &quot;result&quot;: { &quot;allowed&quot;: true, &quot;tokens_left&quot;: 9 } }   Or, when no tokens left in a bucket:  { &quot;result&quot;: { &quot;allowed&quot;: false, &quot;tokens_left&quot;: 0, &quot;allowed_in&quot;: 5208, &quot;server_time&quot;: 1694627573210, } }   In your app code call rate_limit API of Centrifugo PRO every time some action is executed and check allowed flag to allow or discard the action.  Centrifugo PRO also returns allowed_in and server_time fields to help understanding when action will be allowed. These two fields are only appended when tokens_left are less than requested score. allowed_in + server_time will provide you a timestamp in the future (in milliseconds) when action is possible to be executed. So you can delay next action execution till that time if possible.  ","version":"v5","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Distributed rate limit API","url":"/docs/pro/distributed_rate_limit#configuration","content":" To enable distributed rate limiter:  config.json { ... &quot;distributed_rate_limit&quot;: { &quot;enabled&quot;: true, &quot;redis_address&quot;: &quot;localhost:6379&quot; } }   Note, that just like most of other features in Centrifugo it's possible to configure Redis shards here or use Redis Cluster.  ","version":"v5","tagName":"h2"},{"title":"API description​","type":1,"pageTitle":"Distributed rate limit API","url":"/docs/pro/distributed_rate_limit#api-description","content":" Now let's look at API description.  ","version":"v5","tagName":"h2"},{"title":"rate_limit request​","type":1,"pageTitle":"Distributed rate limit API","url":"/docs/pro/distributed_rate_limit#rate_limit-request","content":" Field\tType\tRequired\tDescriptionkey\tstring\tYes\tKey for a bucket - you can construct keys whatever way you like interval\tinteger\tYes\tInterval in milliseconds rate\tinteger\tYes\tAllowed rate per provided interval score\tinteger\tNo\tScore for the current action, if not provided the default score 1 is used  ","version":"v5","tagName":"h3"},{"title":"rate_limit result​","type":1,"pageTitle":"Distributed rate limit API","url":"/docs/pro/distributed_rate_limit#rate_limit-result","content":" Field Name\tType\tRequired\tDescriptionallowed\tbool\tYes\tWhether desired action is allowed at this point in time tokens_left\tinteger\tYes\tHow many tokens left in a bucket allowed_in\tinteger\tNo\tMilliseconds till desired score will be allowed again server_time\tinteger\tNo\tServer time as Unix epoch in milliseconds used to calculate result ","version":"v5","tagName":"h3"},{"title":"Install and run PRO version","type":0,"sectionRef":"#","url":"/docs/pro/install_and_run","content":"","keywords":"","version":"v5"},{"title":"Binary release​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/pro/install_and_run#binary-release","content":" Centrifugo PRO binary releases available on Github. Note that we use a separate repo for PRO releases. Download latest release for your operating system, unpack it and run (see how to set license key below).  If you doubt which distribution you need, then on Linux or MacOS you can use the following command to download and unpack centrifugo binary to your current working directory:  curl -sSLf https://centrifugal.dev/install_pro.sh | sh   ","version":"v5","tagName":"h3"},{"title":"Docker image​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/pro/install_and_run#docker-image","content":" Centrifugo PRO uses a different image from OSS version – centrifugo/centrifugo-pro:  docker run --ulimit nofile=262144:262144 -v /host/dir/with/config/file:/centrifugo -p 8000:8000 centrifugo/centrifugo-pro:v5.2.0 centrifugo -c config.json   ","version":"v5","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/pro/install_and_run#kubernetes","content":" You can use our official Helm chart but make sure you changed Docker image to use PRO version and point to the correct image tag:  values.yaml ... image: registry: docker.io repository: centrifugo/centrifugo-pro tag: v5.2.0   ","version":"v5","tagName":"h3"},{"title":"Debian and Ubuntu​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/pro/install_and_run#debian-and-ubuntu","content":" DEB package available in release assets.  wget https://github.com/centrifugal/centrifugo-pro/releases/download/v5.2.0/centrifugo-pro_5.1.2_amd64.deb sudo dpkg -i centrifugo-pro_5.1.2_amd64.deb   ","version":"v5","tagName":"h3"},{"title":"Centos​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/pro/install_and_run#centos","content":" RPM package available in release assets.  wget https://github.com/centrifugal/centrifugo-pro/releases/download/v5.2.0/centrifugo-pro-5.1.2.x86_64.rpm sudo yum install centrifugo-pro-5.1.2.x86_64.rpm   ","version":"v5","tagName":"h3"},{"title":"Setting PRO license key​","type":1,"pageTitle":"Install and run PRO version","url":"/docs/pro/install_and_run#setting-pro-license-key","content":" Centrifugo PRO inherits all features and configuration options from open-source version. The only difference is that it expects a valid license key on start to avoid sandbox mode limits.  Once you have installed a PRO version and have a license key you can set it in configuration over license field, or pass over environment variables as CENTRIFUGO_LICENSE. Like this:  config.json { ... &quot;license&quot;: &quot;&lt;YOUR_LICENSE_KEY&gt;&quot; }   tip If license properly set then on Centrifugo PRO start you should see license information in logs: owner, license type and expiration date. All PRO features should be unlocked at this point. Warning about sandbox mode in logs on server start must disappear. ","version":"v5","tagName":"h2"},{"title":"Observability enhancements","type":0,"sectionRef":"#","url":"/docs/pro/observability_enhancements","content":"Observability enhancements Centrifugo PRO has some enhancements to exposed metrics. At this moment it provides channel namespace resolution to the following metrics: centrifugo_transport_messages_sentcentrifugo_transport_messages_sent_sizecentrifugo_transport_messages_receivedcentrifugo_transport_messages_received_size Since channel namespace resolution may add some overhead (though negligible in most cases), Centrifugo PRO requires it to be explicitly enabled using two boolean config options (available since Centrifugo PRO v5.1.1): config.json { ... &quot;channel_namespace_for_transport_messages_sent&quot;: true, &quot;channel_namespace_for_transport_messages_received&quot;: true } First option channel_namespace_for_transport_messages_sent enables channel namespace label for: centrifugo_transport_messages_sentcentrifugo_transport_messages_sent_size Second option channel_namespace_for_transport_messages_received enables for: centrifugo_transport_messages_receivedcentrifugo_transport_messages_received_size.","keywords":"","version":"v5"},{"title":"Centrifugo PRO","type":0,"sectionRef":"#","url":"/docs/pro/overview","content":"","keywords":"","version":"v5"},{"title":"Features​","type":1,"pageTitle":"Centrifugo PRO","url":"/docs/pro/overview#features","content":" Centrifugo PRO is packed with the following features:  Everything from Centrifugo OSS🔍 Channel and user tracing allows watching client protocol frames in channel or per user ID in real time.💹 Real-time analytics with ClickHouse for a great system observability, reporting and trending.🛡️ Operation rate limits to protect server from the real-time API misusing and frontend bugs.🔥 Push notification API to manage device tokens and send mobile and browser push notifications.🟢 User status API feature allows understanding activity state for a list of users.🔌 Connections API to query, filter and inspect active connections.✋ User blocking API to block/unblock abusive users by ID.🛑 JWT revoking and invalidation API to revoke tokens by ID and invalidate user's tokens based on issue time.🔔 Channel state events to be notified on the backend about channel occupied and vacated events.💪 Channel capabilities for controlling channel permissions per connection or per subscription.📜 Channel patterns allow defining channel configuration like HTTP routes with parameters.✍️ CEL expressions to write custom efficient permission rules for channel operations.🚀 Faster performance to reduce resource usage on server side.🔮 Singleflight for online presence and history to reduce load on the broker.🍔 Message batching control for advanced tuning of client connection write behaviour.🧐 Observability enhancements for additional more granular system state insights.🪵 CPU and RSS memory usage stats of Centrifugo nodes in admin UI.  Also, explore our Centrifugo PRO planned features board for a concise overview of upcoming features which are currently in progress and enhancements planned for a future.  ","version":"v5","tagName":"h2"},{"title":"Pricing​","type":1,"pageTitle":"Centrifugo PRO","url":"/docs/pro/overview#pricing","content":" Centrifugo PRO requires a license key to run. The pricing information for the license key is available upon request over sales@centrifugal.dev e-mail. Our services are exclusively available to corporate and business clients at this time. We would be happy to learn more about your real-time challenges and how Centrifugo can help you address them. Don't hesitate to ask for an online meeting to discuss the use case in-person.  ","version":"v5","tagName":"h2"},{"title":"Try for free in sandbox mode​","type":1,"pageTitle":"Centrifugo PRO","url":"/docs/pro/overview#try-for-free-in-sandbox-mode","content":" You can try out Centrifugo PRO for free. When you start Centrifugo PRO without license key then it's running in a sandbox mode. Sandbox mode limits the usage of Centrifigo PRO in several ways. For example:  Centrifugo handles up to 20 concurrent connectionsup to 2 server nodes supportedup to 5 API requests per second allowed  This mode should be enough for development and trying out PRO features, but must not be used in production environment as we can introduce additional limitations in the future.  Centrifugo PRO license agreement Centrifugo PRO is distributed by Centrifugal Labs LTD under commercial license which is different from OSS version. By downloading Centrifugo PRO you automatically accept commercial license terms. ","version":"v5","tagName":"h2"},{"title":"Faster performance","type":0,"sectionRef":"#","url":"/docs/pro/performance","content":"","keywords":"","version":"v5"},{"title":"Faster HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#faster-http-api","content":" Centrifugo PRO has an optimized JSON serialization/deserialization for HTTP API.  The effect can be noticeable under load. The exact numbers heavily depend on usage scenario. According to our benchmarks you can expect 10-15% more requests/sec for small message publications over HTTP API, and up to several times throughput boost when you are frequently get lots of messages from a history, see a couple of examples below.  ","version":"v5","tagName":"h2"},{"title":"Faster GRPC API​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#faster-grpc-api","content":" Centrifugo PRO has an optimized Protobuf serialization/deserialization for GRPC API. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v5","tagName":"h2"},{"title":"Faster HTTP proxy​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#faster-http-proxy","content":" Centrifugo PRO has an optimized JSON serialization/deserialization for HTTP proxy. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v5","tagName":"h2"},{"title":"Faster GRPC proxy​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#faster-grpc-proxy","content":" Centrifugo PRO has an optimized Protobuf serialization/deserialization for GRPC API. The effect can be noticeable under load. The exact numbers heavily depend on usage scenario.  ","version":"v5","tagName":"h2"},{"title":"Faster JWT decoding​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#faster-jwt-decoding","content":" Centrifugo PRO has an optimized decoding of JWT claims.  ","version":"v5","tagName":"h2"},{"title":"Faster GRPC unidirectional stream​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#faster-grpc-unidirectional-stream","content":" Centrifugo PRO has an optimized Protobuf deserialization for GRPC unidirectional stream. This only affects deserialization of initial connect command.  ","version":"v5","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#examples","content":" Let's look at quick live comparisons of Centrifugo OSS and Centrifugo PRO regarding HTTP API performance.  ","version":"v5","tagName":"h2"},{"title":"Publish HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#publish-http-api","content":" Sorry, your browser doesn't support embedded video.  In this video you can see a 13% speed up for publish operation. But for more complex API calls with larger payloads the difference can be much bigger. See next example that demonstrates this.  ","version":"v5","tagName":"h3"},{"title":"History HTTP API​","type":1,"pageTitle":"Faster performance","url":"/docs/pro/performance#history-http-api","content":" Sorry, your browser doesn't support embedded video.  In this video you can see an almost 2x overall speed up while asking 100 messages from Centrifugo history API. ","version":"v5","tagName":"h3"},{"title":"Analytics with ClickHouse","type":0,"sectionRef":"#","url":"/docs/pro/analytics","content":"","keywords":"","version":"v5"},{"title":"Configuration​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#configuration","content":" To enable integration with ClickHouse add the following section to a configuration file:  config.json { ... &quot;clickhouse_analytics&quot;: { &quot;enabled&quot;: true, &quot;clickhouse_dsn&quot;: [ &quot;tcp://127.0.0.1:9000&quot;, &quot;tcp://127.0.0.1:9001&quot;, &quot;tcp://127.0.0.1:9002&quot;, &quot;tcp://127.0.0.1:9003&quot; ], &quot;clickhouse_database&quot;: &quot;centrifugo&quot;, &quot;clickhouse_cluster&quot;: &quot;centrifugo_cluster&quot;, &quot;export_connections&quot;: true, &quot;export_subscriptions&quot;: true, &quot;export_operations&quot;: true, &quot;export_publications&quot;: true, &quot;export_notifications&quot;: true, &quot;export_http_headers&quot;: [ &quot;User-Agent&quot;, &quot;Origin&quot;, &quot;X-Real-Ip&quot; ] } }   All ClickHouse analytics options scoped to clickhouse_analytics section of configuration.  Toggle this feature using enabled boolean option.  tip While we have a nested configuration here it's still possible to use environment variables to set options. For example, use CENTRIFUGO_CLICKHOUSE_ANALYTICS_ENABLED env var name for configure enabled option mentioned above. I.e. nesting expressed as _ in Centrifugo.  Centrifugo can export data to different ClickHouse instances, addresses of ClickHouse can be set over clickhouse_dsn option.  You also need to set a ClickHouse cluster name (clickhouse_cluster) and database name clickhouse_database.  export_connections tells Centrifugo to export connection information snapshots. Information about connection will be exported once a connection established and then periodically while connection alive. See below on table structure to see which fields are available.  export_subscriptions tells Centrifugo to export subscription information snapshots. Information about subscription will be exported once a subscription established and then periodically while connection alive. See below on table structure to see which fields are available.  export_operations tells Centrifugo to export individual client operation information. See below on table structure to see which fields are available.  export_publications tells Centrifugo to export publications for channels to a separate ClickHouse table.  export_notifications tells Centrifugo to export push notifications to a separate ClickHouse table.  export_http_headers is a list of HTTP headers to export for connection information.  export_grpc_metadata is a list of metadata keys to export for connection information for GRPC unidirectional transport.  skip_schema_initialization - boolean, default false. By default Centrifugo tries to initialize table schema on start (if not exists). This flag allows skipping initialization process.  skip_ping_on_start - boolean, default false. Centrifugo pings Clickhouse servers by default on start, if any of servers is unavailable – Centrifugo fails to start. This option allow skipping this check thus Centrifugo is able to start even if Clickhouse cluster not working correctly.  ","version":"v5","tagName":"h2"},{"title":"Connections table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#connections-table","content":" SHOW CREATE TABLE centrifugo.connections; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.connections ( `client` String, `user` String, `name` String, `version` String, `transport` String, `headers` Map(String, Array(String)), `metadata` Map(String, Array(String)), `time` DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/connections', '{replica}') PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.connections_distributed; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.connections_distributed ( `client` String, `user` String, `name` String, `version` String, `transport` String, `headers` Map(String, Array(String)), `metadata` Map(String, Array(String)), `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'connections', murmurHash3_64(client)) │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v5","tagName":"h2"},{"title":"Subscriptions table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#subscriptions-table","content":" SHOW CREATE TABLE centrifugo.subscriptions ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.subscriptions ( `client` String, `user` String, `channels` Array(String), `time` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.subscriptions_distributed; ┌─statement───────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.subscriptions_distributed ( `client` String, `user` String, `channels` Array(String), `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'subscriptions', murmurHash3_64(client)) │ └─────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v5","tagName":"h2"},{"title":"Operations table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#operations-table","content":" SHOW CREATE TABLE centrifugo.operations; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations ( `client` String, `user` String, `op` String, `channel` String, `method` String, `error` UInt32, `disconnect` UInt32, `duration` UInt64, `time` DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/operations', '{replica}') PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.operations_distributed; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations_distributed ( `client` String, `user` String, `op` String, `channel` String, `method` String, `error` UInt32, `disconnect` UInt32, `duration` UInt64, `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'operations', murmurHash3_64(client)) │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v5","tagName":"h2"},{"title":"Publications table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#publications-table","content":" SHOW CREATE TABLE centrifugo.publications ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.publications ( `channel` String, `source` String, `size` UInt64, `client` String, `user` String, `time` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.publications_distributed; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations_distributed ( `channel` String, `source` String, `size` UInt64, `client` String, `user` String, `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'publications', murmurHash3_64(channel)) │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v5","tagName":"h2"},{"title":"Notifications table​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#notifications-table","content":" 🚧 This PRO feature is under construction together with push notification API.  SHOW CREATE TABLE centrifugo.notifications ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.notifications ( `uid` String, `provider` String, `type` String, `recipient` String, `device_id` String, `platform` String, `user` String, `msg_id` String, `status` String, `error_message` String, `error_code` String, `time` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(time) ORDER BY time TTL time + toIntervalDay(1) SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   And distributed one:  SHOW CREATE TABLE centrifugo.notifications_distributed; ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE centrifugo.operations_distributed ( `uid` String, `provider` String, `type` String, `recipient` String, `device_id` String, `platform` String, `user` String, `msg_id` String, `status` String, `error_message` String, `error_code` String, `time` DateTime ) ENGINE = Distributed('centrifugo_cluster', 'centrifugo', 'notifications', murmurHash3_64(uid)) │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘   ","version":"v5","tagName":"h2"},{"title":"Query examples​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#query-examples","content":" Show unique users which were connected:  SELECT DISTINCT user FROM centrifugo.connections_distributed; ┌─user─────┐ │ user_1 │ │ user_2 │ │ user_3 │ │ user_4 │ │ user_5 │ └──────────┘   Show total number of publication attempts which were throttled by Centrifugo (received Too many requests error with code 111):  SELECT COUNT(*) FROM centrifugo.operations_distributed WHERE (error = 111) AND (op = 'publish'); ┌─count()─┐ │ 4502 │ └─────────┘   The same for a specific user:  SELECT COUNT(*) FROM centrifugo.operations_distributed WHERE (error = 111) AND (op = 'publish') AND (user = 'user_200'); ┌─count()─┐ │ 1214 │ └─────────┘   Show number of unique users subscribed to a specific channel in last 5 minutes (this is approximate since subscriptions table contain periodic snapshot entries, clients could unsubscribe in between snapshots – this is reflected in operations table):  SELECT COUNT(Distinct(user)) FROM centrifugo.subscriptions_distributed WHERE arrayExists(x -&gt; (x = 'chat:index'), channels) AND (time &gt;= (now() - toIntervalMinute(5))); ┌─uniqExact(user)─┐ │ 101 │ └─────────────────┘   Show top 10 users which called publish operation during last one minute:  SELECT COUNT(op) AS num_ops, user FROM centrifugo.operations_distributed WHERE (op = 'publish') AND (time &gt;= (now() - toIntervalMinute(1))) GROUP BY user ORDER BY num_ops DESC LIMIT 10; ┌─num_ops─┬─user─────┐ │ 56 │ user_200 │ │ 11 │ user_75 │ │ 6 │ user_87 │ │ 6 │ user_65 │ │ 6 │ user_39 │ │ 5 │ user_28 │ │ 5 │ user_63 │ │ 5 │ user_89 │ │ 3 │ user_32 │ │ 3 │ user_52 │ └─────────┴──────────┘   Show total number of push notifications to iOS devices sent during last 24 hours:  SELECT COUNT(*) FROM centrifugo.notifications WHERE (time &gt; (now() - toIntervalHour(24))) AND (platform = 'ios') ┌─count()─┐ │ 31200 │ └─────────┘   ","version":"v5","tagName":"h2"},{"title":"Development​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#development","content":" The recommended way to run ClickHouse in production is with cluster. See an example of such cluster configuration made with Docker Compose.  But during development you may want to run Centrifugo with single instance ClickHouse.  To do this set only one ClickHouse dsn and do not set cluster name:  config.json { ... &quot;clickhouse_analytics&quot;: { &quot;enabled&quot;: true, &quot;clickhouse_dsn&quot;: [ &quot;tcp://127.0.0.1:9000&quot; ], &quot;clickhouse_database&quot;: &quot;centrifugo&quot;, &quot;clickhouse_cluster&quot;: &quot;&quot;, &quot;export_connections&quot;: true, &quot;export_subscriptions&quot;: true, &quot;export_publications&quot;: true, &quot;export_operations&quot;: true, &quot;export_http_headers&quot;: [ &quot;Origin&quot;, &quot;User-Agent&quot; ] } }   Run ClickHouse locally:  docker run -it --rm -v /tmp/clickhouse:/var/lib/clickhouse -p 9000:9000 --name click clickhouse/clickhouse-server   Run ClickHouse client:  docker run -it --rm --link click:clickhouse-server --entrypoint clickhouse-client clickhouse/clickhouse-server --host clickhouse-server   Issue queries:  :) SELECT * FROM centrifugo.operations ┌─client───────────────────────────────┬─user─┬─op──────────┬─channel─────┬─method─┬─error─┬─disconnect─┬─duration─┬────────────────time─┐ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ connecting │ │ │ 0 │ 0 │ 217894 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ connect │ │ │ 0 │ 0 │ 0 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ $chat:index │ │ 0 │ 0 │ 92714 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ presence │ $chat:index │ │ 0 │ 0 │ 3539 │ 2021-07-31 08:15:09 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test1 │ │ 0 │ 0 │ 2402 │ 2021-07-31 08:15:12 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test2 │ │ 0 │ 0 │ 634 │ 2021-07-31 08:15:12 │ │ bd55ae3a-dd44-47cb-a4cc-c41f8e33803b │ 2694 │ subscribe │ test3 │ │ 0 │ 0 │ 412 │ 2021-07-31 08:15:12 │ └──────────────────────────────────────┴──────┴─────────────┴─────────────┴────────┴───────┴────────────┴──────────┴─────────────────────┘   ","version":"v5","tagName":"h2"},{"title":"How export works​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#how-export-works","content":" When ClickHouse analytics enabled Centrifugo nodes start exporting events to ClickHouse. Each node issues insert with events once in 10 seconds (flushing collected events in batches thus making insertion in ClickHouse efficient). Maximum batch size is 100k for each table at the momemt. If insert to ClickHouse failed Centrifugo retries it once and then buffers events in memory (up to 1 million entries). If ClickHouse still unavailable after collecting 1 million events then new events will be dropped until buffer has space. These limits are configurable. Centrifugo PRO uses very efficient code for writing data to ClickHouse, so analytics feature should only add a little overhead for Centrifugo node.  ","version":"v5","tagName":"h2"},{"title":"Exposed metrics​","type":1,"pageTitle":"Analytics with ClickHouse","url":"/docs/pro/analytics#exposed-metrics","content":" Several metrics are exposed to monitor export process health:  centrifugo_clickhouse_analytics_drop_count​  Type: CounterLabels: typeDescription: Total count of drops.Usage: Useful for tracking the number of data drops in ClickHouse analytics, helping identify potential issues with data processing.  centrifugo_clickhouse_analytics_flush_duration_seconds​  Type: SummaryLabels: type, retries, resultDescription: Duration of ClickHouse data flush in seconds.Usage: Helps in monitoring the performance of data flush operations in ClickHouse, aiding in performance tuning and issue resolution.  centrifugo_clickhouse_analytics_batch_size​  Type: SummaryLabels: typeDescription: Distribution of batch sizes for ClickHouse flush.Usage: Useful for understanding the size of data batches being flushed to ClickHouse, helping optimize performance. ","version":"v5","tagName":"h2"},{"title":"CPU and RSS stats","type":0,"sectionRef":"#","url":"/docs/pro/process_stats","content":"CPU and RSS stats A useful addition of Centrifugo PRO is an ability to show CPU and RSS memory usage of each node in admin web UI. Here is how this looks like: The information updated in near real-time (with several seconds delay). It's also available as part of info API.","keywords":"","version":"v5"},{"title":"Push notification API","type":0,"sectionRef":"#","url":"/docs/pro/push_notifications","content":"","keywords":"","version":"v5"},{"title":"Motivation and design choices​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#motivation-and-design-choices","content":" We tried to be practical with our Push Notification API, let's look at its design choices and implementation properties we were able to achieve.  ","version":"v5","tagName":"h2"},{"title":"Storage for tokens​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#storage-for-tokens","content":" To start delivering push notifications in the application, developers usually need to integrate with providers such as FCM, HMS, and APNs. This integration typically requires the storage of device tokens in the application database and the implementation of sending push messages to provider push services.  Centrifugo PRO simplifies the process by providing a backend for device token storage, following best practices in token management. It reacts to errors and periodically removes stale devices/tokens to maintain a working set of device tokens based on provider recommendations.  ","version":"v5","tagName":"h3"},{"title":"Efficient queuing​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#efficient-queuing","content":" Additionally, Centrifugo PRO provides an efficient, scalable queuing mechanism for sending push notifications. Developers can send notifications from the app backend to Centrifugo API with minimal latency and let Centrifugo process sending to FCM, HMS, APNs concurrently using built-in workers. In our tests, we achieved several millions pushes per minute.  Centrifugo PRO also supports delayed push notifications feature – to queue push for a later delivery, so for example you can send notification based on user time zone and let Centrifugo PRO send it when needed.  ","version":"v5","tagName":"h3"},{"title":"Unified secure topics​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#unified-secure-topics","content":" FCM and HMS have a built-in way of sending notification to large groups of devices over topics mechanism (the same for HMS). One problem with native FCM or HMS topics though is that client can subscribe to any topic from the frontend side without any permission check. In today's world this is usually not desired. So Centrifugo PRO re-implements FCM, HMS topics by introducing an additional API to manage device subscriptions to topics.  tip In some cases you may have real-time channels and device subscription topics with matching names – to send messages to both online and offline users. Though it's up to you.  Centrifugo PRO device topic subscriptions also add a way to introduce the missing topic semantics for APNs.  Centrifugo PRO additionally provides an API to create persistent bindings of user to notification topics. Then – as soon as user registers a device – it will be automatically subscribed to its own topics. As soon as user logs out from the app and you update user ID of the device - user topics binded to the device automatically removed/switched. This design solves one of the issues with FCM – if two different users use the same device it's becoming problematic to unsubscribe the device from large number of topics upon logout. Also, as soon as user to topic binding added (using user_topic_update API) – it will be synchronized across all user active devices. You can still manage such persistent subscriptions on the application backend side if you prefer and provide the full list inside device_register call.  ","version":"v5","tagName":"h3"},{"title":"Non-obtrusive proxying​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#non-obtrusive-proxying","content":" Unlike other solutions that combine different provider push sending APIs into a unified API, Centrifugo PRO provides a non-obtrusive proxy for all the mentioned providers. Developers can send notification payloads in a format defined by each provider.  It's also possible to send notifications into native FCM, HMS topics or send to raw FCM, HMS, APNs tokens using Centrifugo PRO's push API, allowing them to combine native provider primitives with those added by Centrifugo (i.e., sending to a list of device IDs or to a list of topics).  ","version":"v5","tagName":"h3"},{"title":"Builtin analytics​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#builtin-analytics","content":" Furthermore, Centrifugo PRO offers the ability to inspect sent push notifications using ClickHouse analytics. Providers may also offer their own analytics, such as FCM, which provides insight into push notification delivery. Centrifugo PRO also offers a way to analyze push notification delivery and interaction using the update_push_status API.  ","version":"v5","tagName":"h3"},{"title":"Steps to integrate​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#steps-to-integrate","content":" Add provider SDK on the frontend side, follow provider instructions for your platform to obtain a push token for a device. For example, for FCM see instructions for iOS, Android, Flutter, Web Browser). The same for HMS or APNs – frontend part should be handled by their native SDKs.Call Centrifugo PRO backend API with the obtained token. From the application backend call Centrifugo device_register API to register the device in Centrifugo PRO storage. Optionally provide list of topics to subscribe device to.Centrifugo returns a registered device object. Pass a generated device ID to the frontend and save it on the frontend together with a token received from FCM.Call Centrifugo send_push_notification API whenever it's time to deliver a push notification.  At any moment you can inspect device storage by calling device_list API.  Once user logs out from the app, you can detach user ID from device by using device_update or remove device with device_remove API.  ","version":"v5","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#configuration","content":" In Centrifugo PRO you can configure one push provider or use all of them – this choice is up to you.  ","version":"v5","tagName":"h2"},{"title":"FCM​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#fcm","content":" As mentioned above Centrifigo uses PostgreSQL for token storage. To enable push notifications make sure database section defined in the configration and fcm is in the push_notifications.enabled_providers list. Centrifugo PRO uses Redis for queuing push notification requests, so Redis address should be configured also. Finally, to integrate with FCM a path to the credentials file must be provided (see how to create one in this instruction). So the full configuration to start sending push notifications over FCM may look like this:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;enabled_providers&quot;: [&quot;fcm&quot;], &quot;fcm_credentials_file_path&quot;: &quot;/path/to/service/account/credentials.json&quot; } }   tip Actually, PostgreSQL database configuration is optional here – you can use push notifications API without it. In this case you will be able to send notifications to FCM, HMS, APNs raw tokens, FCM and HMS native topics and conditions. I.e. using Centrifugo as an efficient proxy for push notifications (for example if you already keep tokens in your database). But sending to device ids and topics, and token/topic management APIs won't be available for usage.  ","version":"v5","tagName":"h3"},{"title":"HMS​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#hms","content":" { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;enabled_providers&quot;: [&quot;hms&quot;], &quot;hms_app_id&quot;: &quot;&lt;your_app_id&gt;&quot;, &quot;hms_app_secret&quot;: &quot;&lt;your_app_secret&gt;&quot;, } }   tip See example how to get app id and app secret here.  ","version":"v5","tagName":"h3"},{"title":"APNs​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#apns","content":" { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;enabled_providers&quot;: [&quot;apns&quot;], &quot;apns_endpoint&quot;: &quot;development&quot;, &quot;apns_bundle_id&quot;: &quot;com.example.your_app&quot;, &quot;apns_auth&quot;: &quot;token&quot;, &quot;apns_token_auth_key_path&quot;: &quot;/path/to/auth/key/file.p8&quot;, &quot;apns_token_key_id&quot;: &quot;&lt;your_key_id&gt;&quot;, &quot;apns_token_team_id&quot;: &quot;your_team_id&quot;, } }   We also support auth over p12 certificates with the following options:  push_notifications.apns_cert_p12_pathpush_notifications.apns_cert_p12_b64push_notifications.apns_cert_p12_password  ","version":"v5","tagName":"h3"},{"title":"Other options​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#other-options","content":" push_notifications.max_inactive_device_days​  This integer option configures the number of days to keep device without updates. By default Centrifugo does not remove inactive devices.  push_notifications.enable_redis_delayed_scheduler​  Boolean option which enables Redis scheduler to process delayed push notifications. It's off by default since produces additional requests to Redis. When using PostgreSQL as push notifications queue engine you don't need to enable sheduler explicitly.  push_notifications.dry_run​  Boolean option, when true Centrifugo PRO does not send push notifications to FCM, APNs, HMS providers but instead just print logs. Useful for development.  push_notifications.dry_run_latency​  Duration. When set together with push_notifications.dry_run every dry-run request will cause some delay in workers emulating real-world latency. Useful for development.  ","version":"v5","tagName":"h3"},{"title":"Use PostgreSQL as queue​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#use-postgresql-as-queue","content":" Centrifugo PRO utilizes Redis Streams as the default queue engine for push notifications. However, it also offers the option to employ PostgreSQL for queuing. It's as simple as:  config.json { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;push_notifications&quot;: { &quot;queue_engine&quot;: &quot;database&quot;, // rest of the options... } }   tip Queue based on Redis streams is generally more efficient, so if you start with PostgreSQL based queue – you have an option to switch to a more performant implementation later. Though in-flight and currently queued push notifications will be lost during a switch.  ","version":"v5","tagName":"h3"},{"title":"API description​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#api-description","content":" ","version":"v5","tagName":"h2"},{"title":"device_register​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#device_register","content":" Registers or updates device information.  device_register request​  Field\tType\tRequired\tDescriptionid\tstring\tNo\tID of the device being registered (provide it when updating). provider\tstring\tYes\tProvider of the device token (valid choices: fcm, hms, apns). token\tstring\tYes\tPush notification token for the device. platform\tstring\tYes\tPlatform of the device (valid choices: ios, android, web). user\tstring\tNo\tUser associated with the device. topics\tarray of strings\tNo\tDevice topic subscriptions. This should be a full list which replaces all the topics previously accociated with the device. User topics managed by UserTopic model will be automatically attached. meta\tmap&lt;string, string&gt;\tNo\tAdditional custom metadata for the device  device_register result​  Field Name\tType\tRequired\tDescriptionid\tstring\tYes\tThe device ID that was registered/updated.  ","version":"v5","tagName":"h3"},{"title":"device_update​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#device_update","content":" Call this method to update device. For example, when user logs out the app and you need to detach user ID from the device.  device_update request​  Field\tType\tRequired\tDescriptionids\trepeated string\tNo\tDevice ids to filter users\trepeated string\tNo\tDevice users filter user_update\tDeviceUserUpdate\tNo\tOptional user update object meta_update\tDeviceMetaUpdate\tNo\tOptional device meta update object topics_update\tDeviceTopicsUpdate\tNo\tOptional topics update object  DeviceUserUpdate:  Field\tType\tRequired\tDescriptionuser\tstring\tYes\tUser to set  DeviceMetaUpdate:  Field\tType\tRequired\tDescriptionmeta\tmap&lt;string, string&gt;\tYes\tMeta to set  DeviceTopicsUpdate:  Field\tType\tRequired\tDescriptionop\tstring\tYes\tOperation to make: add, remove or set topics\trepeated string\tYes\tTopics for the operation  device_update result​  Empty object.  ","version":"v5","tagName":"h3"},{"title":"device_remove​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#device_remove","content":" Removes device from storage. This may be also called when user logs out the app and you don't need its device token after that.  device_remove request​  Field Name\tType\tRequired\tDescriptionids\trepeated string\tNo\tA list of device IDs to be removed users\trepeated string\tNo\tA list of device user IDs to filter devices to remove  device_remove result​  Empty object.  ","version":"v5","tagName":"h3"},{"title":"device_list​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#device_list","content":" Returns a paginated list of registered devices according to request filter conditions.  device_list request​  Field\tType\tRequired\tDescriptionfilter\tDeviceFilter\tYes\tHow to filter results cursor\tstring\tNo\tCursor for pagination (last device id in previous batch, empty for first page). limit\tint32\tNo\tMaximum number of devices to retrieve. include_total_count\tbool\tNo\tFlag indicating whether to include total count for the current filter. include_topics\tbool\tNo\tFlag indicating whether to include topics information for each device. include_meta\tbool\tNo\tFlag indicating whether to include meta information for each device.  DeviceFilter:  Field\tType\tRequired\tDescriptionids\trepeated string\tNo\tList of device IDs to filter results. providers\trepeated string\tNo\tList of device token providers to filter results. platforms\trepeated string\tNo\tList of device platforms to filter results. users\trepeated string\tNo\tList of device users to filter results. topics\trepeated string\tNo\tList of topics to filter results.  device_list result​  Field Name\tType\tRequired\tDescriptionitems\trepeated Device\tYes\tA list of devices next_cursor\tstring\tNo\tCursor string for retreiving the next page, if not set - then no next page exists total_count\tinteger\tNo\tTotal count value (if include_total_count used)  Device:  Field Name\tType\tRequired\tDescriptionid\tstring\tYes\tThe device's ID. provider\tstring\tYes\tThe device's token provider. token\tstring\tYes\tThe device's token. platform\tstring\tYes\tThe device's platform. user\tstring\tNo\tThe user associated with the device. topics\tarray of strings\tNo\tOnly included if include_topics was true meta\tmap&lt;string, string&gt;\tNo\tOnly included if include_meta was true  ","version":"v5","tagName":"h3"},{"title":"device_topic_update​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#device_topic_update","content":" Manage mapping of device to topics.  device_topic_update request​  Field\tType\tRequired\tDescriptiondevice_id\tstring\tYes\tDevice ID. op\tstring\tYes\tadd or remove or set topics\trepeated string\tNo\tList of topics.  device_topic_update result​  Empty object.  ","version":"v5","tagName":"h3"},{"title":"device_topic_list​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#device_topic_list","content":" List device to topic mapping.  device_topic_list request​  Field\tType\tRequired\tDescriptionfilter\tDeviceTopicFilter\tNo\tList of device IDs to filter results. cursor\tstring\tNo\tCursor for pagination (last device id in previous batch, empty for first page). limit\tint32\tNo\tMaximum number of devices to retrieve. include_device\tbool\tNo\tFlag indicating whether to include Device information for each object. include_total_count\tbool\tNo\tFlag indicating whether to include total count info to response.  DeviceTopicFilter:  Field\tType\tRequired\tDescriptiondevice_ids\trepeated string\tNo\tList of device IDs to filter results. device_providers\trepeated string\tNo\tList of device token providers to filter results. device_platforms\trepeated string\tNo\tList of device platforms to filter results. device_users\trepeated string\tNo\tList of device users to filter results. topics\trepeated string\tNo\tList of topics to filter results. topic_prefix\tstring\tNo\tTopic prefix to filter results.  device_topic_list result​  Field Name\tType\tRequired\tDescriptionitems\trepeated DeviceTopic\tYes\tA list of DeviceChannel objects next_cursor\tstring\tNo\tCursor string for retreiving the next page, if not set - then no next page exists total_count\tinteger\tNo\tTotal count value (if include_total_count used)  DeviceTopic:  Field\tType\tRequired\tDescriptionid\tstring\tYes\tID of DeviceTopic object device_id\tstring\tYes\tDevice ID topic\tstring\tYes\tTopic  ","version":"v5","tagName":"h3"},{"title":"user_topic_update​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#user_topic_update","content":" Manage mapping of topics with users. These user topics will be automatically attached to user devices upon registering. And removed from device upon deattaching user.  user_topic_update request​  Field\tType\tRequired\tDescriptionuser\tstring\tYes\tUser ID. op\tstring\tYes\tadd or remove or set topics\trepeated string\tNo\tList of topics.  user_topic_update result​  Empty object.  ","version":"v5","tagName":"h3"},{"title":"user_topic_list​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#user_topic_list","content":" List user to topic mapping.  user_topic_list request​  Field\tType\tRequired\tDescriptionflter\tUserTopicFilter\tNo\tFilter object. cursor\tstring\tNo\tCursor for pagination (last id in previous batch, empty for first page). limit\tint32\tNo\tMaximum number of UserTopic objects to retrieve. include_total_count\tbool\tNo\tFlag indicating whether to include total count info to response.  UserTopicFilter:  Field\tType\tRequired\tDescriptionusers\trepeated string\tNo\tList of users to filter results. topics\trepeated string\tNo\tList of topics to filter results. topic_prefix\tstring\tNo\tChannel prefix to filter results.  user_topic_list result​  Field Name\tType\tDescriptionitems\trepeated UserTopic\tA list of UserTopic objects next_cursor\tstring\tNo total_count\tinteger\tNo  UserTopic:  Field\tType\tRequired\tDescriptionid\tstring\tYes\tID of UserTopic user\tstring\tYes\tUser ID topic\tstring\tYes\tTopic  ","version":"v5","tagName":"h3"},{"title":"send_push_notification​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#send_push_notification","content":" Send push notification to specific device_ids, or to topics, or native provider identifiers like fcm_tokens, or to fcm_topic. Request will be queued by Centrifugo, consumed by Centrifugo built-in workers and sent to the provider API.  send_push_notification request​  Field name\tType\tRequired\tDescriptionrecipient\tPushRecipient\tYes\tRecipient of push notification notification\tPushNotification\tYes\tPush notification to send uid\tstring\tNo\tUnique send id, used for Centrifugo builtin analytics or to cancel delayed push. We recommend using UUID v4 for it send_at\tint64\tNo\tOptional Unix time in the future (in seconds) when to send push notification, push will be queued until that time.  PushRecipient (you must set only one of the following fields):  Field\tType\tRequired\tDescriptionfilter\tDeviceFilter\tNo\tSend to device IDs based on Centrifugo device storage filter fcm_tokens\trepeated string\tNo\tSend to a list of FCM native tokens fcm_topic\tstring\tNo\tSend to a FCM native topic fcm_condition\tstring\tNo\tSend to a FCM native condition hms_tokens\trepeated string\tNo\tSend to a list of HMS native tokens hms_topic\tstring\tNo\tSend to a HMS native topic hms_condition\tstring\tNo\tSend to a HMS native condition apns_tokens\trepeated string\tNo\tSend to a list of APNs native tokens  PushNotification:  Field\tType\tRequired\tDescriptionexpire_at\tint64\tNo\tUnix timestamp when Centrifugo stops attempting to send this notification. Note, it's Centrifugo specific and does not relate to notification TTL fields. We generally recommend to always set this to a reasonable value to protect your app from old push notifications sending fcm\tFcmPushNotification\tNo\tNotification for FCM hms\tHmsPushNotification\tNo\tNotification for HMS apns\tApnsPushNotification\tNo\tNotification for APNs  FcmPushNotification:  Field\tType\tRequired\tDescriptionmessage\tJSON object\tYes\tFCM Message described in FCM docs.  HmsPushNotification:  Field\tType\tRequired\tDescriptionmessage\tJSON object\tYes\tHMS Message described in HMS Push Kit docs.  ApnsPushNotification:  Field\tType\tRequired\tDescriptionheaders\tmap&lt;string, string&gt;\tNo\tAPNs headers payload\tJSON object\tYes\tAPNs payload  send_push_notification result​  Field Name\tType\tDescriptionuid\tstring\tUnique send id, matches uid in request if it was provided  ","version":"v5","tagName":"h3"},{"title":"cancel_push​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#cancel_push","content":" Cancel delayed push notification (which was sent with custom send_at value).  update_push_status request​  Field\tType\tRequired\tDescriptionuid\tstring\tYes\tuid of push notification to cancel  update_push_status result​  Empty object.  ","version":"v5","tagName":"h3"},{"title":"update_push_status​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#update_push_status","content":" This API call is experimental, some changes may happen here.  Centrifugo PRO also allows tracking status of push notification delivery and interaction. It's possible to use update_push_status API to save the updated status of push notification to the notifications analytics table. Then it's possible to build insights into push notification effectiveness by querying the table.  The update_push_status API supposes that you are using uid field with each notification sent and you are using Centrifugo PRO generated device IDs (as described in steps to integrate).  This is a part of server API at the moment, so you need to proxy requests to this endpoint over your backend. We can consider making this API suitable for requests from the client side – please reach out if your use case requires it.  update_push_status request​  Field\tType\tRequired\tDescriptionuid\tstring\tYes\tuid (unique send id) from send_push_notification status\tstring\tYes\tStatus of push notification - delivered or interacted device_id\tstring\tYes\tDevice ID msg_id\tstring\tNo\tMessage ID  update_push_status result​  Empty object.  ","version":"v5","tagName":"h3"},{"title":"Exposed metrics​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#exposed-metrics","content":" Several metrics are available to monitor the state of Centrifugo push worker system:  centrifugo_push_notification_count​  Type: CounterLabels: provider, recipient_type, platform, success, err_codeDescription: Total count of push notifications.Usage: Helps in tracking the number and success rate of push notifications sent, providing insights for optimization and troubleshooting.  centrifugo_push_queue_consuming_lag​  Type: GaugeLabels: provider, queueDescription: Queue consuming lag in seconds.Usage: Useful for monitoring the delay in processing jobs from the queue, helping identify potential bottlenecks and ensuring timely processing.  centrifugo_push_consuming_inflight_jobs​  Type: GaugeLabels: provider, queueDescription: Number of inflight jobs being consumed.Usage: Helps in tracking the load on the job processing system, ensuring that resources are being utilized efficiently.  centrifugo_push_job_duration_seconds​  Type: SummaryLabels: provider, recipient_typeDescription: Duration of push processing job in seconds.Usage: Useful for monitoring the performance of job processing, helping in performance tuning and issue resolution.  ","version":"v5","tagName":"h2"},{"title":"Further reading and tutorials​","type":1,"pageTitle":"Push notification API","url":"/docs/pro/push_notifications#further-reading-and-tutorials","content":" Coming soon. ","version":"v5","tagName":"h2"},{"title":"Operation rate limits","type":0,"sectionRef":"#","url":"/docs/pro/rate_limiting","content":"","keywords":"","version":"v5"},{"title":"In-memory per connection rate limit​","type":1,"pageTitle":"Operation rate limits","url":"/docs/pro/rate_limiting#in-memory-per-connection-rate-limit","content":" In-memory rate limit is an efficient way to limit number of operations allowed on a per-connection basis – i.e. inside each individual real-time connection. Our rate limit implementation uses token bucket algorithm internally.  The list of operations which can be rate limited on a per-connection level is:  subscribepublishhistorypresencepresence_statsrefreshsub_refreshrpc (with optional method resolution)  In addition, Centrifugo allows defining two special buckets containers:  total – define it to limit the total number of commands per interval (all commands sent from client count), these buckets will always be checked if defined, every command from the client always consumes token from total bucketsdefault - define it if you don't want to configure some command buckets explicitly, default buckets will be used in case command buckets is not configured explicitly.  config.json { ... &quot;client_command_rate_limit&quot;: { &quot;enabled&quot;: true, &quot;default&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 60 }, ] }, &quot;total&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 20 }, { &quot;interval&quot;: &quot;60s&quot;, &quot;rate&quot;: 50 }, ] }, &quot;publish&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1 }, ] }, &quot;rpc&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 10 } ], &quot;method_override&quot;: [ { &quot;method&quot;: &quot;update_user_status&quot;, &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;20s&quot;, &quot;rate&quot;: 1 } ] } ] } } }   tip Centrifugo real-time SDKs written in a way that if client receives an error during connect – it will try to reconnect to a server with backoff algorithm. The same for subscribing to channels (i.e. error from subscribe command) – subscription request will be retried with a backoff. Refresh and subscription refresh will be also retried automatically by SDK upon errors after in several seconds. Retries of other commands should be handled manually from the client side if needed – though usually you should choose rate limit limits in a way that normal users of your app never hit the limits.  ","version":"v5","tagName":"h2"},{"title":"In-memory per user rate limit​","type":1,"pageTitle":"Operation rate limits","url":"/docs/pro/rate_limiting#in-memory-per-user-rate-limit","content":" Another type of rate limit in Centrifugo PRO is a per user ID in-memory rate limit. Like per client rate limit this one is also very efficient since also uses in-memory token buckets. The difference is that instead of rate limit per individual client this type of rate limit takes user ID into account.  This type of rate limit only checks commands coming from authenticated users – i.e. with non-empty user ID set. Requests from anonymous users can't be rate limited with it.  The list of operations which can be rate limited is similar to the in-memory rate limit described above. But with additional connect method:  totaldefaultconnectsubscribepublishhistorypresencepresence_statsrefreshsub_refreshrpc (with optional method resolution)  The configuration is very similar:  config.json { ... &quot;user_command_rate_limit&quot;: { &quot;enabled&quot;: true, &quot;default&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 60 }, ] }, &quot;publish&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1 } ] }, &quot;rpc&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 10 } ], &quot;method_override&quot;: [ { &quot;method&quot;: &quot;update_user_status&quot;, &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;20s&quot;, &quot;rate&quot;: 1 } ] } ] } } }   ","version":"v5","tagName":"h2"},{"title":"Redis per user rate limit​","type":1,"pageTitle":"Operation rate limits","url":"/docs/pro/rate_limiting#redis-per-user-rate-limit","content":" The next type of rate limit in Centrifugo PRO is a distributed per user ID rate limit with Redis as a bucket state storage. In this case limits are global for the entire Centrifugo cluster. If one user executed two commands on different Centrifugo nodes, Centrifugo consumes two tokens from the same bucket kept in Redis. Since this rate limit goes to Redis to check limits, it adds some latency to a command processing. Our implementation tries to provide good throughput characteristics though – in our tests single Redis instance can handle more than 100k limit check requests per second. And it's possible to scale Redis in the same ways as for Centrifugo Redis Engine.  This type of rate limit only checks commands coming from authenticated users – i.e. with non-empty user ID set. Requests from anonymous users can't be rate limited with it. The implementation also uses token bucket algorithm internally.  The list of operations which can be rate limited is similar to the in-memory user command rate limit described above. But without special bucket total:  defaultconnectsubscribepublishhistorypresencepresence_statsrefreshsub_refreshrpc (with optional method resolution)  The configuration is very similar:  config.json { ... &quot;redis_user_command_rate_limit&quot;: { &quot;enabled&quot;: true, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;default&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 60 }, ] }, &quot;publish&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 1 } ] }, &quot;rpc&quot;: { &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;1s&quot;, &quot;rate&quot;: 10 } ], &quot;method_override&quot;: [ { &quot;method&quot;: &quot;update_user_status&quot;, &quot;buckets&quot;: [ { &quot;interval&quot;: &quot;20s&quot;, &quot;rate&quot;: 1 } ] } ] } } }   Redis configuration for rate limit feature matches Centrifugo Redis engine configuration. So Centrifugo supports client-side consistent sharding to scale Redis, Redis Sentinel, Redis Cluster for rate limit feature too.  It's also possible to reuse Centrifugo Redis engine by setting use_redis_from_engine option instead of custom rate limit Redis configuration declaration, like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;redis_user_command_rate_limit&quot;: { &quot;enabled&quot;: true, &quot;use_redis_from_engine&quot;: true, ... } }   In this case rate limit will simply connect to Redis instances configured for an Engine.  ","version":"v5","tagName":"h2"},{"title":"Disconnecting abusive or misbehaving connections​","type":1,"pageTitle":"Operation rate limits","url":"/docs/pro/rate_limiting#disconnecting-abusive-or-misbehaving-connections","content":" Above we showed how you can define rate limit strategies to protect server resources and prevent execution of many commands inside the connection and from certain user.  But there are scenarios when abusive or broken connections may generate a significant load on the server just by calling commands and getting error responses due to rate limit or due to other reasons (like malformed command). Centrifugo PRO provides a way to configure error limits per connection to deal with this case.  Error limits are configured as in-memory buckets operating on a per-connection level. When these buckets are full due to lots of errors for an individual connection Centrifugo disconnects the client (with advice to not reconnect, so our SDKs may follow it). This way it's possible to get rid of the connection and rely on HTTP infrastracture tools to deal with client reconnections. Since WebSocket or other our transports (except unidirectional GRPC, but it's usually not available to the public port) are HTTP-based (or start with HTTP request in WebSocket Upgrade case) – developers can use Nginx limit_req_zone directive, Cloudflare rules, iptables, and so on, to protect Centrifugo from unwanted connections.  tip Centrifugo PRO does not count internal errors for the error limit buckets – as internal errors is usually not a client's fault.  The configuration on error limits per connection may look like this:  config.json { ... &quot;client_error_limits&quot;: { &quot;enabled&quot;: true, &quot;total&quot;: { &quot;buckets&quot; : [ { &quot;interval&quot;: &quot;5s&quot;, &quot;rate&quot;: 20 } ] } } }  ","version":"v5","tagName":"h2"},{"title":"Singleflight","type":0,"sectionRef":"#","url":"/docs/pro/singleflight","content":"Singleflight Centrifugo PRO provides an additional boolean option use_singleflight (default false). When this option enabled Centrifugo will automatically try to merge identical requests to history, online presence or presence stats issued at the same time into one real network request. It will do this by using in-memory component called singleflight. tip While it can seem similar, singleflight is not a cache. It only combines identical parallel requests into one. If requests come one after another – they will be sent separately to the broker or presence storage. This option can radically reduce a load on a broker in the following situations: Many clients subscribed to the same channel and in case of massive reconnect scenario try to access history simultaneously to restore a state (whether manually using history API or over automatic recovery feature)Many clients subscribed to the same channel and positioning feature is on so Centrifugo tracks client positionMany clients subscribed to the same channel and in case of massive reconnect scenario try to call presence or presence stats simultaneously Using this option only makes sense with remote engine (Redis, KeyDB, Tarantool), it won't provide a benefit in case of using a Memory engine. To enable: config.json { ... &quot;use_singleflight&quot;: true } Or via CENTRIFUGO_USE_SINGLEFLIGHT environment variable.","keywords":"","version":"v5"},{"title":"Token revocation API","type":0,"sectionRef":"#","url":"/docs/pro/token_revocation","content":"","keywords":"","version":"v5"},{"title":"How it works​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#how-it-works","content":" By default, information about token revocations shared throughout Centrifugo cluster and kept in a process memory. So token revocation information will be lost upon Centrifugo restart.  But it's possible to enable revocation information persistence by configuring a persistence storage – in this case token revocation information will survive Centrifugo restarts.  Centrifugo also automatically expires entries in the storage to keep working set reasonably small. Keeping pool of revoked tokens small allows avoiding expensive database lookups on every check – information is loaded periodically from the database and all checks performed over in-memory data structure – thus token revocation checks are cheap and have a small impact on the overall system performance.  ","version":"v5","tagName":"h2"},{"title":"Configure​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#configure","content":" Token revocation features (both revocation by token ID and user token invalidation by issue time) are enabled by default in Centrifugo PRO (as soon as your JWTs has jti and iat claims you will be able to use revocation APIs). By default revocation information kept in a process memory.  There are two types of persistent engines supported at the moment:  redisdatabase  ","version":"v5","tagName":"h2"},{"title":"Redis persistence engine​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#redis-persistence-engine","content":" Revocation data can be kept in Redis. To enable this configuration should be:  { ... &quot;token_revoke&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; }, &quot;user_tokens_invalidate&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; } }   danger Unlike many other Redis features in Centrifugo consistent sharding is not supported for revocation data. The reason is that we don't want to loose revocation information when additional Redis node added. So only one Redis shard can be provided for token_revoke and user_tokens_invalidate features. This should be fine given that working set of revoked entities should be reasonably small and old entries expire. If you try to set several Redis shards here Centrifugo will exit with an error on start.  caution One more thing you may notice is that Redis configuration here does not have use_redis_from_engine option. The reason is that since Redis is not shardable here reusing Redis configuration here could cause problems at the moment of main Redis scaling – which we want to avoid thus require explicit configuration here.  ","version":"v5","tagName":"h3"},{"title":"Database persistence engine​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#database-persistence-engine","content":" Revocation data can be kept in the relational database. Only PostgreSQL is supported.  To enable this configuration should be like:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;token_revoke&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; }, &quot;user_tokens_invalidate&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; } }   ","version":"v5","tagName":"h3"},{"title":"Revoke token API​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#revoke-token-api","content":" ","version":"v5","tagName":"h2"},{"title":"revoke_token​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#revoke_token","content":" Allows revoking individual tokens. For example, this may be useful when token leakage has been detected and you want to revoke access for a particular tokens. BTW Centrifugo PRO provides user_connections API which has an information about tokens for active users connections (if set in JWT).  caution This API assumes that JWTs you are using contain &quot;jti&quot; claim which is a unique token ID (according to RFC).  Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;uid&quot;: &quot;xxx-xxx-xxx&quot;, &quot;expire_at&quot;: 1635845122}' \\ http://localhost:8000/api/revoke_token   revoke_token params​  Parameter name\tParameter type\tRequired\tDescriptionuid\tstring\tyes\tToken unique ID (JTI claim in case of JWT) expire_at\tint\tno\tUnix time in the future when revocation information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time (matching the expiration time of your JWTs) to keep working set of revocations small (since Centrifugo nodes periodically load all entries from the database table to construct in-memory cache).  revoke_token result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"Invalidate user tokens API​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#invalidate-user-tokens-api","content":" ","version":"v5","tagName":"h2"},{"title":"invalidate_user_tokens​","type":1,"pageTitle":"Token revocation API","url":"/docs/pro/token_revocation#invalidate_user_tokens","content":" Allows revoking all tokens for a user which were issued before a certain time. For example, this may be useful after user changed a password in an application.  caution This API assumes that JWTs you are using contain &quot;iat&quot; claim which is a time token was issued at (according to RFC).  Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;user&quot;: &quot;test&quot;, &quot;issued_before&quot;: 1635845022, &quot;expire_at&quot;: 1635845122}' \\ http://localhost:8000/api/invalidate_user_tokens   invalidate_user_tokens params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to invalidate tokens for issued_before\tint\tno\tAll tokens issued at before this Unix time will be considered revoked (in case of JWT this requires iat to be properly set in JWT), if not provided server uses current time expire_at\tint\tno\tUnix time in the future when revocation information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time (matching the expiration time of your JWTs) to keep working set of revocations small (since Centrifugo nodes periodically load all entries from the database table to construct in-memory cache).  invalidate_user_tokens result​  Empty object. ","version":"v5","tagName":"h3"},{"title":"User and channel tracing","type":0,"sectionRef":"#","url":"/docs/pro/tracing","content":"","keywords":"","version":"v5"},{"title":"Save to a file​","type":1,"pageTitle":"User and channel tracing","url":"/docs/pro/tracing#save-to-a-file","content":" It's possible to connect to the admin tracing endpoint with CURL using the admin session token. And then save tracing output to a file for later processing.  curl -X POST http://localhost:8000/admin/trace -H &quot;Authorization: token &lt;ADMIN_AUTH_TOKEN&gt;&quot; -d '{&quot;type&quot;: &quot;user&quot;, &quot;entity&quot;: &quot;56&quot;}' -o trace.txt   Currently, you should copy the admin auth token from browser developer tools, this may be improved in the future as PRO version evolves. ","version":"v5","tagName":"h3"},{"title":"User blocking API","type":0,"sectionRef":"#","url":"/docs/pro/user_block","content":"","keywords":"","version":"v5"},{"title":"How it works​","type":1,"pageTitle":"User blocking API","url":"/docs/pro/user_block#how-it-works","content":" By default, information about user block/unblock requests shared throughout Centrifugo cluster and kept in memory. So user will be blocked until Centrifugo restart.  But it's possible to enable blocking information persistence by configuring a persistence storage – in this case information will survive Centrifugo restarts.  Centrifugo also automatically expires entries in the storage to keep working set of blocked users reasonably small. Keeping pool of blocked users small allows avoiding expensive database lookups on every check – information is loaded periodically from the storage and all checks performed over in-memory data structure – thus user blocking checks are cheap and have a small impact on the overall system performance.  ","version":"v5","tagName":"h2"},{"title":"Configure​","type":1,"pageTitle":"User blocking API","url":"/docs/pro/user_block#configure","content":" User block feature is enabled by default in Centrifugo PRO (blocking information will be stored in process memory). To keep blocking information persistently you need to configure persistence engine.  There are two types of persistent engines supported at the moment:  redisdatabase  ","version":"v5","tagName":"h2"},{"title":"Redis persistence engine​","type":1,"pageTitle":"User blocking API","url":"/docs/pro/user_block#redis-persistence-engine","content":" Blocking data can be kept in Redis. To enable this configuration should be:  { ... &quot;user_block&quot;: { &quot;persistence_engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot; } }   danger Unlike many other Redis features in Centrifugo consistent sharding is not supported for blocking data. The reason is that we don't want to loose blocking information when additional Redis node added. So only one Redis shard can be provided for user_block feature. This should be fine given that working set of blocked users should be reasonably small and old entries expire. If you try to set several Redis shards here Centrifugo will exit with an error on start.  caution One more thing you may notice is that Redis configuration here does not have use_redis_from_engine option. The reason is that since Redis is not shardable here reusing Redis configuration here could cause problems at the moment of Redis scaling – which we want to avoid thus require explicit configuration here.  ","version":"v5","tagName":"h3"},{"title":"Database persistence engine​","type":1,"pageTitle":"User blocking API","url":"/docs/pro/user_block#database-persistence-engine","content":" Blocking data can be kept in the relational database. Only PostgreSQL is supported.  To enable this configuration should be like:  { ... &quot;database&quot;: { &quot;dsn&quot;: &quot;postgresql://postgres:pass@127.0.0.1:5432/postgres&quot; }, &quot;user_block&quot;: { &quot;persistence_engine&quot;: &quot;database&quot; } }   tip To quickly start local PostgreSQL database: docker run -it --rm -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=pass -p 5432:5432 postgres:15   ","version":"v5","tagName":"h3"},{"title":"Block API​","type":1,"pageTitle":"User blocking API","url":"/docs/pro/user_block#block--api","content":" ","version":"v5","tagName":"h2"},{"title":"block_user​","type":1,"pageTitle":"User blocking API","url":"/docs/pro/user_block#block_user","content":" Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;user&quot;: &quot;2695&quot;, &quot;expire_at&quot;: 1635845122}' \\ http://localhost:8000/api/block_user   block_user params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to block expire_at\tint\tno\tUnix time in the future when user blocking information should expire (Unix seconds). While optional we recommend to use a reasonably small expiration time to keep working set of blocked users small (since Centrifugo nodes periodically load all entries from the storage to construct in-memory cache).  block_user result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"unblock_user​","type":1,"pageTitle":"User blocking API","url":"/docs/pro/user_block#unblock_user","content":" Example:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;user&quot;: &quot;2695&quot;}' \\ http://localhost:8000/api/unblock_user   unblock_user params​  Parameter name\tParameter type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to unblock  unblock_user result​  Empty object at the moment. ","version":"v5","tagName":"h3"},{"title":"User status API","type":0,"sectionRef":"#","url":"/docs/pro/user_status","content":"","keywords":"","version":"v5"},{"title":"Client-side status update RPC​","type":1,"pageTitle":"User status API","url":"/docs/pro/user_status#client-side-status-update-rpc","content":" Centrifugo PRO provides a built-in RPC method of client API called update_user_status. Call it with empty parameters from a client side whenever user performs a useful action that proves it's active status in your app. For example, in Javascript:  await centrifuge.rpc('update_user_status', {});   note Don't forget to debounce this method calls on a client side to avoid exposing RPC on every mouse move event for example.  This RPC call sets user's last active time value in Redis (with sharding and Cluster support). Information about active status will be kept in Redis for a configured time interval, then expire.  ","version":"v5","tagName":"h3"},{"title":"update_user_status server API​","type":1,"pageTitle":"User status API","url":"/docs/pro/user_status#update_user_status-server-api","content":" It's also possible to call update_user_status using Centrifugo server API (for example if you want to force status during application development or you want to proxy status updates over your app backend when using unidirectional transports):  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;users&quot;: [&quot;42&quot;]}' \\ http://localhost:8000/api/update_user_status   Update user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to update status for  Update user status result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"get_user_status server API​","type":1,"pageTitle":"User status API","url":"/docs/pro/user_status#get_user_status-server-api","content":" Now on a backend side you have access to a bulk API to effectively get status of particular users.  Call RPC method of server API (over HTTP or GRPC):  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;users&quot;: [&quot;42&quot;]}' \\ http://localhost:8000/api/get_user_status   You should get a response like this:  { &quot;result&quot;:{ &quot;statuses&quot;:[ { &quot;user&quot;:&quot;42&quot;, &quot;active&quot;:1627107289, &quot;online&quot;:1627107289 } ] } }   In case information about last status update time not available the response will be like this:  { &quot;result&quot;:{ &quot;statuses&quot;:[ { &quot;user&quot;:&quot;42&quot; } ] } }   I.e. status object will present in a response but active field won't be set for status object.  Note that Centrifugo also maintains online field inside user status object. This field updated periodically by Centrifugo itself while user has active connection with a server. So you can draw away statuses in your application: i.e. when user connected (online time) but not using application for a long time (active time).  Get user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to get status for  Get user status result​  Field name\tField type\tOptional\tDescriptionstatuses\tarray of UserStatus\tno\tStatuses for each user in params (same order)  UserStatus​  Field name\tField type\tOptional\tDescriptionuser\tstring\tno\tUser ID active\tinteger\tyes\tLast active time (Unix seconds) online\tinteger\tyes\tLast online time (Unix seconds)  ","version":"v5","tagName":"h3"},{"title":"delete_user_status server API​","type":1,"pageTitle":"User status API","url":"/docs/pro/user_status#delete_user_status-server-api","content":" If you need to clear user status information for some reason there is a delete_user_status server API call:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;users&quot;: [&quot;42&quot;]}' \\ http://localhost:8000/api/delete_user_status   Delete user status params​  Parameter name\tParameter type\tRequired\tDescriptionusers\tarray of strings\tyes\tList of users to delete status for  Delete user status result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"User status API","url":"/docs/pro/user_status#configuration","content":" To enable Redis user status feature:  config.json { ... &quot;user_status&quot;: { &quot;enabled&quot;: true, &quot;redis_address&quot;: &quot;127.0.0.1:6379&quot; } }   Redis configuration for user status feature matches Centrifugo Redis engine configuration. So Centrifugo supports client-side consistent sharding to scale Redis, Redis Sentinel, Redis Cluster for user status feature too.  It's also possible to reuse Centrifugo Redis engine by setting use_redis_from_engine option instead of custom throttling Redis address declaration, like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: &quot;localhost:6379&quot;, &quot;user_status&quot;: { &quot;enabled&quot;: true, &quot;use_redis_from_engine&quot;: true, } }   In this case Redis active status will simply connect to Redis instances configured for Centrifugo Redis engine.  expire_interval is a duration for how long Redis keys will be kept for each user. Expiration time extended on every update. By default expiration time is 31 day. To set it to 1 day:  config.json { ... &quot;user_status&quot;: { ... &quot;expire_interval&quot;: &quot;24h&quot; } }  ","version":"v5","tagName":"h3"},{"title":"Admin web UI","type":0,"sectionRef":"#","url":"/docs/server/admin_web","content":"","keywords":"","version":"v5"},{"title":"Options​","type":1,"pageTitle":"Admin web UI","url":"/docs/server/admin_web#options","content":" admin (boolean, default: false) – enables/disables admin web UIadmin_password (string, default: &quot;&quot;) – this is a password to log into admin web interfaceadmin_secret (string, default: &quot;&quot;) - this is a secret key for authentication token set on successful login.  Make both admin_password and admin_secret strong and keep them in secret.  After configuring, restart Centrifugo and go to http://localhost:8000 (by default) - you should see web interface.  tip Although there is a password based authentication a good advice is to protect web interface by firewall rules in production.    ","version":"v5","tagName":"h2"},{"title":"Using custom web interface​","type":1,"pageTitle":"Admin web UI","url":"/docs/server/admin_web#using-custom-web-interface","content":" If you want to use custom web interface you can specify path to web interface directory dist:  config.json { ..., &quot;admin&quot;: true, &quot;admin_password&quot;: &quot;&lt;PASSWORD&gt;&quot;, &quot;admin_secret&quot;: &quot;&lt;SECRET&gt;&quot;, &quot;admin_web_path&quot;: &quot;&lt;PATH_TO_WEB_DIST&gt;&quot; }   This can be useful if you want to modify official web interface code in some way and test it with Centrifugo.  ","version":"v5","tagName":"h2"},{"title":"Admin insecure mode​","type":1,"pageTitle":"Admin web UI","url":"/docs/server/admin_web#admin-insecure-mode","content":" There is also an option to run Centrifugo in insecure admin mode. In this mode, it's unnecessary to set admin_password and admin_secret in the configuration – you will be automatically logged into the web interface without any password. Note that this mode should only be considered for production if you have protected the admin web interface with firewall rules. Without such protection, anyone on the internet would have full access to the admin functionalities described above. To enable insecure admin mode:  config.json { ..., &quot;admin&quot;: true, &quot;admin_insecure&quot;: true, &quot;admin_password&quot;: &quot;&lt;PASSWORD&gt;&quot;, &quot;admin_secret&quot;: &quot;&lt;SECRET&gt;&quot; }  ","version":"v5","tagName":"h2"},{"title":"Client JWT authentication","type":0,"sectionRef":"#","url":"/docs/server/authentication","content":"","keywords":"","version":"v5"},{"title":"Connection JWT Claims​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#connection-jwt-claims","content":" For connection JWT, Centrifugo uses some standard claims defined in RFC 7519, as well as custom Centrifugo-specific claims.  ","version":"v5","tagName":"h2"},{"title":"sub​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#sub","content":" This standard JWT claim must contain the ID of the current application user (as a string).  If a user is not authenticated in the application but you wish to allow them to connect to Centrifugo, an empty string can be used as the user ID in the sub claim. This facilitates anonymous access. In such cases, you might need to enable the corresponding channel namespace options that allow protocol features for anonymous users.  ","version":"v5","tagName":"h3"},{"title":"exp​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#exp","content":" This claim specifies the UNIX timestamp (in seconds) when the token will expire. It is a standard JWT claim - all JWT libraries across different programming languages provide an API to set it.  If the exp claim is not included, Centrifugo will not expire the connection. When included, a special algorithm will identify connections with an exp in the past and initiate the connection refresh mechanism. The refresh mechanism allows a connection to be extended. If the refresh fails, Centrifugo will eventually close the client connection, which will not be accepted again until new valid and current credentials are provided in the connection token.  The connection expiration mechanism can be utilized in scenarios where you do not want users to remain subscribed to channels after being banned or deactivated in the application. It also serves to protect users from token leakage by setting a reasonably short expiration time.  Choose the exp value judiciously; too short a value can lead to frequent application hits with refresh requests, whereas too long a value can result in delayed user connection deactivation. It's a matter of balance.  Further details on connection expiration can be found below.  ","version":"v5","tagName":"h3"},{"title":"iat​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#iat","content":" This represents the UNIX time when the token was issued (in seconds). Refer to the definition in RFC 7519. This claim is optional but can be advantageous in conjunction with Centrifugo PRO's token revocation features.  ","version":"v5","tagName":"h3"},{"title":"jti​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#jti","content":" This is a unique identifier for the token. Refer to the definition in RFC 7519. This claim is optional but can be beneficial in conjunction with Centrifugo PRO's token revocation features.  ","version":"v5","tagName":"h3"},{"title":"aud​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#aud","content":" By default, Centrifugo does not check JWT audience (rfc7519 aud claim).  But you can force this check by setting token_audience string option:  config.json { &quot;token_audience&quot;: &quot;centrifugo&quot; }   caution Setting token_audience will also affect subscription tokens (used for channel token authorization). If you need to separate connection token configuration and subscription token configuration check out separate subscription token config feature.  ","version":"v5","tagName":"h3"},{"title":"iss​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#iss","content":" By default, Centrifugo does not check JWT issuer (rfc7519 iss claim).  But you can force this check by setting token_issuer string option:  config.json { &quot;token_issuer&quot;: &quot;my_app&quot; }   caution Setting token_issuer will also affect subscription tokens (used for channel token authorization). If you need to separate connection token configuration and subscription token configuration check out separate subscription token config feature.  ","version":"v5","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#info","content":" This optional claim provides additional information about the client's connection for Centrifugo. This information will be included in presence data, join/leave events, and client-side channel publications.  ","version":"v5","tagName":"h3"},{"title":"b64info​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#b64info","content":" For those utilizing the binary Protobuf protocol and requiring the info to be custom bytes, this field should be used.  It contains a base64 encoded representation of your bytes. Centrifugo will decode the base64 back into bytes upon receipt and incorporate the result into the various places described above.  ","version":"v5","tagName":"h3"},{"title":"channels​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#channels","content":" This is an optional array of strings identifying the server-side channels to which the client will be subscribed. Further details can be found in the documentation on server-side subscriptions.  tip It's important to note that the channels claim is sometimes misinterpreted by users as a list of channel permissions. It does not serve that purpose. Instead, using this claim causes the client to be automatically subscribed to the specified channels upon connection, making it unnecessary to invoke the subscribe API from the client side. More information can be found in the server-side subscriptions documentation.  ","version":"v5","tagName":"h3"},{"title":"subs​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#subs","content":" This optional claim is a map of channels with options, providing a more detailed approach to server-side subscriptions compared to the channels claim, as it allows for the annotation of each channel with additional information and data through options.  tip The term subs is shorthand for subscriptions. It should not be confused with the sub claim mentioned earlier, which is a standard JWT claim used to provide a user ID (short for subject). Despite their similar names, these claims serve distinct purposes within a connection JWT.  Example:  { ... &quot;subs&quot;: { &quot;channel1&quot;: { &quot;data&quot;: {&quot;welcome&quot;: &quot;welcome to channel1&quot;} }, &quot;channel2&quot;: { &quot;data&quot;: {&quot;welcome&quot;: &quot;welcome to channel2&quot;} } } }   Subscribe options:​  Field\tType\tOptional\tDescriptioninfo\tJSON object\tyes\tCustom channel info b64info\tstring\tyes\tCustom channel info in Base64 - to pass binary channel info data\tJSON object\tyes\tCustom JSON data to return in subscription context inside Connect reply b64data\tstring\tyes\tSame as data but in Base64 to send binary data override\tOverride object\tyes\tAllows dynamically override some channel options defined in Centrifugo configuration on a per-connection basis (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave position\tBoolValue\tyes\tOverride position recover\tBoolValue\tyes\tOverride recover  BoolValue is an object like this:  { &quot;value&quot;: true/false }   ","version":"v5","tagName":"h3"},{"title":"meta​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#meta","content":" meta is an additional JSON object (e.g., {&quot;key&quot;: &quot;value&quot;}) that is attached to a connection. It differs from info as it is never disclosed to clients within presence and join/leave events; it is only accessible on the server side. It can be included in proxy calls from Centrifugo to the application backend (refer to the proxy_include_connection_meta option). In Centrifugo PRO, there is a connections API method that returns this metadata within the connection description object.  ","version":"v5","tagName":"h3"},{"title":"expire_at​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#expire_at","content":" Although Centrifugo typically uses the exp claim to manage connection expiration, there may be scenarios where you want to separate the token expiration check from the connection expiration time. When the expire_at claim is included in the JWT, Centrifugo uses it to determine the connection expiration time, while the JWT expiration is still verified using the exp claim.  expire_at is a UNIX timestamp indicating when the connection should expire.  To expire the connection at a specific future time, set it to that time.To prevent connection expiration, set it to 0 (token exp claim will still be checked).  ","version":"v5","tagName":"h3"},{"title":"Connection expiration​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#connection-expiration","content":" As mentioned, the exp claim in a connection token is designed to expire the client connection at some point in time. Here's a detailed look at the process when Centrifugo identifies that the connection is going to expire.  First, activate the client expiration mechanism in Centrifugo by providing a connection JWT with an exp claim:  import jwt import time token = jwt.encode({&quot;sub&quot;: &quot;42&quot;, &quot;exp&quot;: int(time.time()) + 10*60}, &quot;secret&quot;, algorithm=&quot;HS256&quot;) print(token)   Assuming the exp claim is set to expire in 10 minutes, the client connects to Centrifugo with this token. Centrifugo will maintain the connection for the specified duration. Once the time elapses, Centrifugo allows a grace period (default is 25 seconds) for the client to refresh its credentials with a new valid token containing an updated exp.  Upon initial connection, the client receives a ttl value in the connect response, indicating the seconds remaining before it must initiate a refresh command with new credentials. Centrifugo SDKs handle this ttl internally and automatically begin the refresh process.  SDKs provide mechanisms to hook into this process and provide a function to get new token. It's up to developer to decide how to load new token from the backend – in web browser this is usually a simple fetch request and response may look like this:  { &quot;token&quot;: token }   You should provide the same connection JWT you issued when the page was initially rendered, but with an updated and valid exp. Our SDKs will then send this token to the Centrifugo server, and the connection will be extended for the period set in the new exp.  When you load new token from your app backend user authentication must be facilitated by your app's session mechanism. So you know for whom you are are going to generate an updated token.  ","version":"v5","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#examples","content":" Let's look at how to generate connection HS256 JWT in Python:  ","version":"v5","tagName":"h2"},{"title":"Simplest token​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#simplest-token","content":"     PythonNodeJS import jwt token = jwt.encode({&quot;sub&quot;: &quot;42&quot;}, &quot;secret&quot;).decode() print(token)   Note that we use the value of token_hmac_secret_key from Centrifugo config here (in this case token_hmac_secret_key value is just secret). The only two who must know the HMAC secret key is your application backend which generates JWT and Centrifugo. You should never reveal the HMAC secret key to your users.  Then you can pass this token to your client side and use it when connecting to Centrifugo:  Using centrifuge-js v3 var centrifuge = new Centrifuge(&quot;ws://localhost:8000/connection/websocket&quot;, { token: token }); centrifuge.connect();   See more details about working with connection tokens and handling token expiration on the client-side in the real-time SDK API spec.  ","version":"v5","tagName":"h3"},{"title":"Token with expiration​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#token-with-expiration","content":" HS256 token that will be valid for 5 minutes:  PythonNodeJS import jwt import time claims = {&quot;sub&quot;: &quot;42&quot;, &quot;exp&quot;: int(time.time()) + 5*60} token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   ","version":"v5","tagName":"h3"},{"title":"Token with additional connection info​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#token-with-additional-connection-info","content":" Let's attach user name:  PythonNodeJS import jwt claims = {&quot;sub&quot;: &quot;42&quot;, &quot;info&quot;: {&quot;name&quot;: &quot;Alexander Emelin&quot;}} token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   ","version":"v5","tagName":"h3"},{"title":"Investigating problems with JWT​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#investigating-problems-with-jwt","content":" You can use jwt.io site to investigate the contents of your tokens. Also, server logs usually contain some useful information.  ","version":"v5","tagName":"h3"},{"title":"JSON Web Key support​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#json-web-key-support","content":" Centrifugo supports JSON Web Key (JWK) spec. This means that it's possible to improve JWT security by providing an endpoint to Centrifugo from where to load JWK (by looking at kid header of JWT).  A mechanism can be enabled by providing token_jwks_public_endpoint string option to Centrifugo (HTTP address).  As soon as token_jwks_public_endpoint set all tokens will be verified using JSON Web Key Set loaded from JWKS endpoint. This makes it impossible to use non-JWK based tokens to connect and subscribe to private channels.  tip Read a tutorial in our blog about using Centrifugo with Keycloak SSO. In that case connection tokens are verified using public key loaded from the JWKS endpoint of Keycloak.  At the moment Centrifugo caches keys loaded from an endpoint for one hour.  Centrifugo will load keys from JWKS endpoint by issuing GET HTTP request with 1 second timeout and one retry in case of failure (not configurable at the moment).  Centrifugo supports the following key types (kty) for JWKs tokens:  RSAEC (since Centrifugo v5.1.0)  Once enabled JWKS used for both connection and channel subscription tokens.  ","version":"v5","tagName":"h2"},{"title":"Dynamic JWKs endpoint​","type":1,"pageTitle":"Client JWT authentication","url":"/docs/server/authentication#dynamic-jwks-endpoint","content":" It's possible to extract variables from iss and aud JWT claims using Go regexp named groups, then use variables extracted during iss or aud matching to construct a JWKS endpoint dynamically upon token validation. In this case JWKS endpoint may be set in config as template.  To achieve this Centrifugo provides two additional options:  token_issuer_regex - match JWT issuer (iss claim) against this regex, extract named groups to variables, variables are then available for jwks endpoint construction.token_audience_regex - match JWT audience (aud claim) against this regex, extract named groups to variables, variables are then available for jwks endpoint construction.  Let's look at the example:  { &quot;token_issuer_regex&quot;: &quot;https://example.com/auth/realms/(?P&lt;realm&gt;[A-z]+)&quot;, &quot;token_jwks_public_endpoint&quot;: &quot;https://keycloak:443/{{realm}}/protocol/openid-connect/certs&quot;, }   To use variable in token_jwks_public_endpoint it must be wrapped in {{ }}.  When using token_issuer_regex and token_audience_regex make sure token_issuer and token_audience not used in the config - otherwise and error will be returned on Centrifugo start.  caution Setting token_issuer_regex and token_audience_regex will also affect subscription tokens (used for channel token authorization). If you need to separate connection token configuration and subscription token configuration check out separate subscription token config feature. ","version":"v5","tagName":"h2"},{"title":"Channel permission model","type":0,"sectionRef":"#","url":"/docs/server/channel_permissions","content":"","keywords":"","version":"v5"},{"title":"Subscribe permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/server/channel_permissions#subscribe-permission-model","content":" By default, client's attempt to subscribe on a channel will be rejected by a server with 103: permission denied error. There are several approaches how to control channel subscribe permissions:  Provide subscription tokenConfigure subscribe proxyUse user-limited channelsUse subscribe_allowed_for_client namespace optionSubscribe capabilities in connection tokenSubscribe capabilities in connect proxy  Below, we are describing those in detail.  Provide subscription token​  A client can provide a subscription token in subscribe request. See the format of the token.  If client provides a valid token then subscription will be accepted. In Centrifugo PRO subscription token can additionally grant publish, history and presence permissions to a client.  caution For namespaces with allow_subscribe_for_client option ON Centrifugo does not allow subscribing on channels starting with private_channel_prefix ($ by default) without token. This limitation exists to help users migrate to Centrifugo v4 without security risks.  Configure subscribe proxy​  If client subscribes on a namespace with configured subscribe proxy then depending on proxy response subscription will be accepted or not.  If a namespace has configured subscribe proxy, but user came with a token – then subscribe proxy is not used, we are relying on token in this case. If a namespace has subscribe proxy, but user subscribes on a user-limited channel – then subscribe proxy is not used also.  Use user-limited channels​  If client subscribes on a user-limited channel and there is a user ID match then subscription will be accepted.  caution User-limited channels must be enabled in a namespace using allow_user_limited_channels option.  Use allow_subscribe_for_client namespace option​  allow_subscribe_for_client allows all authenticated non-anonymous connections to subscribe on all channels in a namespace.  caution Turning this option on effectively makes namespace public – no subscribe permissions will be checked (only the check that current connection is authenticated - i.e. has non-empty user ID). Make sure this is really what you want in terms of channels security.  To additionally allow subscribing to anonymous connections take a look at allow_subscribe_for_anonymous option.  Subscribe capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow user subscribe to channels.  Subscribe capabilities in connect proxy​  Centrifugo PRO only  Connect proxy can return capability object to allow user subscribe to channels.  ","version":"v5","tagName":"h3"},{"title":"Publish permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/server/channel_permissions#publish-permission-model","content":" tip In idiomatic Centrifugo use case data should be published to channels from the application backend (over server API). In this case backend can validate data, save it into persistent storage before publishing in real-time towards connections. When publishing from the client-side backend does not receive publication data at all – it just goes through Centrifugo (except using publish proxy). There are cases when direct publications from the client-side are desired (like typing indicators in chat applications) though.  By default, client's attempt to publish data into a channel will be rejected by a server with 103: permission denied error. There are several approaches how to control channel publish permissions:  Configure publish proxyUse allow_publish_for_subscriber namespace optionUse allow_publish_for_client namespace optionPublish capabilities in connection tokenPublish capability in subscription tokenPublish capabilities in connect proxyPublish capability in subscribe proxy  Use allow_publish_for_client namespace option​  allow_publish_for_client allows publications to channels of a namespace for all client connections.  Use allow_publish_for_subscriber namespace option​  allow_publish_for_subscriber allows publications to channels of a namespace for all connections subscribed on a channel they want to publish data into.  Configure publish proxy​  If client publishes to a namespace with configured publish proxy then depending on proxy response publication will be accepted or not.  Configured publish proxy always used??? (what if user has permission in token or allow_publish_for_client?)  Publish capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow client to publish to channels.  Publish capability in subscription token​  Centrifugo PRO only  Connection token can contain a capability object to allow client to publish to a channel.  Publish capabilities in connect proxy​  Centrifugo PRO only  Connect proxy can return capability object to allow client publish to certain channels.  Publish capability in subscribe proxy​  Centrifugo PRO only  Subscribe proxy can return capability object to allow subscriber publish to channel.  ","version":"v5","tagName":"h3"},{"title":"History permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/server/channel_permissions#history-permission-model","content":" By default, client's attempt to call history from a channel (with history retention configured) will be rejected by a server with 103: permission denied error. There are several approaches how to control channel history permissions.  Use allow_history_for_subscriber namespace option​  allow_history_for_subscriber allows history requests to all channels in a namespace for all client connections subscribed on a channel they want to call history for.  Use allow_history_for_client namespace option​  allow_history_for_client allows history requests to all channels in a namespace for all client connections.  History capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call history for channels.  History capabilities in subscription token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call history from a channel.  History capabilities in connect proxy​  This is a Centrifugo PRO feature.  Connect proxy can return capability object to allow client call history from certain channels.  History capability in subscribe proxy response​  Centrifugo PRO only  Subscribe proxy can return capability object to allow subscriber call history from channel.  ","version":"v5","tagName":"h3"},{"title":"Presence permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/server/channel_permissions#presence-permission-model","content":" By default, client's attempt to call presence from a channel (with channel presence configured) will be rejected by a server with 103: permission denied error. There are several approaches how to control channel presence permissions.  Presence capability in subscribe proxy response​  Subscribe proxy can return capability object to allow subscriber call presence from channel.  Use allow_presence_for_subscriber namespace option​  allow_presence_for_subscriber allows presence requests to all channels in a namespace for all client connections subscribed on a channel they want to call presence for.  Use allow_presence_for_client namespace option​  allow_presence_for_client allows presence requests to all channels in a namespace for all client connections.  Presence capabilities in connection token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call presence for channels.  Presence capabilities in subscription token​  Centrifugo PRO only  Connection token can contain a capability object to allow user call presence of a channel.  Presence capabilities in connect proxy​  Centrifugo PRO only  Connect proxy can return capability object to allow client call presence from certain channels.  ","version":"v5","tagName":"h3"},{"title":"Positioning permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/server/channel_permissions#positioning-permission-model","content":" Server can whether turn on positioning for all channels in a namespace using &quot;force_positioning&quot;: true option or client can create positioned subscriptions (but in this case client must have access to history capability).  ","version":"v5","tagName":"h3"},{"title":"Recovery permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/server/channel_permissions#recovery-permission-model","content":" Server can whether turn on automatic recovery for all channels in a namespace using &quot;force_recovery&quot;: true option or client can create recoverable subscriptions (but in this case client must have access to history capability).  ","version":"v5","tagName":"h3"},{"title":"Join/Leave permission model​","type":1,"pageTitle":"Channel permission model","url":"/docs/server/channel_permissions#joinleave-permission-model","content":" Server can whether force sending join/leave messages to all subscribers for all channels in a namespace using &quot;force_push_join_leave&quot;: true option or client can ask server to include join/leave messages upon subscribing (but in this case client must have access to presence capability). ","version":"v5","tagName":"h3"},{"title":"Channel JWT authorization","type":0,"sectionRef":"#","url":"/docs/server/channel_token_auth","content":"","keywords":"","version":"v5"},{"title":"Subscription JWT claims​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#subscription-jwt-claims","content":" For subscription JWT Centrifugo uses some standard claims defined in rfc7519, also some custom Centrifugo-specific.  ","version":"v5","tagName":"h2"},{"title":"sub​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#sub","content":" This is a standard JWT claim which must contain an ID of the current application user (as string).  The value must match a user in connection JWT – since it's the same real-time connection. The missing claim will mean that token issued for anonymous user (i.e. with empty user ID).  ","version":"v5","tagName":"h3"},{"title":"channel​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#channel","content":" Required. Channel that client tries to subscribe to with this token (string).  ","version":"v5","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#info","content":" Optional. Additional information for connection inside this channel (valid JSON).  ","version":"v5","tagName":"h3"},{"title":"b64info​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#b64info","content":" Optional. Additional information for connection inside this channel in base64 format (string). Will be decoded by Centrifugo to raw bytes.  ","version":"v5","tagName":"h3"},{"title":"exp​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#exp","content":" Optional. This is a standard JWT claim that allows setting private channel subscription token expiration time (a UNIX timestamp in the future, in seconds, as integer) and configures subscription expiration time.  At the moment if the subscription expires client connection will be closed and the client will try to reconnect. In most cases, you don't need this and should prefer using the expiration of the connection JWT to deactivate the connection (see authentication). But if you need more granular per-channel control this may fit your needs.  Once exp is set in token every subscription token must be periodically refreshed. This refresh workflow happens on the client side. Refer to the specific client documentation to see how to refresh subscriptions.  ","version":"v5","tagName":"h3"},{"title":"expire_at​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#expire_at","content":" Optional. By default, Centrifugo looks on exp claim to both check token expiration and configure subscription expiration time. In most cases this is fine, but there could be situations where you want to decouple subscription token expiration check with subscription expiration time. As soon as the expire_at claim is provided (set) in subscription JWT Centrifugo relies on it for setting subscription expiration time (JWT expiration still checked over exp though).  expire_at is a UNIX timestamp seconds when the subscription should expire.  Set it to the future time for expiring subscription at some pointSet it to 0 to disable subscription expiration (but still check token exp claim). This allows implementing a one-time subscription token.  ","version":"v5","tagName":"h3"},{"title":"aud​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#aud","content":" By default, Centrifugo does not check JWT audience (rfc7519 aud claim). But if you set token_audience option as described in client authentication then audience for subscription JWT will also be checked.  ","version":"v5","tagName":"h3"},{"title":"iss​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#iss","content":" By default, Centrifugo does not check JWT issuer (rfc7519 iss claim). But if you set token_issuer option as described in client authentication then issuer for subscription JWT will also be checked.  ","version":"v5","tagName":"h3"},{"title":"iat​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#iat","content":" This is a UNIX time when token was issued (seconds). See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v5","tagName":"h3"},{"title":"jti​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#jti","content":" This is a token unique ID. See definition in RFC. This claim is optional but can be useful together with Centrifugo PRO token revocation features.  ","version":"v5","tagName":"h3"},{"title":"override​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#override","content":" One more claim is override. This is an object which allows overriding channel options for the particular channel subscriber which comes with subscription token.  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\toverride presence channel option join_leave\tBoolValue\tyes\toverride join_leave channel option force_push_join_leave\tBoolValue\tyes\toverride force_push_join_leave channel option force_recovery\tBoolValue\tyes\toverride force_recovery channel option force_positioning\tBoolValue\tyes\toverride force_positioning channel option  BoolValue is an object like this:  { &quot;value&quot;: true/false }   So for example, you want to turn off emitting a presence information for a particular subscriber in a channel:  { ... &quot;override&quot;: { &quot;presence&quot;: { &quot;value&quot;: false } } }   ","version":"v5","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#example","content":" So to generate a subscription token you can use something like this in Python (assuming user ID is 42 and the channel is gossips):      PythonNodeJS import jwt import time claims = {&quot;sub&quot;: &quot;42&quot;, &quot;channel&quot;: &quot;$gossips&quot;, &quot;exp&quot;: int(time.time()) + 3600} token = jwt.encode(claims, &quot;secret&quot;, algorithm=&quot;HS256&quot;).decode() print(token)   Where &quot;secret&quot; is the token_hmac_secret_key from Centrifugo configuration (we use HMAC tokens in this example which relies on a shared secret key, for RSA or ECDSA tokens you need to use a private key known only by your backend).  ","version":"v5","tagName":"h2"},{"title":"gensubtoken cli command​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#gensubtoken-cli-command","content":" During development you can quickly generate valid subscription token using Centrifugo gensubtoken cli command.  ./centrifugo gensubtoken -u 123722 -s channel   You should see an output like this:  HMAC SHA-256 JWT for user &quot;123722&quot; and channel &quot;channel&quot; with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE2NTU0NDg0MzgsImNoYW5uZWwiOiJjaGFubmVsIn0.JyRI3ovNV-abV8VxCmZCD556o2F2mNL1UoU58gNR-uI   But in real app subscription JWT must be generated by your application backend.  ","version":"v5","tagName":"h2"},{"title":"Separate subscription token config​","type":1,"pageTitle":"Channel JWT authorization","url":"/docs/server/channel_token_auth#separate-subscription-token-config","content":" When separate_subscription_token_config boolean option is true Centrifugo does not look at general token options at all when verifying subscription tokens and uses config options starting from subscription_token_ prefix instead.  Here is an example how to use JWKS for connection tokens, but have HMAC-based verification for subscription tokens:  config.json { &quot;token_jwks_public_endpoint&quot;: &quot;https://example.com/openid-connect/certs&quot;, &quot;separate_subscription_token_config&quot;: true, &quot;subscription_token_hmac_secret_key&quot;: &quot;separate_secret_which_must_be_strong&quot; }   All the options which are available for connection token configuration may be re-used for a separate subscription token configuration – just prefix them with subscription_token_ instead of token_. ","version":"v5","tagName":"h2"},{"title":"Channels and namespaces","type":0,"sectionRef":"#","url":"/docs/server/channels","content":"","keywords":"","version":"v5"},{"title":"What is a channel​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#what-is-a-channel","content":" Centrifugo operates on a PUB/SUB model - it has publishers and subscribers. A channel acts as a conduit for publications. Clients can subscribe to a channel to receive all the real-time messages published there. Subscribers to a channel may also request information about the channel's online presence or its history.    A channel is simply a string - names like news, comments, personal_feed are examples of valid channel names. However, there are predefined rules for these strings, as we will discuss later. You can define different behaviors for a channel using a range of available channel options.  Channels are ephemeral – there is no need to create them explicitly. Channels are automatically created by Centrifugo as soon as the first client subscribes. Similarly, when the last subscriber leaves, the channel is automatically cleaned up.  A channel can be part of a channel namespace. Channel namespacing is a mechanism to define different behaviors for various channels within Centrifugo. Using namespaces is the recommended approach to manage channels – enabling only those channel options which are necessary for the specific real-time feature you are implementing with Centrifugo.  caution Ensure you have defined a namespace in the configuration when using channel namespaces. Attempts to subscribe to a channel within an undefined namespace will result in 102: unknown channel errors.  ","version":"v5","tagName":"h2"},{"title":"Channel name rules​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#channel-name-rules","content":" Only ASCII symbols must be used in a channel string.  Channel name length limited by 255 characters by default (controlled by configuration option channel_max_length).  Several symbols in channel names reserved for Centrifugo internal needs:  : – for namespace channel boundary (see below)# – for user channel boundary (see below)$ – for private channel prefix (see below)/ – for Channel Patterns in Centrifugo PRO* – for the future Centrifugo needs&amp; – for the future Centrifugo needs  ","version":"v5","tagName":"h2"},{"title":"namespace boundary (:)​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#namespace-boundary-","content":" : – is a channel namespace boundary. Namespaces are used to set custom options to a group of channels. Each channel belonging to the same namespace will have the same channel options. Read more about about namespaces and channel options below.  If the channel is public:chat - then Centrifugo will apply options to this channel from the channel namespace with the name public.  info A namespace is a inalienable component of the channel name. If a user is subscribed to a channel with a namespace, such as public:chat, then you must publish messages to the public:chat channel for them to be delivered to the user. There is often confusion among developers who try to publish messages to chat, mistakenly believing that the namespace is stripped upon subscription. This is not the case. You must publish exactly to the same channel string you used for subscribing.  ","version":"v5","tagName":"h3"},{"title":"user channel boundary (#)​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#user-channel-boundary-","content":" # symbol serves as the user channel boundary. It acts as a separator to create personal channels for users—referred to as user-limited channels—without requiring a subscription token.  For instance, if the channel is named news#42, then only the user with ID 42 can subscribe to this channel. Centrifugo identifies the user ID from the connection credentials provided in the connection JWT.  To create a user-limited channel within the personal namespace, you might use a name such as personal:user#42.  Furthermore, it's possible to specify multiple user IDs in the channel name, separated by a comma: dialog#42,43. In this case, only users with IDs 42 and 43 are permitted to subscribe to this channel.  This setup is ideal for channels that have a static list of allowed users, such as channels for personal messages to a single user or dialogue channels between specific users. However, for dynamic access management of a channel for numerous users, this type of channel is not appropriate.  tip User-limited channels must be enabled for a channel namespace using allow_user_limited_channels option. See below more information about channel options and channel namespaces.  ","version":"v5","tagName":"h3"},{"title":"private channel prefix ($)​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#private-channel-prefix-","content":" Centrifugo maintains compatibility with its previous versions which had concept of private channels. In earlier versions — specifically Centrifugo v1, v2, and v3—only – only channels beginning with $ required a subscription JWT for subscribing. With Centrifugo v4, this is no longer the case; clients can subscribe to any channel if they have a valid subscription token.  However, for namespaces where the allow_subscribe_for_client option is activated, Centrifugo prohibits subscriptions to channels that start with the private_channel_prefix (which defaults to $) unless a subscription token is provided. This restriction is designed to facilitate a secure migration to Centrifugo v4 or later versions.  ","version":"v5","tagName":"h3"},{"title":"Channel is just a string​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#channel-is-just-a-string","content":" Bear in mind that a channel is uniquely identified by its string representation. Do not assume that channels $news and news are the same; they differ because their strings are not identical. Thus, if a user is subscribed to $news, they will not receive messages published to news.  The channels dialog#42,43 and dialog#43,42 are considered different as well. Centrifugo only applies permission checks when a user subscribes to a channel. So if user-limited channels are enabled then the user with ID 42 will be able to subscribe on both dialog#42,43 and dialog#43,42. But Centrifugo does no magic regarding channel strings when keeping channel-&gt;to-&gt;subscribers map. So if the user subscribed on dialog#42,43 you must publish messages to exactly that channel: dialog#42,43.  The same reasoning applies to channels within namespaces. Channels chat:index and index are not the same — they are distinct and, moreover, they belong to different namespaces. The concept of channel namespaces in Centrifugo will be discussed shortly.  ","version":"v5","tagName":"h3"},{"title":"Channel namespaces​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#channel-namespaces","content":" Centrifugo allows configuring a list of channel namespaces. Namespaces are optional but super-useful.  A namespace acts as a container for options that are applied to channels starting with the namespace name. I.e. if you defined namespace with a name personal in config, then all the channels starting with personal:, like personal:1 or personal:2, will inherit options defined for personal namespace. This provides great control over channel behavior, so you have a flexible way to define different channel options for various real-time features in the application.  Namespace has a name, and can contain all the channel options. Namespace name is required to be set. Name of namespace must be unique, must consist of letters, numbers, underscores, or hyphens and be more than 2 symbols length i.e. satisfy regexp ^[-a-zA-Z0-9_]{2,}$.  When you want to use specific namespace options your channel must be prefixed with namespace name and : separator: public:messages, gossips:messages are two channels in public and gossips namespaces.  Centrifugo looks for : symbol in the channel name, if found – extracts the namespace name, and applies all the configured namespace channel options while processing protocol commands from a client or server API calls.  All things together here is an example of config.json which includes some top-level channel options set and has 2 additional channel namespaces configured:  config.json { &quot;token_hmac_secret_key&quot;: &quot;very-long-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;presence&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;30s&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;facts&quot;, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot; }, { &quot;name&quot;: &quot;gossips&quot; } ] }   Channel news will use globally defined channel options.Channel facts:sport will use facts namespace options.Channel gossips:sport will use gossips namespace options.Channel xxx:hello will result into subscription error since there is no xxx namespace defined in the configuration above.  Channel namespaces also work with private channels and user-limited channels. For example, if you have a namespace called dialogs then the private channel can be constructed as $dialogs:gossips, user-limited channel can be constructed as dialogs:dialog#1,2.  note There is no inheritance in channel options and namespaces – for example, you defined presence: true on a top level of configuration and then defined a namespace – that namespace won't have online presence enabled - you must enable it for a namespace explicitly.  There are many options which can be set for channel namespace (on top-level and to named one) to modify behavior of channels belonging to a namespace. Below we describe all these options.  ","version":"v5","tagName":"h2"},{"title":"Channel options​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#channel-options","content":" Channel behavior can be modified by using channel options. Channel options can be defined on configuration top-level and for every namespace.  ","version":"v5","tagName":"h2"},{"title":"presence​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#presence","content":" presence (boolean, default false) – enable/disable online presence information for channels in a namespace.  Online presence is information about clients currently subscribed to the channel. It contains each subscriber's client ID, user ID, connection info, and channel info. By default, this option is off so no presence information will be available for channels.  Let's say you have a channel chat:index and 2 users (with ID 2694 and 56) subscribed to it. And user 2694 has 2 connections to Centrifugo in different browser tabs. In presence data you may see sth like this:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;chat:index&quot;}' \\ http://localhost:8000/api/presence { &quot;result&quot;: { &quot;presence&quot;: { &quot;66fdf8d1-06f0-4375-9fac-db959d6ee8d6&quot;: { &quot;user&quot;: &quot;2694&quot;, &quot;client&quot;: &quot;66fdf8d1-06f0-4375-9fac-db959d6ee8d6&quot;, &quot;conn_info&quot;: {&quot;name&quot;: &quot;Alex&quot;} }, &quot;d4516dd3-0b6e-4cfe-84e8-0342fd2bb20c&quot;: { &quot;user&quot;: &quot;2694&quot;, &quot;client&quot;: &quot;d4516dd3-0b6e-4cfe-84e8-0342fd2bb20c&quot;, &quot;conn_info&quot;: {&quot;name&quot;: &quot;Alex&quot;} } &quot;g3216dd3-1b6e-tcfe-14e8-1342fd2bb20c&quot;: { &quot;user&quot;: &quot;56&quot;, &quot;client&quot;: &quot;g3216dd3-1b6e-tcfe-14e8-1342fd2bb20c&quot;, &quot;conn_info&quot;: {&quot;name&quot;: &quot;Alice&quot;} } } } }   To call presence API from the client connection side client must have permission to do so. See presence permission model.  caution Enabling channel online presence adds some overhead since Centrifugo needs to maintain an additional data structure (in a process memory or in a broker memory/disk). So only use it for channels where presence is required.  See more details about online presence design.  ","version":"v5","tagName":"h3"},{"title":"join_leave​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#join_leave","content":" join_leave (boolean, default false) – enable/disable sending join and leave messages when the client subscribes to a channel (unsubscribes from a channel). Join/leave event includes information about the connection that triggered an event – client ID, user ID, connection info, and channel info (similar to entry inside presence information).  Enabling join_leave means that Join/Leave messages will start being emitted, but by default they are not delivered to clients subscribed to a channel. You need to force this using namespace option force_push_join_leave or explicitly provide intent from a client-side (in this case client must have permission to call presence API).  caution Keep in mind that join/leave messages can generate a huge number of messages in a system if turned on for channels with a large number of active subscribers. If you have channels with a large number of subscribers consider avoiding using this feature. It's hard to say what is &quot;large&quot; for you though – just estimate the load based on the fact that each subscribe/unsubscribe event in a channel with N subscribers will result into N messages broadcasted to all. If all clients reconnect at the same time the amount of generated messages is N^2.  Join/leave messages distributed only with at most once delivery guarantee.  ","version":"v5","tagName":"h3"},{"title":"force_push_join_leave​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#force_push_join_leave","content":" Boolean, default false.  When on all clients will receive join/leave events for a channel in a namespace automatically – without explicit intent to consume join/leave messages from the client side.  If pushing join/leave is not forced then client can provide a corresponding Subscription option to enable it – but it should have permissions to access channel presence (by having an explicit capability or if allowed on a namespace level).  ","version":"v5","tagName":"h3"},{"title":"history_size​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#history_size","content":" history_size (integer, default 0) – history size (amount of messages) for channels. As Centrifugo keeps all history messages in process memory (or in a broker memory) it's very important to limit the maximum amount of messages in channel history with a reasonable value. history_size defines the maximum amount of messages that Centrifugo will keep for each channel in the namespace. As soon as history has more messages than defined by history size – old messages will be evicted.  Setting only history_size is not enough to enable history in channels – you also need to wisely configure history_ttl option (see below).  caution Enabling channel history adds some overhead (both memory and CPU) since Centrifugo needs to maintain an additional data structure (in a process memory or a broker memory/disk). So only use history for channels where it's required.  ","version":"v5","tagName":"h3"},{"title":"history_ttl​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#history_ttl","content":" history_ttl (duration, default 0s) – interval how long to keep channel history messages (with seconds precision).  As all history is storing in process memory (or in a broker memory) it is also very important to get rid of old history data for unused (inactive for a long time) channels.  By default history TTL duration is zero – this means that channel history is disabled.  Again – to turn on history you should wisely configure both history_size and history_ttl options.  For example for top-level channels (which do not belong to a namespace):  config.json { ... &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;60s&quot; }   Let's look at example. You enabled history for a namespace chat and sent two messages in channel chat:index. Then history will contain sth like this:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;chat:index&quot;, &quot;limit&quot;: 100}' \\ http://localhost:8000/api/history { &quot;result&quot;: { &quot;publications&quot;: [ { &quot;data&quot;: { &quot;input&quot;: &quot;1&quot; }, &quot;offset&quot;: 1 }, { &quot;data&quot;: { &quot;input&quot;: &quot;2&quot; }, &quot;offset&quot;: 2 } ], &quot;epoch&quot;: &quot;gWuY&quot;, &quot;offset&quot;: 2 } }   To call history API from the client connection side client must have permission to do so. See history permission model.  See additional information about offsets and epoch in History and recovery chapter.  tip History persistence properties are dictated by Centrifugo engine used. For example, when using memory engine history is only kept till Centrifugo node restart. In Redis engine case persistence is determined by a Redis server persistence configuration (same for KeyDB and Tarantool).  ","version":"v5","tagName":"h3"},{"title":"history_meta_ttl​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#history_meta_ttl","content":" history_meta_ttl (duration) – sets a time of history stream metadata expiration (with seconds precision).  When using a history in a channel, Centrifugo keeps some metadata for each channel stream. Metadata includes the latest stream offset and its epoch value. In some cases, when channels are created for а short time and then not used anymore, created metadata can stay in memory while not useful. For example, you can have a personal user channel but after using your app for a while user left it forever. From a long-term perspective, this can be an unwanted memory growth. Setting a reasonable value to this option can help to expire metadata faster (or slower if you need it). The rule of thumb here is to keep this value much bigger than maximum history TTL used in Centrifugo configuration.  If not specified Centrifugo uses a global history_meta_ttl which is 30 days. This should be a good default for most use cases to avoid tweaking history_meta_ttl on a namespace level at all.  ","version":"v5","tagName":"h3"},{"title":"force_positioning​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#force_positioning","content":" force_positioning (boolean, default false) – when the force_positioning option is on Centrifugo forces all subscriptions in a namespace to be positioned. I.e. Centrifugo will try to compensate at most once delivery of PUB/SUB broker checking client position inside a stream.  If Centrifugo detects a bad position of the client (i.e. potential message loss) it disconnects a client with the Insufficient state disconnect code. Also, when the position option is enabled Centrifugo exposes the current stream top offset and current epoch in subscribe reply making it possible for a client to manually recover its state upon disconnect using history API.  force_positioning option must be used in conjunction with reasonably configured message history for a channel i.e. history_size and history_ttl must be set (because Centrifugo uses channel history to check client position in a stream).  If positioning is not forced then client can provide a corresponding Subscription option to enable it – but it should have permissions to access channel history (by having an explicit capability or if allowed on a namespace level).  ","version":"v5","tagName":"h3"},{"title":"force_recovery​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#force_recovery","content":" force_recovery (boolean, default false) – when the position option is on Centrifugo forces all subscriptions in a namespace to be recoverable. When enabled Centrifugo will try to recover missed publications in channels after a client reconnects for some reason (bad internet connection for example). Also when the recovery feature is on Centrifugo automatically enables properties of the force_positioning option described above.  force_recovery option must be used in conjunction with reasonably configured message history for channel i.e. history_size and history_ttl must be set (because Centrifugo uses channel history to recover messages).  If recovery is not forced then client can provide a corresponding Subscription option to enable it – but it should have permissions to access channel history (by having an explicit capability or if allowed on a namespace level).  tip Not all real-time events require this feature turned on so think wisely when you need this. When this option is turned on your application should be designed in a way to tolerate duplicate messages coming from a channel (currently Centrifugo returns recovered publications in order and without duplicates but this is an implementation detail that can be theoretically changed in the future). See more details about how recovery works in special chapter.  ","version":"v5","tagName":"h3"},{"title":"allow_subscribe_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_subscribe_for_client","content":" allow_subscribe_for_client (boolean, default false) – when on all non-anonymous clients will be able to subscribe to any channel in a namespace. To additionally allow anonymous users to subscribe turn on allow_subscribe_for_anonymous (see below).  caution Turning this option on effectively makes namespace public – no subscribe permissions will be checked (only the check that current connection is authenticated - i.e. has non-empty user ID). Make sure this is really what you want in terms of channels security.  ","version":"v5","tagName":"h3"},{"title":"allow_subscribe_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_subscribe_for_anonymous","content":" allow_subscribe_for_anonymous (boolean, default false) – turn on if anonymous clients (with empty user ID) should be able to subscribe on channels in a namespace.  ","version":"v5","tagName":"h3"},{"title":"allow_publish_for_subscriber​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_publish_for_subscriber","content":" allow_publish_for_subscriber (boolean, default false) - when the allow_publish_for_subscriber option is enabled client can publish into a channel in namespace directly from the client side over real-time connection but only if client subscribed to that channel.  danger Keep in mind that in this case subscriber can publish any payload to a channel – Centrifugo does not validate input at all. Your app backend won't receive those messages - publications just go through Centrifugo towards channel subscribers. Consider always validate messages which are being published to channels (i.e. using server API to publish after validating input on the backend side, or using publish proxy - see idiomatic usage).  allow_publish_for_subscriber (or allow_publish_for_client mentioned below) option still can be useful to send something without backend-side validation and saving it into a database – for example, this option may be handy for demos and quick prototyping real-time app ideas.  ","version":"v5","tagName":"h3"},{"title":"allow_publish_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_publish_for_client","content":" allow_publish_for_client (boolean, default false) – when on allows clients to publish messages into channels directly (from a client-side). It's like allow_publish_for_subscriber – but client should not be a channel subscriber to publish.  danger Keep in mind that in this case client can publish any payload to a channel – Centrifugo does not validate input at all. Your app backend won't receive those messages - publications just go through Centrifugo towards channel subscribers. Consider always validate messages which are being published to channels (i.e. using server API to publish after validating input on the backend side, or using publish proxy - see idiomatic usage).  ","version":"v5","tagName":"h3"},{"title":"allow_publish_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_publish_for_anonymous","content":" allow_publish_for_anonymous (boolean, default false) – turn on if anonymous clients should be able to publish into channels in a namespace.  ","version":"v5","tagName":"h3"},{"title":"allow_history_for_subscriber​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_history_for_subscriber","content":" allow_history_for_subscriber (boolean, default false) – allows clients who subscribed on a channel to call history API from that channel.  ","version":"v5","tagName":"h3"},{"title":"allow_history_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_history_for_client","content":" allow_history_for_client (boolean, default false) – allows all clients to call history information in a namespace.  ","version":"v5","tagName":"h3"},{"title":"allow_history_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_history_for_anonymous","content":" allow_history_for_anonymous (boolean, default false) – turn on if anonymous clients should be able to call history from channels in a namespace.  ","version":"v5","tagName":"h3"},{"title":"allow_presence_for_subscriber​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_presence_for_subscriber","content":" allow_presence_for_subscriber (boolean, default false) – allows clients who subscribed on a channel to call presence information from that channel.  ","version":"v5","tagName":"h3"},{"title":"allow_presence_for_client​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_presence_for_client","content":" allow_presence_for_client (boolean, default false) – allows all clients to call presence information in a namespace.  ","version":"v5","tagName":"h3"},{"title":"allow_presence_for_anonymous​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_presence_for_anonymous","content":" allow_presence_for_anonymous (boolean, default false) – turn on if anonymous clients should be able to call presence from channels in a namespace.  ","version":"v5","tagName":"h3"},{"title":"allow_user_limited_channels​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#allow_user_limited_channels","content":" allow_user_limited_channels (boolean, default false) - allows using user-limited channels in a namespace for checking subscribe permission.  note If client subscribes to a user-limited channel while this option is off then server rejects subscription with 103: permission denied error.  ","version":"v5","tagName":"h3"},{"title":"channel_regex​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#channel_regex","content":" channel_regex (string, default &quot;&quot;) – is an option to set a regular expression for channels allowed in the namespace. By default Centrifugo does not limit channel name variations. For example, if you have a namespace chat, then channel names inside this namespace are not really limited, it can be chat:index, chat:1, chat:2, chat:zzz and so on. But if you want to be strict and know possible channel patterns you can use channel_regex option. This is especially useful in namespaces where all clients can subscribe to channels.  For example, let's only allow digits after chat: for channel names in a chat namespace:  { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;chat&quot;, &quot;allow_subscribe_for_client&quot;: true, &quot;channel_regex&quot;: &quot;^[\\d+]$&quot; } ] }   danger Note, that we are skipping chat: part in regex. Since namespace prefix is the same for all channels in a namespace we only match the rest (after the prefix) of channel name.  Channel regex only checked for client-side subscriptions, if you are using server-side subscriptions Centrifugo won't check the regex.  Centrifugo uses Go language regexp package for regular expressions.  ","version":"v5","tagName":"h3"},{"title":"proxy_subscribe​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#proxy_subscribe","content":" proxy_subscribe (boolean, default false) – turns on subscribe proxy, more info in proxy chapter  ","version":"v5","tagName":"h3"},{"title":"proxy_publish​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#proxy_publish","content":" proxy_publish (boolean, default false) – turns on publish proxy, more info in proxy chapter  ","version":"v5","tagName":"h3"},{"title":"proxy_sub_refresh​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#proxy_sub_refresh","content":" proxy_sub_refresh (boolean, default false) – turns on sub refresh proxy, more info in proxy chapter  ","version":"v5","tagName":"h3"},{"title":"proxy_subscribe_stream​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#proxy_subscribe_stream","content":" proxy_subscribe_stream (boolean, default false) - turns on subscribe stream proxy, see subscription streams  ","version":"v5","tagName":"h3"},{"title":"subscribe_proxy_name​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#subscribe_proxy_name","content":" subscribe_proxy_name (string, default &quot;&quot;) – turns on subscribe proxy when granular proxy mode is used. Note that proxy_subscribe option defined above is ignored in granular proxy mode.  ","version":"v5","tagName":"h3"},{"title":"publish_proxy_name​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#publish_proxy_name","content":" publish_proxy_name (string, default &quot;&quot;) – turns on publish proxy when granular proxy mode is used. Note that proxy_publish option defined above is ignored in granular proxy mode.  ","version":"v5","tagName":"h3"},{"title":"sub_refresh_proxy_name​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#sub_refresh_proxy_name","content":" sub_refresh_proxy_name (string, default &quot;&quot;) – turns on sub refresh proxy when granular proxy mode is used. Note that proxy_sub_refresh option defined above is ignored in granular proxy mode.  ","version":"v5","tagName":"h3"},{"title":"subscribe_stream_proxy_name​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#subscribe_stream_proxy_name","content":" subscribe_stream_proxy_name (string, default &quot;&quot;) – turns on subscribe stream proxy when granular proxy mode is used. Note that proxy_subscribe_stream option defined above is ignored in granular proxy mode.  ","version":"v5","tagName":"h3"},{"title":"Channel config examples​","type":1,"pageTitle":"Channels and namespaces","url":"/docs/server/channels#channel-config-examples","content":" Let's look at how to set some of these options in a config. In this example we turning on presence, history features, forcing publication recovery. Also allowing all client connections (including anonymous users) to subscribe to channels and call publish, history, presence APIs if subscribed.  config.json { &quot;token_hmac_secret_key&quot;: &quot;my-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;presence&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot;, &quot;force_recovery&quot;: true, &quot;allow_subscribe_for_client&quot;: true, &quot;allow_subscribe_for_anonymous&quot;: true, &quot;allow_publish_for_subscriber&quot;: true, &quot;allow_publish_for_anonymous&quot;: true, &quot;allow_history_for_subscriber&quot;: true, &quot;allow_history_for_anonymous&quot;: true, &quot;allow_presence_for_subscriber&quot;: true, &quot;allow_presence_for_anonymous&quot;: true }   Here we set channel options on config top-level – these options will affect channels without namespace. In many cases defining namespaces is a recommended approach so you can manage options for every real-time feature separately. With namespaces the above config may transform to:  config.json { &quot;token_hmac_secret_key&quot;: &quot;my-secret-key&quot;, &quot;api_key&quot;: &quot;secret-api-key&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;feed&quot;, &quot;presence&quot;: true, &quot;history_size&quot;: 10, &quot;history_ttl&quot;: &quot;300s&quot;, &quot;force_recovery&quot;: true, &quot;allow_subscribe_for_client&quot;: true, &quot;allow_subscribe_for_anonymous&quot;: true, &quot;allow_publish_for_subscriber&quot;: true, &quot;allow_publish_for_anonymous&quot;: true, &quot;allow_history_for_subscriber&quot;: true, &quot;allow_history_for_anonymous&quot;: true, &quot;allow_presence_for_subscriber&quot;: true, &quot;allow_presence_for_anonymous&quot;: true } ] }   In this case channels should be prefixed with feed: to follow the behavior configured for a feed namespace. ","version":"v5","tagName":"h2"},{"title":"Error and disconnect codes","type":0,"sectionRef":"#","url":"/docs/server/codes","content":"","keywords":"","version":"v5"},{"title":"Client error codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#client-error-codes","content":" Client errors are errors that can be returned to a client in replies to commands. This is specific for bidirectional client protocol only. For example, an error can be returned inside a reply to a subscribe command issued by a client.  Here is the list of Centrifugo built-in client error codes (with proxy feature you have a way to use custom error codes in replies or reuse existing).  ","version":"v5","tagName":"h2"},{"title":"Internal​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#internal","content":" Code: 100 Message: &quot;internal server error&quot; Temporary: true   Error Internal means server error, if returned this is a signal that something went wrong with a server itself and client most probably not guilty.  ","version":"v5","tagName":"h3"},{"title":"Unauthorized​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#unauthorized","content":" Code: 101 Message: &quot;unauthorized&quot;   Error Unauthorized says that request is unauthorized.  ","version":"v5","tagName":"h3"},{"title":"Unknown Channel​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#unknown-channel","content":" Code: 102 Message: &quot;unknown channel&quot;   Error Unknown Channel means that channel name does not exist.  Usually this is returned when client uses channel with a namespace which is not defined in Centrifugo configuration.  ","version":"v5","tagName":"h3"},{"title":"Permission Denied​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#permission-denied","content":" Code: 103 Message: &quot;permission denied&quot;   Error Permission Denied means that access to resource not allowed.  ","version":"v5","tagName":"h3"},{"title":"Method Not Found​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#method-not-found","content":" Code: 104 Message: &quot;method not found&quot;   Error Method Not Found means that method sent in command does not exist.  ","version":"v5","tagName":"h3"},{"title":"Already Subscribed​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#already-subscribed","content":" Code: 105 Message: &quot;already subscribed&quot;   Error Already Subscribed returned when a client attempts to subscribe to a channel to which it is already subscribed.  In Centrifugo, a client can only have one subscription to a specific channel.  When using client-side subscriptions, this error may signal a bug in the SDK or the SDK being used in a way that was not planned. This error may also be returned by the server when a client tries to subscribe to a channel but is already subscribed to it using server-side subscriptions.  ","version":"v5","tagName":"h3"},{"title":"Limit Exceeded​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#limit-exceeded","content":" Code: 106 Message: &quot;limit exceeded&quot;   Error Limit Exceeded says that some sort of limit exceeded, server logs should give more detailed information. See also ErrorTooManyRequests which is more specific for rate limiting purposes.  ","version":"v5","tagName":"h3"},{"title":"Bad Request​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#bad-request","content":" Code: 107 Message: &quot;bad request&quot;   Error Bad Request says that server can not process received data because it is malformed. Retrying request does not make sense.  ","version":"v5","tagName":"h3"},{"title":"Not Available​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#not-available","content":" Code: 108 Message: &quot;not available&quot;   Error Not Available means that resource is not enabled.  For example, this can be returned when trying to access history or presence in a channel that is not configured for having history or presence features.  ","version":"v5","tagName":"h3"},{"title":"Token Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#token-expired","content":" Code: 109 Message: &quot;token expired&quot;   Error Token Expired indicates that connection token expired. Our SDKs handle it in a special way by updating token.  ","version":"v5","tagName":"h3"},{"title":"Expired​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#expired","content":" Code: 110 Message: &quot;expired&quot;   Error Expired indicates that connection expired (no token involved).  ","version":"v5","tagName":"h3"},{"title":"Too Many Requests​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#too-many-requests","content":" Code: 111 Message: &quot;too many requests&quot; Temporary: true   Error Too Many Requests means that server rejected request due to rate limiting strategies.  ","version":"v5","tagName":"h3"},{"title":"Unrecoverable Position​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#unrecoverable-position","content":" Code: 112 Message: &quot;unrecoverable position&quot;   Error Unrecoverable Position means that stream does not contain required range of publications to fulfill a history query.  This can happen due to wrong epoch passed.  ","version":"v5","tagName":"h3"},{"title":"Client disconnect codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#client-disconnect-codes","content":" Client can be disconnected by a Centrifugo server with custom code and string reason. Here is the list of Centrifugo built-in disconnect codes (with proxy feature you have a way to use custom disconnect codes).  note We expect that in most situations developers don't need to programmatically deal with handling various disconnect codes, but since Centrifugo sends them and codes shown in server metrics – they are documented. We expect these codes are mostly useful for logs and metrics.  ","version":"v5","tagName":"h2"},{"title":"DisconnectConnectionClosed​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#disconnectconnectionclosed","content":" Code: 3000 Reason: &quot;connection closed&quot;   DisconnectConnectionClosed is a special Disconnect object used when client connection was closed without any advice from a server side. This can be a clean disconnect, or temporary disconnect of the client due to internet connection loss. Server can not distinguish the actual reason of disconnect.  ","version":"v5","tagName":"h3"},{"title":"Non-terminal disconnect codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#non-terminal-disconnect-codes","content":" Client will reconnect after receiving such codes.  Shutdown​  Code: 3001 Reason: &quot;shutdown&quot;   Disconnect Shutdown may be sent when node is going to shut down.  DisconnectServerError​  Code: 3004 Reason: &quot;internal server error&quot;   DisconnectServerError issued when internal error occurred on server.  DisconnectExpired​  Code: 3005 Reason: &quot;connection expired&quot;   DisconnectSubExpired​  Code: 3006 Reason: &quot;subscription expired&quot;   DisconnectSubExpired issued when client subscription expired.  DisconnectSlow​  Code: 3008 Reason: &quot;slow&quot;   DisconnectSlow issued when client can't read messages fast enough.  DisconnectWriteError​  Code: 3009 Reason: &quot;write error&quot;   DisconnectWriteError issued when an error occurred while writing to client connection.  DisconnectInsufficientState​  Code: 3010 Reason: &quot;insufficient state&quot;   DisconnectInsufficientState issued when server detects wrong client position in channel Publication stream. Disconnect allows clien to restore missed publications on reconnect.  DisconnectForceReconnect​  Code: 3011 Reason: &quot;force reconnect&quot;   DisconnectForceReconnect issued when server disconnects connection for some reason and whants it to reconnect.  DisconnectNoPong​  Code: 3012 Reason: &quot;no pong&quot;   DisconnectNoPong may be issued when server disconnects bidirectional connection due to no pong received to application-level server-to-client pings in a configured time.  DisconnectTooManyRequests​  Code: 3013 Reason: &quot;too many requests&quot;   DisconnectTooManyRequests may be issued when client sends too many commands to a server.  ","version":"v5","tagName":"h3"},{"title":"Terminal disconnect codes​","type":1,"pageTitle":"Error and disconnect codes","url":"/docs/server/codes#terminal-disconnect-codes","content":" Client won't reconnect upon receiving such code.  DisconnectInvalidToken​  Code: 3500 Reason: &quot;invalid token&quot;   DisconnectInvalidToken issued when client came with invalid token.  DisconnectBadRequest​  Code: 3501 Reason: &quot;bad request&quot;   DisconnectBadRequest issued when client uses malformed protocol frames.  DisconnectStale​  Code: 3502 Reason: &quot;stale&quot;   DisconnectStale issued to close connection that did not become authenticated in configured interval after dialing.  DisconnectForceNoReconnect​  Code: 3503 Reason: &quot;force disconnect&quot;   DisconnectForceNoReconnect issued when server disconnects connection and asks it to not reconnect again.  DisconnectConnectionLimit​  Code: 3504 Reason: &quot;connection limit&quot;   DisconnectConnectionLimit can be issued when client connection exceeds a configured connection limit (per user ID or due to other rule).  DisconnectChannelLimit​  Code: 3505 Reason: &quot;channel limit&quot;   DisconnectChannelLimit can be issued when client connection exceeds a configured channel limit.  DisconnectInappropriateProtocol​  Code: 3506 Reason: &quot;inappropriate protocol&quot;   DisconnectInappropriateProtocol can be issued when client connection format can not handle incoming data. For example, this happens when JSON-based clients receive binary data in a channel. This is usually an indicator of programmer error, JSON clients can not handle binary.  DisconnectPermissionDenied​  Code: 3507 Reason: &quot;permission denied&quot;   DisconnectPermissionDenied may be issued when client attempts accessing a server without enough permissions.  DisconnectNotAvailable​  Code: 3508 Reason: &quot;not available&quot;   DisconnectNotAvailable may be issued when ErrorNotAvailable does not fit message type, for example we issue DisconnectNotAvailable when client sends asynchronous message without MessageHandler set on server side.  DisconnectTooManyErrors​  Code: 3509 Reason: &quot;too many errors&quot;   DisconnectTooManyErrors may be issued when client generates too many errors. ","version":"v5","tagName":"h3"},{"title":"Configure Centrifugo","type":0,"sectionRef":"#","url":"/docs/server/configuration","content":"","keywords":"","version":"v5"},{"title":"Configuration sources​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#configuration-sources","content":" Centrifugo can be configured in several ways: using command-line flags (highest priority), environment variables (second priority after flags), configuration file (lowest priority).  ","version":"v5","tagName":"h2"},{"title":"Command-line flags​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#command-line-flags","content":" Centrifugo supports several command-line flags. See centrifugo -h for available flags. Command-line flags limited to most frequently used. In general, we suggest to avoid using flags for configuring Centrifugo in a production environment – prefer using environment variables or configuration file.  Command-line options have the highest priority when set than other ways to configure Centrifugo.  ","version":"v5","tagName":"h3"},{"title":"OS environment variables​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#os-environment-variables","content":" All Centrifugo options can be set over env in the format CENTRIFUGO_&lt;OPTION_NAME&gt; (i.e. option name with CENTRIFUGO_ prefix, all in uppercase).  Setting options over env is mostly straightforward except namespaces – see how to set namespaces via env. Environment variables have the second priority after flags.  Boolean options can be set using strings according to Go language ParseBool function. I.e. to set true you can just use &quot;true&quot; value for an environment variable (or simply &quot;1&quot;). To set false use &quot;false&quot; or &quot;0&quot;. Example:  export CENTRIFUGO_PROMETHEUS=&quot;1&quot;   Also, array options, like allowed_origins can be set over environment variables as a single string where values separated by a space. For example:  export CENTRIFUGO_ALLOWED_ORIGINS=&quot;https://mysite1.example.com https://mysite2.example.com&quot;   For a nested object configuration (which we have, for example, in Centrifugo PRO ClickHouse analytics) it's still possible to use environment variables to set options. In this case replace nesting with _ when constructing environment variable name.  Empty environment variables are considered unset (!) and will fall back to the next configuration source.  ","version":"v5","tagName":"h3"},{"title":"Configuration file​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#configuration-file","content":" Configuration file supports all options mentioned in Centrifugo documentation and can be in one of three supported formats: JSON, YAML, or TOML. Config file options have the lowest priority among configuration sources (i.e. option set over environment variable is preferred over the same option in config file).  A simple way to start with Centrifugo is to run:  centrifugo genconfig   This command generates config.json configuration file in a current directory. This file already has the minimal number of options set. So it's then possible to start Centrifugo:  centrifugo -c config.json   ","version":"v5","tagName":"h3"},{"title":"Config file formats​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#config-file-formats","content":" Centrifugo supports three configuration file formats: JSON, YAML, or TOML.  ","version":"v5","tagName":"h2"},{"title":"JSON config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#json-config-format","content":" Here is an example of Centrifugo JSON configuration file:  config.json { &quot;allowed_origins&quot;: [&quot;http://localhost:3000&quot;], &quot;token_hmac_secret_key&quot;: &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot;, &quot;api_key&quot;: &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; }   token_hmac_secret_key used to check JWT signature (more info about JWT in authentication chapter). If you are using connect proxy then you may use Centrifugo without JWT.  api_key used for Centrifugo API endpoint authorization, see more in chapter about server HTTP API. Keep both values secret and never reveal them to clients.  allowed_origins option described below.  ","version":"v5","tagName":"h3"},{"title":"TOML config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#toml-config-format","content":" Centrifugo also supports TOML format for configuration file:  centrifugo --config=config.toml   Where config.toml contains:  config.toml allowed_origins: [ &quot;http://localhost:3000&quot; ] token_hmac_secret_key = &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot; api_key = &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; log_level = &quot;debug&quot;   In the example above we also defined logging level to be debug which is useful to have while developing an application. In the production environment debug logging can be too chatty.  ","version":"v5","tagName":"h3"},{"title":"YAML config format​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#yaml-config-format","content":" YAML format is also supported:  config.yaml allowed_origins: - &quot;http://localhost:3000&quot; token_hmac_secret_key: &quot;&lt;YOUR-SECRET-STRING-HERE&gt;&quot; api_key: &quot;&lt;YOUR-API-KEY-HERE&gt;&quot; log_level: debug   With YAML remember to use spaces, not tabs when writing a configuration file.  ","version":"v5","tagName":"h3"},{"title":"Important options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#important-options","content":" Let's describe some important options you can configure when running Centrifugo.  ","version":"v5","tagName":"h2"},{"title":"allowed_origins​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#allowed_origins","content":" This option allows setting an array of allowed origin patterns (array of strings) for WebSocket and SockJS endpoints to prevent CSRF or WebSocket hijacking attacks. Also, it's used for HTTP-based unidirectional transports to enable CORS for configured origins.  As soon as allowed_origins is defined every connection request with Origin set will be checked against each pattern in an array.  Connection requests without Origin header set are passing through without any checks (i.e. always allowed).  For example, a client connects to Centrifugo from a web browser application on http://localhost:3000. In this case, allowed_origins should be configured in this way:  &quot;allowed_origins&quot;: [ &quot;http://localhost:3000&quot; ]   When connecting from https://example.com:  &quot;allowed_origins&quot;: [ &quot;https://example.com&quot; ]   Origin pattern can contain wildcard symbol * to match subdomains:  &quot;allowed_origins&quot;: [ &quot;https://*.example.com&quot; ]   – in this case requests with Origin header like https://foo.example.com or https://bar.example.com will pass the check.  It's also possible to allow all origins in the following way (but this is discouraged and insecure when using connect proxy feature):  &quot;allowed_origins&quot;: [ &quot;*&quot; ]   ","version":"v5","tagName":"h3"},{"title":"address​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#address","content":" Bind your Centrifugo to a specific interface address (string, by default &quot;&quot; - listen on all available interfaces).  ","version":"v5","tagName":"h3"},{"title":"port​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#port","content":" Port to bind Centrifugo to (string, by default &quot;8000&quot;).  ","version":"v5","tagName":"h3"},{"title":"engine​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#engine","content":" Engine to use - memory, redis or tarantool. It's a string option, by default memory. Read more about engines in special chapter.  ","version":"v5","tagName":"h3"},{"title":"Advanced options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#advanced-options","content":" These options allow tweaking server behavior, in most cases default values are good to start with.  ","version":"v5","tagName":"h2"},{"title":"client_channel_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_channel_limit","content":" Default: 128  Sets the maximum number of different channel subscriptions a single client can have.  tip When designing an application avoid subscribing to an unlimited number of channels per one client. Keep number of subscriptions for each client reasonably small – this will help keeping handshake process lightweight and fast.  ","version":"v5","tagName":"h3"},{"title":"channel_max_length​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#channel_max_length","content":" Default: 255  Sets the maximum length of the channel name.  ","version":"v5","tagName":"h3"},{"title":"client_user_connection_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_user_connection_limit","content":" Default: 0  The maximum number of connections from a user (with known user ID) to Centrifugo node. By default, unlimited.  The important thing to emphasize is that client_user_connection_limit works only per one Centrifugo node and exists mostly to protect Centrifugo from many connections from a single user – but not for business logic limitations. This means that if you set this to 1 and scale nodes – say run 10 Centrifugo nodes – then a user will be able to create 10 connections (one to each node).  ","version":"v5","tagName":"h3"},{"title":"client_connection_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_connection_limit","content":" Default: 0  When set to a value &gt; 0 client_connection_limit limits the max number of connections single Centrifugo node can handle. It acts on HTTP middleware level and stops processing request if the condition met. It logs a warning into logs in this case and increments centrifugo_node_client_connection_limit Prometheus counter. Client SDKs will attempt reconnecting.  Some motivation behind this option may be found in this issue.  Note, that at this point client_connection_limit does not affect connections coming over GRPC unidirectional transport.  ","version":"v5","tagName":"h3"},{"title":"client_connection_rate_limit​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_connection_rate_limit","content":" Default: 0  client_connection_rate_limit sets the maximum number of HTTP requests to establish a new real-time connection a single Centrifugo node will accept per second (on real-time transport endpoints). All requests outside the limit will get 503 Service Unavailable code in response. Our SDKs handle this with backoff reconnection.  By default, no limit is used.  Note, that at this point client_connection_rate_limit does not affect connections coming over GRPC unidirectional transport.  ","version":"v5","tagName":"h3"},{"title":"client_queue_max_size​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_queue_max_size","content":" Default: 1048576  Maximum client message queue size in bytes to close slow reader connections. By default - 1mb.  ","version":"v5","tagName":"h3"},{"title":"client_concurrency​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_concurrency","content":" Default: 0  client_concurrency when set tells Centrifugo that commands from a client must be processed concurrently.  By default, concurrency disabled – Centrifugo processes commands received from a client one by one. This means that if a client issues two RPC requests to a server then Centrifugo will process the first one, then the second one. If the first RPC call is slow then the client will wait for the second RPC response much longer than it could (even if the second RPC is very fast). If you set client_concurrency to some value greater than 1 then commands will be processed concurrently (in parallel) in separate goroutines (with maximum concurrency level capped by client_concurrency value). Thus, this option can effectively reduce the latency of individual requests. Since separate goroutines are involved in processing this mode adds some performance and memory overhead – though it should be pretty negligible in most cases. This option applies to all commands from a client (including subscribe, publish, presence, etc).  ","version":"v5","tagName":"h3"},{"title":"client_stale_close_delay​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_stale_close_delay","content":" Duration, default: 10s  This option allows tuning the maximum time Centrifugo will wait for the connect frame (which contains authentication information) from the client after establishing connection. Default value should be reasonable for most use cases.  ","version":"v5","tagName":"h3"},{"title":"client_user_id_http_header​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#client_user_id_http_header","content":" String, default: &quot;&quot;  Available since v5.1.1  Usually to authenticate client connections with Centrifugo you need to use JWT authentication or connect proxy. Sometimes though it may be convenient to pass user ID information in incoming HTTP request headers. This is usually the case when application backend infrastructure has some authentication proxy (like Envoy, etc). This proxy may set authenticated user ID to some header and proxy requests further to Centrifugo.  When client_user_id_http_header is set to some non-empty header name Centrifugo will try to extract the authenticated user ID for client connections from that header. This mechanism works for all real-time transports based on HTTP (this also includes WebSocket since it starts with HTTP Upgrade request). Example:  config.json { ... &quot;client_user_id_http_header&quot;: &quot;X-User-Id&quot; }   When using this way for user authentication – you can not set connection expiration and additional connection info which is possible to do using other authentication ways mentioned above.  Security warning When using authentication over proxy ensure your proxy strips the header you are using for auth if it comes from the client or forbids such requests to avoid malicious usage. Only your authentication proxy must set the header with user ID.  ","version":"v5","tagName":"h3"},{"title":"allow_anonymous_connect_without_token​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#allow_anonymous_connect_without_token","content":" Default: false  Enable a mode when all clients can connect to Centrifugo without JWT. In this case, all connections without a token will be treated as anonymous (i.e. with empty user ID). Access to channel operations should be explicitly enabled for anonymous connections.  ","version":"v5","tagName":"h3"},{"title":"disallow_anonymous_connection_tokens​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#disallow_anonymous_connection_tokens","content":" Default: false  When the option is set Centrifugo won't accept connections from anonymous users even if they provided a valid JWT. I.e. if token is valid, but sub claim is empty – then Centrifugo closes connection with advice to not reconnect again.  ","version":"v5","tagName":"h3"},{"title":"gomaxprocs​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#gomaxprocs","content":" Default: 0  By default, Centrifugo runs on all available CPU cores (also Centrifugo can look at cgroup limits when rnning in Docker/Kubernetes). To limit the number of cores Centrifugo can utilize in one moment use this option.  ","version":"v5","tagName":"h3"},{"title":"Endpoint configuration​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#endpoint-configuration","content":" After Centrifugo started there are several endpoints available.  ","version":"v5","tagName":"h2"},{"title":"Default endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#default-endpoints","content":" Bidirectional WebSocket default endpoint:  ws://localhost:8000/connection/websocket   Bidirectional emulation with HTTP-streaming (disabled by default):  ws://localhost:8000/connection/http_stream   Bidirectional emulation with SSE (EventSource) (disabled by default):  ws://localhost:8000/connection/sse   Bidirectional SockJS default endpoint (disabled by default):  http://localhost:8000/connection/sockjs   Unidirectional EventSource endpoint (disabled by default):  http://localhost:8000/connection/uni_sse   Unidirectional HTTP streaming endpoint (disabled by default):  http://localhost:8000/connection/uni_http_stream   Unidirectional WebSocket endpoint (disabled by default):  http://localhost:8000/connection/uni_websocket   Unidirectional SSE (EventSource) endpoint (disabled by default):  http://localhost:8000/connection/uni_sse   Server HTTP API endpoint:  http://localhost:8000/api   By default, all endpoints work on port 8000. This can be changed with port option:  { &quot;port&quot;: 9000 }   In production setup, you may have a proper domain name in endpoint addresses above instead of localhost. While domain name and port parts can differ depending on setup – URL paths stay the same: /connection/sockjs, /connection/websocket, /api etc.  ","version":"v5","tagName":"h3"},{"title":"Admin endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#admin-endpoints","content":" Admin web UI endpoint works on root path by default, i.e. http://localhost:8000.  For more details about admin web UI, refer to the Admin web UI documentation.  ","version":"v5","tagName":"h3"},{"title":"Debug endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#debug-endpoints","content":" Next, when Centrifugo started in debug mode some extra debug endpoints become available. To start in debug mode add debug option to config:  { ... &quot;debug&quot;: true }   And endpoint:  http://localhost:8000/debug/pprof/   – will show useful information about the internal state of Centrifugo instance. This info is especially helpful when troubleshooting. See wiki page for more info.  ","version":"v5","tagName":"h3"},{"title":"Health check endpoint​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#health-check-endpoint","content":" Use health boolean option (by default false) to enable the health check endpoint which will be available on path /health. Also available over command-line flag:  centrifugo -c config.json --health   ","version":"v5","tagName":"h3"},{"title":"Swagger UI for server API​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#swagger-ui-for-server-api","content":" Use swagger boolean option (by default false) to enable Swagger UI for server HTTP API. UI will be available on path /swagger. Also available over command-line flag:  centrifugo -c config.json --swagger   ","version":"v5","tagName":"h3"},{"title":"Custom internal ports​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#custom-internal-ports","content":" We strongly recommend not expose API, admin, debug, health, and Prometheus endpoints to the Internet. The following Centrifugo endpoints are considered internal:  API endpoint (/api) - for HTTP API requestsAdmin web interface endpoints (/, /admin/auth, /admin/api) - used by web interfacePrometheus endpoint (/metrics) - used for exposing server metrics in Prometheus formatHealth check endpoint (/health) - used to do health checksDebug endpoints (/debug/pprof) - used to inspect internal server stateSwagger UI endpoint (/swagger) - used for showing embedded Swagger UI for server HTTP API  It's a good practice to protect all these endpoints with a firewall. For example, it's possible to configure in location section of the Nginx configuration.  Though sometimes you don't have access to a per-location configuration in your proxy/load balancer software. For example when using Amazon ELB. In this case, you can change ports on which your internal endpoints work.  To run internal endpoints on custom port use internal_port option:  { ... &quot;internal_port&quot;: 9000 }   So admin web interface will work on address:  http://localhost:9000   Also, debug page will be available on a new custom port too:  http://localhost:9000/debug/pprof/   The same for API and Prometheus endpoints.  ","version":"v5","tagName":"h3"},{"title":"Disable default endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#disable-default-endpoints","content":" To disable websocket endpoint set websocket_disable boolean option to true.  To disable API endpoint set api_disable boolean option to true.  ","version":"v5","tagName":"h3"},{"title":"Customize handler endpoints​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#customize-handler-endpoints","content":" It's possible to customize server HTTP handler endpoints. To do this Centrifugo supports several options:  admin_handler_prefix (default &quot;&quot;) - to control Admin panel URL prefixwebsocket_handler_prefix (default &quot;/connection/websocket&quot;) - to control WebSocket URL prefixhttp_stream_handler_prefix (default &quot;/connection/http_stream&quot;) - to control HTTP-streaming URL prefixsse_handler_prefix (default &quot;/connection/sse&quot;) - to control SSE/EventSource URL prefixemulation_handler_prefix (default &quot;/emulation&quot;) - to control emulation endpoint prefixsockjs_handler_prefix (default &quot;/connection/sockjs&quot;) - to control SockJS URL prefixuni_sse_handler_prefix (default &quot;/connection/uni_sse&quot;) - to control unidirectional Eventsource URL prefixuni_http_stream_handler_prefix (default &quot;/connection/uni_http_stream&quot;) - to control unidirectional HTTP streaming URL prefixuni_websocket_handler_prefix (default &quot;/connection/uni_websocket&quot;) - to control unidirectional WebSocket URL prefixapi_handler_prefix (default &quot;/api&quot;) - to control HTTP API URL prefixprometheus_handler_prefix (default &quot;/metrics&quot;) - to control Prometheus URL prefixhealth_handler_prefix (default &quot;/health&quot;) - to control health check URL prefix  ","version":"v5","tagName":"h3"},{"title":"Signal handling​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#signal-handling","content":" It's possible to send HUP signal to Centrifugo to reload a configuration:  kill -HUP &lt;PID&gt;   Though at moment this will only reload token secrets and channel options (top-level and namespaces).  Centrifugo tries to gracefully shut down client connections when SIGINT or SIGTERM signals are received. By default, the maximum graceful shutdown period is 30 seconds but can be changed using shutdown_timeout (integer, in seconds) configuration option.  ","version":"v5","tagName":"h2"},{"title":"Insecure modes​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#insecure-modes","content":" ","version":"v5","tagName":"h2"},{"title":"Insecure client connection​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#insecure-client-connection","content":" The boolean option client_insecure (default false) allows connecting to Centrifugo without JWT token. In this mode, there is no user authentication involved. It also disables permission checks on client API level - for presence and history calls. This mode can be useful for demo projects based on Centrifugo, integration tests, local projects, or real-time application prototyping. Don't use it in production until you 100% know what you are doing.  ","version":"v5","tagName":"h3"},{"title":"Disable client token signature check​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#disable-client-token-signature-check","content":" Available since Centrifugo v5.0.4  The boolean option client_insecure_skip_token_signature_verify (default false), if enabled – tells Centrifugo to skip JWT signature verification - for both connection and subscription tokens. This is absolutely insecure and must only be used for development and testing purposes. Token claims are parsed as usual - so token should still follow JWT format.  ","version":"v5","tagName":"h3"},{"title":"Insecure API mode​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#insecure-api-mode","content":" This mode can be enabled using the boolean option api_insecure (default false). When on there is no need to provide API key in HTTP requests. When using this mode everyone that has access to /api endpoint can send any command to server. Enabling this option can be reasonable if /api endpoint is protected by firewall rules.  The option is also useful in development to simplify sending API commands to Centrifugo using CURL for example without specifying Authorization header in requests.  ","version":"v5","tagName":"h3"},{"title":"Insecure admin mode​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#insecure-admin-mode","content":" This mode can be enabled using the boolean option admin_insecure (default false). When on there is no authentication in the admin web interface. Again - this is not secure but can be justified if you protected the admin interface by firewall rules or you want to use basic authentication for the Centrifugo admin interface (configured on proxy level).  ","version":"v5","tagName":"h3"},{"title":"Setting time duration options​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#setting-time-duration-options","content":" Time durations in Centrifugo can be set using strings where duration value and unit are both provided. For example, to set 5 seconds duration use &quot;5s&quot;.  The minimal time resolution is 1ms. Some options of Centrifugo only support second precision (for example history_ttl channel option).  Valid time units are &quot;ms&quot; (milliseconds), &quot;s&quot; (seconds), &quot;m&quot; (minutes), &quot;h&quot; (hours).  Some examples:  &quot;1000ms&quot; // 1000 milliseconds &quot;1s&quot; // 1 second &quot;12h&quot; // 12 hours &quot;720h&quot; // 30 days   ","version":"v5","tagName":"h2"},{"title":"Setting namespaces over env​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#setting-namespaces-over-env","content":" While setting most options in Centrifugo over env is pretty straightforward setting namespaces is a bit special:  CENTRIFUGO_NAMESPACES='[{&quot;name&quot;: &quot;ns1&quot;}, {&quot;name&quot;: &quot;ns2&quot;}]' ./centrifugo   I.e. CENTRIFUGO_NAMESPACES environment variable should be a valid JSON string that represents namespaces array.  ","version":"v5","tagName":"h2"},{"title":"Anonymous usage stats​","type":1,"pageTitle":"Configure Centrifugo","url":"/docs/server/configuration#anonymous-usage-stats","content":" Centrifugo periodically sends anonymous usage information (once in 24 hours). That information is impersonal and does not include sensitive data, passwords, IP addresses, hostnames, etc. Only counters to estimate version and installation size distribution, and feature usage.  Please do not disable usage stats sending without reason. If you depend on Centrifugo – sure you are interested in further project improvements. Usage stats help us understand Centrifugo use cases better, concentrate on widely-used features, and be confident we are moving in the right direction. Developing in the dark is hard, and decisions may be non-optimal.  To disable sending usage stats set usage_stats_disable option:  config.json { &quot;usage_stats_disable&quot;: true }  ","version":"v5","tagName":"h2"},{"title":"Helper CLI commands","type":0,"sectionRef":"#","url":"/docs/server/console_commands","content":"","keywords":"","version":"v5"},{"title":"version​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/server/console_commands#version","content":" To show Centrifugo version and exit run:  centrifugo version   ","version":"v5","tagName":"h2"},{"title":"genconfig​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/server/console_commands#genconfig","content":" Another command is genconfig:  centrifugo genconfig -c config.json   It will automatically generate the minimal required configuration file. This is mostly useful for development.  If any errors happen – program will exit with error message and exit code 1.  genconfig also supports generation of YAML and TOML configuration file formats - just provide an extension to a file:  centrifugo genconfig -c config.toml   ","version":"v5","tagName":"h2"},{"title":"checkconfig​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/server/console_commands#checkconfig","content":" Centrifugo has special command to check configuration file checkconfig:  centrifugo checkconfig --config=config.json   If any errors found during validation – program will exit with error message and exit code 1.  ","version":"v5","tagName":"h2"},{"title":"gentoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/server/console_commands#gentoken","content":" Another command is gentoken:  centrifugo gentoken -c config.json -u 28282   It will automatically generate HMAC SHA-256 based token for user with ID 28282 (which expires in 1 week).  You can change token TTL with -t flag (number of seconds):  centrifugo gentoken -c config.json -u 28282 -t 3600   This way generated token will be valid for 1 hour.  If any errors happen – program will exit with error message and exit code 1.  This command is mostly useful for development.  ","version":"v5","tagName":"h2"},{"title":"gensubtoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/server/console_commands#gensubtoken","content":" Another command is gensubtoken:  centrifugo gensubtoken -c config.json -u 28282 -s channel   It will automatically generate HMAC SHA-256 based subscription token for channel channel and user with ID 28282 (which expires in 1 week).  You can change token TTL with -t flag (number of seconds):  centrifugo gentoken -c config.json -u 28282 -s channel -t 3600   This way generated token will be valid for 1 hour.  If any errors happen – program will exit with error message and exit code 1.  This command is mostly useful for development.  ","version":"v5","tagName":"h2"},{"title":"checktoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/server/console_commands#checktoken","content":" One more command is checktoken:  centrifugo checktoken -c config.json &lt;TOKEN&gt;   It will validate your connection JWT, so you can test it before using while developing application.  If any errors happen or validation failed – program will exit with error message and exit code 1.  This is mostly useful for development.  ","version":"v5","tagName":"h2"},{"title":"checksubtoken​","type":1,"pageTitle":"Helper CLI commands","url":"/docs/server/console_commands#checksubtoken","content":" One more command is checksubtoken:  centrifugo checksubtoken -c config.json &lt;TOKEN&gt;   It will validate your subscription JWT, so you can test it before using while developing application.  If any errors happen or validation failed – program will exit with error message and exit code 1.  This is mostly useful for development. ","version":"v5","tagName":"h2"},{"title":"Built-in API command async consumers","type":0,"sectionRef":"#","url":"/docs/server/consumers","content":"","keywords":"","version":"v5"},{"title":"Supported consumers​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#supported-consumers","content":" The following built-in async consumers are available at this point:  Consumer from PostgreSQL outbox table (since Centrifugo v5.2.0)Consumer from Kafka topics (since Centrifugo v5.2.0)  ","version":"v5","tagName":"h2"},{"title":"How it works​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#how-it-works","content":" Consumers expect to consume messages which represent Centrifugo server API commands. I.e. while in synchronous server API you are using HTTP or GRPC to send commands – with asynchronous consumers you are inserting API command to PostgreSQL outbox table, or delivering to Kafka topic – and it will be soon consumed and processed asynchronously by Centrifugo.  Async consumers only process commands which modify state – such as publish, broadcast, unsubscribe, disconnect, etc. Sending read commands for async execution simply does not make any sense and they will be ignored. Also, batch method is not supported.  Centrifugo only supports JSON payloads for asynchronous commands coming to consumers for now. If you need binary format – reach out with your use case.  If Centrifugo encounters an error while processing consumed messages – then internal errors will be retried, all other errors logged on error level – and the message will be marked as processed. The processing logic for broadcast API is special: if any of the publications to any channel from broadcast channels array failed – then the entire broadcast command will be retried. To prevent duplicate messages being published during such retries – consider using idempotency_key in the broadcast command.  tip Our Chat/Messenger tutorial shows PostgreSQL outbox and Kafka consumer in action. It also shows techniques to avoid duplicate messages (idempotent publications) and deal with late message delivery (idempotent processing on client side). Whether you need those techniques – depends on the nature of app. Various real-time features may require different ways of sending real-time events. Both synchronous API calls and async calls have its own advantages and trade-offs. We also talk about this in Asynchronous message streaming to Centrifugo with Benthos blog post.  ","version":"v5","tagName":"h2"},{"title":"Common consumer options​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#common-consumer-options","content":" Consumers can be set in the configuration using consumers array:  { ... &quot;consumers&quot;: [ { &quot;name&quot;: &quot;xxx&quot;, &quot;type&quot;: &quot;postgresql&quot;, &quot;postgresql&quot;: {...} }, { &quot;name&quot;: &quot;yyy&quot;, &quot;type&quot;: &quot;kafka&quot;, &quot;kafka&quot;: {...} }, ] }   On top level each consumer object has the following fields:  name - string (required), described name of consumer. Must be unique for each consumer and match the regex ^[a-zA-Z0-9_]{2,} - i.e. latin symbols, digits and underscores and be at least 2 symbols. This name will be used for logging purposes, metrics, also to override some options with environment variables.type - string (required), type of consumer. At this point can be postgresql or kafkadisabled - boolean (default: false), when set to true allows disabling the consumer  To provide consumers over environment variable provide CENTRIFUGO_CONSUMERS var with JSON array serialized to string. It's also possible to override some specific consumer options over environment variables – see below.  ","version":"v5","tagName":"h2"},{"title":"PostgreSQL outbox consumer​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#postgresql-outbox-consumer","content":" Centrifugo can natively integrate with PostgreSQL table for Transactional outbox pattern. The table in PostgreSQL must have predefined format Centrifugo expects:  CREATE TABLE IF NOT EXISTS centrifugo_outbox ( id BIGSERIAL PRIMARY KEY, method text NOT NULL, payload JSONB NOT NULL, partition INTEGER NOT NULL default 0, created_at TIMESTAMP WITH TIME ZONE DEFAULT now() NOT NULL );   Then configure consumer of postgresql type in Centrifugo config:  { ... &quot;consumers&quot;: [ { &quot;name&quot;: &quot;my_postgresql_consumer&quot;, &quot;type&quot;: &quot;postgresql&quot;, &quot;postgresql&quot;: { &quot;dsn&quot;: &quot;postgresql://user:password@localhost:5432/db&quot;, &quot;outbox_table_name&quot;: &quot;centrifugo_outbox&quot;, &quot;num_partitions&quot;: 1, &quot;partition_select_limit&quot;: 100, &quot;partition_poll_interval&quot;: &quot;300ms&quot; } } ] }   Here is how you can insert row in outbox table to publish into Centrifugo channel:  INSERT INTO centrifugo_outbox (method, payload, partition) VALUES ('publish', '{&quot;channel&quot;: &quot;updates&quot;, &quot;data&quot;: {&quot;text&quot;: &quot;Hello, world!&quot;}}', 0);   Centrifugo supports LISTEN/NOTIFY mechanism of PostgreSQL to be notified about new data in the outbox table. To enable it you need first create a trigger in PostgreSQL:  CREATE OR REPLACE FUNCTION centrifugo_notify_partition_change() RETURNS TRIGGER AS $$ BEGIN PERFORM pg_notify('centrifugo_partition_change', NEW.partition::text); RETURN NEW; END; $$ LANGUAGE plpgsql; CREATE OR REPLACE TRIGGER centrifugo_notify_partition_trigger AFTER INSERT ON chat_outbox FOR EACH ROW EXECUTE FUNCTION centrifugo_notify_partition_change();   And then update consumer config – add &quot;partition_notification_channel&quot; option to it:  { ... &quot;consumers&quot;: [ { &quot;name&quot;: &quot;my_postgresql_consumer&quot;, &quot;type&quot;: &quot;postgresql&quot;, &quot;postgresql&quot;: { ... &quot;partition_notification_channel&quot;: &quot;centrifugo_partition_change&quot; } } ] }   ","version":"v5","tagName":"h2"},{"title":"PostgreSQL consumer options​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#postgresql-consumer-options","content":" dsn - string (required), DSN to PostgreSQL database, ex. &quot;postgresql://user:password@localhost:5432/db&quot;. To override dsn over environment variables use CENTRIFUGO_CONSUMERS_POSTGRESQL_&lt;CONSUMER_NAME&gt;_DSN.outbox_table_name - string (required), the name of outbox table in selected database, ex. &quot;centrifugo_outbox&quot;num_partitions - integer (default: 1), the number of partitions to use. Centrifugo keeps strict order of commands per-partition by default. This option provides a way to create concurrent consumers each consuming from different partition of outbox table. Note, that partition numbers in start with 0, so when using 1 as num_partitions insert data with partition == 0 to the outbox table.partition_select_limit - integer (default: 100) – max number of commands to select in one query to outbox table.partition_poll_interval - duration (default: &quot;300ms&quot;) - polling interval for each partitionpartition_notification_channel - string (default: &quot;&quot;) - optional name of LISTEN/NOTIFY channel to trigger consuming upon data added to outbox partition.  ","version":"v5","tagName":"h3"},{"title":"Kafka consumer​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#kafka-consumer","content":" Another built-in consumer – is Kafka topics consumer. To configure Centrifugo to consume Kafka topic:   ... &quot;consumers&quot;: [ { &quot;name&quot;: &quot;my_kafka_consumer&quot;, &quot;type&quot;: &quot;kafka&quot;, &quot;kafka&quot;: { &quot;brokers&quot;: [&quot;localhost:9092&quot;], &quot;topics&quot;: [&quot;postgres.public.chat_cdc&quot;], &quot;consumer_group&quot;: &quot;centrifugo&quot; } } ] }   Then simply put message in the following format to Kafka topic:  { &quot;method&quot;: &quot;publish&quot;, &quot;payload&quot;: { &quot;channel&quot;: &quot;mychannel&quot;, &quot;data&quot;: {} } }   – and it will be consumed by Centrifugo and reliably processed.  ","version":"v5","tagName":"h2"},{"title":"Kafka consumer options​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#kafka-consumer-options","content":" brokers - array of strings (required), points Centrifugo to Kafka brokers. To override brokers over environment variables use CENTRIFUGO_CONSUMERS_KAFKA_&lt;CONSUMER_NAME&gt;_BROKERS – string with broker addresses separated by space.topics - array of string (required), tells which topics to consumeconsumer_group - string (required), sets the name of consumer group to usemax_poll_records - integer (default: 100) - sets the maximum number of records to fetch from Kafka during a single poll operation.sasl_mechanism - only &quot;plain&quot; is now supportedsasl_user - string, user for plain SASL auth. To override sasl_user over environment variables use CENTRIFUGO_CONSUMERS_KAFKA_&lt;CONSUMER_NAME&gt;_SASL_USER.sasl_password - string, password for plain SASL auth. To override sasl_password over environment variables use CENTRIFUGO_CONSUMERS_KAFKA_&lt;CONSUMER_NAME&gt;_SASL_PASSWORD.tls - boolean (default: false) - enables TLS, if true – then all the TLS options may be additionally set.  ","version":"v5","tagName":"h3"},{"title":"Kafka TLS options​","type":1,"pageTitle":"Built-in API command async consumers","url":"/docs/server/consumers#kafka-tls-options","content":" tls_cert – string representing the path on disk to the TLS certificate. This is typically used for the server certificate in TLS connections.tls_key – string representing the path on disk to the TLS key associated with the certificate.tls_cert_pem – string containing the PEM (Privacy Enhanced Mail) encoded TLS certificate.tls_key_pem – string containing the PEM encoded TLS key.tls_root_ca – string representing the path on disk to the root CA certificate. This is used to verify the server certificate.tls_root_ca_pem – string containing the PEM encoded root CA certificate.tls_client_ca – string representing the path on disk to the client CA certificate. This is used in mutual TLStls_client_ca_pem – string containing the PEM encoded client CA certificate.tls_insecure_skip_verify – boolean indicating whether to skip verifying the server's certificate chain and host name. If true, TLS accepts any certificate presented by the server and any host name in that certificate.server_name – string specifying the server name to use for server certificate verification if tls_insecure_skip_verify is false. This is also used in the client's SNI (Server Name Indication) extension in TLS handshake. ","version":"v5","tagName":"h3"},{"title":"Engines and scalability","type":0,"sectionRef":"#","url":"/docs/server/engines","content":"","keywords":"","version":"v5"},{"title":"Memory engine​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#memory-engine","content":" Used by default. Supports only one node. Nice choice to start with. Supports all features keeping everything in Centrifugo node process memory. You don't need to install Redis when using this engine.  Advantages:  Super fast since it does not involve network at allDoes not require separate broker setup  Disadvantages:  Does not allow scaling nodes (actually you still can scale Centrifugo with Memory engine but you have to publish data into each Centrifugo node and you won't have consistent history and presence state throughout Centrifugo nodes)Does not persist message history in channels between Centrifugo restarts.  ","version":"v5","tagName":"h2"},{"title":"Memory engine options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#memory-engine-options","content":" history_meta_ttl​  Duration, default 2160h (90 days).  history_meta_ttl sets a time of history stream metadata expiration.  When using a history in a channel, Centrifugo keeps some metadata for it. Metadata includes the latest stream offset and its epoch value. In some cases, when channels are created for а short time and then not used anymore, created metadata can stay in memory while not useful. For example, you can have a personal user channel but after using your app for a while user left it forever. From a long-term perspective, this can be an unwanted memory growth. Setting a reasonable value to this option can help to expire metadata faster (or slower if you need it). The rule of thumb here is to keep this value much bigger than maximum history TTL used in Centrifugo configuration.  ","version":"v5","tagName":"h3"},{"title":"Redis engine​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#redis-engine","content":" Redis is an open-source, in-memory data structure store, used as a database, cache, and message broker.  Centrifugo Redis engine allows scaling Centrifugo nodes to different machines. Nodes will use Redis as a message broker (utilizing Redis PUB/SUB for node communication) and keep presence and history data in Redis.  Minimal Redis version is 5.0.1  With Redis it's possible to come to the architecture like this:    ","version":"v5","tagName":"h2"},{"title":"Redis engine options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#redis-engine-options","content":" Several configuration options related to Redis engine.  redis_address​  String, default &quot;127.0.0.1:6379&quot; - Redis server address.  redis_password​  String, default &quot;&quot; - Redis password.  redis_user​  String, default &quot;&quot; - Redis user for ACL-based auth.  redis_db​  Integer, default 0 - number of Redis db to use.  redis_prefix​  String, default &quot;centrifugo&quot; – custom prefix to use for channels and keys in Redis.  redis_use_lists​  Boolean, default false – turns on using Redis Lists instead of Stream data structure for keeping history (not recommended, keeping this for backwards compatibility mostly).  redis_force_resp2​  Boolean, default false. If set to true it forces using RESP2 protocol for communicating with Redis. By default, Redis client used by Centrifugo tries to detect supported Redis protocol automatically trying RESP3 first.  history_meta_ttl​  Duration, default 2160h (90 days).  history_meta_ttl sets a time of history stream metadata expiration.  Similar to a Memory engine Redis engine also looks at history_meta_ttl option. Meta key in Redis is a HASH that contains the current offset number in channel and the stream epoch value.  When using a history in a channel, Centrifugo saves metadata for it. Metadata includes the latest stream offset and its epoch value. In some cases, when channels are created for а short time and then not used anymore, created metadata can stay in memory while not useful. For example, you can have a personal user channel but after using your app for a while user left it forever. From a long-term perspective, this can be an unwanted memory growth. Setting a reasonable value to this option can help. The rule of thumb here is to keep this value much bigger than maximum history TTL used in Centrifugo configuration.  ","version":"v5","tagName":"h3"},{"title":"Configuring Redis TLS​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#configuring-redis-tls","content":" Some options may help you configuring TLS-protected communication between Centrifugo and Redis.  redis_tls​  Boolean, default false - enable Redis TLS connection.  redis_tls_insecure_skip_verify​  Boolean, default false - disable Redis TLS host verification. Centrifugo v4 also supports alias for this option – redis_tls_skip_verify – but it will be removed in v5.  redis_tls_cert​  String, default &quot;&quot; – path to TLS cert file. If you prefer passing certificate as a string instead of path to the file then use redis_tls_cert_pem option.  redis_tls_key​  String, default &quot;&quot; – path to TLS key file. If you prefer passing cert key as a string instead of path to the file then use redis_tls_key_pem option.  redis_tls_root_ca​  String, default &quot;&quot; – path to TLS root CA file (in PEM format) to use. If you prefer passing root CA PEM as a string instead of path to the file then use redis_tls_root_ca_pem option.  redis_tls_server_name​  String, default &quot;&quot; – used to verify the hostname on the returned certificates. It is also included in the client's handshake to support virtual hosting unless it is an IP address.  ","version":"v5","tagName":"h3"},{"title":"Scaling with Redis tutorial​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#scaling-with-redis-tutorial","content":" Let's see how to start several Centrifugo nodes using the Redis Engine. We will start 3 Centrifugo nodes and all those nodes will be connected via Redis.  First, you should have Redis running. As soon as it's running - we can launch 3 Centrifugo instances. Open your terminal and start the first one:  centrifugo --config=config.json --port=8000 --engine=redis --redis_address=127.0.0.1:6379   If your Redis is on the same machine and runs on its default port you can omit redis_address option in the command above.  Then open another terminal and start another Centrifugo instance:  centrifugo --config=config.json --port=8001 --engine=redis --redis_address=127.0.0.1:6379   Note that we use another port number (8001) as port 8000 is already busy by our first Centrifugo instance. If you are starting Centrifugo instances on different machines then you most probably can use the same port number (8000 or whatever you want) for all instances.  And finally, let's start the third instance:  centrifugo --config=config.json --port=8002 --engine=redis --redis_address=127.0.0.1:6379   Now you have 3 Centrifugo instances running on ports 8000, 8001, 8002 and clients can connect to any of them. You can also send API requests to any of those nodes – as all nodes connected over Redis PUB/SUB message will be delivered to all interested clients on all nodes.  To load balance clients between nodes you can use Nginx – you can find its configuration here in the documentation.  tip In the production environment you will most probably run Centrifugo nodes on different hosts, so there will be no need to use different port numbers.  Here is a live example where we locally start two Centrifugo nodes both connected to local Redis:  Sorry, your browser doesn't support embedded video.  ","version":"v5","tagName":"h3"},{"title":"Redis Sentinel for high availability​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#redis-sentinel-for-high-availability","content":" Centrifugo supports the official way to add high availability to Redis - Redis Sentinel.  For this you only need to utilize 2 Redis Engine options: redis_sentinel_address and redis_sentinel_master_name:  redis_sentinel_address (string, default &quot;&quot;) - comma separated list of Sentinel addresses for HA. At least one known server required.redis_sentinel_master_name (string, default &quot;&quot;) - name of Redis master Sentinel monitors  Also:  redis_sentinel_password – optional string password for your Sentinel, works with Redis Sentinel &gt;= 5.0.1redis_sentinel_user - optional string user (used only in Redis ACL-based auth).  So you can start Centrifugo which will use Sentinels to discover Redis master instances like this:  centrifugo --config=config.json   Where config.json:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_sentinel_address&quot;: &quot;127.0.0.1:26379&quot;, &quot;redis_sentinel_master_name&quot;: &quot;mymaster&quot; }   Sentinel configuration file may look like this (for 3-node Sentinel setup with quorum 2):  port 26379 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 10000 sentinel failover-timeout mymaster 60000   You can find how to properly set up Sentinels in official documentation.  Note that when your Redis master instance is down there will be a small downtime interval until Sentinels discover a problem and come to a quorum decision about a new master. The length of this period depends on Sentinel configuration.  ","version":"v5","tagName":"h3"},{"title":"Redis Sentinel TLS​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#redis-sentinel-tls","content":" To configure TLS for Redis Sentinel use the following options.  redis_sentinel_tls​  Boolean, default false - enable Redis TLS connection.  redis_sentinel_tls_insecure_skip_verify​  Boolean, default false - disable Redis TLS host verification. Centrifugo v4 also supports alias for this option – redis_sentinel_tls_skip_verify – but it will be removed in v5.  redis_sentinel_tls_cert​  String, default &quot;&quot; – path to TLS cert file. If you prefer passing certificate as a string instead of path to the file then use redis_sentinel_tls_cert_pem option.  redis_sentinel_tls_key​  String, default &quot;&quot; – path to TLS key file. If you prefer passing cert key as a string instead of path to the file then use redis_sentinel_tls_key_pem option.  redis_sentinel_tls_root_ca​  String, default &quot;&quot; – path to TLS root CA file (in PEM format) to use. If you prefer passing root CA PEM as a string instead of path to the file then use redis_sentinel_tls_root_ca_pem option.  redis_sentinel_tls_server_name​  String, default &quot;&quot; – used to verify the hostname on the returned certificates. It is also included in the client's handshake to support virtual hosting unless it is an IP address.  ","version":"v5","tagName":"h3"},{"title":"Haproxy instead of Sentinel configuration​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#haproxy-instead-of-sentinel-configuration","content":" Alternatively, you can use Haproxy between Centrifugo and Redis to let it properly balance traffic to Redis master. In this case, you still need to configure Sentinels but you can omit Sentinel specifics from Centrifugo configuration and just use Redis address as in a simple non-HA case.  For example, you can use something like this in Haproxy config:  listen redis server redis-01 127.0.0.1:6380 check port 6380 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 server redis-02 127.0.0.1:6381 check port 6381 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 backup bind *:16379 mode tcp option tcpka option tcplog option tcp-check tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK balance roundrobin   And then just point Centrifugo to this Haproxy:  centrifugo --config=config.json --engine=redis --redis_address=&quot;localhost:16379&quot;   ","version":"v5","tagName":"h3"},{"title":"Redis sharding​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#redis-sharding","content":" Centrifugo has built-in Redis sharding support.  This resolves the situation when Redis becoming a bottleneck on a large Centrifugo setup. Redis is a single-threaded server, it's very fast but its power is not infinite so when your Redis approaches 100% CPU usage then the sharding feature can help your application to scale.  At moment Centrifugo supports a simple comma-based approach to configuring Redis shards. Let's just look at examples.  To start Centrifugo with 2 Redis shards on localhost running on port 6379 and port 6380 use config like this:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: [ &quot;127.0.0.1:6379&quot;, &quot;127.0.0.1:6380&quot;, ] }   To start Centrifugo with Redis instances on different hosts:  config.json { ... &quot;engine&quot;: &quot;redis&quot;, &quot;redis_address&quot;: [ &quot;192.168.1.34:6379&quot;, &quot;192.168.1.35:6379&quot;, ] }   If you also need to customize AUTH password, Redis DB number then you can use an extended address notation.  note Due to how Redis PUB/SUB works it's not possible (and it's pretty useless anyway) to run shards in one Redis instance using different Redis DB numbers.  When sharding enabled Centrifugo will spread channels and history/presence keys over configured Redis instances using a consistent hashing algorithm. At moment we use Jump consistent hash algorithm (see paper and implementation).  ","version":"v5","tagName":"h3"},{"title":"Redis cluster​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#redis-cluster","content":" Running Centrifugo with Redis cluster is simple and can be achieved using redis_cluster_address option. This is an array of strings. Each element of the array is a comma-separated Redis cluster seed node. For example:  { ... &quot;redis_cluster_address&quot;: [ &quot;localhost:30001,localhost:30002,localhost:30003&quot; ] }   You don't need to list all Redis cluster nodes in config – only several working nodes are enough to start.  To set the same over environment variable:  CENTRIFUGO_REDIS_CLUSTER_ADDRESS=&quot;localhost:30001&quot; CENTRIFUGO_ENGINE=redis ./centrifugo   If you need to shard data between several Redis clusters then simply add one more string with seed nodes of another cluster to this array:  { ... &quot;redis_cluster_address&quot;: [ &quot;localhost:30001,localhost:30002,localhost:30003&quot;, &quot;localhost:30101,localhost:30102,localhost:30103&quot; ] }   Sharding between different Redis clusters can make sense due to the fact how PUB/SUB works in the Redis cluster. It does not scale linearly when adding nodes as all PUB/SUB messages got copied to every cluster node. See this discussion for more information on topic. To spread data between different Redis clusters Centrifugo uses the same consistent hashing algorithm described above (i.e. Jump).  To reproduce the same over environment variable use space to separate different clusters:  CENTRIFUGO_REDIS_CLUSTER_ADDRESS=&quot;localhost:30001,localhost:30002 localhost:30101,localhost:30102&quot; CENTRIFUGO_ENGINE=redis ./centrifugo   ","version":"v5","tagName":"h3"},{"title":"Other Redis compatible​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#other-redis-compatible","content":" When using Redis engine it's possible to point Centrifugo not only to Redis itself, but also to the other Redis compatible server. Such servers may work just fine if implement Redis protocol and support all the data structures Centrifugo uses and have PUB/SUB implemented.  Some known options:  AWS Elasticache – it was reported to work, but we suggest you testing the setup including failover tests and work under load.KeyDB – should work fine with Centrifugo, no known problems at this point regarding Centrifugo compatibility.DragonflyDB - should work fine starting from DragonflyDB 1.3.0 and with redis_force_resp2 Centrifugo option on. We have not tested a Redis Cluster emulation mode provided by DragonflyDB yet. We suggest you testing the setup including failover tests and work under load.  ","version":"v5","tagName":"h2"},{"title":"Tarantool engine​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#tarantool-engine","content":" EXPERIMENTAL  Tarantool is a fast and flexible in-memory storage with different persistence/replication schemes and LuaJIT for writing custom logic on the Tarantool side. It allows implementing Centrifugo engine with unique characteristics.  caution EXPERIMENTAL status of Tarantool integration means that we are still going to improve it and there could be breaking changes as integration evolves.  There are many ways to operate Tarantool in production and it's hard to distribute Centrifugo Tarantool engine in a way that suits everyone. Centrifugo tries to fit generic case by providing centrifugal/tarantool-centrifuge module and by providing ready-to-use centrifugal/rotor project based on centrifugal/tarantool-centrifuge and Tarantool Cartridge.  info To be honest we bet on the community help to push this integration further. Tarantool provides an incredible performance boost for presence and history operations (up to 5x more RPS compared to the Redis Engine) and a pretty fast PUB/SUB (comparable to what Redis Engine provides). Let's see what we can build together.  There are several supported Tarantool topologies to which Centrifugo can connect:  One standalone Tarantool instanceMany standalone Tarantool instances and consistently shard data between themTarantool running in CartridgeTarantool with replica and automatic failover in CartridgeMany Tarantool instances (or leader-follower setup) in Cartridge with consistent client-side sharding between themTarantool with synchronous replication (Raft-based, Tarantool &gt;= 2.7)  After running Tarantool you can point Centrifugo to it (and of course scale Centrifugo nodes):  config.json { ... &quot;engine&quot;: &quot;tarantool&quot;, &quot;tarantool_address&quot;: &quot;127.0.0.1:3301&quot; }   See centrifugal/rotor repo for ready-to-use engine based on Tarantool Cartridge framework.  See centrifugal/tarantool-centrifuge repo for examples on how to run engine with Standalone single Tarantool instance or with Raft-based synchronous replication.  ","version":"v5","tagName":"h2"},{"title":"Tarantool engine options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#tarantool-engine-options","content":" tarantool_address​  String or array of strings. Default tcp://127.0.0.1:3301.  Connection address to Tarantool.  tarantool_mode​  String, default standalone  A mode how to connect to Tarantool. Default is standalone which connects to a single Tarantool instance address. Possible values are: leader-follower (connects to a setup with Tarantool master and async replicas) and leader-follower-raft (connects to a Tarantool with synchronous Raft-based replication).  All modes support client-side consistent sharding (similar to what Redis engine provides).  tarantool_user​  String, default &quot;&quot;. Allows setting a user.  tarantool_password​  String, default &quot;&quot;. Allows setting a password.  history_meta_ttl​  Duration, default 2160h.  Same option as for Memory engine and Redis engine also applies to Tarantool case.  ","version":"v5","tagName":"h3"},{"title":"Nats broker​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#nats-broker","content":" It's possible to scale with Nats PUB/SUB server. Keep in mind, that Nats is called a broker here, not an Engine – Nats integration only implements PUB/SUB part of Engine, so carefully read limitations below.  Limitations:  Nats integration works only for unreliable at most once PUB/SUB. This means that history, presence, and message recovery Centrifugo features won't be available.Nats wildcard channel subscriptions with symbols * and &gt; not supported.  First start Nats server:  $ nats-server [3569] 2020/07/08 20:28:44.324269 [INF] Starting nats-server version 2.1.7 [3569] 2020/07/08 20:28:44.324400 [INF] Git commit [not set] [3569] 2020/07/08 20:28:44.325600 [INF] Listening for client connections on 0.0.0.0:4222 [3569] 2020/07/08 20:28:44.325612 [INF] Server id is NDAM7GEHUXAKS5SGMA3QE6ZSO4IQUJP6EL3G2E2LJYREVMAMIOBE7JT4 [3569] 2020/07/08 20:28:44.325617 [INF] Server is ready   Then start Centrifugo with broker option:  centrifugo --broker=nats --config=config.json   And one more Centrifugo on another port (of course in real life you will start another Centrifugo on another machine):  centrifugo --broker=nats --config=config.json --port=8001   Now you can scale connections over Centrifugo instances, instances will be connected over Nats server.  ","version":"v5","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Engines and scalability","url":"/docs/server/engines#options","content":" nats_url​  String, default nats://127.0.0.1:4222.  Connection url in format nats://derek:pass@localhost:4222.  nats_prefix​  String, default centrifugo.  Prefix for channels used by Centrifugo inside Nats.  nats_dial_timeout​  Duration, default 1s.  Timeout for dialing with Nats.  nats_write_timeout​  Duration, default 1s.  Write (and flush) timeout for a connection to Nats. ","version":"v5","tagName":"h3"},{"title":"History and recovery","type":0,"sectionRef":"#","url":"/docs/server/history_and_recovery","content":"","keywords":"","version":"v5"},{"title":"History design​","type":1,"pageTitle":"History and recovery","url":"/docs/server/history_and_recovery#history-design","content":" History properties configured on a namespace level, to enable history both history_size and history_ttl should be set to a value greater than zero.  Centrifugo is designed with an idea that history streams are ephemeral (can be created on the fly without explicit create call from Centrifugo users) and can expire or can be lost at any moment. Centrifugo provides a way for a client to understand that channel history lost. In this case, the main application database should be the source of truth and state recovery.  When history is on every message published into a channel is saved into a history stream. The persistence properties of history are dictated by a Centrifugo engine used. For example, in the case of the Memory engine, all history is stored in process memory. So as soon as Centrifugo restarted all history is cleared. When using Redis engine history is kept in Redis Stream data structure - persistence properties are then inherited from Redis persistence configuration (the same for KeyDB engine). For Tarantool history is kept inside Tarantool spaces.  Each publication when added to history has an offset field. This is an incremental uint64 field. Each stream is identified by the epoch field - which is an arbitrary string. As soon as the underlying engine loses data epoch field will change for a stream thus letting consumers know that stream can't be used as the source of state recovery anymore.  tip History in channels is not enabled by default. See how to enable it over channel options. History is available in both server and client API.  ","version":"v5","tagName":"h2"},{"title":"History iteration API​","type":1,"pageTitle":"History and recovery","url":"/docs/server/history_and_recovery#history-iteration-api","content":" History iteration based on three fields:  limitsincereverse  Combining these fields you can iterate over a stream in both directions.  Get current stream top offset and epoch:  history(limit: 0, since: null, reverse: false)   Get full history from the current beginning (but up to client_history_max_publication_limit, which is 300 by default):  history(limit: -1, since: null, reverse: false)   Get full history from the current end (but up to client_history_max_publication_limit, which is 300 by default):  history(limit: -1, since: null, reverse: true)   Get history from the current beginning (up to 10):  history(limit: 10, since: null, reverse: false)   Get history from the current end in reversed direction (up to 10):  history(limit: 10, since: null, reverse: true)   Get up to 10 publications since known stream position (described by offset and epoch values):  history(limit: 10, since: {offset: 0, epoch: &quot;epoch&quot;}, reverse: false)   Get up to 10 publications since known stream position (described by offset and epoch values) but in reversed direction (from last to earliest):  history(limit: 10, since: {offset: 11, epoch: &quot;epoch&quot;}, reverse: true)   Here is an example program in Go which endlessly iterates over stream both ends (using gocent API library), upon reaching the end of stream the iteration goes in reversed direction (not really useful in real world but fun):  // Iterate by 10. limit := 10 // Paginate in reversed order first, then invert it. reverse := true // Start with nil StreamPosition, then fill it with value while paginating. var sp *gocent.StreamPosition for { historyResult, err = c.History( ctx, channel, gocent.WithLimit(limit), gocent.WithReverse(reverse), gocent.WithSince(sp), ) if err != nil { log.Fatalf(&quot;Error calling history: %v&quot;, err) } for _, pub := range historyResult.Publications { log.Println(pub.Offset, &quot;=&gt;&quot;, string(pub.Data)) sp = &amp;gocent.StreamPosition{ Offset: pub.Offset, Epoch: historyResult.Epoch, } } if len(historyResult.Publications) &lt; limit { // Got all pubs, invert pagination direction. reverse = !reverse log.Println(&quot;end of stream reached, change iteration direction&quot;) } }   ","version":"v5","tagName":"h2"},{"title":"Automatic message recovery​","type":1,"pageTitle":"History and recovery","url":"/docs/server/history_and_recovery#automatic-message-recovery","content":" One of the most interesting features of Centrifugo is automatic message recovery after short network disconnects. This mechanism allows a client to automatically restore missed publications on successful resubscribe to a channel after being disconnected for a while.  In general, you could query your application backend for the actual state on every client reconnect - but the message recovery feature allows Centrifugo to deal with this and restore missed publications from the history cache thus radically reducing the load on your application backend and your main application database in some scenarios (when many clients reconnect at the same time).  danger Message recovery protocol feature designed to be used together with reasonably small history stream size as all missed publications sent towards the client in one protocol frame on resubscribing to a channel. Thus, it is mostly suitable for short-time disconnects. It helps a lot to survive a reconnect storm when many clients reconnect at one moment (balancer reload, network glitch) - but it's not a good idea to recover a long list of missed messages after clients being offline for a long time.  To enable recovery mechanism for channels set the force_recovery boolean configuration option to true on the configuration file top-level or for a channel namespace. Make sure to enable this option in namespaces where history is on. It's also possible to ask for enabling recovery from the client-side when configuring Subscription object – in this case client must have a permission to call history API.  When re-subscribing on channels Centrifugo will return missed publications to a client in a subscribe Reply, also it will return a special recovered boolean flag to indicate whether all missed publications successfully recovered after a disconnect or not.  The number of publications that is possible to automatically recover is controlled by the client_recovery_max_publication_limit option which is 300 by default.  Centrifugo recovery model based on two fields in the protocol: offset and epoch. All fields are managed automatically by Centrifugo client SDKs (for bidirectional transport).  The recovery process works this way:  Let's suppose client subscribes on a channel with recovery on.Client receives subscribe reply from Centrifugo with two values: stream epoch and stream top offset, those values are saved by an SDK implementation.Every received publication has incremental offset, client SDK increments locally saved offset on each publication from a channel.Let's say at this point client disconnected for a while.Upon resubscribing to a channel SDK provides last seen epoch anf offset to Centrifugo.Centrifugo tries to load all the missed publications starting from the stream position provided by a client.If Centrifugo decides it can successfully recover client's state – then all missed publications returned in subscribe reply and client receives recovered: true in subscribed event context, and publication event handler of Subscription is called for every missed publication. Otherwise no publications returned and recovered flag of subscribed event context is set to false.  epoch is useful for cases when history storage is temporary and can lose the history stream entirely. In this case comparing epoch values gives Centrifugo a tip that while publications exist and theoretically have same offsets as before - the stream is actually different, so it's impossible to use it for the recovery process.  To summarize, here is a list of possible scenarios when Centrifugo can't recover client's state for a channel and provides recovered: false flag in subscribed event context:  number of missed publications exceeds client_recovery_max_publication_limit optionnumber of missed publications exceeds history_size namespace optionclient was away for a long time and history stream expired according to history_ttl namespace optionstorage used by Centrifugo engine lost the stream (restart, number of shards changed, cleared by the administrator, etc.)  Having said this all, Centrifugo recovery is nice to keep the continuity of the connection and subscription. This speed-ups resubscribe in many cases and helps the backend to survive mass reconnect scenario since you avoid lots of requests for state loading. For setups with millions of connections this can be a life-saver. But we recommend applications to always have a way to load the state from the main application database. For example, on app reload.  You can also manually implement your own recovery logic on top of the basic PUB/SUB possibilities that Centrifugo provides. As we said above you can simply ask your backend for an actual state after every client resubscribe completely bypassing the recovery mechanism described here. Also, it's possible to manually iterate over the Centrifugo stream using the history iteration API described above. ","version":"v5","tagName":"h2"},{"title":"Infrastructure tuning","type":0,"sectionRef":"#","url":"/docs/server/infra_tuning","content":"","keywords":"","version":"v5"},{"title":"Open files limit​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/server/infra_tuning#open-files-limit","content":" You should increase a max number of open files Centrifugo process can open if you want to handle more connections.  To get the current open files limit run:  ulimit -n   On Linux you can check limits for a running process using:  cat /proc/&lt;PID&gt;/limits   The open file limit shows approximately how many clients your server can handle. Each connection consumes one file descriptor. On most operating systems this limit is 128-256 by default.  See this document to get more info on how to increase this number.  If you install Centrifugo using RPM from repo then it automatically sets max open files limit to 65536.  You may also need to increase max open files for Nginx (or any other proxy before Centrifugo).  ","version":"v5","tagName":"h3"},{"title":"Ephemeral port exhaustion​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/server/infra_tuning#ephemeral-port-exhaustion","content":" Ephemeral ports exhaustion problem can happen between your load balancer and Centrifugo server. If your clients connect directly to Centrifugo without any load balancer or reverse proxy software between then you are most likely won't have this problem. But load balancing is a very common thing.  The problem arises due to the fact that each TCP connection uniquely identified in the OS by the 4-part-tuple:  source ip | source port | destination ip | destination port   On load balancer/server boundary you are limited in 65536 possible variants by default. Actually due to some OS limits not all 65536 ports are available for usage (usually about 15k ports available by default).  In order to eliminate a problem you can:  Increase the ephemeral port range by tuning ip_local_port_range optionDeploy more Centrifugo server instances to load balance acrossDeploy more load balancer instancesUse virtual network interfaces  See a post in Pusher blog about this problem and more detailed solution steps.  ","version":"v5","tagName":"h3"},{"title":"Sockets in TIME_WAIT state​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/server/infra_tuning#sockets-in-time_wait-state","content":" On load balancer/server boundary one more problem can arise: sockets in TIME_WAIT state.  Under load when lots of connections and disconnections happen socket descriptors can stay in TIME_WAIT state. Those descriptors can not be reused for a while. So you can get various errors when using Centrifugo. For example something like (99: Cannot assign requested address) while connecting to upstream in Nginx error log and 502 on client side.  Look how many socket descriptors in TIME_WAIT state.  netstat -an |grep TIME_WAIT | grep &lt;CENTRIFUGO_PID&gt; | wc -l   Nice article about TIME_WAIT sockets: http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html  The advices here are similar to ephemeral port exhaustion problem:  Increase the ephemeral port range by tuning ip_local_port_range optionDeploy more Centrifugo server instances to load balance acrossDeploy more load balancer instancesUse virtual network interfaces  ","version":"v5","tagName":"h3"},{"title":"Proxy max connections​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/server/infra_tuning#proxy-max-connections","content":" Proxies like Nginx and Envoy have default limits on maximum number of connections which can be established.  Make sure you have a reasonable limit for max number of incoming and outgoing connections in your proxy configuration.  ","version":"v5","tagName":"h3"},{"title":"Conntrack table​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/server/infra_tuning#conntrack-table","content":" More rare (since default limit is usually sufficient) your possible number of connections can be limited by conntrack table. Netfilter framework which is part of iptables keeps information about all connections and has limited size for this information. See how to see its limits and instructions to increase in this article.  ","version":"v5","tagName":"h3"},{"title":"Additional server protection​","type":1,"pageTitle":"Infrastructure tuning","url":"/docs/server/infra_tuning#additional-server-protection","content":" You should also consider adding additional protection to your Centrifugo endpoints. Centrifugo itself provides several options (described in configuration section) regarding server protection from the malicious behavior. Though an additional layer of DDOS protection on network or infrastructure level is highly recommended. For example, you may want to limit the number of connections coming from particular IP address.  Here we list some possible ways you can use to protect your Centrifugo installation:  Adding Nginx limit_conn_zone configurationUsing stick tables of HaproxyConfiguring rate limiting rules with Cloudflare  The list is not exhaustive of course. ","version":"v5","tagName":"h3"},{"title":"Load balancing","type":0,"sectionRef":"#","url":"/docs/server/load_balancing","content":"","keywords":"","version":"v5"},{"title":"Nginx configuration​","type":1,"pageTitle":"Load balancing","url":"/docs/server/load_balancing#nginx-configuration","content":" Although it's possible to use Centrifugo without any reverse proxy before it, it's still a good idea to keep Centrifugo behind mature reverse proxy to deal with edge cases when handling HTTP/Websocket connections from the wild. Also you probably want some sort of load balancing eventually between Centrifugo nodes so that proxy can be such a balancer too.  In this section we will look at Nginx configuration to deploy Centrifugo.  Minimal Nginx version – 1.3.13 because it was the first version that can proxy Websocket connections.  There are 2 ways: running Centrifugo server as separate service on its own domain or embed it to a location of your web site (for example to /centrifugo).  ","version":"v5","tagName":"h2"},{"title":"Separate domain for Centrifugo​","type":1,"pageTitle":"Load balancing","url":"/docs/server/load_balancing#separate-domain-for-centrifugo","content":" upstream centrifugo { # uncomment ip_hash if using SockJS transport with many upstream servers. #ip_hash; server 127.0.0.1:8000; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } #server { #\tlisten 80; #\tserver_name centrifugo.example.com; #\trewrite ^(.*) https://$server_name$1 permanent; #} server { server_name centrifugo.example.com; listen 80; #listen 443 ssl; #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #ssl_certificate /etc/nginx/ssl/server.crt; #ssl_certificate_key /etc/nginx/ssl/server.key; include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; tcp_nodelay on; gzip on; gzip_min_length 1000; gzip_proxied any; # Only retry if there was a communication error, not a timeout # on the Centrifugo server (to avoid propagating &quot;queries of death&quot; # to all frontends) proxy_next_upstream error; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; location /connection { proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_http_version 1.1; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } location / { proxy_pass http://centrifugo; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } }   ","version":"v5","tagName":"h3"},{"title":"Embed to a location of web site​","type":1,"pageTitle":"Load balancing","url":"/docs/server/load_balancing#embed-to-a-location-of-web-site","content":" upstream centrifugo { # uncomment ip_hash if using SockJS transport with many upstream servers. #ip_hash; server 127.0.0.1:8000; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { # ... your web site Nginx config location /centrifugo/ { rewrite ^/centrifugo/(.*) /$1 break; proxy_pass http://centrifugo; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; } location /centrifugo/connection { rewrite ^/centrifugo(.*) $1 break; proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } }   ","version":"v5","tagName":"h3"},{"title":"worker_connections​","type":1,"pageTitle":"Load balancing","url":"/docs/server/load_balancing#worker_connections","content":" You may also need to update worker_connections option of Nginx:  events { worker_connections 65535; }  ","version":"v5","tagName":"h3"},{"title":"Metrics monitoring","type":0,"sectionRef":"#","url":"/docs/server/monitoring","content":"","keywords":"","version":"v5"},{"title":"Prometheus​","type":1,"pageTitle":"Metrics monitoring","url":"/docs/server/monitoring#prometheus","content":" To enable Prometheus endpoint start Centrifugo with prometheus option on:  config.json { ... &quot;prometheus&quot;: true }   This will enable /metrics endpoint so the Centrifugo instance can be monitored by your Prometheus server.  ","version":"v5","tagName":"h3"},{"title":"Graphite​","type":1,"pageTitle":"Metrics monitoring","url":"/docs/server/monitoring#graphite","content":" To enable automatic export to Graphite (via TCP):  config.json { ... &quot;graphite&quot;: true, &quot;graphite_host&quot;: &quot;localhost&quot;, &quot;graphite_port&quot;: 2003 }   By default, stats will be aggregated over 10 seconds intervals inside Centrifugo and then pushed to Graphite over TCP connection.  If you need to change this aggregation interval use the graphite_interval option (in seconds, default 10).  ","version":"v5","tagName":"h3"},{"title":"Grafana dashboard​","type":1,"pageTitle":"Metrics monitoring","url":"/docs/server/monitoring#grafana-dashboard","content":" Check out Centrifugo official Grafana dashboard for Prometheus storage. You can import that dashboard to your Grafana, point to Prometheus storage – and enjoy visualized metrics.   ","version":"v5","tagName":"h3"},{"title":"Server observability","type":0,"sectionRef":"#","url":"/docs/server/observability","content":"","keywords":"","version":"v5"},{"title":"Metrics​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#metrics","content":" ","version":"v5","tagName":"h2"},{"title":"Prometheus metrics​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#prometheus-metrics","content":" To enable Prometheus endpoint start Centrifugo with prometheus option on:  config.json { ... &quot;prometheus&quot;: true }   This will enable /metrics endpoint so the Centrifugo instance can be monitored by your Prometheus server.  ","version":"v5","tagName":"h3"},{"title":"Graphite metrics​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#graphite-metrics","content":" To enable automatic export to Graphite (via TCP):  config.json { ... &quot;graphite&quot;: true, &quot;graphite_host&quot;: &quot;localhost&quot;, &quot;graphite_port&quot;: 2003 }   By default, stats will be aggregated over 10 seconds intervals inside Centrifugo and then pushed to Graphite over TCP connection.  If you need to change this aggregation interval use the graphite_interval option (in seconds, default 10).  ","version":"v5","tagName":"h3"},{"title":"Grafana dashboard​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#grafana-dashboard","content":" Check out Centrifugo official Grafana dashboard for Prometheus storage. You can import that dashboard to your Grafana, point to Prometheus storage – and enjoy visualized metrics.    ","version":"v5","tagName":"h3"},{"title":"Exposed metrics​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#exposed-metrics","content":" Here is a description of various metrics exposed by Centrifugo.  centrifugo_node_messages_sent_count​  Type: CounterLabels: typeDescription: Tracks the number of messages sent by a node to the broker.Usage: Use this metric to monitor the outgoing message rate and detect any anomalies or spikes in the data flow.  centrifugo_node_messages_received_count​  Type: CounterLabels: typeDescription: Measures the number of messages received from the broker.Usage: Helps in understanding the incoming message rate and ensures the node is receiving data as expected.  centrifugo_node_action_count​  Type: CounterLabels: actionDescription: Counts the number of various actions called within the node.Usage: Useful for tracking specific actions' usage and frequency.  centrifugo_node_num_clients​  Type: GaugeDescription: Shows the current number of clients connected to the node.Usage: Monitor the client connections to ensure the node is not reaching its capacity.  centrifugo_node_num_users​  Type: GaugeDescription: Displays the number of unique users connected to the node.Usage: Helps in understanding user engagement and capacity planning.  centrifugo_node_num_subscriptions​  Type: GaugeDescription: Indicates the number of active subscriptions.Usage: Use this to monitor the subscription levels and identify any potential issues or required optimizations.  centrifugo_node_num_nodes​  Type: GaugeDescription: Shows the total number of nodes in the cluster.Usage: Essential for monitoring the size of the cluster and ensuring that all nodes are operational.  centrifugo_node_build​  Type: GaugeLabels: versionDescription: Provides build information of the node.Usage: Helps in tracking the version of the application running across different environments.  centrifugo_node_num_channels​  Type: GaugeDescription: Counts the number of channels with one or more subscribers.Usage: Useful for monitoring the activity and utilization of channels.  centrifugo_node_survey_duration_seconds​  Type: SummaryLabels: opDescription: Captures the duration of surveys conducted by the node.Usage: Helps in performance monitoring and identifying any delays or issues in survey operations.  centrifugo_client_num_reply_errors​  Type: CounterLabels: method, codeDescription: Counts the number of errors in replies sent to clients.Usage: Critical for error monitoring and ensuring smooth client interactions.  centrifugo_client_num_server_disconnects​  Type: CounterLabels: codeDescription: Tracks the number of server-initiated disconnects.Usage: Use this to monitor the health of client connections and identify potential issues with the server.  centrifugo_client_command_duration_seconds​  Type: SummaryLabels: methodDescription: Measures the duration of commands executed by clients.Usage: Essential for performance monitoring and ensuring timely responses to client commands.  centrifugo_client_recover​  Type: CounterLabels: recoveredDescription: Counts the number of recover operations performed.Usage: Helps in tracking the system's resilience and recovery mechanisms.  centrifugo_client_connection_limit_reached_total​  Type: CounterLabels: NoneDescription: Number of refused connections due to the node client connection limit.Usage: Useful for monitoring the load on the Centrifugo node and identifying when clients are being refused connections due to reaching the connection limit.  centrifugo_transport_connect_count​  Type: CounterLabels: transportDescription: Measures the number of connections to specific transports.Usage: Use this to monitor the usage of different transports and ensure they are functioning as expected.  centrifugo_transport_messages_sent​  Type: CounterLabels: transport, frame_type, channel_namespaceDescription: Tracks the number of messages sent to client connections over specific transports.Usage: Essential for understanding the data flow and performance of different transports.  centrifugo_transport_messages_sent_size​  Type: CounterLabels: transport, frame_type, channel_namespaceDescription: Measures the size of messages (in bytes) sent to client connections over specific transports.Usage: Helps in monitoring the network bandwidth usage and optimizing the data transfer.  centrifugo_transport_messages_received​  Type: CounterLabels: transport, frame_type, channel_namespaceDescription: Counts the number of messages received from client connections over specific transports.Usage: Important for ensuring that messages are being successfully received and processed.  centrifugo_transport_messages_received_size​  Type: CounterLabels: transport, frame_type, channel_namespaceDescription: Measures the size of messages (in bytes) received from client connections over specific transports.Usage: Use this metric to monitor the incoming data size and optimize the application's performance.  centrifugo_proxy_duration_seconds​  Type: Summary &amp; HistogramLabels: protocol, typeDescription: Captures the duration of proxy calls.Usage: Critical for understanding the performance of proxy calls and identifying any potential bottlenecks or issues.  centrifugo_proxy_errors​  Type: CounterLabels: protocol, typeDescription: Counts the number of errors occurred during proxy calls.Usage: Helps in monitoring the reliability of proxy services and ensuring error-free operations.  centrifugo_granular_proxy_duration_seconds​  Type: Summary &amp; HistogramLabels: type, nameDescription: Measures the duration of granular proxy calls.Usage: Use this to get more detailed insights into the performance of granular proxy operations.  centrifugo_granular_proxy_errors​  Type: CounterLabels: type, nameDescription: Counts the number of errors in granular proxy calls.Usage: Essential for error tracking and ensuring the stability of granular proxy services.  centrifugo_api_command_duration_seconds​  Type: SummaryLabels: protocol, methodDescription: Tracks the duration of API commands.Usage: Helps in monitoring the API performance and ensuring timely responses.  centrifugo_api_command_duration_seconds_histogram​  Type: HistogramLabels: protocol, methodDescription: Tracks the duration of API commands.Usage: Helps in monitoring the API performance and ensuring timely responses.  ","version":"v5","tagName":"h3"},{"title":"Traces​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#traces","content":" ","version":"v5","tagName":"h2"},{"title":"OpenTelemetry​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#opentelemetry","content":" At this point Centrifugo can export traces for HTTP and GRPC server API requests in OpenTelemetry format.  To enable:  { ... &quot;opentelemetry&quot;: true, &quot;opentelemetry_api&quot;: true }   OpenTelemetry must be explicitly turned on to avoid tracing overhead when it's not needed.  To configure OpenTelemetry export behaviour we are relying on OpenTelemetry environment vars supporting only HTTP export endpoints for now.  So a simple example to run Centrifugo with server API tracing would be running Jaeger with COLLECTOR_OTLP_ENABLED:  docker run --rm -it --name jaeger \\ -e COLLECTOR_OTLP_ENABLED=true \\ -p 16686:16686 \\ -p 4318:4318 \\ jaegertracing/all-in-one:latest   Then start Centrifugo:  OTEL_EXPORTER_OTLP_ENDPOINT=&quot;http://localhost:4318&quot; CENTRIFUGO_OPENTELEMETRY=1 CENTRIFUGO_OPENTELEMETRY_API=1 ./centrifugo   Send some API requests - and open http://localhost:16686 to see traces in Jaeger UI.  By default, Centrifugo exports traces in http/protobuf format. If you want to use GRPC exporter then it's possible to turn it on by setting environment variable OTEL_EXPORTER_OTLP_PROTOCOL to grpc (GRPC exporter format supported since Centrifugo v5.0.3).  ","version":"v5","tagName":"h3"},{"title":"Logs​","type":1,"pageTitle":"Server observability","url":"/docs/server/observability#logs","content":" Logging may be configured using log_level option. It may have the following values:  nonetracedebuginfo (default)warnerror  We generally do not recommend anything below info to be used in production.  By default Centrifugo logs to STDOUT. Usually this is what you need when running servers on modern infrastructures. Logging into file may be configured using log_file option. ","version":"v5","tagName":"h2"},{"title":"Online presence","type":0,"sectionRef":"#","url":"/docs/server/presence","content":"","keywords":"","version":"v5"},{"title":"Enabling online presence​","type":1,"pageTitle":"Online presence","url":"/docs/server/presence#enabling-online-presence","content":" To enable online presence, you need to set the presence option to true for the specific channel namespace in your Centrifugo configuration.  { &quot;namespaces&quot;: [{ &quot;namespace&quot;: &quot;public&quot;, &quot;presence&quot;: true }] }   After enabling this you can query presence information over server HTTP/GRPC presence call:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: YOUR_API_KEY&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;public:test&quot;}' \\ http://localhost:8000/api/presence   See description of presence API.  Also, a shorter version of presence which only contains two counters - number of clients and number of unique users in channel - may be called:  curl --header &quot;Content-Type: application/json&quot; \\ --header &quot;X-API-Key: YOUR_API_KEY&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;public:test&quot;}' \\ http://localhost:8000/api/presence_stats   See description of presence stats API.  ","version":"v5","tagName":"h2"},{"title":"Retrieving presence on the client side​","type":1,"pageTitle":"Online presence","url":"/docs/server/presence#retrieving-presence-on-the-client-side","content":" Once presence enabled, you can retrieve the presence information on the client side too by calling the presence method on the channel.  To do this you need to give the client permission to call presence. Once done, presence may be retrieved from the subscription:  const resp = await subscription.presence(channel);   It's also available on the top-level of the client (for example, if you need to call presence for server-side subscription):  const resp = await client.presence(channel);   If the permission check has passed successfully – both methods will return an object containing information about currently subscribed clients.  Also, presenceStats method is avalable:  const resp = await subscription.presenceStats(channel);   ","version":"v5","tagName":"h2"},{"title":"Join and leave events​","type":1,"pageTitle":"Online presence","url":"/docs/server/presence#join-and-leave-events","content":" It's also possible to enable real-time tracking of users joining or leaving a channel by listening to join and leave events on the client side.  By default, Centrifugo does not send these events and they must be explicitly turned on for channel namespace:  { &quot;namespaces&quot;: [{ &quot;namespace&quot;: &quot;public&quot;, &quot;presence&quot;: true, &quot;join_leave&quot;: true, &quot;force_push_join_leave&quot;: true }] }   Then on the client side:  subscription.on('join', function(joinCtx) { console.log('client joined:', joinCtx); }); subscription.on('leave', function(leaveCtx) { console.log('client left:', leaveCtx); });   And the same on client top-level for the needs of server-side subscriptions (analogous to the presence call described above).  These events provide real-time updates and can be used to keep track of user activity and manage live interactions.  You can combine join/leave events with presence information and maintain a list of currently active subscribers - for example show the list of online players in the game room updated in real-time.  ","version":"v5","tagName":"h2"},{"title":"Implementation notes​","type":1,"pageTitle":"Online presence","url":"/docs/server/presence#implementation-notes","content":" The online presence feature might increase the load on your Centrifugo server, since Centrifugo need to maintain an addition data structure. Therefore, it is recommended to use this feature judiciously based on your server's capability and the necessity of real-time presence data in your application.  Always make sure to secure the presence data, as it could expose sensitive information about user activity in your application. Ensure appropriate security measures are in place.  Join and leave events delivered with at most once guarantee.  See more about presence design in design overview chapter.  Also check out FAQ which mentions scalability concerns for presence data and join/leave events.  ","version":"v5","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Online presence","url":"/docs/server/presence#conclusion","content":" The online presence feature of Centrifugo is a highly useful tool for real-time applications. It provides instant and live data about user activity, which can be critical for interactive features in chats, collaborative tools, multiplayer games, or live tracking systems. Make sure to configure and use this feature appropriately to get the most out of your real-time application. ","version":"v5","tagName":"h2"},{"title":"Proxy subscription streams","type":0,"sectionRef":"#","url":"/docs/server/proxy_streams","content":"","keywords":"","version":"v5"},{"title":"Scalability concerns​","type":1,"pageTitle":"Proxy subscription streams","url":"/docs/server/proxy_streams#scalability-concerns","content":" Using proxy subscription streams increases resource usage on both Centrifugo and app backend sides because it involves more moving parts such as goroutines, additional buffers, connections, etc.  The feature is quite niche. Read carefully the motivation described in this doc. If you don't really need proxy streams – prefer using Centrifugo usual approach by always publishing messages to channels over Centrifugo publish API whenever an event happens. This is efficient and Centrifugo just drops messages in case of no active subscribers in a channel. I.e. follow our idiomatic guidelines.  tip Use proxy subscription streams only when really needed. Specifically, proxy subscription stream may be very useful to stream data for a limited time upon some user action in the app.  At the same time proxy subscription streams should scale well horizontally with adding more servers. But scaling GRPC is more involved and using GRPC streams results into more resources utilized than with the common Centrifugo approach, so make sure the resource consumption is sufficient for your system by performing load tests with your expected load profile.  The thing is that sometimes proxy streams is the only way to achieve the desired behaviour – at that point they shine even though require more resources and developer effort. Also, not every use case involves tens of thousands of subscriptions/connections to worry about – be realistic about your practical situation.  ","version":"v5","tagName":"h3"},{"title":"Motivation and design​","type":1,"pageTitle":"Proxy subscription streams","url":"/docs/server/proxy_streams#motivation-and-design","content":" Here is a diagram which shows the sequence of events happening when using subscription streams:    Subscription streams generally solve a task of integrating with third-party streaming providers or external process, possibly with custom filtering. They come into play when it's not feasible to continuously stream all data to various channels, and when you need to deallocate resources on the backend side as soon as stream is not needed anymore.  Subscription streams may be also considered as streaming requests – an isolated way to stream something from the backend to the client or from the client to the backend.  Let's describe a real-life use case. Say you have Loki for keeping logs, it provides a streaming API for tailing logs. You decided to stream logs towards your app's clients. When client subscribes to some channel in Centrifugo and the unidirectional stream established between Centrifugo and your backend – you can make sure client has proper permissions for the requested resource and backend then starts tailing Loki logs (or other third-party system, this may be Twitter streaming API, MQTT broker, GraphQL subscription, or streaming query to the real-time database such as RethinkDB). As soon as backend receives log events from Loki it transfers them towards client over Centrifugo.  Client can provide custom data upon subscribing to a channel which makes it possible to pass query filters from the frontend app to the backend. In the example with Loki above this may be a LogQL query.  In case of proxy subscription streams all the client authentication may be delegated to common Centrifugo mechanisms, so when the channel stream is established you know the ID of user (obtained by Centrifugo from JWT auth process or over connect proxy). You can additionally check channel permissions at the moment of stream establishement.  As soon as client unsubscribes from the channel – Centrifugo closes the unidirectional GRPC stream – so your backend will notice that. If client disconnects – stream is closed also.  If for some reason connection between Centrifugo and backend is closed – then Centrifugo will unsubscribe a client with insufficient state reason and a client will soon resubscribe to a channel (managed automatically by our SDKs).  You may wonder – what about the same channel name used for subscribing to such a stream by different connections. Proxy stream is an individual link between a client and a backend – Centrifugo transfers stream data published to the GRPC stream by the backend only to the client connection to whom the stream belongs. I.e. messages sent by the backend to GRPC stream are not broadcasted to other channel subscribers. But if you will use server API for publishing – then message will be broadcasted to all channel subscribers even if they are currently using proxy stream within that channel.  Presence and join/leave features will work as usual for channels with subscription proxy streams. If different connections use the same channel they will be able to use presence (if enabled) to see who else is currently in the channel, and may receive join/leave messages (if enabled).  Channel history for proxy subscription streams For the case of proxy subscription streams Centrifugo channel history and recovery features do not really make sense. Proxy stream is an individual direct link between client and your backend through Centrifugo which is always re-established from scratch upon re-subscription or connection drops. The benefit of the history and its semantics are not clear in this case and can only bring undesired overhead (because Centrifugo will have to use broker, now messages just go directly towards connections without broker/engine involved at all).  Only for client-side subscriptions Subscription streams work only with client-side subscriptions (i.e. when client explicitly subscribes to a channel on the application's frontend side). Server-side subscriptions won't initiate a GRPC stream to the backend.  Don't forget that Centrifugo namespace system is very flexible – so you can always combine different approaches using different channel namespaces. You can always use subscription streams only for some channels belonging to a specific namespace.  ","version":"v5","tagName":"h3"},{"title":"Unidirectional subscription streams​","type":1,"pageTitle":"Proxy subscription streams","url":"/docs/server/proxy_streams#unidirectional-subscription-streams","content":" From the configuration point of view subscription streams may be enabled for channel namespace just as additional type of proxy. The important difference is that only GRPC endpoints may be used - as we are using GRPC streaming RPCs for this functionality.  You can configure subscription streams for channels very similar to how subscribe proxy is configured.  First, configure subscribe stream proxy, pointing it to the backend which implements our proxy stream GRPC service contract:  config.json { ... &quot;proxy_subscribe_stream_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_subscribe_stream_timeout&quot;: &quot;3s&quot; }   Only grpc:// endpoints are supported since we are heavily relying on GRPC streaming ecosystem here. In this case proxy_subscribe_stream_timeout defines a time how long Centrifugo waits for a first message from a stream which contains subscription details to transfer to a client.  Then you can enable subscription streams for channels on a namespace level:  config.json { ... &quot;proxy_subscribe_stream_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_subscribe_stream_timeout&quot;: &quot;3s&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;streams&quot;, &quot;proxy_subscribe_stream&quot;: true } ] }   info You can not use subscribe, publish, sub_refresh proxy configurations together with stream proxy configuration inside one channel namespace.  That's it on Centrifugo side. Now on the app backend you should implement GRPC service according to the following definitions:  service CentrifugoProxy { ... // SubscribeUnidirectional allows handling unidirectional subscription streams. rpc SubscribeUnidirectional(SubscribeRequest) returns (stream StreamSubscribeResponse); ... }   GRPC service definitions can be found in the Centrifugo repository: proxy.proto - same as we described before, probably you already have a service which implements some methods from it. If you don't – just follow GRPC tutorials for your programming language to generate server stubs from our Protobuf schema – and you are ready to describe stream logic.  Here we are looking at unidirectional subscription stream – so the next thing to do is to implement streaming handler on the application backend side which contains stream business logic, i.e. implement SubscribeUnidirectional streaming rpc handler.  tip You can write GRPC handlers to handle proxy subscription streams in any language with good GRPC support.  A basic example of such handler in Go may look like this (error handling skipped for brevity):  package main import ( &quot;fmt&quot; &quot;log&quot; &quot;math&quot; &quot;net&quot; &quot;strconv&quot; &quot;time&quot; pb &quot;example/proxyproto&quot; &quot;google.golang.org/grpc&quot; ) type streamServer struct { pb.UnimplementedCentrifugoProxyServer } func (s *streamerServer) SubscribeUnidirectional( req *pb.SubscribeRequest, stream pb.CentrifugoProxy_SubscribeUnidirectionalServer, ) error { started := time.Now() fmt.Println(&quot;unidirectional subscribe called with request&quot;, req) defer func() { fmt.Println(&quot;unidirectional subscribe finished, elapsed&quot;, time.Since(started)) }() _ = stream.Send(&amp;pb.StreamSubscribeResponse{ SubscribeResponse: &amp;pb.SubscribeResponse{}, }) // Now publish data to a stream every 1 second. for { select { case &lt;-stream.Context().Done(): return stream.Context().Err() case &lt;-time.After(1000 * time.Millisecond): } pub := &amp;pb.Publication{Data: []byte(`{&quot;input&quot;: &quot;` + strconv.Itoa(i) + `&quot;}`)} _ = stream.Send(&amp;pb.StreamSubscribeResponse{Publication: pub}) } } func main() { lis, _ := net.Listen(&quot;tcp&quot;, &quot;:12000&quot;) s := grpc.NewServer(grpc.MaxConcurrentStreams(math.MaxUint32)) pb.RegisterCentrifugoProxyServer(s, &amp;streamServer{}) _ = s.Serve(lis) }   tip Note we have increased grpc.MaxConcurrentStreams for server to handle more simultaneous streams than allowed by default. Usually default is 100 but can differ in various GRPC server implementations. If you expect more streams then you need a bigger value.  Centrifugo has some rules about messages in streams. Upon stream establishement Centrifugo expects backend to send first message from a stream - this is a StreamSubscribeResponse with SubscribeResponse in it. Centrifugo waits for this message before replying to the client's subscription command. This way we can communicate initial state with a client and make sure streaming is properly established with all permission checks passed. After sending initial message you can send events (publications) as they appear in your system.  Now everything should be ready to test it out from the client side: just subscribe to a channel where stream proxy is on with our SDK – and you will see your stream handler called and data streamed from it to a client. For example, with our Javascript SDK:  const client = new Centrifuge('ws://localhost:8000/connection/websocket', { getToken: getTokenImplementation }); client.connect(); const sub = client.newSubscription('streams:123e4567-e89b-12d3-a456-426614174000', { data: {} }).on('publication', function(ctx) { console.log(&quot;received publication from a channel&quot;, ctx.data); }); sub.subscribe();   Again, while we are still looking for a proper semantics of subscription streams we recommend using unique channel names for all on-demand streams you are establishing.  ","version":"v5","tagName":"h3"},{"title":"Bidirectional subscription streams​","type":1,"pageTitle":"Proxy subscription streams","url":"/docs/server/proxy_streams#bidirectional-subscription-streams","content":" In addition to unidirectional streams, Centrifugo supports bidirectional streams upon client channel subscription. In this case client gets a possibility to stream any data to the backend utilizing bidirectional communication. Client can send messages to a bidirectional stream by using .publish(data) method of a Subscription object.  In terms of general design bidirectional streams behave similar to unidirectional streams as described above.  When enabling subscription streams, Centrifugo uses unidirectional GRPC streams by default – as those should fit most of the use cases proxy subscription streams were introduced for. To tell Centrifugo use bidirectional streaming add proxy_subscribe_stream_bidirectional flag to the namespace configuration:  config.json { ... &quot;proxy_subscribe_stream_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_subscribe_stream_timeout&quot;: &quot;3s&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;streams&quot;, &quot;proxy_subscribe_stream&quot;: true, &quot;proxy_subscribe_stream_bidirectional&quot;: true } ] }   On the backend you need to implement the following streaming handler:  service CentrifugoProxy { ... // SubscribeBidirectional allows handling bidirectional subscription streams. rpc SubscribeBidirectional(stream StreamSubscribeRequest) returns (stream StreamSubscribeResponse); ... }   The first StreamSubscribeRequest message in stream will contain SubscribeRequest and Centrifugo expects StreamSubscribeResponse with SubscribeResponse from the backend – just like in unidirectional case described above.  An example of such handler in Go language which echoes back all publications from client (error handling skipped for brevity):  func (s *streamerServer) SubscribeBidirectional( stream pb.CentrifugoProxy_SubscribeBidirectionalServer, ) error { started := time.Now() fmt.Println(&quot;bidirectional subscribe called&quot;) defer func() { fmt.Println(&quot;bidirectional subscribe finished, elapsed&quot;, time.Since(started)) }() // First message always contains SubscribeRequest. req, _ := stream.Recv() fmt.Println(&quot;subscribe request received&quot;, req.SubscribeRequest) _ = stream.Send(&amp;pb.StreamSubscribeResponse{ SubscribeResponse: &amp;pb.SubscribeResponse{}, }) // The following messages contain publications from client. for { req, _ = stream.Recv() data := req.Publication.Data fmt.Println(&quot;data from client&quot;, string(data)) var cd clientData pub := &amp;pb.Publication{Data: data} _ = stream.Send(&amp;pb.StreamSubscribeResponse{Publication: pub}) } }   ","version":"v5","tagName":"h3"},{"title":"Granular proxy mode​","type":1,"pageTitle":"Proxy subscription streams","url":"/docs/server/proxy_streams#granular-proxy-mode","content":" Granular proxy mode works with subscription streams in the same manner as for other Centrifugo proxy types.  Here is an example how you can define different subscribe stream proxies for different namespaces:  { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [ { &quot;name&quot;: &quot;stream_1&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3000&quot;, &quot;timeout&quot;: &quot;500ms&quot;, }, { &quot;name&quot;: &quot;stream_2&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3001&quot;, &quot;timeout&quot;: &quot;500ms&quot;, } ], &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;ns1&quot;, &quot;subscribe_stream_proxy_name&quot;: &quot;stream_1&quot; }, { &quot;name&quot;: &quot;ns2&quot;, &quot;subscribe_stream_proxy_name&quot;: &quot;stream_2&quot; } ] }   ","version":"v5","tagName":"h2"},{"title":"Full example​","type":1,"pageTitle":"Proxy subscription streams","url":"/docs/server/proxy_streams#full-example","content":" Full example which demonstrates proxy subscribe stream backend implemented in Go language may be found in Centrifugo examples repo. ","version":"v5","tagName":"h2"},{"title":"Server-side subscriptions","type":0,"sectionRef":"#","url":"/docs/server/server_subs","content":"","keywords":"","version":"v5"},{"title":"Dynamic server-side subscriptions​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/server/server_subs#dynamic-server-side-subscriptions","content":" See subscribe and unsubscribe server API  ","version":"v5","tagName":"h3"},{"title":"Automatic personal channel subscription​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/server/server_subs#automatic-personal-channel-subscription","content":" It's possible to automatically subscribe a user to a personal server-side channel.  To enable this you need to enable the user_subscribe_to_personal boolean option (by default false). As soon as you do this every connection with a non-empty user ID will be automatically subscribed to a personal user-limited channel. Anonymous users with empty user IDs won't be subscribed to any channel.  For example, if you set this option and the user with ID 87334 connects to Centrifugo it will be automatically subscribed to channel #87334 and you can process personal publications on the client-side in the same way as shown above.  As you can see by default generated personal channel name belongs to the default namespace (i.e. no explicit namespace used). To set custom namespace name use user_personal_channel_namespace option (string, default &quot;&quot;) – i.e. the name of namespace from configured configuration namespaces array. In this case, if you set user_personal_channel_namespace to personal for example – then the automatically generated personal channel will be personal:#87334 – user will be automatically subscribed to it on connect and you can use this channel name to publish personal notifications to the online user.  ","version":"v5","tagName":"h3"},{"title":"Maintain single user connection​","type":1,"pageTitle":"Server-side subscriptions","url":"/docs/server/server_subs#maintain-single-user-connection","content":" Usage of personal channel subscription also opens a road to enable one more feature: maintaining only a single connection for each user globally around all Centrifugo nodes.  user_personal_single_connection boolean option (default false) turns on a mode in which Centrifugo will try to maintain only a single connection for each user at the same moment. As soon as the user establishes a connection other connections from the same user will be closed with connection limit reason (client won't try to automatically reconnect).  This feature works with a help of presence information inside a personal channel. So presence should be turned on in a personal channel.  Example config:  config.json { &quot;user_subscribe_to_personal&quot;: true, &quot;user_personal_single_connection&quot;: true, &quot;user_personal_channel_namespace&quot;: &quot;personal&quot;, &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;personal&quot;, &quot;presence&quot;: true } ] }   note Centrifugo can't guarantee that other user connections will be closed – since Disconnect messages are distributed around Centrifugo nodes with at most once guarantee. So don't add critical business logic based on this feature to your application. Though this should work just fine most of the time if the connection between the Centrifugo node and PUB/SUB broker is OK. ","version":"v5","tagName":"h3"},{"title":"Configure TLS","type":0,"sectionRef":"#","url":"/docs/server/tls","content":"","keywords":"","version":"v5"},{"title":"Using crt and key files​","type":1,"pageTitle":"Configure TLS","url":"/docs/server/tls#using-crt-and-key-files","content":" In first way you already have cert and key files. For development you can create self-signed certificate - see this instruction as example.  config.json { ... &quot;tls&quot;: true, &quot;tls_key&quot;: &quot;server.key&quot;, &quot;tls_cert&quot;: &quot;server.crt&quot; }   And run:  ./centrifugo --config=config.json   ","version":"v5","tagName":"h3"},{"title":"Automatic certificates​","type":1,"pageTitle":"Configure TLS","url":"/docs/server/tls#automatic-certificates","content":" For automatic certificates from Let's Encrypt add into configuration file:  config.json { ... &quot;tls_autocert&quot;: true, &quot;tls_autocert_host_whitelist&quot;: &quot;www.example.com&quot;, &quot;tls_autocert_cache_dir&quot;: &quot;/tmp/certs&quot;, &quot;tls_autocert_email&quot;: &quot;user@example.com&quot;, &quot;tls_autocert_http&quot;: true, &quot;tls_autocert_http_addr&quot;: &quot;:80&quot; }   tls_autocert (boolean) says Centrifugo that you want automatic certificate handling using ACME provider.  tls_autocert_host_whitelist (string) is a string with your app domain address. This can be comma-separated list. It's optional but recommended for extra security.  tls_autocert_cache_dir (string) is a path to a folder to cache issued certificate files. This is optional but will increase performance.  tls_autocert_email (string) is optional - it's an email address ACME provider will send notifications about problems with your certificates.  tls_autocert_http (boolean) is an option to handle http_01 ACME challenge on non-TLS port.  tls_autocert_http_addr (string) can be used to set address for handling http_01 ACME challenge (default is :80)  When configured correctly and your domain is valid (localhost will not work) - certificates will be retrieved on first request to Centrifugo.  Also Let's Encrypt certificates will be automatically renewed.  There are two options that allow Centrifugo to support TLS client connections from older browsers such as Chrome 49 on Windows XP and IE8 on XP:  tls_autocert_force_rsa - this is a boolean option, by default false. When enabled it forces autocert manager generate certificates with 2048-bit RSA keys.tls_autocert_server_name - string option, allows to set server name for client handshake hello. This can be useful to deal with old browsers without SNI support - see comment  grpc_api_tls_disable boolean flag allows to disable TLS for GRPC API server but keep it on for HTTP endpoints.  uni_grpc_tls_disable boolean flag allows to disable TLS for GRPC uni stream server but keep it on for HTTP endpoints.  ","version":"v5","tagName":"h3"},{"title":"TLS for GRPC API​","type":1,"pageTitle":"Configure TLS","url":"/docs/server/tls#tls-for-grpc-api","content":" You can provide custom certificate files to configure TLS for GRPC API server.  grpc_api_tls boolean flag enables TLS for GRPC API server, requires an X509 certificate and a key filegrpc_api_tls_cert string provides a path to an X509 certificate file for GRPC API servergrpc_api_tls_key string provides a path to an X509 certificate key for GRPC API server  ","version":"v5","tagName":"h3"},{"title":"TLS for GRPC unidirectional stream​","type":1,"pageTitle":"Configure TLS","url":"/docs/server/tls#tls-for-grpc-unidirectional-stream","content":" You can provide custom certificate files to configure TLS for GRPC unidirectional stream endpoint.  uni_grpc_tls boolean flag enables TLS for GRPC server, requires an X509 certificate and a key fileuni_grpc_tls_cert string provides a path to an X509 certificate file for GRPC uni stream serveruni_grpc_tls_key string provides a path to an X509 certificate key for GRPC uni stream server ","version":"v5","tagName":"h3"},{"title":"Client protocol","type":0,"sectionRef":"#","url":"/docs/transports/client_protocol","content":"","keywords":"","version":"v5"},{"title":"Protobuf schema​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#protobuf-schema","content":" Centrifugo is built on top of Centrifuge library for Go. Centrifuge library uses its own framing for wrapping Centrifuge-specific messages – synchronous commands from a client to a server (which expect replies from a server) and asynchronous pushes.  Centrifuge client protocol is defined by a Protobuf schema. This is the source of truth.  tip At the moment Protobuf schema contains some fields which are only used in client protocol v1. This is for backwards compatibility – server supports clients connecting over both client protocol v2 and client protocol v1. Client protocol v1 is considered deprecated and will be removed at some point in the future (giving enough time to our users to migrate).  ","version":"v5","tagName":"h2"},{"title":"Command-Reply​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#command-reply","content":" In bidirectional case client sends Command to a server and server sends Reply to a client. I.e. all communication between client and server is a bidirectional exchange of Command and Reply messages.  Each Command has id field. This is an incremental uint32 field. This field will be echoed in a server replies to commands so client could match a certain Reply to Command sent before. This is important since Websocket is an asynchronous transport where server and client both send messages at any moment and there is no builtin request-response matching. Having id allows matching a reply with a command send before on SDK level.  In JSON case client can send command like this:  {&quot;id&quot;: 1, &quot;subscribe&quot;: {&quot;channel&quot;: &quot;example&quot;}}   And client can expect something like this in response:  {&quot;id&quot;: 1, &quot;subscribe&quot;: {}}   Reply for different commands has corresponding field with command result (&quot;subscribe&quot; in example above).  Reply can also contain error if Command processing resulted into an error on a server. error is optional and if Reply does not have error then it means that Command processed successfully and client can handle result object appropriately.  error looks like this in JSON case:  { &quot;code&quot;: 100, &quot;message&quot;: &quot;internal server error&quot;, &quot;temporary&quot;: true }   I.e. reply with error may look like this:  {&quot;id&quot;: 1, &quot;error&quot;: {&quot;code&quot;: 100, &quot;message&quot;: &quot;internal server error&quot;}}   We will talk more about error handling below.  Centrifuge library defines several command types client can issue. A well-written client must be aware of all those commands and client workflow.  Current commands:  connect – sent to authenticate connection, sth like hello from a client which can carry authentication token and arbitrary data.subscribe – sent to subscribe to a channelunsubscribe - sent to unsubscribe from a channelpublish - sent to publish data into a channelpresence - sent to request presence information from a channelpresence_stats - sent to request presence stats information from a channelhistory - sent to request history information for a channelsend - sent to send async message to a server (this command is a bit special since it must not contain id - as we don't wait for any response from a server in this case).rpc - sent to send RPC to a channel (execute arbitrary logic and wait for response)refresh - sent to refresh connection tokensub_refresh - sent to refresh channel subscription token  ","version":"v5","tagName":"h2"},{"title":"Asynchronous pushes​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#asynchronous-pushes","content":" The special type of Reply is asynchronous Reply. Such replies have no id field set (or id can be equal to zero). Async replies can come to a client at any moment - not as reaction to issued Command but as a message from a server to a client at arbitrary time. For example, this can be a message published into channel.  There are several types of asynchronous messages that can come from a server to a client.  pub is a message published into channeljoin messages sent when someone joined (subscribed on) channel.leave messages sent when someone left (unsubscribed from) channel.unsubscribe message sent when a server unsubscribed current client from a channel:subscribe may be sent when a server subscribes client to a channel.disconnect may be sent be a server before closing connection and contains disconnect code/reasonmessage may be sent when server sends asynchronous message to a clientconnect push can be sent in unidirectional transport caserefresh may be sent when a server refreshes client credentials (useful in unidirectional transports)  ","version":"v5","tagName":"h2"},{"title":"Top level batching​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#top-level-batching","content":" To reduce number of system calls one request from a client to a server and one response from a server to a client can have more than one Command or Reply. This allows reducing number of system calls for writing and reading data.  When JSON format used then many Command can be sent from client to server in JSON streaming line-delimited format. I.e. each individual Command encoded to JSON and then commands joined together using new line symbol \\n:  {&quot;id&quot;: 1, &quot;subscribe&quot;: {&quot;channel&quot;: &quot;ch1&quot;}} {&quot;id&quot;: 2, &quot;subscribe&quot;: {&quot;channel&quot;: &quot;ch2&quot;}}   Here is an example how we do this in Javascript client when JSON format used:  function encodeCommands(commands) { const encodedCommands = []; for (const i in commands) { if (commands.hasOwnProperty(i)) { encodedCommands.push(JSON.stringify(commands[i])); } } return encodedCommands.join('\\n'); }   info This doc uses JSON format for examples because it's human-readable. Everything said here for JSON is also true for Protobuf encoded case. There is a difference how several individual Command or server Reply joined into one request – see details below. Also, in JSON format bytes fields transformed into embedded JSON by Centrifugo.  When Protobuf format used then many Command can be sent from a client to a server in a length-delimited format where each individual Command marshaled to bytes prepended by varint length. See existing client implementations for encoding example.  The same rules relate to many Reply in one response from server to client. Line-delimited JSON and varint-length prefixed Protobuf also used there.  tip Server can even send reply to a command and asynchronous message batched together in a one frame.  For example here is how we read server response and extracting individual replies in Javascript client when JSON format used:  function decodeReplies(data) { const replies = []; const encodedReplies = data.split('\\n'); for (const i in encodedReplies) { if (encodedReplies.hasOwnProperty(i)) { if (!encodedReplies[i]) { continue; } const reply = JSON.parse(encodedReplies[i]); replies.push(reply); } } return replies; }   For Protobuf case see existing client implementations for decoding example.  ","version":"v5","tagName":"h2"},{"title":"Ping Pong​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#ping-pong","content":" To maintain connection alive and detect broken connections server periodically sends empty commands to clients and expects empty replies from them.  When client does not receive ping from a server for some time it can consider connection broken and try to reconnect. Usually a server sends pings every 25 seconds.  ","version":"v5","tagName":"h2"},{"title":"Handle disconnects​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#handle-disconnects","content":" Client should handle disconnect advices from server. In websocket case disconnect advice is sent in CLOSE Websocket frame. Disconnect advice contains uint32 code and human-readable string reason.  ","version":"v5","tagName":"h2"},{"title":"Handle errors​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#handle-errors","content":" This section contains advices to error handling in client implementations.  Errors can happen during various operations and can be handled in special way in context of some commands to tolerate network and server problems.  Errors during connect must result in full client reconnect with exponential backoff strategy. The special case is error with code 110 which signals that connection token already expired. As we said above client should update its connection JWT before connecting to server again.  Errors during subscribe must result in full client reconnect in case of internal error (code 100). And be sent to subscribe error event handler of subscription if received error is persistent. Persistent errors are errors like permission denied, bad request, namespace not found etc. Persistent errors in most situation mean a mistake from developers side.  The special corner case is client-side timeout during subscribe operation. As protocol is asynchronous it's possible in this case that server will eventually subscribe client on channel but client will think that it's not subscribed. It's possible to retry subscription request and tolerate already subscribed (code 105) error as expected. But the simplest solution is to reconnect entirely as this is simpler and gives client a chance to connect to working server instance.  Errors during rpc-like operations can be just returned to caller - i.e. user javascript code. Calls like history and presence are idempotent. You should be accurate with non-idempotent operations like publish - in case of client timeout it's possible to send the same message into channel twice if retry publish after timeout - so users of libraries must care about this case – making sure they have some protection from displaying message twice on client side (maybe some sort of unique key in payload).  ","version":"v5","tagName":"h2"},{"title":"Additional notes​","type":1,"pageTitle":"Client protocol","url":"/docs/transports/client_protocol#additional-notes","content":" Client protocol does not allow one client connection to subscribe to the same channel twice. In this case client will receive already subscribed error in a reply to a subscribe command. ","version":"v5","tagName":"h2"},{"title":"Client real-time SDKs","type":0,"sectionRef":"#","url":"/docs/transports/client_sdk","content":"","keywords":"","version":"v5"},{"title":"List of client SDKs​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/transports/client_sdk#list-of-client-sdks","content":" Here is a list of SDKs maintained by Centrifugal Labs:  centrifuge-js – for a browser, NodeJS and React Nativecentrifuge-go - for Go languagecentrifuge-dart - for Dart and Flutter (mobile and web)centrifuge-swift – for native iOS developmentcentrifuge-java – for native Android development and general Java  SDKs driven by the community:  centrifuge-csharp - SDK in C# for .NET and Unity 2022.3+  See a description of client protocol if you want to write a custom bidirectional connector or eager to learn how Centrifugo protocol internals are structured. In case of any question how protocol works take a look at existing SDK source code or reach out in the community rooms.  ","version":"v5","tagName":"h2"},{"title":"Protobuf and JSON formats in SDKs​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/transports/client_sdk#protobuf-and-json-formats-in-sdks","content":" Centrifugo real-time SDKs work using two possible serialization formats: JSON and Protobuf. The entire bidirectional client protocol is described by the Protobuf schema. But those Protobuf messages may be also encoded as JSON objects (in JSON representation bytes fields in the Protobuf schema is replaced by the embedded JSON object in Centrifugo case).  Our Javascript SDK - centrifuge-js - uses JSON serialization for protocol frames by default. This makes communication with Centrifugo server convenient as we are exchanging human-readable JSON frames between client and server. And it makes it possible to use centrifuge-js without extra dependency to protobuf.js library. It's possible to switch to Protobuf protocol with centrifuge-js SDK though, in case you want more compact Centrifuge protocol representation, faster decode/encode speeds on Centrifugo server side, or payloads you need to pass are custom binary. See more details on how to use centrifuge-js with Protobuf serialization in README.  centrifuge-go real-time SDK for Go language also supports both JSON and Protobuf formats when communicating with Centrifugo server.  Other SDKs, like centrifuge-dart, centrifuge-swift, centrifuge-java work using only Protobuf serialization for Centrifuge protocol internally. So they utilize the fastest and the most compact wire representation by default. Note, that while internally in those SDKs the serialization format is Protobuf, you can still send JSON towards these clients as JSON objects may be encoded as UTF-8 bytes. So these SDKs may work with both custom binary and JSON payloads.  There are some important notes about JSON and Protobuf interoperability mentioned in our FAQ.  ","version":"v5","tagName":"h2"},{"title":"SDK feature matrix​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/transports/client_sdk#sdk-feature-matrix","content":" Below you can find an information regarding support of different features in our official client SDKs  ","version":"v5","tagName":"h2"},{"title":"Connection related features​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/transports/client_sdk#connection-related-features","content":" Client feature\tjs\tdart\tswift\tgo\tjavaconnect to a server\t✅\t✅\t✅\t✅\t✅ setting client options\t✅\t✅\t✅\t✅\t✅ automatic reconnect with backoff algorithm\t✅\t✅\t✅\t✅\t✅ client state changes\t✅\t✅\t✅\t✅\t✅ command-reply\t✅\t✅\t✅\t✅\t✅ command timeouts\t✅\t✅\t✅\t✅\t✅ async pushes\t✅\t✅\t✅\t✅\t✅ ping-pong\t✅\t✅\t✅\t✅\t✅ connection token refresh\t✅\t✅\t✅\t✅\t✅ handle disconnect advice from server\t✅\t✅\t✅\t✅\t✅ server-side subscriptions\t✅\t✅\t✅\t✅\t✅ batching API\t✅ bidirectional WebSocket emulation\t✅   ","version":"v5","tagName":"h3"},{"title":"Client-side subscription related features​","type":1,"pageTitle":"Client real-time SDKs","url":"/docs/transports/client_sdk#client-side-subscription-related-features","content":" Client feature\tjs\tdart\tswift\tgo\tjavasubscrbe to a channel\t✅\t✅\t✅\t✅\t✅ setting subscription options\t✅\t✅\t✅\t✅\t✅ automatic resubscribe with backoff algorithm\t✅\t✅\t✅\t✅\t✅ subscription state changes\t✅\t✅\t✅\t✅\t✅ subscription command-reply\t✅\t✅\t✅\t✅\t✅ subscription async pushes\t✅\t✅\t✅\t✅\t✅ subscription token refresh\t✅\t✅\t✅\t✅\t✅ handle unsubscribe advice from server\t✅\t✅\t✅\t✅\t✅ manage subscription registry\t✅\t✅\t✅\t✅\t✅ optimistic subscriptions\t✅  ","version":"v5","tagName":"h3"},{"title":"HTTP streaming, with bidirectional emulation","type":0,"sectionRef":"#","url":"/docs/transports/http_stream","content":"","keywords":"","version":"v5"},{"title":"Options​","type":1,"pageTitle":"HTTP streaming, with bidirectional emulation","url":"/docs/transports/http_stream#options","content":" ","version":"v5","tagName":"h2"},{"title":"http_stream​","type":1,"pageTitle":"HTTP streaming, with bidirectional emulation","url":"/docs/transports/http_stream#http_stream","content":" Boolean, default: false.  Enables HTTP streaming endpoint. And enables emulation endpoint (/emulation by default) to accept emulation HTTP requests from clients.  config.json { ... &quot;http_stream&quot;: true }   When enabling http_stream you can connect to /connection/http_stream from centrifuge-js. Note that our bidirectional emulation also uses /emulation endpoint of Centrifugo to send requests from client to server. This is required because HTTP streaming is a unidirectional transport in its nature. So we use HTTP call to send data from client to server and proxy this call to the correct Centrifugo node which handles the connection. Thus achieving bidirectional behaviour - see details about Centrifugo bidirectional emulation layer. Make sure /emulation endpoint is available for requests from the client side too. If required, you can also control both HTTP streaming connection url prefix and emulation endpoint prefix, see customizing endpoints.  ","version":"v5","tagName":"h3"},{"title":"http_stream_max_request_body_size​","type":1,"pageTitle":"HTTP streaming, with bidirectional emulation","url":"/docs/transports/http_stream#http_stream_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes. ","version":"v5","tagName":"h3"},{"title":"Real-time transports","type":0,"sectionRef":"#","url":"/docs/transports/overview","content":"","keywords":"","version":"v5"},{"title":"Bidirectional​","type":1,"pageTitle":"Real-time transports","url":"/docs/transports/overview#bidirectional","content":" Bidirectional transports are capable to serve all Centrifugo features. These transports are the main Centrifugo focus and where Centrifugo really shines.  Bidirectional transports come with a cost that developers need to use a special client connector library (SDK) which speaks Centrifugo client protocol. The reason why we need a special client connector library is that a bidirectional connection is asynchronous – it's required to match requests to responses, properly manage connection state, handle request queueing/timeouts/errors, etc. And of course to multiplex subscriptions to different channels over a single connection.  Centrifugo has several official client SDKs for popular environments. All of them work over WebSocket transport. Our Javascript SDK also offers bidirectional fallbacks over HTTP-Streaming, Server-Sent Events (SSE), SockJS, and has an experimental support for WebTransport.  ","version":"v5","tagName":"h2"},{"title":"Unidirectional​","type":1,"pageTitle":"Real-time transports","url":"/docs/transports/overview#unidirectional","content":" Unidirectional transports suit well for simple use-cases with stable subscriptions, usually known at connection time.  The advantage is that unidirectional transports do not require special client connectors - developers can use native browser APIs (like WebSocket, EventSource/SSE, HTTP-streaming), or GRPC generated code to receive real-time updates from Centrifugo. Thus avoiding dependency to a client connector that abstracts bidirectional communication.  The drawback is that with unidirectional transports you are not inheriting all Centrifugo features out of the box (like dynamic subscriptions/unsubscriptions, automatic message recovery on reconnect, possibility to send RPC calls over persistent connection). But some of the missing client APIs can be mimicked by using calls to Centrifugo server API (i.e. over client -&gt; application backend -&gt; Centrifugo).  Learn more about unidirectional protocol and available unidirectional transports.  ","version":"v5","tagName":"h2"},{"title":"PING/PONG behavior​","type":1,"pageTitle":"Real-time transports","url":"/docs/transports/overview#pingpong-behavior","content":" Centrifugo server periodically sends pings to clients and expects pong from clients that works over bidirectional transports. Sending ping and receiving pong allows to find broken connections faster. Centrifugo sends pings on the Centrifugo client protocol level, thus it's possible for clients to handle ping messages on the client side to make sure connection is not broken (our bidirectional SDKs do this automatically).  By default Centrifugo sends pings every 25 seconds. This may be changed using ping_interval option (duration, default &quot;25s&quot;).  Centrifugo expects pong message from bidirectional client SDK after sending ping to it. By default, it waits no more than 8 seconds before closing a connection. This may be changed using pong_timeout option (duration, default &quot;8s&quot;).  In most cases default ping/pong intervals are fine so you don't really need to tweak them. Reducing timeouts may help you to find non-gracefully closed connections faster, but will increase network traffic and CPU resource usage since ping/pongs are sent faster.  caution ping_interval must be greater than pong_timeout in the current implementation.  Here is a scheme how ping/pong works in bidirectional and unidirectional client scenarios:   ","version":"v5","tagName":"h2"},{"title":"SockJS","type":0,"sectionRef":"#","url":"/docs/transports/sockjs","content":"","keywords":"","version":"v5"},{"title":"SockJS caveats​","type":1,"pageTitle":"SockJS","url":"/docs/transports/sockjs#sockjs-caveats","content":" caution There are several important caveats to know when using SockJS – see below.  ","version":"v5","tagName":"h2"},{"title":"Sticky sessions​","type":1,"pageTitle":"SockJS","url":"/docs/transports/sockjs#sticky-sessions","content":" First is that you need to use sticky sessions mechanism if you have more than one Centrifugo nodes running behind a load balancer. This mechanism usually supported by load balancers (for example Nginx). Sticky sessions mean that all requests from the same client will come to the same Centrifugo node. This is necessary because SockJS maintains connection session in process memory thus allowing bidirectional communication between a client and a server. Sticky mechanism not required if you only use one Centrifugo node on a backend.  For example, with Nginx sticky support can be enabled with ip_hash directive for upstream:  upstream centrifugo { ip_hash; server 127.0.0.1:8000; server 127.0.0.2:8000; }   With this configuration Nginx will proxy connections with the same ip address to the same upstream backend.  But ip_hash; is not the best choice in this case, because there could be situations where a lot of different connections are coming with the same IP address (behind proxies) and the load balancing system won't be fair.  So the best solution would be using something like nginx-sticky-module which uses setting a special cookie to track the upstream server for a client.  ","version":"v5","tagName":"h3"},{"title":"Browser only​","type":1,"pageTitle":"SockJS","url":"/docs/transports/sockjs#browser-only","content":" SockJS is only supported by centrifuge-js – i.e. our browser client. There is no much sense to use SockJS outside of a browser these days.  ","version":"v5","tagName":"h3"},{"title":"JSON only​","type":1,"pageTitle":"SockJS","url":"/docs/transports/sockjs#json-only","content":" One more thing to be aware of is that SockJS does not support binary data, so there is no option to use Centrifugo Protobuf protocol on top of SockJS (unlike WebSocket). Only JSON payloads can be transferred.  ","version":"v5","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"SockJS","url":"/docs/transports/sockjs#options","content":" ","version":"v5","tagName":"h2"},{"title":"sockjs​","type":1,"pageTitle":"SockJS","url":"/docs/transports/sockjs#sockjs","content":" Boolean, default: false.  Enables SockJS transport.  ","version":"v5","tagName":"h3"},{"title":"sockjs_url​","type":1,"pageTitle":"SockJS","url":"/docs/transports/sockjs#sockjs_url","content":" Default: https://cdn.jsdelivr.net/npm/sockjs-client@1/dist/sockjs.min.js  Link to SockJS url which is required when iframe-based HTTP fallbacks are in use. ","version":"v5","tagName":"h3"},{"title":"SSE (EventSource), with bidirectional emulation","type":0,"sectionRef":"#","url":"/docs/transports/sse","content":"","keywords":"","version":"v5"},{"title":"Options​","type":1,"pageTitle":"SSE (EventSource), with bidirectional emulation","url":"/docs/transports/sse#options","content":" ","version":"v5","tagName":"h2"},{"title":"sse​","type":1,"pageTitle":"SSE (EventSource), with bidirectional emulation","url":"/docs/transports/sse#sse","content":" Boolean, default: false.  Enables SSE (EventSource) endpoint. And enables emulation endpoint (/emulation by default) to accept emulation HTTP requests from clients.  config.json { ... &quot;sse&quot;: true }   When enabling sse you can connect to /connection/sse from centrifuge-js. Note that our bidirectional emulation also uses /emulation endpoint of Centrifugo to send requests from client to server. This is required because SSE/EventSource is a unidirectional transport in its nature. So we use HTTP call to send data from client to server and proxy this call to the correct Centrifugo node which handles the connection. Thus achieving bidirectional behaviour - see details about Centrifugo bidirectional emulation layer. Make sure /emulation endpoint is available for requests from the client side too. If required, you can also control both SSE connection url prefix and emulation endpoint prefix, see customizing endpoints.  ","version":"v5","tagName":"h3"},{"title":"sse_max_request_body_size​","type":1,"pageTitle":"SSE (EventSource), with bidirectional emulation","url":"/docs/transports/sse#sse_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes when using HTTP POST requests to connect (browsers are using GET so it's not applied). ","version":"v5","tagName":"h3"},{"title":"Proxy events to the backend","type":0,"sectionRef":"#","url":"/docs/server/proxy","content":"","keywords":"","version":"v5"},{"title":"HTTP proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#http-proxy","content":" HTTP proxy in Centrifugo converts client connection events into HTTP calls to the application backend.  ","version":"v5","tagName":"h2"},{"title":"HTTP request structure​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#http-request-structure","content":" All HTTP proxy calls are HTTP POST requests that will be sent from Centrifugo to configured endpoints with a configured timeout. These requests will have some headers copied from the original client request (see details below) and include JSON body which varies depending on call type (for example data sent by a client in RPC call etc, see more details about JSON bodies below).  ","version":"v5","tagName":"h3"},{"title":"Proxy HTTP headers​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#proxy-http-headers","content":" The good thing about Centrifugo HTTP proxy is that it transparently proxies original HTTP request headers in a request to app backend. In most cases this allows achieving transparent authentication on the application backend side. But it's required to provide an explicit list of HTTP headers you want to be proxied, for example:  config.json { ... &quot;proxy_http_headers&quot;: [ &quot;Origin&quot;, &quot;User-Agent&quot;, &quot;Cookie&quot;, &quot;Authorization&quot;, &quot;X-Real-Ip&quot;, &quot;X-Forwarded-For&quot;, &quot;X-Request-Id&quot; ] }   Alternatively, you can set a list of headers via an environment variable (space separated):  export CENTRIFUGO_PROXY_HTTP_HEADERS=&quot;Cookie User-Agent X-B3-TraceId X-B3-SpanId&quot; ./centrifugo   note Centrifugo forces the Content-Type header to be application/json in all HTTP proxy requests since it sends the body in JSON format to the application backend.  Starting from Centrifugo v5.0.2 it's possible to configure static set of headers to be appended to all HTTP proxy requests:  config.json { ... &quot;proxy_static_http_headers&quot;: { &quot;X-Custom-Header&quot;: &quot;custom value&quot; } }   proxy_static_http_headers is a map with string keys and string values. You may also set it over environment variable using JSON object string:  export CENTRIFUGO_PROXY_STATIC_HTTP_HEADERS='{&quot;X-Custom-Header&quot;: &quot;custom value&quot;}'   Static headers may be overriden by the header from client connection request if you proxy the header with the same name inside proxy_http_headers option showed above.  ","version":"v5","tagName":"h3"},{"title":"Proxy GRPC metadata​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#proxy-grpc-metadata","content":" When GRPC unidirectional stream is used as a client transport then you may want to proxy GRPC metadata from the client request. In this case you may configure proxy_grpc_metadata option. This is an array of string metadata keys which will be proxied. These metadata keys transformed to HTTP headers of proxy request. By default no metadata keys are proxied.  See below the table of rules how metadata and headers proxied in transport/proxy different scenarios.  ","version":"v5","tagName":"h3"},{"title":"Connect proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#connect-proxy","content":" With the following options in the configuration file:  { ... &quot;proxy_connect_endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;proxy_connect_timeout&quot;: &quot;1s&quot; }   – connection requests without JWT set will be proxied to proxy_connect_endpoint URL endpoint. On your backend side, you can authenticate the incoming connection and return client credentials to Centrifugo in response to the proxied request.  danger Make sure you properly configured allowed_origins Centrifugo option or check request origin on your backend side upon receiving connect request from Centrifugo. Otherwise, your site can be vulnerable to CSRF attacks if you are using WebSocket transport for client connections.  Yes, this means you don't need to generate JWT and pass it to a client-side and can rely on a cookie while authenticating the user. Centrifugo should work on the same domain in this case so your site cookie could be passed to Centrifugo by browsers. In many cases your existing session mechanism will provide user authentication details to the connect proxy handler.  tip If you want to pass some custom authentication token from a client side (not in Centrifugo JWT format) but force request to be proxied then you may put it in a cookie or use connection request custom data field (available in all our transports). This data can contain arbitrary payload you want to pass from a client to a server.  This also means that every new connection from a user will result in an HTTP POST request to your application backend. While with JWT token you usually generate it once on application page reload, if client reconnects due to Centrifugo restart or internet connection loss it uses the same JWT it had before thus usually no additional requests are generated during reconnect process (until JWT expired).    Payload example that will be sent to app backend when client without token wants to establish a connection with Centrifugo and proxy_connect_endpoint is set to non-empty URL string:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot; }   Expected response example:  {&quot;result&quot;: {&quot;user&quot;: &quot;56&quot;}}   This response allows connecting and tells Centrifugo the ID of a user. See below the full list of supported fields in the result.  Several app examples which use connect proxy can be found in our blog:  With NodeJSWith DjangoWith Laravel  Connect request fields​  This is what sent from Centrifugo to application backend in case of connect proxy request.  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) name\tstring\tyes\toptional name of the client (this field will only be set if provided by a client on connect) version\tstring\tyes\toptional version of the client (this field will only be set if provided by a client on connect) data\tJSON\tyes\toptional data from client (this field will only be set if provided by a client on connect) b64data\tstring\tyes\toptional data from the client in base64 format (if the binary proxy mode is used) channels\tArray of strings\tyes\tlist of server-side channels client want to subscribe to, the application server must check permissions and add allowed channels to result  Connect result fields​  This is what application returns to Centrifugo inside result field in case of connect proxy request.  Field\tType\tOptional\tDescriptionuser\tstring\tno\tuser ID (calculated on app backend based on request cookie header for example). Return it as an empty string for accepting unauthenticated requests expire_at\tinteger\tyes\ta timestamp (Unix seconds in the future) when connection must be considered expired. If not set or set to 0 connection won't expire at all info\tJSON\tyes\ta connection info JSON b64info\tstring\tyes\tbinary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages data\tJSON\tyes\ta custom data to send to the client in connect command response. b64data\tstring\tyes\ta custom data to send to the client in the connect command response for binary connections, will be decoded to raw bytes on Centrifugo side before sending to client channels\tarray of strings\tyes\tallows providing a list of server-side channels to subscribe connection to. See more details about server-side subscriptions subs\tmap of SubscribeOptions\tyes\tmap of channels with options to subscribe connection to. See more details about server-side subscriptions meta\tJSON object (ex. {&quot;key&quot;: &quot;value&quot;})\tyes\ta custom data to attach to connection (this won't be exposed to client-side)  Options​  proxy_connect_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  Example​  Here is the simplest example of the connect handler in Tornado Python framework (note that in a real system you need to authenticate the user on your backend side, here we just return &quot;56&quot; as user ID):  class CentrifugoConnectHandler(tornado.web.RequestHandler): def check_xsrf_cookie(self): pass def post(self): self.set_header('Content-Type', 'application/json; charset=&quot;utf-8&quot;') data = json.dumps({ 'result': { 'user': '56' } }) self.write(data) def main(): options.parse_command_line() app = tornado.web.Application([ (r'/centrifugo/connect', CentrifugoConnectHandler), ]) app.listen(3000) tornado.ioloop.IOLoop.instance().start() if __name__ == '__main__': main()   This example should help you to implement a similar HTTP handler in any language/framework you are using on the backend side.  We also have a tutorial in the blog about Centrifugo integration with NodeJS which uses connect proxy and native session middleware of Express.js to authenticate connections. Even if you are not using NodeJS on a backend a tutorial can help you understand the idea.  What if connection is unauthenticated/unauthorized to connect?​  In this case return a disconnect object as a response. See Return custom disconnect section. Depending on whether you want connection to reconnect or not (usually not) you can select the appropriate disconnect code. Sth like this in response:  { &quot;disconnect&quot;: { &quot;code&quot;: 4501, &quot;reason&quot;: &quot;unauthorized&quot; } }   – may be sufficient enough. Choosing codes and reason is up to the developer, but follow the rules described in Return custom disconnect section.  ","version":"v5","tagName":"h3"},{"title":"Refresh proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#refresh-proxy","content":" With the following options in the configuration file:  { ... &quot;proxy_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/refresh&quot;, &quot;proxy_refresh_timeout&quot;: &quot;1s&quot; }   – Centrifugo will call proxy_refresh_endpoint when it's time to refresh the connection. Centrifugo itself will ask your backend about connection validity instead of refresh workflow on the client-side.  The payload sent to app backend in refresh request (when the connection is going to expire):  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot; }   Expected response example:  {&quot;result&quot;: {&quot;expire_at&quot;: 1565436268}}   Refresh request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc.) protocol\tstring\tno\tprotocol type used by client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Refresh result fields​  Field\tType\tOptional\tDescriptionexpired\tbool\tyes\ta flag to mark the connection as expired - the client will be disconnected expire_at\tinteger\tyes\ta timestamp in the future when connection must be considered expired info\tJSON\tyes\ta connection info JSON b64info\tstring\tyes\tbinary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages  Options​  proxy_refresh_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  ","version":"v5","tagName":"h3"},{"title":"RPC proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#rpc-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_rpc_endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;proxy_rpc_timeout&quot;: &quot;1s&quot; }   RPC calls over client connection will be proxied to proxy_rpc_endpoint. This allows a developer to utilize WebSocket connection (or any other bidirectional transport Centrifugo supports) in a bidirectional way.  Payload example sent to app backend in RPC request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;method&quot;: &quot;getCurrentPrice&quot;, &quot;data&quot;:{&quot;params&quot;: {&quot;object_id&quot;: 12}} }   Expected response example:  {&quot;result&quot;: {&quot;data&quot;: {&quot;answer&quot;: &quot;2019&quot;}}}   RPC request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket or sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process method\tstring\tyes\tan RPC method string, if the client does not use named RPC call then method will be omitted data\tJSON\tyes\tRPC custom data sent by client b64data\tstring\tyes\twill be set instead of data field for binary proxy mode meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  RPC result fields​  Field\tType\tOptional\tDescriptiondata\tJSON\tyes\tRPC response - any valid JSON is supported b64data\tstring\tyes\tcan be set instead of data for binary response encoded in base64 format  Options​  proxy_rpc_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  See below on how to return a custom error.  ","version":"v5","tagName":"h3"},{"title":"Subscribe proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#subscribe-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot; }   – subscribe requests sent over client connection will be proxied to proxy_subscribe_endpoint. This allows you to check the access of the client to a channel.  info Subscribe proxy does not proxy subscriptions with token and subscriptions to user-limited channels at the moment. That's because those are already providing channel access control. Subscribe proxy assumes that all the permission management happens on the backend side when processing proxy request. So if you need to get subscribe proxy requests for all channels in the system - do not use subscription tokens and user-limited channels.  Unlike proxy types described above subscribe proxy must be enabled per channel namespace. This means that every namespace (including global/default one) has a boolean option proxy_subscribe that enables subscribe proxy for channels in a namespace.  So to enable subscribe proxy for channels without namespace define proxy_subscribe on a top configuration level:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;proxy_subscribe&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_subscribe_endpoint&quot;: &quot;http://localhost:3000/centrifugo/subscribe&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;proxy_subscribe&quot;: true }] }   Payload example sent to app backend in subscribe request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;chat:index&quot; }   Expected response example if subscription is allowed:  {&quot;result&quot;: {}}   Subscribe request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket or sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\ta string channel client wants to subscribe to meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true) data\tJSON\tyes\tcustom data from client sent with subscription request (this field will only be set if provided by a client on subscribe). b64data\tstring\tyes\toptional subscription data from the client in base64 format (if the binary proxy mode is used).  Subscribe result fields​  Field\tType\tOptional\tDescriptioninfo\tJSON\tyes\ta channel info JSON b64info\tstring\tyes\ta binary connection channel info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using data\tJSON\tyes\ta custom data to send to the client in subscribe command reply. b64data\tstring\tyes\ta custom data to send to the client in subscribe command reply, will be decoded to raw bytes on Centrifugo side before sending to client override\tOverride object\tyes\tAllows dynamically override some channel options defined in Centrifugo configuration on a per-connection basis (see below available fields) expire_at\tinteger\tyes\ta timestamp (Unix seconds in the future) when subscription must be considered expired. If not set or set to 0 subscription won't expire at all. Supported since Centrifugo v5.0.4  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave force_push_join_leave\tBoolValue\tyes\tOverride force_push_join_leave force_positioning\tBoolValue\tyes\tOverride force_positioning force_recovery\tBoolValue\tyes\tOverride force_recovery  BoolValue is an object like this:  { &quot;value&quot;: true/false }   See below on how to return an error in case you don't want to allow subscribing.  Options​  proxy_subscribe_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  What if connection is not allowed to subscribe?​  In this case you can return error object as a subscribe handler response. See return custom error section.  In general, frontend applications should not try to subscribe to channels for which access is not allowed. But these situations can happen or malicious user can try to subscribe to a channel. In most scenarios returning:  { &quot;error&quot;: { &quot;code&quot;: 403, &quot;message&quot;: &quot;permission denied&quot; } }   – is sufficient enough. Error code may be not 403 actually, no real reason to force HTTP semantics here - so it's up to Centrifugo user to decide. Just keep it in range [400, 1999] as described here.  If case of returning response above, on client side unsubscribed event of Subscription object will be called with error code 403. Subscription won't resubscribe automatically after that.  ","version":"v5","tagName":"h3"},{"title":"Publish proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#publish-proxy","content":" With the following option in the configuration file:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot; }   – publish calls sent by a client will be proxied to proxy_publish_endpoint.  This request happens BEFORE a message is published to a channel, so your backend can validate whether a client can publish data to a channel. An important thing here is that publication to the channel can fail after your backend successfully validated publish request (for example publish to Redis by Centrifugo returned an error). In this case, your backend won't know about the error that happened but this error will propagate to the client-side.    Like the subscribe proxy, publish proxy must be enabled per channel namespace. This means that every namespace (including the global/default one) has a boolean option proxy_publish that enables publish proxy for channels in the namespace. All other namespace options will be taken into account before making a proxy request, so you also need to turn on the publish option too.  So to enable publish proxy for channels without namespace define proxy_publish and publish on a top configuration level:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_publish_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true }] }   Keep in mind that this will only work if the publish channel option is on for a channel namespace (or for a global top-level namespace).  Payload example sent to app backend in a publish request:  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;chat:index&quot;, &quot;data&quot;:{&quot;input&quot;:&quot;hello&quot;} }   Expected response example if publish is allowed:  {&quot;result&quot;: {}}   Publish request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs) protocol\tstring\tno\tprotocol type used by the client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\ta string channel client wants to publish to data\tJSON\tyes\tdata sent by client b64data\tstring\tyes\twill be set instead of data field for binary proxy mode meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Publish result fields​  Field\tType\tOptional\tDescriptiondata\tJSON\tyes\tan optional JSON data to send into a channel instead of original data sent by a client b64data\tstring\tyes\ta binary data encoded in base64 format, the meaning is the same as for data above, will be decoded to raw bytes on Centrifugo side before publishing skip_history\tbool\tyes\twhen set to true Centrifugo won't save publication to the channel history  See below on how to return an error in case you don't want to allow publishing.  Options​  proxy_publish_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  ","version":"v5","tagName":"h3"},{"title":"Sub refresh proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#sub-refresh-proxy","content":" With the following options in the configuration file:  { ... &quot;proxy_sub_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/sub_refresh&quot;, &quot;proxy_sub_refresh_timeout&quot;: &quot;1s&quot; }   – Centrifugo will call proxy_sub_refresh_endpoint when it's time to refresh the subscription. Centrifugo itself will ask your backend about subscription validity instead of subscription refresh workflow on the client-side.  Like subscribe and publish proxy types, sub refresh proxy must be enabled per channel namespace. This means that every namespace (including the global/default one) has a boolean option proxy_sub_refresh that enables sub refresh proxy for channels in the namespace. Only subscriptions which have expiration time will be validated over sub refresh proxy endpoint.  Sub refresh proxy may be used as a periodical Subscription liveness callback from Centrifugo to app backend.  caution In the current implementation the delay of Subscription refresh requests from Centrifugo to application backend may be up to one minute (was implemented this way from a simplicity and efficiency perspective). We assume this should be enough for many scenarios. But this may be improved if needed. Please reach us out with a detailed description of your use case where you want more accurate requests to refresh subscriptions.  So to enable sub refresh proxy for channels without namespace define proxy_sub_refresh on a top configuration level:  { ... &quot;proxy_sub_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/sub_refresh&quot;, &quot;proxy_sub_refresh&quot;: true }   Or for channels in namespace sun:  { ... &quot;proxy_sub_refresh_endpoint&quot;: &quot;http://localhost:3000/centrifugo/publish&quot;, &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;sun&quot;, &quot;proxy_sub_refresh&quot;: true }] }   The payload sent to app backend in sub refresh request (when the subscription is going to expire):  { &quot;client&quot;:&quot;9336a229-2400-4ebc-8c50-0a643d22e8a0&quot;, &quot;transport&quot;:&quot;websocket&quot;, &quot;protocol&quot;: &quot;json&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;user&quot;:&quot;56&quot;, &quot;channel&quot;: &quot;channel&quot; }   Expected response example:  {&quot;result&quot;: {&quot;expire_at&quot;: 1565436268}}   Sub refresh request fields​  Field\tType\tOptional\tDescriptionclient\tstring\tno\tunique client ID generated by Centrifugo for each incoming connection transport\tstring\tno\ttransport name (ex. websocket, sockjs, uni_sse etc.) protocol\tstring\tno\tprotocol type used by client (json or protobuf at moment) encoding\tstring\tno\tprotocol encoding type used (json or binary at moment) user\tstring\tno\ta connection user ID obtained during authentication process channel\tstring\tno\tchannel for which Subscription is going to expire meta\tJSON\tyes\ta connection attached meta (off by default, enable with &quot;proxy_include_connection_meta&quot;: true)  Sub refresh result fields​  Field\tType\tOptional\tDescriptionexpired\tbool\tyes\ta flag to mark the subscription as expired - the client will be disconnected expire_at\tinteger\tyes\ta timestamp in the future (Unix seconds) when subscription must be considered expired info\tJSON\tyes\ta channel info JSON b64info\tstring\tyes\tbinary channel info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages  Options​  proxy_sub_refresh_timeout (duration) config option controls timeout of HTTP POST request sent to app backend. By default 1s.  ","version":"v5","tagName":"h3"},{"title":"Return custom error​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#return-custom-error","content":" Application backend can return JSON object that contains an error to return it to the client:  { &quot;error&quot;: { &quot;code&quot;: 1000, &quot;message&quot;: &quot;custom error&quot; } }   Applications must use error codes in range [400, 1999]. Error code field is uint32 internally.  note Returning custom error does not apply to response for refresh and sub refresh proxy requests as there is no sense in returning an error (will not reach client anyway). I.e. custom error is only processed for connect, subscribe, publish and rpc proxy types.  ","version":"v5","tagName":"h3"},{"title":"Return custom disconnect​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#return-custom-disconnect","content":" Application backend can return JSON object that contains a custom disconnect object to disconnect client in a custom way:  { &quot;disconnect&quot;: { &quot;code&quot;: 4500, &quot;reason&quot;: &quot;disconnect reason&quot; } }   Application must use numbers in the range 4000-4999 for custom disconnect codes:  codes in range [4000, 4499] give client an advice to reconnectcodes in range [4500, 4999] are terminal codes – client won't reconnect upon receiving it.  Code is uint32 internally. Numbers outside of 4000-4999 range are reserved by Centrifugo internal protocol. Keep in mind that due to WebSocket protocol limitations and Centrifugo internal protocol needs you need to keep disconnect reason string no longer than 32 ASCII symbols (i.e. 32 bytes max).  note Returning custom disconnect does not apply to response for refresh and sub refresh proxy requests as there is no way to control disconnect at moment - the client will always be disconnected with expired disconnect reason. I.e. custom disconnect is only processed for connect, subscribe, publish and rpc proxy types.  ","version":"v5","tagName":"h3"},{"title":"GRPC proxy​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#grpc-proxy","content":" Centrifugo can also proxy connection events to your backend over GRPC instead of HTTP. In this case, Centrifugo acts as a GRPC client and your backend acts as a GRPC server.  GRPC service definitions can be found in the Centrifugo repository: proxy.proto.  tip GRPC proxy inherits all the fields for HTTP proxy – so you can refer to field descriptions for HTTP above. Both proxy types in Centrifugo share the same Protobuf schema definitions.  Every proxy call in this case is a unary GRPC call. Centrifugo puts client headers into GRPC metadata (since GRPC doesn't have headers concept).  All you need to do to enable proxying over GRPC instead of HTTP is to use grpc schema in endpoint, for example for the connect proxy:  config.json { ... &quot;proxy_connect_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_connect_timeout&quot;: &quot;1s&quot; }   Refresh proxy:  config.json { ... &quot;proxy_refresh_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_refresh_timeout&quot;: &quot;1s&quot; }   Or for RPC proxy:  config.json { ... &quot;proxy_rpc_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_rpc_timeout&quot;: &quot;1s&quot; }   For publish proxy in namespace chat:  config.json { ... &quot;proxy_publish_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_publish_timeout&quot;: &quot;1s&quot; &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;chat&quot;, &quot;publish&quot;: true, &quot;proxy_publish&quot;: true } ] }   Use subscribe proxy for all channels without namespaces:  config.json { ... &quot;proxy_subscribe_endpoint&quot;: &quot;grpc://localhost:12000&quot;, &quot;proxy_subscribe_timeout&quot;: &quot;1s&quot;, &quot;proxy_subscribe&quot;: true }   So the same as for HTTP, just the different endpoint scheme.  ","version":"v5","tagName":"h2"},{"title":"GRPC proxy options​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#grpc-proxy-options","content":" Some additional options exist to control GRPC proxy behavior.  proxy_grpc_cert_file​  String, default: &quot;&quot;.  Path to cert file for secure TLS connection. If not set then an insecure connection with the backend endpoint is used.  proxy_grpc_credentials_key​  String, default &quot;&quot; (i.e. not used).  Add custom key to per-RPC credentials.  proxy_grpc_credentials_value​  String, default &quot;&quot; (i.e. not used).  A custom value for proxy_grpc_credentials_key.  proxy_grpc_compression​  Bool, default false (i.e. not used).  If true then gzip compression will be used for each GRPC proxy call (available since Centrifugo v5.1.0).  ","version":"v5","tagName":"h3"},{"title":"GRPC proxy example​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#grpc-proxy-example","content":" We have an example of backend server (written in Go language) which can react to events from Centrifugo over GRPC. For other programming languages the approach is similar, i.e.:  Copy proxy Protobuf definitionsGenerate GRPC codeRun backend service with you custom business logicPoint Centrifugo to it.  ","version":"v5","tagName":"h3"},{"title":"Header proxy rules​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#header-proxy-rules","content":" Centrifugo not only supports HTTP-based client transports but also GRPC-based (for example GRPC unidirectional stream). Here is a table with rules used to proxy headers/metadata in various scenarios:  Client protocol type\tProxy type\tClient headers\tClient metadataHTTP\tHTTP\tIn proxy request headers\tN/A GRPC\tGRPC\tN/A\tIn proxy request metadata HTTP\tGRPC\tIn proxy request metadata\tN/A GRPC\tHTTP\tN/A\tIn proxy request headers  ","version":"v5","tagName":"h2"},{"title":"Binary mode​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#binary-mode","content":" As you may noticed there are several fields in request/result description of various proxy calls which use base64 encoding.  Centrifugo can work with binary Protobuf protocol (in case of bidirectional WebSocket transport). All our bidirectional clients support this.  Most Centrifugo users use JSON for custom payloads: i.e. for data sent to a channel, for connection info attached while authenticating (which becomes part of presence response, join/leave messages and added to Publication client info when message published from a client side).  But since HTTP proxy works with JSON format (i.e. sends requests with JSON body) – it can not properly pass binary data to application backend. Arbitrary binary data can't be encoded into JSON.  In this case it's possible to turn Centrifugo proxy into binary mode by using:  config.json { ... &quot;proxy_binary_encoding&quot;: true }   Once enabled this option tells Centrifugo to use base64 format in requests and utilize fields like b64data, b64info with payloads encoded to base64 instead of their JSON field analogues.  While this feature is useful for HTTP proxy it's not really required if you are using GRPC proxy – since GRPC allows passing binary data just fine.  Regarding b64 fields in proxy results – just use base64 fields when required – Centrifugo is smart enough to detect that you are using base64 field and will pick payload from it, decode from base64 automatically and will pass further to connections in binary format.  ","version":"v5","tagName":"h2"},{"title":"Granular proxy mode​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#granular-proxy-mode","content":" By default, with proxy configuration shown above, you can only define a global proxy settings and one endpoint for each type of proxy (i.e. one for connect proxy, one for subscribe proxy, and so on). Also, you can configure only one set of headers to proxy which will be used by each proxy type. This may be sufficient for many use cases, but what if you need a more granular control? For example, use different subscribe proxy endpoints for different channel namespaces (i.e. when using microservice architecture).  Centrifugo v3.1.0 introduced a new mode for proxy configuration called granular proxy mode. In this mode it's possible to configure subscribe and publish proxy behaviour on per-namespace level, use different set of headers passed to the proxy endpoint in each proxy type. Also, Centrifugo v3.1.0 introduced a concept of rpc namespaces (in addition to channel namespaces) – together with granular proxy mode this allows configuring rpc proxies on per rpc namespace basis.  ","version":"v5","tagName":"h2"},{"title":"Enable granular proxy mode​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#enable-granular-proxy-mode","content":" Since the change is rather radical it requires a separate boolean option granular_proxy_mode to be enabled. As soon as this option set Centrifugo does not use proxy configuration rules described above and follows the rules described below.  config.json { ... &quot;granular_proxy_mode&quot;: true }   ","version":"v5","tagName":"h3"},{"title":"Defining a list of proxies​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#defining-a-list-of-proxies","content":" When using granular proxy mode on configuration top level you can define &quot;proxies&quot; array with a list of different proxy objects. Each proxy object in an array should have at least two required fields: name and endpoint.  Here is an example:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [ { &quot;name&quot;: &quot;connect&quot;, &quot;endpoint&quot;: &quot;http://localhost:3000/centrifugo/connect&quot;, &quot;timeout&quot;: &quot;500ms&quot;, &quot;http_headers&quot;: [&quot;Cookie&quot;] }, { &quot;name&quot;: &quot;refresh&quot;, &quot;endpoint&quot;: &quot;http://localhost:3000/centrifugo/refresh&quot;, &quot;timeout&quot;: &quot;500ms&quot; }, { &quot;name&quot;: &quot;subscribe1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/subscribe&quot; }, { &quot;name&quot;: &quot;publish1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/publish&quot; }, { &quot;name&quot;: &quot;rpc1&quot;, &quot;endpoint&quot;: &quot;http://localhost:3001/centrifugo/rpc&quot; }, { &quot;name&quot;: &quot;subscribe2&quot;, &quot;endpoint&quot;: &quot;http://localhost:3002/centrifugo/subscribe&quot; }, { &quot;name&quot;: &quot;publish2&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3002&quot; } { &quot;name&quot;: &quot;rpc2&quot;, &quot;endpoint&quot;: &quot;grpc://localhost:3002&quot; } ] }   Let's look at all fields for a proxy object which is possible to set for each proxy inside &quot;proxies&quot; array.  Field name\tField type\tRequired\tDescriptionname\tstring\tyes\tUnique name of proxy used for referencing in configuration, must match regexp ^[-a-zA-Z0-9_.]{2,}$ endpoint\tstring\tyes\tHTTP or GRPC endpoint in the same format as in default proxy mode. For example, http://localhost:3000/path for HTTP or grpc://localhost:3000 for GRPC. timeout\tduration (string)\tno\tProxy request timeout, default &quot;1s&quot; http_headers\tarray of strings\tno\tList of headers to proxy, by default no headers static_http_headers\tmap[string]string\tno\tStatic set of headers to add to HTTP proxy requests. Note these headers only appended to HTTP proxy requests from Centrifugo to backend. Available since Centrifugo v5.0.2 grpc_metadata\tarray of strings\tno\tList of GRPC metadata keys to proxy, by default no metadata keys binary_encoding\tbool\tno\tUse base64 for payloads include_connection_meta\tbool\tno\tInclude meta information (attached on connect) grpc_cert_file\tstring\tno\tPath to cert file for secure TLS connection. If not set then an insecure connection with the backend endpoint is used. grpc_credentials_key\tstring\tno\tAdd custom key to per-RPC credentials. grpc_credentials_value\tstring\tno\tA custom value for grpc_credentials_key. grpc_compression\tbool\tno\tIf true then gzip compression will be used for each GRPC proxy call (available since Centrifugo v5.1.0)  ","version":"v5","tagName":"h3"},{"title":"Granular connect and refresh​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#granular-connect-and-refresh","content":" As soon as you defined a list of proxies you can reference them by a name to use a specific proxy configuration for a specific event.  To enable connect proxy:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;connect_proxy_name&quot;: &quot;connect&quot; }   We have an example of Centrifugo integration with NodeJS which uses granular proxy mode. Even if you are not using NodeJS on a backend an example can help you understand the idea.  Let's also add refresh proxy:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;connect_proxy_name&quot;: &quot;connect&quot;, &quot;refresh_proxy_name&quot;: &quot;refresh&quot; }   ","version":"v5","tagName":"h3"},{"title":"Granular subscribe, publish, sub refresh​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#granular-subscribe-publish-sub-refresh","content":" Subscribe, publish and sub refresh proxies work per-namespace. This means that subscribe_proxy_name, publish_proxy_name and sub_refresh_proxy_name are just channel namespace options. So it's possible to define these options on configuration top-level (for channels in default top-level namespace) or inside namespace object.  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;ns1&quot;, &quot;subscribe_proxy_name&quot;: &quot;subscribe1&quot;, &quot;publish&quot;: true, &quot;publish_proxy_name&quot;: &quot;publish1&quot; }, { &quot;name&quot;: &quot;ns2&quot;, &quot;subscribe_proxy_name&quot;: &quot;subscribe2&quot;, &quot;publish&quot;: true, &quot;publish_proxy_name&quot;: &quot;publish2&quot; } ] }   If namespace does not have &quot;subscribe_proxy_name&quot; or &quot;subscribe_proxy_name&quot; is empty then no subscribe proxy will be used for a namespace.  If namespace does not have &quot;publish_proxy_name&quot; or &quot;publish_proxy_name&quot; is empty then no publish proxy will be used for a namespace.  If namespace does not have &quot;sub_refresh_proxy_name&quot; or &quot;sub_refresh_proxy_name&quot; is empty then no sub refresh proxy will be used for a namespace.  tip You can define subscribe_proxy_name, publish_proxy_name, sub_refresh_proxy_name on configuration top level – and in this case publish, subscribe and sub refresh requests for channels without explicit namespace will be proxied using this proxy. The same mechanics as for other channel options in Centrifugo.  ","version":"v5","tagName":"h3"},{"title":"Granular RPC​","type":1,"pageTitle":"Proxy events to the backend","url":"/docs/server/proxy#granular-rpc","content":" Analogous to channel namespaces it's possible to configure rpc namespaces:  config.json { ... &quot;granular_proxy_mode&quot;: true, &quot;proxies&quot;: [...], &quot;namespaces&quot;: [...], &quot;rpc_namespaces&quot;: [ { &quot;name&quot;: &quot;rpc_ns1&quot;, &quot;rpc_proxy_name&quot;: &quot;rpc1&quot;, }, { &quot;name&quot;: &quot;rpc_ns2&quot;, &quot;rpc_proxy_name&quot;: &quot;rpc2&quot; } ] }   The mechanics is the same as for channel namespaces. RPC requests with RPC method like rpc_ns1:test will use rpc proxy rpc1, RPC requests with RPC method like rpc_ns2:test will use rpc proxy rpc2. So Centrifugo uses : as RPC namespace boundary in RPC method (just like it does for channel namespaces).  Just like channel namespaces RPC namespaces should have a name which match ^[-a-zA-Z0-9_.]{2,}$ regexp pattern – this is validated on Centrifugo start.  tip The same as for channel namespaces and channel options you can define rpc_proxy_name on configuration top level – and in this case RPC calls without explicit namespace in RPC method will be proxied using this proxy. ","version":"v5","tagName":"h3"},{"title":"Unidirectional client protocol","type":0,"sectionRef":"#","url":"/docs/transports/uni_client_protocol","content":"","keywords":"","version":"v5"},{"title":"Unidirectional message types​","type":1,"pageTitle":"Unidirectional client protocol","url":"/docs/transports/uni_client_protocol#unidirectional-message-types","content":" In case of unidirectional transports Centrifugo will send Push frames to the connection. Push frames defined by client protocol schema. I.e. Centrifugo reuses a part of its bidirectional protocol for unidirectional communication. Push message defined as:  message Push { string channel = 2; Publication pub = 4; Join join = 5; Leave leave = 6; Unsubscribe unsubscribe = 7; Message message = 8; Subscribe subscribe = 9; Connect connect = 10; Disconnect disconnect = 11; Refresh refresh = 12; }   tip Some numbers in Protobuf definitions skipped for backwards compatibility with previous client protocol version.  So unidirectional connection will receive various pushes. Every push contains one of the following objects:  PublicationJoinLeaveUnsubscribeMessageSubscribeConnectDisconnectRefresh  Some pushes belong to a channel which may be set on Push top level.  All you need to do is look at Push, process messages you are interested in and ignore others. In most cases you will be most interested in pushes which contain Connect or Publication messages.  For example, according to protocol schema Publication message type looks like this:  message Publication { bytes data = 4; ClientInfo info = 5; uint64 offset = 6; map&lt;string, string&gt; tags = 7; }   tip In JSON protocol case Centrifugo replaces bytes type with embedded JSON.  Just try using any unidirectional transport and you will quickly get the idea. ","version":"v5","tagName":"h3"},{"title":"Server API walkthrough","type":0,"sectionRef":"#","url":"/docs/server/server_api","content":"","keywords":"","version":"v5"},{"title":"HTTP API​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#http-api","content":" Server HTTP API works on /api path prefix (by default). The request format is super-simple: this is an HTTP POST request to a specific method API path with application/json Content-Type, X-API-Key header and with JSON body.  Instead of many words, here is an example how to call publish method:  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;test&quot;, &quot;data&quot;: {&quot;value&quot;: &quot;test_value&quot;}}' \\ http://localhost:8000/api/publish   tip You can just use one of our available HTTP API libraries or use Centrifugo GRPC API to avoid manually constructing requests structures.  Below we look at all aspects of HTTP API in detail, starting with information about authorization.  ","version":"v5","tagName":"h2"},{"title":"HTTP API authorization​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#http-api-authorization","content":" HTTP API is protected by api_key set in Centrifugo configuration. I.e. api_key option must be added to config, like:  config.json { ... &quot;api_key&quot;: &quot;&lt;YOUR_API_KEY&gt;&quot; }   This API key must be set in the request X-API-Key header in this way:  X-API-Key: &lt;YOUR_API_KEY&gt;   It's also possible to pass API key over URL query param. Simply add ?api_key=&lt;YOUR_API_KEY&gt; query param to the API endpoint. Keep in mind that passing the API key in the X-API-Key header is a recommended way as it is considered more secure.  To disable API key check on Centrifugo side you can use api_insecure configuration option. Use it in development only or make sure to protect the API endpoint by proxy or firewall rules in production – to prevent anyone with access to the endpoint to send commands over your unprotected Centrifugo API.  API key auth is not very safe for man-in-the-middle so we also recommended protecting Centrifugo with TLS.  ","version":"v5","tagName":"h2"},{"title":"API methods​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#api-methods","content":" Server API supports many methods. Let's describe them starting with the most important publish operation.  ","version":"v5","tagName":"h2"},{"title":"publish​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#publish","content":" Publish method allows publishing data into a channel (we call this message publication in Centrifugo). Most probably this is a command you'll use most of the time.  Here is an example of publishing message to Centrifugo:  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;chat&quot;, &quot;data&quot;: {&quot;text&quot;: &quot;hello&quot;}}' \\ http://localhost:8000/api/publish   In case of successful publish you will get a response like this:  { &quot;result&quot;: {} }   As an additional example, let's take a look how to publish to Centrifugo with requests library for Python:  import json import requests api_key = &quot;YOUR_API_KEY&quot; data = json.dumps({ &quot;channel&quot;: &quot;docs&quot;, &quot;data&quot;: { &quot;content&quot;: &quot;1&quot; } }) headers = {'Content-type': 'application/json', 'X-API-Key': api_key} resp = requests.post(&quot;https://centrifuge.example.com/api/publish&quot;, data=data, headers=headers) print(resp.json())   In case of publication error, response object will contain error field. For example, let's publish to an unknown namespace (not defined in Centrifugo configuration):  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;unknown:chat&quot;, &quot;data&quot;: {&quot;text&quot;: &quot;hello&quot;}}' \\ http://localhost:8000/api/publish   In response you will also get 200 OK, but payload will contain error field instead of result:  { &quot;error&quot;: { &quot;code&quot;: 102, &quot;message&quot;: &quot;namespace not found&quot; } }   error object contains error code and message - this is also the same for other commands described below.  Publish request​  Field name\tField type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to publish data\tany JSON\tyes\tCustom JSON data to publish into a channel skip_history\tbool\tno\tSkip adding publication to history for this request tags\tmap[string]string\tno\tPublication tags - map with arbitrary string keys and values which is attached to publication and will be delivered to clients b64data\tstring\tno\tCustom binary data to publish into a channel encoded to base64 so it's possible to use HTTP API to send binary to clients. Centrifugo will decode it from base64 before publishing. In case of GRPC you can publish binary using data field. idempotency_key\tstring\tno\tOptional idempotency key to drop duplicate publications upon retries. It acts per channel. Centrifugo currently keeps the cache of idempotent publish results during 5 minutes window. Available since Centrifugo v5.2.0  Publish result​  Field name\tField type\tOptional\tDescriptionoffset\tinteger\tyes\tOffset of publication in history stream epoch\tstring\tyes\tEpoch of current stream  ","version":"v5","tagName":"h3"},{"title":"broadcast​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#broadcast","content":" broadcast is similar to publish but allows to efficiently send the same data into many channels:  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channels&quot;: [&quot;user:1&quot;, &quot;user:2&quot;], &quot;data&quot;: {&quot;text&quot;: &quot;hello&quot;}}' \\ http://localhost:8000/api/broadcast   Broadcast request​  Field name\tField type\tRequired\tDescriptionchannels\tArray of strings\tyes\tList of channels to publish data to data\tany JSON\tyes\tCustom JSON data to publish into each channel skip_history\tbool\tno\tSkip adding publications to channels' history for this request tags\tmap[string]string\tno\tPublication tags - map with arbitrary string keys and values which is attached to publication and will be delivered to clients b64data\tstring\tno\tCustom binary data to publish into a channel encoded to base64 so it's possible to use HTTP API to send binary to clients. Centrifugo will decode it from base64 before publishing. In case of GRPC you can publish binary using data field. idempotency_key\tstring\tno\tOptional idempotency key to drop duplicate publications upon retries. It acts per channel. Centrifugo currently keeps the cache of idempotent publish results during 5 minutes window. Available since Centrifugo v5.2.0  Broadcast result​  Field name\tField type\tOptional\tDescriptionresponses\tArray of publish responses\tno\tResponses for each individual publish (with possible error and publish result)  ","version":"v5","tagName":"h3"},{"title":"subscribe​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#subscribe","content":" subscribe allows subscribing active user's sessions to a channel. Note, it's mostly for dynamic server-side subscriptions.  Subscribe request​  Field name\tField type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to subscribe channel\tstring\tyes\tName of channel to subscribe user to info\tany JSON\tno\tAttach custom data to subscription (will be used in presence and join/leave messages) b64info\tstring\tno\tinfo in base64 for binary mode (will be decoded by Centrifugo) client\tstring\tno\tSpecific client ID to subscribe (user still required to be set, will ignore other user connections with different client IDs) session\tstring\tno\tSpecific client session to subscribe (user still required to be set) data\tany JSON\tno\tCustom subscription data (will be sent to client in Subscribe push) b64data\tstring\tno\tSame as data but in base64 format (will be decoded by Centrifugo) recover_since\tStreamPosition object\tno\tStream position to recover from override\tOverride object\tno\tAllows dynamically override some channel options defined in Centrifugo configuration (see below available fields)  Override object​  Field\tType\tOptional\tDescriptionpresence\tBoolValue\tyes\tOverride presence join_leave\tBoolValue\tyes\tOverride join_leave force_push_join_leave\tBoolValue\tyes\tOverride force_push_join_leave force_positioning\tBoolValue\tyes\tOverride force_positioning force_recovery\tBoolValue\tyes\tOverride force_recovery  BoolValue is an object like this:  { &quot;value&quot;: true/false }   Subscribe result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"unsubscribe​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#unsubscribe","content":" unsubscribe allows unsubscribing user from a channel.  Unsubscribe request​  Field name\tField type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to unsubscribe channel\tstring\tyes\tName of channel to unsubscribe user to client\tstring\tno\tSpecific client ID to unsubscribe (user still required to be set) session\tstring\tno\tSpecific client session to disconnect (user still required to be set).  Unsubscribe result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"disconnect​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#disconnect","content":" disconnect allows disconnecting a user by ID.  Disconnect request​  Field name\tField type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to disconnect client\tstring\tno\tSpecific client ID to disconnect (user still required to be set) session\tstring\tno\tSpecific client session to disconnect (user still required to be set). whitelist\tArray of strings\tno\tArray of client IDs to keep disconnect\tDisconnect object\tno\tProvide custom disconnect object, see below  Disconnect object​  Field name\tField type\tRequired\tDescriptioncode\tint\tyes\tDisconnect code reason\tstring\tyes\tDisconnect reason  Disconnect result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"refresh​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#refresh","content":" refresh allows refreshing user connection (mostly useful when unidirectional transports are used).  Refresh request​  Field name\tField type\tRequired\tDescriptionuser\tstring\tyes\tUser ID to refresh client\tstring\tno\tClient ID to refresh (user still required to be set) session\tstring\tno\tSpecific client session to refresh (user still required to be set). expired\tbool\tno\tMark connection as expired and close with Disconnect Expired reason expire_at\tint\tno\tUnix time (in seconds) in the future when the connection will expire  Refresh result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"presence​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#presence","content":" presence allows getting channel online presence information (all clients currently subscribed on this channel).  tip Presence in channels is not enabled by default. See how to enable it over channel options. Also check out dedicated chapter about it.  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;chat&quot;}' \\ http://localhost:8000/api/presence   Example response:  { &quot;result&quot;: { &quot;presence&quot;: { &quot;c54313b2-0442-499a-a70c-051f8588020f&quot;: { &quot;client&quot;: &quot;c54313b2-0442-499a-a70c-051f8588020f&quot;, &quot;user&quot;: &quot;42&quot; }, &quot;adad13b1-0442-499a-a70c-051f858802da&quot;: { &quot;client&quot;: &quot;adad13b1-0442-499a-a70c-051f858802da&quot;, &quot;user&quot;: &quot;42&quot; } } } }   Presence request​  Field name\tField type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call presence from  Presence result​  Field name\tField type\tOptional\tDescriptionpresence\tMap of client ID (string) to ClientInfo object\tno\tOffset of publication in history stream  ClientInfo​  Field name\tField type\tOptional\tDescriptionclient\tstring\tno\tClient ID user\tstring\tno\tUser ID conn_info\tJSON\tyes\tOptional connection info chan_info\tJSON\tyes\tOptional channel info  ","version":"v5","tagName":"h3"},{"title":"presence_stats​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#presence_stats","content":" presence_stats allows getting short channel presence information - number of clients and number of unique users (based on user ID).  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;chat&quot;}' \\ http://localhost:8000/api/presence_stats   Example response:  { &quot;result&quot;: { &quot;num_clients&quot;: 0, &quot;num_users&quot;: 0 } }   Presence stats request​  Field name\tField type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call presence from  Presence stats result​  Field name\tField type\tOptional\tDescriptionnum_clients\tinteger\tno\tTotal number of clients in channel num_users\tinteger\tno\tTotal number of unique users in channel  ","version":"v5","tagName":"h3"},{"title":"history​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#history","content":" history allows getting channel history information (list of last messages published into the channel). By default if no limit parameter set in request history call will only return current stream position information - i.e. offset and epoch fields. To get publications you must explicitly provide limit parameter. See also history API description in special doc chapter.  tip History in channels is not enabled by default. See how to enable it over channel options.  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;channel&quot;: &quot;chat&quot;, &quot;limit&quot;: 2}' \\ http://localhost:8000/api/history   Example response:  { &quot;result&quot;: { &quot;epoch&quot;: &quot;qFhv&quot;, &quot;offset&quot;: 4, &quot;publications&quot;: [ { &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; }, &quot;offset&quot;: 2 }, { &quot;data&quot;: { &quot;text&quot;: &quot;hello&quot; }, &quot;offset&quot;: 3 } ] } }   History request​  Field name\tField type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to call history from limit\tint\tno\tLimit number of returned publications, if not set in request then only current stream position information will present in result (without any publications) since\tStreamPosition object\tno\tTo return publications after this position reverse\tbool\tno\tIterate in reversed order (from latest to earliest)  StreamPosition​  Field name\tField type\tRequired\tDescriptionoffset\tinteger\tyes\tOffset in a stream epoch\tstring\tyes\tStream epoch  History result​  Field name\tField type\tOptional\tDescriptionpublications\tArray of publication objects\tyes\tList of publications in channel offset\tinteger\tyes\tTop offset in history stream epoch\tstring\tyes\tEpoch of current stream  ","version":"v5","tagName":"h3"},{"title":"history_remove​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#history_remove","content":" history_remove allows removing publications in channel history. Current top stream position meta data kept untouched to avoid client disconnects due to insufficient state.  History remove request​  Field name\tField type\tRequired\tDescriptionchannel\tstring\tyes\tName of channel to remove history  History remove result​  Empty object at the moment.  ","version":"v5","tagName":"h3"},{"title":"channels​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#channels","content":" channels return active channels (with one or more active subscribers in it).  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{}' \\ http://localhost:8000/api/channels   Channels request​  Field name\tField type\tRequired\tDescriptionpattern\tstring\tno\tPattern to filter channels, we are using gobwas/glob library for matching  Channels result​  Field name\tField type\tOptional\tDescriptionchannels\tMap of string to ChannelInfo\tno\tMap where key is channel and value is ChannelInfo (see below)  ChannelInfo​  Field name\tField type\tOptional\tDescriptionnum_clients\tinteger\tno\tTotal number of connections currently subscribed to a channel  caution Keep in mind that since the channels method by default returns all active channels it can be really heavy for massive deployments. Centrifugo does not provide a way to paginate over channels list. At the moment we mostly suppose that channels API call will be used in the development process or for administrative/debug purposes, and in not very massive Centrifugo setups (with no more than 10k active channels). A better and scalable approach for huge setups could be a real-time analytics approach described here.  ","version":"v5","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#info","content":" info method allows getting information about running Centrifugo nodes.  Example response:  { &quot;result&quot;: { &quot;nodes&quot;: [ { &quot;name&quot;: &quot;Alexanders-MacBook-Pro.local_8000&quot;, &quot;num_channels&quot;: 0, &quot;num_clients&quot;: 0, &quot;num_users&quot;: 0, &quot;uid&quot;: &quot;f844a2ed-5edf-4815-b83c-271974003db9&quot;, &quot;uptime&quot;: 0, &quot;version&quot;: &quot;&quot; } ] } }   Info request​  Empty object at the moment.  Info result​  Field name\tField type\tOptional\tDescriptionnodes\tArray of Node objects\tno\tInformation about all nodes in a cluster  ","version":"v5","tagName":"h3"},{"title":"batch​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#batch","content":" Batch allows sending many commands in one request. Commands processed sequentially by Centrifugo, users should check individual error in each returned reply. Useful to avoid RTT latency penalty for each command sent, this is an analogue of pipelining.  Example with two publications in one request:  curl --header &quot;X-API-Key: &lt;API_KEY&gt;&quot; \\ --request POST \\ --data '{&quot;commands&quot;: [{&quot;publish&quot;: {&quot;channel&quot;: &quot;test1&quot;, &quot;data&quot;: {}}}, {&quot;publish&quot;: {&quot;channel&quot;: &quot;x:test2&quot;, &quot;data&quot;: {}}}]}' \\ http://localhost:8000/api/batch   Example response:  { &quot;replies&quot;:[ {&quot;publish&quot;:{}}, {&quot;error&quot;:{&quot;code&quot;:102,&quot;message&quot;:&quot;unknown channel&quot;}} ] }   Starting from Centrifugo v5.2.0 it's also possible to pass &quot;parallel&quot;: true on batch data top level to make batch commands processing parallel on Centrifugo side. This may provide reduced latency (especially in case of using Redis engine).  ","version":"v5","tagName":"h3"},{"title":"HTTP API libraries​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#http-api-libraries","content":" Sending an API request to Centrifugo is a simple task to do in any programming language - this is just a POST request with JSON payload in body and Authorization header.  But we have several official HTTP API libraries for different languages, to help developers to avoid constructing proper HTTP requests manually:  cent for Pythonphpcent for PHPgocent for Gorubycent for Ruby  Also, there are API libraries created by community:  crystalcent API client for Crystal languagecent.js API client for NodeJSCentrifugo.AspNetCore API client for ASP.NET Core  tip Also, keep in mind that Centrifugo has GRPC API so you can automatically generate client API code for your language.  ","version":"v5","tagName":"h2"},{"title":"GRPC API​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#grpc-api","content":" Centrifugo also supports GRPC API. With GRPC it's possible to communicate with Centrifugo using a more compact binary representation of commands and use the power of HTTP/2 which is the transport behind GRPC.  GRPC API is also useful if you want to publish binary data to Centrifugo channels.  tip GRPC API allows calling all commands described in HTTP API doc, actually both GRPC and HTTP API in Centrifugo based on the same Protobuf schema definition. So refer to the HTTP API description doc for the parameter and the result field description.  You can enable GRPC API in Centrifugo using grpc_api option:  config.json { ... &quot;grpc_api&quot;: true }   By default, GRPC will be served on port 10000 but you can change it using the grpc_api_port option.  Now, as soon as Centrifugo started – you can send GRPC commands to it. To do this get our API Protocol Buffer definitions from this file.  Then see GRPC docs specific to your language to find out how to generate client code from definitions and use generated code to communicate with Centrifugo.  ","version":"v5","tagName":"h2"},{"title":"GRPC example for Python​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#grpc-example-for-python","content":" For example for Python you need to run sth like this according to GRPC docs:  pip install grpcio-tools python -m grpc_tools.protoc -I ./ --python_out=. --grpc_python_out=. api.proto   As soon as you run the command you will have 2 generated files: api_pb2.py and api_pb2_grpc.py. Now all you need is to write a simple program that uses generated code and sends GRPC requests to Centrifugo:  import grpc import api_pb2_grpc as api_grpc import api_pb2 as api_pb channel = grpc.insecure_channel('localhost:10000') stub = api_grpc.CentrifugoApiStub(channel) try: resp = stub.Info(api_pb.InfoRequest()) except grpc.RpcError as err: # GRPC level error. print(err.code(), err.details()) else: if resp.error.code: # Centrifugo server level error. print(resp.error.code, resp.error.message) else: print(resp.result)   Note that you need to explicitly handle Centrifugo API level error which is not transformed automatically into GRPC protocol-level error.  ","version":"v5","tagName":"h3"},{"title":"GRPC example for Go​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#grpc-example-for-go","content":" Here is a simple example of how to run Centrifugo with the GRPC Go client.  You need protoc, protoc-gen-go and protoc-gen-go-grpc installed.  First start Centrifugo itself with GRPC API enabled:  CENTRIFUGO_GRPC_API=1 centrifugo --config config.json   In another terminal tab:  mkdir centrifugo_grpc_example cd centrifugo_grpc_example/ touch main.go go mod init centrifugo_example mkdir apiproto cd apiproto wget https://raw.githubusercontent.com/centrifugal/centrifugo/master/internal/apiproto/api.proto -O api.proto   Run protoc to generate code:  protoc -I ./ api.proto --go_out=. --go-grpc_out=.   Put the following code to main.go file (created on the last step above):  package main import ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;centrifugo_example/apiproto&quot; &quot;google.golang.org/grpc&quot; ) func main() { conn, err := grpc.Dial(&quot;localhost:10000&quot;, grpc.WithInsecure()) if err != nil { log.Fatalln(err) } defer conn.Close() client := apiproto.NewCentrifugoApiClient(conn) for { resp, err := client.Publish(context.Background(), &amp;apiproto.PublishRequest{ Channel: &quot;chat:index&quot;, Data: []byte(`{&quot;input&quot;: &quot;hello from GRPC&quot;}`), }) if err != nil { log.Printf(&quot;Transport level error: %v&quot;, err) } else { if resp.GetError() != nil { respError := resp.GetError() log.Printf(&quot;Error %d (%s)&quot;, respError.Code, respError.Message) } else { log.Println(&quot;Successfully published&quot;) } } time.Sleep(time.Second) } }   Then run:  go run main.go   The program starts and periodically publishes the same payload into chat:index channel.  ","version":"v5","tagName":"h3"},{"title":"GRPC API key authorization​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#grpc-api-key-authorization","content":" You can also set grpc_api_key (string) in Centrifugo configuration to protect GRPC API with key. In this case, you should set per RPC metadata with key authorization and value apikey &lt;KEY&gt;. For example in Go language:  package main import ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;centrifugo_example/apiproto&quot; &quot;google.golang.org/grpc&quot; ) type keyAuth struct { key string } func (t keyAuth) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) { return map[string]string{ &quot;authorization&quot;: &quot;apikey &quot; + t.key, }, nil } func (t keyAuth) RequireTransportSecurity() bool { return false } func main() { conn, err := grpc.Dial(&quot;localhost:10000&quot;, grpc.WithInsecure(), grpc.WithPerRPCCredentials(keyAuth{&quot;xxx&quot;})) if err != nil { log.Fatalln(err) } defer conn.Close() client := apiproto.NewCentrifugoClient(conn) for { resp, err := client.Publish(context.Background(), &amp;PublishRequest{ Channel: &quot;chat:index&quot;, Data: []byte(`{&quot;input&quot;: &quot;hello from GRPC&quot;}`), }) if err != nil { log.Printf(&quot;Transport level error: %v&quot;, err) } else { if resp.GetError() != nil { respError := resp.GetError() log.Printf(&quot;Error %d (%s)&quot;, respError.Code, respError.Message) } else { log.Println(&quot;Successfully published&quot;) } } time.Sleep(time.Second) } }   For other languages refer to GRPC docs.  ","version":"v5","tagName":"h3"},{"title":"Transport error mode​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#transport-error-mode","content":" By default, Centrifugo server API never returns transport level errors - for example it always returns 200 OK for HTTP API and never returns GRPC transport-level errors. Centrifugo returns its custom errors from API calls inside optional error field of response as we showed above in this doc. This means that API call to Centrifigo API may returns 200 OK, but in the error field you may find Centrifugo-specific 100: internal error.  Since Centrifugo v5.1.0 Centrifigo has an option to use transport-native error codes instead of Centrifugo error field in the response. The main motivation is make API calls friendly to integrate with the network ecosystem - for automatic retries, better logging, etc. In many situations this may be more obvious for humans also.  Let's show an example. Without any special options HTTP request to Centrifigo server API which contains error in response looks like this:  ❯ echo '{}' | http POST &quot;http://localhost:8000/api/publish&quot; HTTP/1.1 200 OK Content-Length: 46 Content-Type: application/json Date: Sat, 19 Aug 2023 07:23:40 GMT { &quot;error&quot;: { &quot;code&quot;: 107, &quot;message&quot;: &quot;bad request&quot; } }   Note - it returns 200 OK even though response contains error field. With transport error mode request-response may be transformed into the following:  ❯ echo '{}' | http POST &quot;http://localhost:8000/api/publish&quot; &quot;X-Centrifugo-Error-Mode: transport&quot; HTTP/1.1 400 Bad Request Content-Length: 36 Content-Type: application/json Date: Sat, 19 Aug 2023 07:23:59 GMT { &quot;code&quot;: 107, &quot;message&quot;: &quot;bad request&quot; }   Transport error mode may be turned on globally:  using &quot;api_error_mode&quot;: &quot;transport&quot; option for HTTP server APIusing &quot;grpc_api_error_mode&quot;: &quot;transport&quot; option for GRPC server API  Also, this mode may be used on per-request basis:  by setting custom header X-Centrifugo-Error-Mode: transport for HTTP (as we just showed in the example)adding custom metadata key x-centrifugo-error-mode: transport for GRPC  caution Note, that transport error mode does not help a lot with Batch and Broadcast APIs which are quite special because these calls contain many independent operations. For these calls you still need to look at individual error objects in response.  To achieve the goal we have an internal matching of Centrifugo API error codes to HTTP and GRPC error codes.  ","version":"v5","tagName":"h2"},{"title":"Centrifugo error code to HTTP code​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#centrifugo-error-code-to-http-code","content":" func MapErrorToHTTPCode(err *Error) int { switch err.Code { case ErrorInternal.Code: // 100 -&gt; HTTP 500 return http.StatusInternalServerError case ErrorUnknownChannel.Code, ErrorNotFound.Code: // 102, 104 -&gt; HTTP 404 return http.StatusNotFound case ErrorBadRequest.Code, ErrorNotAvailable.Code: // 107, 108 -&gt; HTTP 400 return http.StatusBadRequest case ErrorUnrecoverablePosition.Code: // 112 -&gt; HTTP 416 return http.StatusRequestedRangeNotSatisfiable case ErrorConflict.Code: // 113 -&gt; HTTP 409 return http.StatusConflict default: // Default to Internal Error for unmapped errors. // In general should be avoided - all new API errors must be explicitly described here. return http.StatusInternalServerError // HTTP 500 } }   ","version":"v5","tagName":"h3"},{"title":"Centrifugo error code to GRPC code​","type":1,"pageTitle":"Server API walkthrough","url":"/docs/server/server_api#centrifugo-error-code-to-grpc-code","content":" func MapErrorToGRPCCode(err *Error) codes.Code { switch err.Code { case ErrorInternal.Code: // 100 return codes.Internal case ErrorUnknownChannel.Code, ErrorNotFound.Code: // 102, 104 return codes.NotFound case ErrorBadRequest.Code, ErrorNotAvailable.Code: // 107, 108 return codes.InvalidArgument case ErrorUnrecoverablePosition.Code: // 112 return codes.OutOfRange case ErrorConflict.Code: // 113 return codes.AlreadyExists default: // Default to Internal Error for unmapped errors. // In general should be avoided - all new API errors must be explicitly described here. return codes.Internal } }  ","version":"v5","tagName":"h3"},{"title":"Unidirectional GRPC","type":0,"sectionRef":"#","url":"/docs/transports/uni_grpc","content":"","keywords":"","version":"v5"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#supported-data-formats","content":" JSON and binary.  ","version":"v5","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#options","content":" ","version":"v5","tagName":"h2"},{"title":"uni_grpc​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#uni_grpc","content":" Boolean, default: false.  Enables unidirectional GRPC endpoint.  config.json { ... &quot;uni_grpc&quot;: true }   ","version":"v5","tagName":"h3"},{"title":"uni_grpc_port​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#uni_grpc_port","content":" String, default &quot;11000&quot;.  Port to listen on.  ","version":"v5","tagName":"h3"},{"title":"uni_grpc_address​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#uni_grpc_address","content":" String, default &quot;&quot; (listen on all interfaces)  Address to bind uni GRPC to.  ","version":"v5","tagName":"h3"},{"title":"uni_grpc_max_receive_message_size​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#uni_grpc_max_receive_message_size","content":" Default: 65536 (64KB)  Maximum allowed size of a first connect message received from GRPC connection in bytes.  ","version":"v5","tagName":"h3"},{"title":"uni_grpc_tls​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#uni_grpc_tls","content":" Boolean, default: false  Enable custom TLS for unidirectional GRPC server.  ","version":"v5","tagName":"h3"},{"title":"uni_grpc_tls_cert​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#uni_grpc_tls_cert","content":" String, default: &quot;&quot;.  Path to cert file.  ","version":"v5","tagName":"h3"},{"title":"uni_grpc_tls_key​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#uni_grpc_tls_key","content":" String, default: &quot;&quot;.  Path to key file.  ","version":"v5","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Unidirectional GRPC","url":"/docs/transports/uni_grpc#example","content":" We don't have example here yet. In general, the algorithm is like this:  Copy Protobuf definitionsGenerate GRPC client codeUse generated code to connect to CentrifugoProcess Push messages, drop unknown Pushes, handle those necessary for the application. ","version":"v5","tagName":"h2"},{"title":"Unidirectional HTTP streaming","type":0,"sectionRef":"#","url":"/docs/transports/uni_http_stream","content":"","keywords":"","version":"v5"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#connect-command","content":" It's possible to pass initial connect command by posting a JSON body to a streaming endpoint.  Refer to the full Connect command description – it's the same as for unidirectional WebSocket.  ","version":"v5","tagName":"h2"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#supported-data-formats","content":" JSON  ","version":"v5","tagName":"h2"},{"title":"Pings​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#pings","content":" Centrifugo will send different message types to a connection. Every message is JSON encoded. A special JSON value null used as a PING message. You can simply ignore it on a client side upon receiving. You can ignore such messages or use them to detect broken connections (nothing received from a server for a long time).  ","version":"v5","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#options","content":" ","version":"v5","tagName":"h2"},{"title":"uni_http_stream​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#uni_http_stream","content":" Boolean, default: false.  Enables unidirectional HTTP streaming endpoint.  config.json { ... &quot;uni_http_stream&quot;: true }   ","version":"v5","tagName":"h3"},{"title":"uni_http_stream_max_request_body_size​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#uni_http_stream_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes.  ","version":"v5","tagName":"h3"},{"title":"Connecting using CURL​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#connecting-using-curl","content":" Let's look how simple it is to connect to Centrifugo using HTTP streaming.  We will start from scratch, generate new configuration file:  centrifugo genconfig   Turn on uni HTTP stream and automatically subscribe users to personal channel upon connect:  config.json { ... &quot;uni_http_stream&quot;: true, &quot;user_subscribe_to_personal&quot;: true }   Run Centrifugo:  centrifugo -c config.json   In separate terminal window create token for a user:  ❯ go run main.go gentoken -u user12 HMAC SHA-256 JWT for user user12 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMTIiLCJleHAiOjE2MjUwNzMyODh9.BxmS4R-X6YXMxLfXNhYRzeHvtu_M2NCaXF6HNu7VnDM   Then connect to Centrifugo uni HTTP stream endpoint with simple CURL POST request:  curl -X POST http://localhost:8000/connection/uni_http_stream \\ -d '{&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyMTIiLCJleHAiOjE2MjUwNzMyODh9.BxmS4R-X6YXMxLfXNhYRzeHvtu_M2NCaXF6HNu7VnDM&quot;}'   Open one more terminal window and publish message to a personal user channel:  curl -X POST http://localhost:8000/api/publish \\ -d '{&quot;channel&quot;: &quot;#user12&quot;, &quot;data&quot;: {&quot;input&quot;: &quot;hello&quot;}}' \\ -H &quot;Authorization: apikey 9230f514-34d2-4971-ace2-851c656e81dc&quot;   You should see this messages coming from server.  {} messages are pings from a server.  That's all, happy streaming!  ","version":"v5","tagName":"h2"},{"title":"Browser example​","type":1,"pageTitle":"Unidirectional HTTP streaming","url":"/docs/transports/uni_http_stream#browser-example","content":" A basic browser will come soon as we update docs for v4. ","version":"v5","tagName":"h2"},{"title":"Client SDK API","type":0,"sectionRef":"#","url":"/docs/transports/client_api","content":"","keywords":"","version":"v5"},{"title":"Client connection states​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#client-connection-states","content":" Client connection has 4 states:  disconnectedconnectingconnectedclosed  note closed state is only implemented by SDKs where it makes sense (need to clean up allocated resources when app gracefully shuts down – for example in Java SDK we close thread executors used internally).  When a new Client is created it has a disconnected state. To connect to a server connect() method must be called. After calling connect Client moves to the connecting state. If a Client can't connect to a server it attempts to create a connection with an exponential backoff algorithm (with full jitter). If a connection to a server is successful then the state becomes connected.  If a connection is lost (due to a missing network for example, or due to reconnect advice received from a server, or due to some client-side error that can't be recovered without reconnecting) Client goes to the connecting state again. In this state Client tries to reconnect (again, with an exponential backoff algorithm).  The Client's state can become disconnected. This happens when Client's disconnect() method was called by a developer. Also, this can happen due to server advice from a server, or due to a terminal problem that happened on the client-side.  Here is a program where we create a Client instance, set callbacks to listen to state updates and establish a connection with a server:      JavascriptDartSwiftJavaGo const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); client.on('connecting', function(ctx) { console.log('connecting', ctx); }); client.on('connected', function(ctx) { console.log('connected', ctx); }); client.on('disconnected', function(ctx) { console.log('disconnected', ctx); }); client.connect();   In case of successful connection Client states will transition like this:  disconnected (initial) -&gt; connecting (on('connecting') called) -&gt; connected (on('connected') called).  In case of already connected Client temporary lost a connection with a server and then successfully reconnected:  connected -&gt; connecting (on('connecting') called) -&gt; connected (on('connected') called).  In case of already connected Client temporary lost a connection with a server, but got a terminal error upon reconnection:  connected -&gt; connecting (on('connecting') called) -&gt; disconnected (on('disconnected') called).  In case of already connected Client came across terminal condition (for example, if during a connection token refresh application found that user has no permission to connect anymore):  connected -&gt; disconnected (on('disconnected') called).  Both connecting and disconnected events have numeric code and human-readable string reason in their context, so you can look at them and find the exact reason why the Client went to the connecting state or to the disconnected state.  This diagram demonstrates possible Client state transitions:    You can also listen for all errors happening internally (which are not reflected by state changes, for example, transport errors happening on initial connect, transport during reconnect, connection token refresh related errors, etc) while the client works by using error event:  client.on('error', function(ctx) { console.log('client error', ctx); });   If you want to disconnect from a server call .disconnect() method:  client.disconnect();   In this case on('disconnected') will be called. You can call connect() again when you need to establish a connection.  closed state implemented in SDKs where resources like internal queues, thread executors, etc must be cleaned up when the Client is not needed anymore. All subscriptions should automatically go to the unsubscribed state upon closing. The client is not usable after going to a closed state.  ","version":"v5","tagName":"h2"},{"title":"Client common options​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#client-common-options","content":" There are several common options available when creating Client instance.  option to set connection token and callback to get connection token upon expiration (see below mode details)option to set connect dataoption to configure operation timeouttweaks for reconnect backoff algorithm (min delay, max delay)configure max delay of server pings (to detect broken connection)configure headers to send in WebSocket upgrade request (except centrifuge-js)configure client name and version for analytics purpose  ","version":"v5","tagName":"h2"},{"title":"Client methods​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#client-methods","content":" connect() – connect to a serverdisconnect() - disconnect from a serverclose() - close Client if not needed anymoresend(data) - send asynchronous message to a serverrpc(method, data) - send arbitrary RPC and wait for response  ","version":"v5","tagName":"h2"},{"title":"Client connection token​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#client-connection-token","content":" All SDKs support connecting to Centrifugo with JWT. Initial connection token can be set in Client option upon initialization. Example:  const client = new Centrifuge('ws://localhost:8000/connection/websocket', { token: 'JWT-GENERATED-ON-BACKEND-SIDE' });   If the token sets connection expiration then the client SDK will keep the token refreshed. It does this by calling a special callback function. This callback must return a new token. If a new token with updated connection expiration is returned from callback then it's sent to Centrifugo. In case of error returned by your callback SDK will retry the operation after some jittered time.  An example:  async function getToken() { if (!loggedIn) { return &quot;&quot;; // Empty token or pre-generated token for anonymous users. } // Fetch your application backend. const res = await fetch('http://localhost:8000/centrifugo/connection_token'); if (!res.ok) { if (res.status === 403) { // Return special error to not proceed with token refreshes, // client will be disconnected. throw new Centrifuge.UnauthorizedError(); } // Any other error thrown will result into token refresh re-attempts. throw new Error(`Unexpected status code ${res.status}`); } const data = await res.json(); return data.token; } const client = new Centrifuge( 'ws://localhost:8000/connection/websocket', { token: 'JWT-GENERATED-ON-BACKEND-SIDE', // Optional, getToken is enough. getToken: getToken } );   tip If initial token is not provided, but getToken is specified – then SDK should assume that developer wants to use token authentication. In this case SDK should attempt to get a connection token before establishing an initial connection.  ","version":"v5","tagName":"h2"},{"title":"Connection PING/PONG​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#connection-pingpong","content":" PINGs sent by a server, a client should answer with PONGs upon receiving PING. If a client does not receive PING from a server for a long time (ping interval + configured delay) – the connection is considered broken and will be re-established.  ","version":"v5","tagName":"h2"},{"title":"Subscription states​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#subscription-states","content":" Client allows subscribing on channels. This can be done by creating Subscription object.  const sub = centrifuge.newSubscription(channel); sub.subscribe();   When anewSubscription method is called Client allocates a new Subscription instance and saves it in the internal subscription registry. Having a registry of allocated subscriptions allows SDK to manage resubscribes upon reconnecting to a server. Centrifugo connectors do not allow creating two subscriptions to the same channel – in this case, newSubscription can throw an exception.  Subscription has 3 states:  unsubscribedsubscribingsubscribed  When a new Subscription is created it has an unsubscribed state.  To initiate the actual process of subscribing to a channel subscribe() method of Subscription instance should be called. After calling subscribe() Subscription moves to subscribing state.  If subscription to a channel is not successful then depending on error type subscription can automatically resubscribe (with exponential backoff) or go to an unsubscribed state (upon non-temporary error). If subscription to a channel is successful then the state becomes subscribed.  JavascriptDartSwiftJavaGo const sub = client.newSubscription(channel); sub.on('subscribing', function(ctx) { console.log('subscribing'); }); sub.on('subscribed', function(ctx) { console.log('subscribed'); }); sub.on('unsubscribed', function(ctx) { console.log('unsubscribed'); }); sub.subscribe();   Subscriptions also go to subscribing state when Client connection (i.e. transport) becomes unavailable. Upon connection re-establishement all subscriptions which are not in unsubscribed state will resubscribe automatically.  In case of successful subscription states will transition like this:  unsubscribed (initial) -&gt; subscribing (on('subscribing') called) -&gt; subscribed (on('subscribed') called).  In case of connected and subscribed Client temporary lost a connection with a server and then succesfully reconnected and resubscribed:  subscribed -&gt; subscribing (on('subscribing') called) -&gt; subscribed (on('subscribed') called).  Both subscribing and unsubscribed events have numeric code and human-readable string reason in their context, so you can look at them and find the exact reason why Subscription went to subscribing state or to unsubscribed state.  This diagram demonstrates possible Subscription state transitions:    You can listen for all errors happening internally in Subscription (which are not reflected by state changes, for example, temporary subscribe errors, subscription token related errors, etc) by using error event:  sub.on('error', function(ctx) { console.log(&quot;subscription error&quot;, ctx); });   If you want to unsubscribe from a channel call .unsubscribe() method:  sub.unsubscribe();   In this case on('unsubscribed') will be called. Subscription still kept in Client's registry, but no resubscription attempts will be made. You can call subscribe() again when you need Subscription again. Or you can remove Subscription from Client's registry (see below).  ","version":"v5","tagName":"h2"},{"title":"Subscription management​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#subscription-management","content":" The client SDK provides several methods to manage the internal registry of client-side subscriptions.  newSubscription(channel, options) allocates a new Subscription in the registry or throws an exception if the Subscription is already there. We will discuss common Subscription options below.  getSubscription(channel) returns the existing Subscription by a channel from the registry (or null if it does not exist).  removeSubscription(sub) removes Subscription from Client's registry. Subscription is automatically unsubscribed before being removed. Use this to free resources if you don't need a Subscription to a channel anymore.  subscriptions() returns all registered subscriptions, so you can iterate over all and do some action if required (for example, you want to unsubscribe/remove all subscriptions).  ","version":"v5","tagName":"h2"},{"title":"Listen to channel publications​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#listen-to-channel-publications","content":" Of course the main point of having Subscriptions is the ability to listen for publications (i.e. messages published to a channel).  sub.on('publication', function(ctx) { console.log(&quot;received publication&quot;, ctx); });   Publication context has several fields:  data - publication payload, this can be JSON or binary dataoffset - optional offset inside history stream, this is an incremental numbertags - optional tags, this is a map with string keys and string valuesinfo - optional information about client connection who published this (only exists if publication comes from client-side publish() API).  So minimal code where we connect to a server and listen for messages published into example channel may look like:  JavascriptDartSwiftJavaGo const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); const sub = client.newSubscription('example').on('publication', function(ctx) { console.log(&quot;received publication from a channel&quot;, ctx.data); }); sub.subscribe(); client.connect();   Note, that we can call subscribe() before making a connection to a server – and this will work just fine, subscription goes to subscribing state and will be subscribed upon succesfull connection. And of course, it's possible to call .subscribe() after .connect().  ","version":"v5","tagName":"h2"},{"title":"Subscription recovery state​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#subscription-recovery-state","content":" Subscriptions to channels with recovery option enabled maintain stream position information internally. On every publication received this information updated and used to recover missed publications upon resubscribe (caused by reconnect for example).  When you call unsubscribe() Subscription position state is not cleared. So it's possible to call subscribe() later and catch up a state.  The recovery process result – i.e. whether all missed publications recovered or not – can be found in on('subscribed') event context. Centrifuge protocol provides two fields:  wasRecovering - boolean flag that tells whether recovery was used during subscription process resulted into subscribed state. Can be useful if you want to distinguish first subscribe attempt (when subscription does not have any position information yet)recovered - boolean flag that tells whether Centrifugo thinks that all missed publications can be successfully recovered and there is no need to load state from the main application database. It's always false when wasRecovering is false.  ","version":"v5","tagName":"h2"},{"title":"Subscription common options​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#subscription-common-options","content":" There are several common options available when creating Subscription instance.  option to set subscription token and callback to get subscription token upon expiration (see below more details)option to set subscription data (attached to every subscribe/resubscribe request)options to tweak resubscribe backoff algorithmoption to start Subscription since known Stream Position (i.e. attempt recovery on first subscribe)option to ask server to make subscription positioned (if not forced by a server)option to ask server to make subscription recoverable (if not forced by a server)option to ask server to push Join/Leave messages (if not forced by a server)  ","version":"v5","tagName":"h2"},{"title":"Subscription methods​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#subscription-methods","content":" subscribe() – start subscribing to a channelunsubscribe() - unsubscribe from a channelpublish(data) - publish data to Subscription channelhistory(options) - request Subscription channel historypresence() - request Subscription channel online presence informationpresenceStats() - request Subscription channel online presence stats information (number of client connections and unique users in a channel).  ","version":"v5","tagName":"h2"},{"title":"Subscription token​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#subscription-token","content":" All SDKs support subscribing to Centrifugo channels with JWT. Channel subscription token can be set as a Subscription option upon initialization. Example:  const sub = centrifuge.newSubscription(channel, { token: 'JWT-GENERATED-ON-BACKEND-SIDE' }); sub.subscribe();   If token sets subscription expiration client SDK will keep token refreshed. It does this by calling special callback function. This callback must return a new token. If new token with updated subscription expiration returned from a calbback then it's sent to Centrifugo. If your callback returns an empty string – this means user has no permission to subscribe to a channel anymore and subscription will be unsubscribed. In case of error returned by your callback SDK will retry operation after some jittered time.  An example:  async function getToken(ctx) { // Fetch your application backend. const res = await fetch('http://localhost:8000/centrifugo/subscription_token', { method: 'POST', headers: new Headers({ 'Content-Type': 'application/json' }), body: JSON.stringify({ channel: ctx.channel }) }); if (!res.ok) { if (res.status === 403) { // Return special error to not proceed with token refreshes, // client will be disconnected. throw new Centrifuge.UnauthorizedError(); } // Any other error thrown will result into token refresh re-attempts. throw new Error(`Unexpected status code ${res.status}`); } const data = await res.json(); return data.token; } const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); const sub = centrifuge.newSubscription(channel, { token: 'JWT-GENERATED-ON-BACKEND-SIDE', // Optional, getToken is enough. getToken: getToken }); sub.subscribe();   tip If initial token is not provided, but getToken is specified – then SDK should assume that developer wants to use token authorization for a channel subscription. In this case SDK should attempt to get a subscription token before initial subscribe.  ","version":"v5","tagName":"h2"},{"title":"Server-side subscriptions​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#server-side-subscriptions","content":" We encourage using client-side subscriptions where possible as they provide a better control and isolation from connection. But in some cases you may want to use server-side subscriptions (i.e. subscriptions created by server upon connection establishment).  Technically, client SDK keeps server-side subscriptions in internal registry (similar to client-side subscriptions but without possibility to control them).  To listen for server-side subscription events use callbacks as shown in example below:  const client = new Centrifuge('ws://localhost:8000/connection/websocket', {}); client.on('subscribed', function(ctx) { // Called when subscribed to a server-side channel upon Client moving to // connected state or during connection lifetime if server sends Subscribe // push message. console.log('subscribed to server-side channel', ctx.channel); }); client.on('subscribing', function(ctx) { // Called when existing connection lost (Client reconnects) or Client // explicitly disconnected. Client continue keeping server-side subscription // registry with stream position information where applicable. console.log('subscribing to server-side channel', ctx.channel); }); client.on('unsubscribed', function(ctx) { // Called when server sent unsubscribe push or server-side subscription // previously existed in SDK registry disappeared upon Client reconnect. console.log('unsubscribed from server-side channel', ctx.channel); }); client.on('publication', function(ctx) { // Called when server sends Publication over server-side subscription. console.log('publication receive from server-side channel', ctx.channel, ctx.data); }); client.connect();   Server-side subscription events mostly mimic events of client-side subscriptions. But again – they do not provide control to the client and managed entirely by a server side.  Additionally, Client has several top-level methods to call with server-side subscription related operations:  publish(channel, data)history(channel, options)presence(channel)presenceStats(channel)  ","version":"v5","tagName":"h2"},{"title":"Error codes​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#error-codes","content":" Server can return error codes in range 100-1999. Error codes in interval 0-399 reserved by Centrifuge/Centrifugo server. Codes in range [400, 1999] may be returned by application code built on top of Centrifuge/Centrifugo.  Server errors contain a temporary boolean flag which works as a signal that error may be fixed by a later retry.  Errors with codes 0-100 can be used by client-side implementation. Client-side errors may not have code attached at all since in many languages error can be distinguished by its type.  ","version":"v5","tagName":"h2"},{"title":"Unsubscribe codes​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#unsubscribe-codes","content":" Server may return unsubscribe codes. Server unsubscribe codes must be in range [2000, 2999].  Unsubscribe codes &gt;= 2500 coming from server to client result into automatic resubscribe attempt (i.e. client goes to subscribing state). Codes &lt; 2500 result into going to unsubscribed state.  Client implementation can use codes &lt; 2000 for client-side specific unsubscribe reasons.  ","version":"v5","tagName":"h2"},{"title":"Disconnect codes​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#disconnect-codes","content":" Server may send custom disconnect codes to a client. Custom disconnect codes must be in range [3000, 4999].  Client automatically reconnects upon receiving code in range 3000-3499, 4000-4499 (i.e. Client goes to connecting state). Other codes result into going to disconnected state.  Client implementation can use codes &lt; 3000 for client-side specific disconnect reasons.  ","version":"v5","tagName":"h2"},{"title":"RPC​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#rpc","content":" An SDK provides a way to send RPC to a server. RPC is a call that is not related to channels at all. It's just a way to call the server method from the client-side over the real-time connection. RPC is only available when RPC proxy configured (so Centrifugo proxies the RPC to your application backend).  const rpcRequest = {'key': 'value'}; const data = await centrifuge.namedRPC('example_method', rpcRequest);   ","version":"v5","tagName":"h2"},{"title":"Channel history API​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#channel-history-api","content":" SDK provides a method to call publication history inside a channel (only for channels where history is enabled) to get last publications in a channel.  Get stream current top position:  const resp = await subscription.history(); console.log(resp.offset); console.log(resp.epoch);   Get up to 10 publications from history since known stream position:  const resp = await subscription.history({limit: 10, since: {offset: 0, epoch: '...'}}); console.log(resp.publications);   Get up to 10 publications from history since current stream beginning:  const resp = await subscription.history({limit: 10}); console.log(resp.publications);   Get up to 10 publications from history since current stream end in reversed order (last to first):  const resp = await subscription.history({limit: 10, reverse: true}); console.log(resp.publications);   ","version":"v5","tagName":"h2"},{"title":"Presence and presence stats API​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#presence-and-presence-stats-api","content":" Once subscribed client can call presence and presence stats information inside channel (only for channels where presence configured):  For presence (full information about active subscribers in channel):  const resp = await subscription.presence(); // resp contains presence information - a map client IDs as keys // and client information as values.   For presence stats (just a number of clients and unique users in a channel):  const resp = await subscription.presenceStats(); // resp contains a number of clients and a number of unique users.   ","version":"v5","tagName":"h2"},{"title":"SDK common best practices​","type":1,"pageTitle":"Client SDK API","url":"/docs/transports/client_api#sdk-common-best-practices","content":" Callbacks must be fast. Avoid blocking operations inside event handlers. Callbacks caused by protocol messages received from a server are called synchronously and connection read loop is blocked while such callbacks are being executed. Consider doing heavy work asynchronously.Do not blindly rely on the current Client or Subscription state when making client API calls – state can change at any moment, so don't forget to handle errors.Disconnect from a server when a mobile application goes to the background since a mobile OS can kill the connection at some point without any callbacks called. ","version":"v5","tagName":"h2"},{"title":"Unidirectional SSE (EventSource)","type":0,"sectionRef":"#","url":"/docs/transports/uni_sse","content":"","keywords":"","version":"v5"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/transports/uni_sse#connect-command","content":" Unfortunately SSE specification does not allow POST requests from a web browser, so the only way to pass initial connect command is over URL params. Centrifugo is looking for cf_connect URL param for connect command. Connect command value expected to be a JSON-encoded string, properly encoded into URL. For example:  const url = new URL('http://localhost:8000/connection/uni_sse'); url.searchParams.append(&quot;cf_connect&quot;, JSON.stringify({ 'token': '&lt;JWT&gt;' })); const eventSource = new EventSource(url);   Refer to the full Connect command description – it's the same as for unidirectional WebSocket.  The length of URL query should be kept less than 2048 characters to work throughout browsers. This should be more than enough for most use cases.  tip Centrifugo unidirectional SSE endpoint also supports POST requests. While it's not very useful for browsers which only allow GET requests for EventSource this can be useful when connecting from a mobile device. In this case you must send the connect object as a JSON body of a POST request (instead of using cf_connect URL parameter), similar to what we have in unidirectional HTTP streaming transport case.  ","version":"v5","tagName":"h2"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/transports/uni_sse#supported-data-formats","content":" JSON  ","version":"v5","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/transports/uni_sse#options","content":" ","version":"v5","tagName":"h2"},{"title":"uni_sse​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/transports/uni_sse#uni_sse","content":" Boolean, default: false.  Enables unidirectional SSE (EventSource) endpoint.  config.json { ... &quot;uni_sse&quot;: true }   ","version":"v5","tagName":"h3"},{"title":"uni_sse_max_request_body_size​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/transports/uni_sse#uni_sse_max_request_body_size","content":" Default: 65536 (64KB)  Maximum allowed size of a initial HTTP POST request in bytes when using HTTP POST requests to connect (browsers are using GET so it's not applied).  ","version":"v5","tagName":"h3"},{"title":"Browser example​","type":1,"pageTitle":"Unidirectional SSE (EventSource)","url":"/docs/transports/uni_sse#browser-example","content":" Coming soon. ","version":"v5","tagName":"h2"},{"title":"Unidirectional WebSocket","type":0,"sectionRef":"#","url":"/docs/transports/uni_websocket","content":"","keywords":"","version":"v5"},{"title":"Connect command​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#connect-command","content":" It's possible to send connect command as first WebSocket message (as JSON).  Field name\tField type\tRequired\tDescriptiontoken\tstring\tno\tConnection JWT, not required when using the connect proxy feature. data\tany JSON\tno\tCustom JSON connection data name\tstring\tno\tApplication name version\tstring\tno\tApplication version subs\tmap of channel to SubscribeRequest\tno\tPass an information about desired subscriptions to a server  ","version":"v5","tagName":"h2"},{"title":"SubscribeRequest​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#subscriberequest","content":" Field name\tField type\tRequired\tDescriptionrecover\tboolean\tno\tWhether a client wants to recover from a certain position offset\tinteger\tno\tKnown stream position offset when recover is used epoch\tstring\tno\tKnown stream position epoch when recover is used  ","version":"v5","tagName":"h3"},{"title":"Supported data formats​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#supported-data-formats","content":" JSON  ","version":"v5","tagName":"h2"},{"title":"Pings​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#pings","content":" Centrifugo uses empty commands ({} in JSON case) as pings for unidirectional WS. You can ignore such messages or use them to detect broken connections (nothing received from a server for a long time).  ","version":"v5","tagName":"h2"},{"title":"Options​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#options","content":" ","version":"v5","tagName":"h2"},{"title":"uni_websocket​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#uni_websocket","content":" Boolean, default: false.  Enables unidirectional WebSocket endpoint.  config.json { ... &quot;uni_websocket&quot;: true }   ","version":"v5","tagName":"h3"},{"title":"uni_websocket_message_size_limit​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#uni_websocket_message_size_limit","content":" Default: 65536 (64KB)  Maximum allowed size of a first connect message received from WebSocket connection in bytes.  ","version":"v5","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Unidirectional WebSocket","url":"/docs/transports/uni_websocket#example","content":" Let's connect to a unidirectional WebSocket endpoint using wscat tool – it allows connecting to WebSocket servers interactively from a terminal.  First, run Centrifugo with uni_websocket enabled. Also let's enable automatic personal channel subscriptions for users. Configuration example:  config.json { &quot;token_hmac_secret_key&quot;: &quot;secret&quot;, &quot;uni_websocket&quot;:true, &quot;user_subscribe_to_personal&quot;: true }   Run Centrifugo:  ./centrifugo -c config.json   In another terminal:  ❯ ./centrifugo gentoken -c config.json -u test_user HMAC SHA-256 JWT for user test_user with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ0ZXN0X3VzZXIiLCJleHAiOjE2MzAxMzAxNzB9.u7anX-VYXywX1p1lv9UC9CAu04vpA6LgG5gsw5lz1Iw   Install wscat and run:  wscat -c &quot;ws://localhost:8000/connection/uni_websocket&quot;   This will establish a connection with a server and you then can send connect command to a server:  ❯ wscat -c &quot;ws://localhost:8000/connection/uni_websocket&quot; Connected (press CTRL+C to quit) &gt; {&quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ0ZXN0X3VzZXIiLCJleHAiOjE2NTY1MDMwNDV9.3UYL-UCUBp27TybeBK7Z0OenwdsKwCMRe46fuEjJnzI&quot;, &quot;subs&quot;: {&quot;abc&quot;: {}}} &lt; {&quot;connect&quot;:{&quot;client&quot;:&quot;bfd28799-b958-4791-b9e9-b011eaef68c1&quot;,&quot;version&quot;:&quot;0.0.0&quot;,&quot;subs&quot;:{&quot;#test_user&quot;:{}},&quot;expires&quot;:true,&quot;ttl&quot;:604407,&quot;ping&quot;:25,&quot;session&quot;:&quot;57b1287b-44ec-45c8-93fc-696c5294af25&quot;}}   The connection will receive server pings (empty commands {}) periodically. You can try to publish something to #test_user or abc channels (using Centrifugo server API or using admin UI) – and the message should come to the connection we just established. ","version":"v5","tagName":"h2"},{"title":"WebSocket","type":0,"sectionRef":"#","url":"/docs/transports/websocket","content":"","keywords":"","version":"v5"},{"title":"Options​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#options","content":" ","version":"v5","tagName":"h2"},{"title":"websocket_message_size_limit​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#websocket_message_size_limit","content":" Default: 65536 (64KB)  Maximum allowed size of a message received from WebSocket connection in bytes.  ","version":"v5","tagName":"h3"},{"title":"websocket_read_buffer_size​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#websocket_read_buffer_size","content":" In bytes, by default 0 which tells Centrifugo to reuse read buffer from HTTP server for WebSocket connection (usually 4096 bytes in size). If set to a lower value can reduce memory usage per WebSocket connection (but can increase number of system calls depending on average message size).  config.json { ... &quot;websocket_read_buffer_size&quot;: 512 }   ","version":"v5","tagName":"h3"},{"title":"websocket_write_buffer_size​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#websocket_write_buffer_size","content":" In bytes, by default 0 which tells Centrifugo to reuse write buffer from HTTP server for WebSocket connection (usually 4096 bytes in size). If set to a lower value can reduce memory usage per WebSocket connection (but HTTP buffer won't be reused):  config.json { ... &quot;websocket_write_buffer_size&quot;: 512 }   ","version":"v5","tagName":"h3"},{"title":"websocket_use_write_buffer_pool​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#websocket_use_write_buffer_pool","content":" If you have a few writes then websocket_use_write_buffer_pool (boolean, default false) option can reduce memory usage of Centrifugo a bit as there won't be separate write buffer binded to each WebSocket connection.  ","version":"v5","tagName":"h3"},{"title":"websocket_compression​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#websocket_compression","content":" An experimental feature for raw WebSocket endpoint - permessage-deflate compression for websocket messages. Btw look at great article about websocket compression. WebSocket compression can reduce an amount of traffic travelling over the wire.  We consider this experimental because this websocket compression is experimental in Gorilla Websocket library that Centrifugo uses internally.  caution Enabling WebSocket compression will result in much slower Centrifugo performance and more memory usage – depending on your message rate this can be very noticeable.  To enable WebSocket compression for raw WebSocket endpoint set websocket_compression to true in a configuration file. After this clients that support permessage-deflate will negotiate compression with server automatically. Note that enabling compression does not mean that every connection will use it - this depends on client support for this feature.  Another option is websocket_compression_min_size. Default 0. This is a minimal size of message in bytes for which we use deflate compression when writing it to client's connection. Default value 0 means that we will compress all messages when websocket_compression enabled and compression support negotiated with client.  It's also possible to control websocket compression level defined at compress/flate By default when compression with a client negotiated Centrifugo uses compression level 1 (BestSpeed). If you want to set custom compression level use websocket_compression_level configuration option.  ","version":"v5","tagName":"h3"},{"title":"Protobuf binary protocol​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#protobuf-binary-protocol","content":" In most cases you will use Centrifugo with JSON protocol which is used by default. It consists of simple human-readable frames that can be easily inspected. Also it's a very simple task to publish JSON encoded data to HTTP API endpoint. You may want to use binary Protobuf client protocol if:  you want less traffic on wire as Protobuf is very compactyou want maximum performance on server-side as Protobuf encoding/decoding is very efficientyou can sacrifice human-readable JSON for your application  Binary protobuf protocol only works for raw Websocket connections (as SockJS can't deal with binary). With most clients to use binary you just need to provide query parameter format to Websocket URL, so final URL look like:  wss://centrifugo.example.com/connection/websocket?format=protobuf   After doing this Centrifugo will use binary frames to pass data between client and server. Your application specific payload can be random bytes.  tip You still can continue to encode your application specific data as JSON when using Protobuf protocol thus have a possibility to co-exist with clients that use JSON protocol on the same Centrifugo installation inside the same channels.  ","version":"v5","tagName":"h2"},{"title":"Debugging with Postman, wscat, etc​","type":1,"pageTitle":"WebSocket","url":"/docs/transports/websocket#debugging-with-postman-wscat-etc","content":" Centrifugo supports a special url parameter for bidirectional websocket which turns on using native WebSocket frame ping-pong mechanism instead of server-to-client application level pings Centrifugo uses by default. This simplifies debugging Centrifugo protocol with tools like Postman, wscat, websocat, etc.  By default, it may be inconvenient due to the fact Centrifugo sends periodic ping message to the client ({} in JSON protocol scenario) and expects pong response back within some time period. Otherwise Centrifugo closes connection. This results in problems with mentioned tools because you had to manually send {} pong message upon ping message. So typical session in wscat could look like this:  ❯ wscat --connect ws://localhost:8000/connection/websocket Connected (press CTRL+C to quit) &gt; {&quot;id&quot;: 1, &quot;connect&quot;: {}} &lt; {&quot;id&quot;:1,&quot;connect&quot;:{&quot;client&quot;:&quot;9ac9de4e-5289-4ad6-9aa7-8447f007083e&quot;,&quot;version&quot;:&quot;0.0.0&quot;,&quot;ping&quot;:25,&quot;pong&quot;:true}} &lt; {} Disconnected (code: 3012, reason: &quot;no pong&quot;)   The parameter is called cf_ws_frame_ping_pong, to use it connect to Centrifugo bidirectional WebSocket endpoint like ws://localhost:8000/connection/websocket?cf_ws_frame_ping_pong=true. Here is an example which demonstrates working with Postman WebSocket where we connect to local Centrifugo and subscribe to two channels test1 and test2:    You can then proceed to Centrifugo admin web UI, publish something to these channels and see publications in Postman.  Note, how we sent several JSON commands in one WebSocket frame to Centrifugo from Postman in the example above - this is possible since Centrifugo protocol supports batches of commands in line-delimited format.  We consider this feature to be used only for debugging, in production prefer using our SDKs without using cf_ws_frame_ping_pong parameter – because app-level ping-pong is more efficient and our SDKs detect broken connections due to it. ","version":"v5","tagName":"h2"},{"title":"WebTransport","type":0,"sectionRef":"#","url":"/docs/transports/webtransport","content":"WebTransport WebTransport is an API offering low-latency, bidirectional, client-server messaging on top of HTTP/3 (with QUIC under the hood). See Using WebTransport article that gives a good overview of it. danger WebTransport support in Centrifugo is EXPERIMENTAL and not recommended for production usage. WebTransport IETF specification is not finished yet and may have breaking changes. To use WebTransport you first need to run HTTP/3 experimental server and enable webtransport endpoint: config.json { &quot;http3&quot;: true, &quot;tls&quot;: true, &quot;tls_cert&quot;: &quot;path/to/crt&quot;, &quot;tls_key&quot;: &quot;path/to/key&quot;, &quot;webtransport&quot;: true } In HTTP/3 and WebTransport case TLS is required. tip At the time of writing only Chrome (since v97) supports WebTransport API. If you are experimenting with self-signed certificates you may need to run Chrome with flags to force HTTP/3 on origin and ignore certificate errors: /path/to/your/Chrome --origin-to-force-quic-on=localhost:8000 --ignore-certificate-errors-spki-list=TSZTiMjLG+DNjESXdJh3f+S8C+RhsFCav7T24VNuCPQ= Where the value of --ignore-certificate-errors-spki-list is a certificate fingerprint obtained this way: openssl x509 -in server.crt -pubkey -noout | openssl pkey -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64 With not self-signed certs things should work just fine in Chrome. Here is a video tutorial that shows this in action: After starting Centrifugo with HTTP/3 and WebTransport endpoint you can connect to that endpoint (by default – /connection/webtransport) using centrifuge-js. For example, let's enable WebTransport and will use WebSocket as a fallback option: const transports = [ { transport: 'webtransport', endpoint: 'https://localhost:8000/connection/webtransport' }, { transport: 'websocket', endpoint: 'wss://localhost:8000/connection/websocket' } ]; const centrifuge = new Centrifuge(transports); centrifuge.connect() Note, that we are using secure schemes here – https:// and wss://. While in WebSocket case you could opt for non-TLS communication, in WebTransport case non-TLS http:// scheme is simply not supported by the specification. Also, Chrome may not automatically close WebTransport sessions upon browser reload, so consider adding: addEventListener(&quot;unload&quot;, (event) =&gt; { centrifuge.disconnect() }); tip Make sure you run Centrifugo without load balancer or reverse proxy in front, or make sure your proxy can proxy HTTP/3 traffic to Centrifugo. In Centrifugo case, we utilize a single bidirectional stream of WebTransport to pass our protocol between client and server. Both JSON and Protobuf communication are supported. There are some issues with the proper passing of the disconnect advice in some cases, otherwise it's fully functional. Obviously, due to the limited WebTransport support in browsers at the moment, possible breaking changes in the WebTransport specification it's an experimental feature. And it's not recommended for production usage for now. At some point in the future, it may become a reasonable alternative to WebSocket.","keywords":"","version":"v5"},{"title":"Setting up backend and database","type":0,"sectionRef":"#","url":"/docs/tutorial/backend","content":"","keywords":"","version":"v5"},{"title":"Start Django project​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#start-django-project","content":" To start with Django project you will need Python 3. As soon as you have it run:  python3 -m venv env ./env/bin/activate python -m pip install Django python -m django --version django-admin startproject app mv app backend   This will create backend directory in your current directory with the following contents:  backend/ manage.py app/ __init__.py asgi.py settings.py urls.py wsgi.py   The app directory contains core settings and things to run Django app. For the main chat business logic let's create a new Django app:  cd backend python manage.py startapp chat   This will create chat folder with sth like this inside:  chat/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py views.py   We need to tell our project that the chat app is installed. Edit the app/settings.py file and add 'chat' to the INSTALLED_APPS setting. It'll look like this:  backend/app/settings.py INSTALLED_APPS = [ 'chat', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', ]   Our backend service will expose REST API for the frontend. The simplest way to add REST in Django is to use Django Rest framework:  pip install djangorestframework pip freeze &gt; requirements.txt   Update INSTALLED_APPS:  backend/app/settings.py INSTALLED_APPS = [ 'rest_framework', 'chat', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', ]   For the main database we will use PostgreSQL here. Add db to docker-compose.yml:  docker-compose.yml version: '3.8' services: db: image: postgres:15 volumes: - ./postgres_data:/var/lib/postgresql/data/ healthcheck: test: [ &quot;CMD&quot;, &quot;pg_isready&quot;, &quot;-U&quot;, &quot;grandchat&quot; ] interval: 1s timeout: 5s retries: 10 environment: - POSTGRES_USER=grandchat - POSTGRES_PASSWORD=grandchat - POSTGRES_DB=grandchat expose: - 5432 ports: - 5432:5432   And properly set DATABASES in Django app settings:  backend/app/settings.py DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': 'grandchat', 'USER': 'grandchat', 'PASSWORD': 'grandchat', 'HOST': 'db', 'PORT': '5432', } }   Note that in this example we are running everything in Docker, that's why database host is db - it matches the service name in docker-compose.yml.  Let's also serve Django application when we are running docker compose. We will serve Django using Gunicorn web server. To achieve that create custom Dockerfile inside backend directory:  backend/Dockerfile FROM python:3.11.4-slim-buster WORKDIR /usr/src/app ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 RUN pip install --upgrade pip COPY ./requirements.txt . RUN pip install -r requirements.txt COPY . . CMD [&quot;gunicorn&quot;, &quot;app.wsgi&quot;, &quot;--reload&quot;, &quot;--access-logfile&quot;, &quot;-&quot;, \\ &quot;--workers&quot;, &quot;2&quot;, &quot;--bind&quot;, &quot;0.0.0.0:8000&quot;]   Here we are using gunicorn with hot reload here to simplify development, of course you won't do this in production.  Now add backend service to docker-compose.yml:  docker-compose.yml backend: build: ./backend volumes: - ./backend:/usr/src/app expose: - 8000 depends_on: db: condition: service_healthy   Note that we pass backend dir to the container, also passing and installing dependencies, as a result we will get Django app served and with hot reload upon source code changes.  ","version":"v5","tagName":"h2"},{"title":"Creating models​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#creating-models","content":" Django is great to quickly create domain models required for our messenger. Here is what we need:  User – for user model we will use Django's built-in User model hereRoom - the model that describes chat room with unique nameRoomMember – users can join and leave rooms, so this model contains many to many relationship between User and RoomMessage - this describes a message sent to room by some user (belongs to Room, has User – the author of message)  Add the following to chat/models.py:  backend/chat/models.py from django.db import models from django.contrib.auth.models import User class Room(models.Model): name = models.CharField(max_length=100, unique=True) version = models.PositiveBigIntegerField(default=0) created_at = models.DateTimeField(auto_now_add=True) bumped_at = models.DateTimeField(auto_now_add=True) last_message = models.ForeignKey( 'Message', related_name='last_message_rooms', on_delete=models.SET_NULL, null=True, blank=True, ) def increment_version(self): self.version += 1 self.save() return self.version def __str__(self): return self.name class RoomMember(models.Model): room = models.ForeignKey(Room, related_name='memberships', on_delete=models.CASCADE) user = models.ForeignKey(User, related_name='rooms', on_delete=models.CASCADE) joined_at = models.DateTimeField(auto_now_add=True) class Meta: unique_together = ('room', 'user') def __str__(self): return f&quot;{self.user.username} in {self.room.name}&quot; class Message(models.Model): room = models.ForeignKey(Room, related_name='messages', on_delete=models.CASCADE) # Note, message may have null user – we consider such messages &quot;system&quot;. These messages # initiated by the backend and have no user author. We are not using such messages in # the example currently, but leave the opportunity to extend. user = models.ForeignKey( User, related_name='messages', on_delete=models.CASCADE, null=True) content = models.TextField() created_at = models.DateTimeField(auto_now_add=True)   Having the models we now need to make database migrations and create tables for them. First run the app:  docker compose up --build   And from another terminal tab run:  docker compose exec backend python manage.py makemigrations docker compose exec backend python manage.py migrate   Let's also create an admin user (or better two!):  docker compose exec backend python manage.py createsuperuser   At this point we have a Django app with a configured database that has all the required tables for our app core entities. To access the app we will add one more element – Nginx reverse proxy. It's usually optional while developing, but in our case it's super-useful since we are building SPA-application and want to serve both frontend and backend from the same domain. But before moving to Nginx configuration we need to add some views to Django app – for user login/logout, and api for rooms, membership and messages.  ","version":"v5","tagName":"h2"},{"title":"Adding backend API​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#adding-backend-api","content":" We need to create some APIs for the application:  An endpoint to return CSRF tokenEndpoints for user login/logoutEndpoints for chat API – listing and searching rooms, listing and creating messages, joining/leaving chat rooms.  CSRF and login/logout endpoints are rather trivial to implement with Django. For chat API using Django Rest Framework (DRF) simplifies the task for us drastically. We already defined models above, with DRF we just need to define serializers and viewsets for the desired routes.  ","version":"v5","tagName":"h2"},{"title":"GET /api/csrf/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#get-apicsrf","content":" We need a way to let the frontend to load CSRF token. Refer to the Django Session-based Auth for Single Page Apps article which explains why we need to do that.  backend/app/views.py from django.http import JsonResponse from django.middleware.csrf import get_token def get_csrf(request): return JsonResponse({}, headers={'X-CSRFToken': get_token(request)})   ","version":"v5","tagName":"h3"},{"title":"POST /api/login/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#post-apilogin","content":" Simply using Django's functions for authenticating user here:  backend/app/views.py import json from django.contrib.auth import authenticate, login from django.http import JsonResponse from django.views.decorators.http import require_POST @require_POST def login_view(request): credentials = json.loads(request.body) username = credentials.get('username') password = credentials.get('password') if not username or not password: return JsonResponse({'detail': 'provide username and password'}, status=400) user = authenticate(username=username, password=password) if not user: return JsonResponse({'detail': 'invalid credentials'}, status=400) login(request, user) return JsonResponse({'user': {'id': user.pk, 'username': user.username}})   ","version":"v5","tagName":"h3"},{"title":"POST /api/logout/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#post-apilogout","content":" Simply using Django's functions for log user out here:  backend/app/views.py import json from django.contrib.auth import logout from django.http import JsonResponse from django.views.decorators.http import require_POST @require_POST def logout_view(request): if not request.user.is_authenticated: return JsonResponse({'detail': 'must be authenticated'}, status=403) logout(request) return JsonResponse({})   ","version":"v5","tagName":"h3"},{"title":"GET /api/rooms/search/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#get-apiroomssearch","content":" For rooms search we will simply return all the rooms sorted by name. As mentioned before for the restful layer we work with models using Django Rest framework. This means we need to tell DRF how to serialize models describing Serializer class and then we can use serializers in DRF predefined viewsets to create views.  backend/chat/serializers.py class RoomSearchSerializer(serializers.ModelSerializer): is_member = serializers.BooleanField(read_only=True) class Meta: model = Room fields = ['id', 'name', 'created_at', 'updated_at', 'is_member']   And:  backend/chat/views.py class RoomSearchViewSet(viewsets.ModelViewSet): serializer_class = RoomSearchSerializer permission_classes = [IsAuthenticated] def get_queryset(self): user = self.request.user user_membership = RoomMember.objects.filter( room=OuterRef('pk'), user=user ) return Room.objects.annotate( is_member=Exists(user_membership) ).order_by('name')   ","version":"v5","tagName":"h3"},{"title":"GET /api/rooms/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#get-apirooms","content":" backend/chat/serializers.py class UserSerializer(serializers.ModelSerializer): class Meta: model = User fields = ['id', 'username'] class LastMessageSerializer(serializers.ModelSerializer): user = UserSerializer(read_only=True) class Meta: model = Message fields = ['id', 'content', 'user', 'created_at'] class RoomSerializer(serializers.ModelSerializer): member_count = serializers.SerializerMethodField() last_message = LastMessageSerializer(read_only=True) def get_member_count(self, obj): return obj.member_count class Meta: model = Room fields = ['id', 'name', 'version', 'member_count', 'last_message']   And:  backend/chat/views.py class RoomListViewSet(ListModelMixin, GenericViewSet): serializer_class = RoomSerializer permission_classes = [IsAuthenticated] def get_queryset(self): return Room.objects.annotate( member_count=Count('memberships') ).filter( memberships__user_id=self.request.user.pk ).prefetch_related('last_message', 'last_message__user').order_by('-memberships__joined_at')   ","version":"v5","tagName":"h3"},{"title":"GET /api/rooms/:room_id/messages/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#get-apiroomsmessages","content":" backend/chat/serializers.py class MessageRoomSerializer(serializers.ModelSerializer): class Meta: model = Room fields = ['id', 'version'] class MessageSerializer(serializers.ModelSerializer): user = UserSerializer(read_only=True) room = MessageRoomSerializer(read_only=True) class Meta: model = Message fields = ['id', 'content', 'user', 'room', 'created_at']   backend/chat/views.py class MessageListCreateAPIView(ListCreateAPIView): serializer_class = MessageSerializer permission_classes = [IsAuthenticated] def get_queryset(self): room_id = self.kwargs['room_id'] get_object_or_404(RoomMember, user=self.request.user, room_id=room_id) return Message.objects.filter( room_id=room_id).prefetch_related('user', 'room').order_by('-created_at') @transaction.atomic def create(self, request, *args, **kwargs): # Will be shown below.   ","version":"v5","tagName":"h3"},{"title":"POST /api/rooms/:room_id/messages/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#post-apiroomsmessages","content":" backend/chat/views.py class MessageListCreateAPIView(ListCreateAPIView): serializer_class = MessageSerializer permission_classes = [IsAuthenticated] def get_queryset(self): # Shown above. @transaction.atomic def create(self, request, *args, **kwargs): room_id = self.kwargs['room_id'] room = Room.objects.select_for_update().get(id=room_id) room.increment_version() channels = self.get_room_member_channels(room_id) serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) obj = serializer.save(room=room, user=request.user) room.last_message = obj room.bumped_at = timezone.now() room.save() headers = self.get_success_headers(serializer.data) return Response(serializer.data, status=status.HTTP_201_CREATED, headers=headers)   Note we make actions here in transaction (using @transaction.atomic), and also use select_for_update method to lock the room while we are working with it. This allows us to atomically increment Room version on every change. We will show how having incremental version inside each room helps us on the frontend side later in the tutorial.  While creating new message we set room.bumped_at to current time – so that we have a desired sort on the frontend side.  ","version":"v5","tagName":"h3"},{"title":"POST /api/rooms/:room_id/join/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#post-apiroomsjoin","content":" backend/chat/serializers.py class RoomMemberSerializer(serializers.ModelSerializer): user = UserSerializer(read_only=True) room = RoomSerializer(read_only=True) class Meta: model = RoomMember fields = ['room', 'user']   backend/chat/views.py class JoinRoomView(APIView): permission_classes = [IsAuthenticated] @transaction.atomic def post(self, request, room_id): room = Room.objects.select_for_update().get(id=room_id) room.increment_version() if RoomMember.objects.filter(user=request.user, room=room).exists(): return Response({&quot;message&quot;: &quot;already a member&quot;}, status=status.HTTP_409_CONFLICT) obj, _ = RoomMember.objects.get_or_create(user=request.user, room=room) channels = self.get_room_member_channels(room_id) obj.room.member_count = len(channels) body = RoomMemberSerializer(obj).data return Response(body, status=status.HTTP_200_OK)   Here we add the current request user into the room.  ","version":"v5","tagName":"h3"},{"title":"POST /api/rooms/:room_id/leave/​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#post-apiroomsleave","content":" backend/chat/views.py class LeaveRoomView(APIView): permission_classes = [IsAuthenticated] @transaction.atomic def post(self, request, room_id): room = Room.objects.select_for_update().get(id=room_id) room.increment_version() channels = self.get_room_member_channels(room_id) obj = get_object_or_404(RoomMember, user=request.user, room=room) obj.room.member_count = len(channels) - 1 pk = obj.pk obj.delete() body = RoomMemberSerializer(obj).data return Response(body, status=status.HTTP_200_OK)   Here we remove the current request user from the room.  ","version":"v5","tagName":"h3"},{"title":"Register urls​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#register-urls","content":" After serializers and view written, we just need to add urls to route requests to views:  backend/chat/urls.py from django.urls import path from .views import RoomListViewSet, RoomDetailViewSet, RoomSearchViewSet, \\ MessageListCreateAPIView, JoinRoomView, LeaveRoomView urlpatterns = [ path('rooms/', RoomListViewSet.as_view({'get': 'list'}), name='room-list'), path('rooms/&lt;int:pk&gt;/', RoomDetailViewSet.as_view({'get': 'retrieve'}), name='room-detail'), path('search/', RoomSearchViewSet.as_view({'get': 'list'}), name='room-search'), path('rooms/&lt;int:room_id&gt;/messages/', MessageListCreateAPIView.as_view(), name='room-messages'), path('rooms/&lt;int:room_id&gt;/join/', JoinRoomView.as_view(), name='join-room'), path('rooms/&lt;int:room_id&gt;/leave/', LeaveRoomView.as_view(), name='leave-room') ]   And in app/urls.py:  backend/app/urls.py from django.contrib import admin from django.urls import path, include from django.contrib.staticfiles.urls import staticfiles_urlpatterns from . import views urlpatterns = [ path('admin/', admin.site.urls), path('api/csrf/', views.get_csrf, name='api-csrf'), path('api/token/connection/', views.get_connection_token, name='api-connection-token'), path('api/token/subscription/', views.get_subscription_token, name='api-subscription-token'), path('api/login/', views.login_view, name='api-login'), path('api/logout/', views.logout_view, name='api-logout'), path('api/', include('chat.urls')), ] urlpatterns += staticfiles_urlpatterns()   So we included all the views we wrote, included chat application urls.  ","version":"v5","tagName":"h3"},{"title":"Adding admin models​","type":1,"pageTitle":"Setting up backend and database","url":"/docs/tutorial/backend#adding-admin-models","content":" We also serving Django built-in admin - it will allow us to create some rooms to play with. In the example source code you may find some additional code in backend/chat/admin.py which registers models in Django admin. After adding Nginx you will be able to start the app and go to http://localhost:9000/admin – and create some rooms. Let's add Nginx now. ","version":"v5","tagName":"h3"},{"title":"Integrating Centrifugo for real-time event delivery","type":0,"sectionRef":"#","url":"/docs/tutorial/centrifugo","content":"","keywords":"","version":"v5"},{"title":"Adding Centrifugo connection​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#adding-centrifugo-connection","content":" Our next goal is to connect to Centrifugo from the frontend app. We will do this right after user authenticated and chat layout loaded.  To add real-time WebSocket connection you need to install centrifuge-js - Centrifugo SDK for Javascript.  npm install centrifuge   Then import it in App.jsx:  import { Centrifuge, PublicationContext, SubscriptionStateContext, SubscribedContext, SubscriptionState } from 'centrifuge';   We also imported some types we will be using in the app.  To establish a connection with Centrifugo as soon as user authenticated in the app we can use useEffect React hook with the dependency on userInfo:  useEffect(() =&gt; { if (!userInfo.id) { return; } let centrifuge: Centrifuge | null = null; const init = async () =&gt; { centrifuge = new Centrifuge(WS_ENDPOINT, { debug: true }) centrifuge.connect() } // As soon as we get authenticated user – init our app. init() return () =&gt; { if (centrifuge) { console.log(&quot;disconnect Centrifuge&quot;) centrifuge.disconnect() } } }, [userInfo])   When user logs out and userInfo.id is not set – the connection to server is closed as we do centrifuge.disconnect() in useEffect cleanup function.  But if you run the code like this – connection won't be established. That's bad news! But we also have good news - this means that Centrifugo supports secure communication and we need to authenticate connection upon establishing! Let's do this.  ","version":"v5","tagName":"h2"},{"title":"Adding JWT connection authentication​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#adding-jwt-connection-authentication","content":" Change Centrifuge constructor to:  centrifuge = new Centrifuge(WS_ENDPOINT, { getToken: getConnectionToken, debug: true })   Where getConnectionToken is function like this:  export const getConnectionToken = async () =&gt; { const response = await axios.get(`${API_ENDPOINT_BASE}/api/token/connection/`, {}) return response.data.token; }   I.e. it makes request to the backend and receives connection JWT in response. Again – frontend makes request to the backend to get Centrifugo connection token. Of course we should implement the view on the backend which processes such requests and generates tokens for authenticated users.  The token must follow specification described in Client JWT authentication chapter. Long story short – it's just a JWT from rfc7519, we can use any JWT library to generate it.  Let's extend backend/app/views.py with this view:  backend/app/views.py import jwt from django.conf import settings def get_connection_token(request): if not request.user.is_authenticated: return JsonResponse({'detail': 'unauthorized'}, status=401) token_claims = { 'sub': str(request.user.pk), 'exp': int(time.time()) + 120 } token = jwt.encode(token_claims, settings.CENTRIFUGO_TOKEN_SECRET) return JsonResponse({'token': token})   – where jwt import is a PyJWT library (pip install PyJWT). We generate JWT where sub claim is set to current user ID and token expires in 2 minutes.  Note, we are using settings.CENTRIFUGO_TOKEN_SECRET here, we need to include this option to backend/app/settings.py:  backend/app/settings.py # CENTRIFUGO_TOKEN_SECRET is used to create connection and subscription JWT. # SECURITY WARNING: make it strong, keep it in secret, never send to the frontend! CENTRIFUGO_TOKEN_SECRET = 'secret'   It must match the value of &quot;token_hmac_secret_key&quot; option from Centrifugo configuration.  Don't forget to include this view to urls.py configuration, and then you can finally connect to Centrifugo from the frontend: upon page load centrifuge-js SDK makes request to the backend to load connection token, establishes WebSocket connection with Centrifugo passing connection token. Centrifugo validates token and since secrets match Centrifugo may be sure the token contains valid information about user.  ","version":"v5","tagName":"h2"},{"title":"Subscribing on personal channel​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#subscribing-on-personal-channel","content":" Awesome! Though simply being connecting is not that useful. We want to receive real-time data from Centrifugo. But how Centrifugo will understand how to route published data? Of course due to channel concept. Client can subscribe to channel to receive all messages published to that channel.  As mentioned before – for this sort of app using a single individual channel for each user makes a lot of sense.  You can ask – could we simply subscribe to all room channels current user is member of? It may be a good thing if you know that users won't have too many groups, let's say 10-100 max. Going above this number will make UI less efficient. Consider user who is a member of a thousand of groups – it will require a very heavyweight initial subscribe request. What if user is member of 10k groups? So moving all the routing complexity to the backend having a single individual channel on the frontend seems a more reasonable approach for our app. And this will also help us to simpify state recovery later.  We already have namespace personal configured in Centrifugo – so let's use it to construct individual channel for each user.  const personalChannel = 'personal:' + userInfo.id   So for user with id 1 we will have channel personal:1, for user 2 – personal:2 – and so on. Of course in messenger app we do not want one user to be able to subscribe on the channel belonging to another user. So we will use subscription token auth for channels here. It's also a JWT loaded from the backend. But this JWT must additionally include channel claim. So in React we can create Subscription object this way:  export const getSubscriptionToken = async (channel: string) =&gt; { const response = await axios.get(`${API_ENDPOINT_BASE}/api/token/subscription/`, { params: { channel: channel } }); return response.data.token; } const getPersonalChannelSubscriptionToken = async () =&gt; { return getSubscriptionToken(personalChannel) } const sub = centrifuge.newSubscription(personalChannel, { getToken: getPersonalChannelSubscriptionToken }) sub.on('publication', (ctx: PublicationContext) =&gt; { // Used to process incoming channel publications. We will talk about it soon. onPublication(ctx.data) }) sub.subscribe()   Note that we additionally attach channel URL query param when requesting backend – so the backend understands which channel to generate subscription JWT for.  On the backend side we check permission to subscribe and return subscription token:  backend/app/views.py def get_subscription_token(request): if not request.user.is_authenticated: return JsonResponse({'detail': 'unauthorized'}, status=401) channel = request.GET.get('channel') if channel != f'personal:{request.user.pk}': return JsonResponse({'detail': 'permission denied'}, status=403) token_claims = { 'sub': str(request.user.pk), 'exp': int(time.time()) + 300, 'channel': channel } token = jwt.encode(token_claims, settings.CENTRIFUGO_TOKEN_SECRET) return JsonResponse({'token': token})   Please refer to client SDK spec for more information about error handling scenarios.  Let's also finish up the logic with real-time subscription status now:  sub.on('state', (ctx: SubscriptionStateContext) =&gt; { if (ctx.newState == SubscriptionState.Subscribed) { setRealTimeStatus('🟢') } else { setRealTimeStatus('🔴') } })   There are several subscription states in all our SDKs - unsubscribed, subscribing, subscribed. You can also listen for them separately for more granular logic and get more detailed information about the reason of subscription loss. See client SDK spec for more detailed description.  Now we should be able to connect (and authenticate) and subscribe to channel (with authorization). Try to open browser tools network tab and see WebSocket frames exchanged between client and server (we showed how to see this in quickstart).  ","version":"v5","tagName":"h2"},{"title":"Publish real-time messages​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#publish-real-time-messages","content":" Now we have real-time WebSocket connection which is subscribed to user individual channel. It's time to start publishing messages upon changes in chat rooms. In out case, we send a real-time message in one of the following scenarios:  someone sends a message to a chat roomuser joins a roomuser leaves a room  But we want all chat room members to receive events. If user 1 sends a messages to chat room, we need to find all current members of this room and publish real-time message to each personal channel. I.e. if three users with IDs 1, 2 and 3 are members of some room – then we need to publish message to three channels personal:1, personal:2 and personal:3. So all the members will be notified about event in real-time.  To efficiently publish message to many channels Centrifugo provides broadcast API. Let's use HTTP API of Centrifugo:  backend/chat/views.py import requests from django.conf import settings class CentrifugoMixin: # A helper method to return the list of channels for all current members of specific room. # So that the change in the room may be broadcasted to all the members. def get_room_member_channels(self, room_id): members = RoomMember.objects.filter(room_id=room_id).values_list('user', flat=True) return [f'personal:{user_id}' for user_id in members] def broadcast_room(self, room_id, broadcast_payload): # Using Centrifugo HTTP API is the simplest way to send real-time message, and usually # it provides the best latency. The trade-off here is that error here may result in # lost real-time event. Depending on the application requirements this may be fine or not. def broadcast(): session = requests.Session() retries = Retry(total=1, backoff_factor=1, status_forcelist=[500, 502, 503, 504]) session.mount('http://', HTTPAdapter(max_retries=retries)) try: session.post( &quot;http://centrifugo:8000/api/broadcast&quot;, data=json.dumps(broadcast_payload), headers={ 'Content-type': 'application/json', 'X-API-Key': settings.CENTRIFUGO_HTTP_API_KEY, 'X-Centrifugo-Error-Mode': 'transport' } ) except requests.exceptions.RequestException as e: logging.error(e) # We need to use on_commit here to not send notification to Centrifugo before # changes applied to the database. Since we are inside transaction.atomic block # broadcast will happen only after successful transaction commit. transaction.on_commit(broadcast) class MessageListCreateAPIView(ListCreateAPIView, CentrifugoMixin): # Same as before @transaction.atomic def create(self, request, *args, **kwargs): room_id = self.kwargs['room_id'] room = Room.objects.select_for_update().get(id=room_id) room.increment_version() channels = self.get_room_member_channels(room_id) serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) obj = serializer.save(room=room, user=request.user) room.last_message = obj room.save() # This is where we add code to broadcast over Centrifugo API. broadcast_payload = { 'channels': channels, 'data': { 'type': 'message_added', 'body': serializer.data }, 'idempotency_key': f'message_{serializer.data[&quot;id&quot;]}' } self.broadcast_room(room_id, broadcast_payload) headers = self.get_success_headers(serializer.data) return Response(serializer.data, status=status.HTTP_201_CREATED, headers=headers)   Let's mention some important things.  We do broadcasts only after successful commit, using Django's transaction.on_commit hook. Otherwise transaction we could get an error on transaction commit - but send misleading real-time message.  Here we use requests library for making HTTP requests (pip install requests) and do some retries which is nice to deal with temporary network issues.  We construct list of channels using values_list method of Djanfo queryset to make query more efficient.  We also using settings.CENTRIFUGO_HTTP_API_KEY which is set in settings.py and matches api_key option from Centrifugo configuration file:  # CENTRIFUGO_HTTP_API_KEY is used for auth in Centrifugo server HTTP API. # SECURITY WARNING: make it strong, keep it in secret! CENTRIFUGO_HTTP_API_KEY = 'api_key'   Note the following:  'idempotency_key': f'message_{serializer.data[&quot;id&quot;]}'   When publishing we provide idempotency_key to Centrifugo – this allows effectively dropping duplicate publications during configurable time window on Centrifugo side.  Another important thing is how we designed the data of the real-time event – note we've included event type field on top level. In this case message_added. This approach allows easily expanding possible event types – so the frontend may distinguish between them and process accordingly.  We can extend JoinRoomView and LeaveRoomView with similar code to also broadcast room membership events:  backend/chat/views.py class JoinRoomView(APIView, CentrifugoMixin): # Some code skipped here .... @transaction.atomic def post(self, request, room_id): # Some code skipped here .... obj, _ = RoomMember.objects.get_or_create(user=request.user, room=room) channels = self.get_room_member_channels(room_id) obj.room.member_count = len(channels) body = RoomMemberSerializer(obj).data broadcast_payload = { 'channels': channels, 'data': { 'type': 'user_joined', 'body': body }, 'idempotency_key': f'user_joined_{obj.pk}' } self.broadcast_room(room_id, broadcast_payload) return Response(body, status=status.HTTP_200_OK) class LeaveRoomView(APIView, CentrifugoMixin): # Some code skipped here .... @transaction.atomic def post(self, request, room_id): # Some code skipped here .... obj = get_object_or_404(RoomMember, user=request.user, room=room) obj.room.member_count = len(channels) - 1 pk = obj.pk obj.delete() body = RoomMemberSerializer(obj).data broadcast_payload = { 'channels': channels, 'data': { 'type': 'user_left', 'body': body }, 'idempotency_key': f'user_left_{pk}' } self.broadcast_room(room_id, broadcast_payload) return Response(body, status=status.HTTP_200_OK)   We also would like to mention the concept of room version. Each room has version field in our app, we increment it by one every time we make some room updates. We then attach version to every event we publish. This technique may be useful to avoid processing non-actual real-time messages on the client side. This is especially useful if we use outbox or CDC techniques where delivery latency increases and a chance to get real-time message which is not actual (i.e. app already loaded more &quot;fresh&quot; state from the backend) increases.  ","version":"v5","tagName":"h2"},{"title":"Handle real-time messages​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#handle-real-time-messages","content":" As we already shown above the entrypoint for incoming real-time messages on the frontend side is on('publication') callback of Subscription object.  sub.on('publication', (ctx: PublicationContext) =&gt; { onPublication(ctx.data) })   Where onPublication is:  const onPublication = (publication: any) =&gt; { setMessageQueue(prevQueue =&gt; [...prevQueue, publication]); };   In our app example we process the messages using asynchronous queue. To be honest, it's hard to give the universal receipt here – it seems to be a good approach for our example, but probably in your own app you will organise message processing differently.  const [chatState, dispatch] = useReducer(reducer, initialChatState); const [messageQueue, setMessageQueue] = useState&lt;any[]&gt;([]); useEffect(() =&gt; { if (messageQueue.length === 0) { return; // Return if no messages to process. } const processUserJoined = async (body: any) =&gt; { // We will describe this very soon. } const processUserLeft = async (body: any) =&gt; { // We will describe this very soon. } const processMessageAdded = async (body: any) =&gt; { // We will describe this very soon. }; const processMessage = async () =&gt; { const message = messageQueue[0]; const { type, body } = message switch (type) { case 'message_added': { await processMessageAdded(body); break } case 'user_joined': { await processUserJoined(body); break } case 'user_left': { await processUserLeft(body); break } default: console.log('unsupported message type', type, body) } // Remove the processed message from the queue setMessageQueue(prevQueue =&gt; prevQueue.slice(1)); }; processMessage(); }, [messageQueue, chatState]);   ","version":"v5","tagName":"h2"},{"title":"Handle message added event​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#handle-message-added-event","content":" Let's look what's going on inside processMessageAdded function:  const processMessageAdded = async (body: any) =&gt; { const roomId = body.room.id const newMessage = body let room = chatState.roomsById[roomId] if (!room) { room = await fetchRoom(roomId) dispatch({ type: &quot;ADD_ROOMS&quot;, payload: { rooms: [room] } }) } let messages = chatState.messagesByRoomId[roomId] if (!messages) { const messages = await fetchMessages(roomId) dispatch({ type: &quot;ADD_MESSAGES&quot;, payload: { roomId: roomId, messages: messages } }) return; } dispatch({ type: &quot;ADD_MESSAGES&quot;, payload: { roomId: roomId, messages: [newMessage] } }) }   We load the room if it was not loaded yet, load room's messages if it's first time we see a message in the room.  ","version":"v5","tagName":"h2"},{"title":"Handle user joined event​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#handle-user-joined-event","content":" const processUserJoined = async (body: any) =&gt; { const roomId = body.room.id const roomVersion = body.room.version let room = chatState.roomsById[roomId] if (!room) { room = await fetchRoom(roomId) if (room === null) { return } dispatch({ type: &quot;ADD_ROOMS&quot;, payload: { rooms: [room] } }) } else { dispatch({ type: &quot;SET_ROOM_MEMBER_COUNT&quot;, payload: { roomId: roomId, version: roomVersion, memberCount: body.room.member_count } }) } }   ","version":"v5","tagName":"h2"},{"title":"Handle user left event​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#handle-user-left-event","content":" const processUserLeft = async (body: any) =&gt; { const roomId = body.room.id const roomVersion = body.room.version const leftUserId = body.user.id let room = chatState.roomsById[roomId] if (room) { if (room.version &gt;= roomVersion) { console.error(`Outdated version for room ID ${roomId}.`); return } if (userInfo.id == leftUserId) { dispatch({ type: &quot;DELETE_ROOM&quot;, payload: { roomId: roomId } }) } else { dispatch({ type: &quot;SET_ROOM_MEMBER_COUNT&quot;, payload: { roomId: roomId, version: roomVersion, memberCount: body.room.member_count } }) } } else if (userInfo.id != leftUserId) { room = await fetchRoom(roomId) dispatch({ type: &quot;ADD_ROOMS&quot;, payload: { rooms: [room] } }) } }   ","version":"v5","tagName":"h2"},{"title":"We did it​","type":1,"pageTitle":"Integrating Centrifugo for real-time event delivery","url":"/docs/tutorial/centrifugo#we-did-it","content":" Awesome – we now have an application with real-time features powered by Centrifugo! Messages and room membership changes are now delivered to users in real-time. Though, it's not the end of our journey. So please, take a break – and then proceed to the next part.   ","version":"v5","tagName":"h2"},{"title":"Appendix #1: Possible Improvements","type":0,"sectionRef":"#","url":"/docs/tutorial/improvements","content":"Appendix #1: Possible Improvements There are still many areas for improvement in GrandChat, but we had to halt at a certain point to prevent the tutorial from becoming a book. If you enjoyed the tutorial and wish to enhance GrandChat further, here are some bright ideas: 💡 Implement a 2-column layout on wide desktop screens – a list of chat rooms on the left and room details on the right. As mentioned in the beginning, this is already achievable with some rearrangement of React components and CSS. 💡 Provide non-admin users with the ability to create new rooms, perhaps creating private rooms for one-to-one communication that are not visible on the &quot;Discover&quot; page. One-to-one chats may just be a subset of our current chat room implementation. At some point, you may add a property to the room defining the room type, allowing for different behavior in rooms of different types. 💡 Enhance the frontend by adding more strict types – leveraging the full power of TypeScript. While using any in some places helped us evolve quickly during the tutorial, strict typing in production will eventually save you time. 💡 Introduce &quot;system&quot; messages, such as displaying messages about users who joined/left inside the room detail view. In this case, the message won't have a user author. We've already made the user field of the Message model nullable to support this scenario. 💡 Pagination was left out of scope here - loading 100 rooms and 100 last messages in rooms. Ideally, we want to lazily load more items too (if scrolled to the end). The backend API implemented here already supports pagination, making it a nice challenge to add it to the app. 💡 Display the number of users in the chat room who are currently online using Centrifugo online presence. For rooms with many active members, consider using a parallel batch request to Centrifugo to get online presence or opt for an implementation using some approximation, like we provide in Centrifugo PRO user status feature. 💡 Save message delivery/read statuses to the application database and show them in the UI. On the chat list screen, highlight chat rooms with unread messages. 💡 Add typing notifications for more interactivity. While this may seem simple, it's actually not – you have to think about debouncing and probably use room-specific channels for efficient publishing. 💡 We are not handling errors everywhere on the client side to prevent further complexity in the tutorial. However, for production, proper error handling is necessary. The basic thing to do is to show an Unrecoverable Error screen, which we already have for some errors in the example. Users can reload the page after encountering it to start from scratch. 💡 Support markdown as message content and add the ability to attach media to messages. Remember that messages should only have a link to media files; do not attempt to pass file content over WebSocket. 💡 Add push notifications to engage offline users to come back to the app or notify them about important messages, such as when someone mentions a user in the room. Centrifugo PRO provides a push notifications API, but you can also use any third-party service. 💡 There is one more possible issue in application state sync we've decided not to solve here – it may occur during the initial load of data from the backend upon page load. If a real-time message comes after the state is loaded but before a real-time subscription is established for the first time, the message won't be shown until page reload. There are multiple ways to fix this, such as establishing a real-time connection/subscription first and then loading the initial chat state and applying messages received while the state was loading. Or get stream top offset from Centrifigo history API before initial state load, then use it for the initial subscribe. Alternatively, silently re-sync the state in the background after setting up a real-time subscription to a personal channel. 💡 Integrate with the ChatGPT API and introduce chatbots with AI skills. In this case, you may additionally send all the messages in chat rooms to Kafka to create an extensible chatbot platform that can be a completely isolated service from the chat core. The possibilities are limitless!","keywords":"","version":"v5"},{"title":"Building WebSocket chat (messenger) app from scratch","type":0,"sectionRef":"#","url":"/docs/tutorial/intro","content":"","keywords":"","version":"v5"},{"title":"Application tech stack​","type":1,"pageTitle":"Building WebSocket chat (messenger) app from scratch","url":"/docs/tutorial/intro#application-tech-stack","content":" Centrifugo is completely agnostic to the technology stack, seamlessly integrating with any frontend or backend technologies. However, for the purpose of this tutorial, we needed to choose specific technologies to illustrate the entire process of building a real-time WebSocket app:  💎 On the frontend, we utilize React and Typescript, with a help of the tooling provided by Vite. The frontend is designed as a Single-Page Application (SPA) that communicates with the backend through a REST API.  💎 For the backend, we employ Python's Django framework, complemented by Django REST Framework to implement the server API. The backend relies on PostgreSQL as its primary database.  💎 Centrifugo will handle WebSocket connections, providing a real-time transport layer for delivering events instantly to users. The backend will communicate with Centrifugo synchronously over Centrifugo HTTP API, and asynchronously using transactional outbox or CDC approach with Kafka Connect.  💎 Nginx acts as a reverse proxy for all public endpoints of the app, facilitating the serving of frontend and backend endpoints from the same domain. This configuration is essential for secure HTTP-only cookie authentication of frontend-to-backend communication.  💎 To handle connection authentication in Centrifugo and perform channel permission checks, we use JWT (JSON Web Token) in the app. This ensures secure real-time communication and helps the backend to deal with a reconnect storm – a problem which becomes very important at scale in WebSocket applications that deal with many real-time connections.    The tutorial is quite lengthy, and it will likely grow larger over time. The primary objective here is to illustrate the process of building a real-time app in detail. Even if you are not familiar with Django or React but wish to grasp Centrifugo concepts, consider reading this tutorial. After going through the entire content, you should feel much more comfortable with Centrifugo design and idiomatic approach to integrate with it.  ","version":"v5","tagName":"h2"},{"title":"Straight to the source code​","type":1,"pageTitle":"Building WebSocket chat (messenger) app from scratch","url":"/docs/tutorial/intro#straight-to-the-source-code","content":" The complete source code for the app we build may be found on Github. If you have Docker, you will be able to run the app locally quickly using just a few Docker Compose commands.  If certain steps in the tutorial appear unclear, remember that you can refer to the source code. Or ask in our communities.  ","version":"v5","tagName":"h2"},{"title":"Centrifugo vs Django Channels​","type":1,"pageTitle":"Building WebSocket chat (messenger) app from scratch","url":"/docs/tutorial/intro#centrifugo-vs-django-channels","content":" Before we begin, a brief note about Django and real-time: Python developers are likely familiar with Django's popular framework for building real-time applications – Django Channels. However, with Centrifugo, you can gain several important advantages:  🔥 More features out-of-the-box, including a history cache, missed message recovery, online presence, admin web UI, excellent observability, support for more real-time transports, Protobuf protocol, etc.  🔥 Centrifugo serves as a universal real-time component, allowing you to decouple your real-time transport layer from the application core. You can integrate Centrifugo into any of your future projects, regardless of the programming language used in the backend.  🔥 It's possible to use a traditional Django approach for writing application business logic — there's no need to use ASGI if you prefer not to. Centrifugo is easy to integrate into existing Django applications working on top of WSGI. And of course it's possible to combine using ASGI in Django with Centrifugo integration.  🔥 You get an amazing scalable performance. Centrifugo is fast and supports sharding by channel to scale further. The use of JWT for authentication and channel authorization enables handling millions of concurrent connections with a reasonable number of Django backend instances. We will demonstrate that achieving chat rooms with tens of thousands of online users and minimal delivery latency is straightforward with Centrifugo. This is something Django Channels users might find challenging without investing considerable time in thinking about how to scale the app properly. ","version":"v5","tagName":"h2"},{"title":"App layout and behavior","type":0,"sectionRef":"#","url":"/docs/tutorial/layout","content":"","keywords":"","version":"v5"},{"title":"App screens​","type":1,"pageTitle":"App layout and behavior","url":"/docs/tutorial/layout#app-screens","content":" We tried to find a good balance which screens to include into the app. The goal was to keep the result minimal while still showcasing the ideas covered in the tutorial. Our completed app consists of four screens.  ","version":"v5","tagName":"h2"},{"title":"Login Screen​","type":1,"pageTitle":"App layout and behavior","url":"/docs/tutorial/layout#login-screen","content":" One of the goals for the tutorial is showing the app with user authentication. To show how to tell Centrifugo which user connects and build permissions for channels around the particular user.  Nothing too special here – we will use native Django user/password authentication. Django already has built-in User model and functions to support user login/logout workflow. So we just use this.This screen does not include any real-time features. As soon as user logs into the app with its username/password pair we see the Chat Room List Screen.  ","version":"v5","tagName":"h3"},{"title":"Chat Room List Screen​","type":1,"pageTitle":"App layout and behavior","url":"/docs/tutorial/layout#chat-room-list-screen","content":" This one shows rooms current user joined. So user can click on any to go to Chat Room Detail Screen.This screen includes a couple of elements to emphasize. First one is a green circle on top right. This is a Centrifugo real-time subscription status. As soon as user connected to Centrifugo and subscribed to the personal message stream (personal channel) - the indicator is green 🟢. Otherwise - it's red 🔴.  The next element - the number of cats users who joined the specific room. This counter is synchronized in real-time. For example, if someone joins the room from Chat Room Search Screen (at which we will look shortly) – the counter will be instantly synchronized. We also automatically add/remove rooms if current user joins/leaves some room from within Chat Room Search Screen opened in another browser tab or another device.  For every room we show the beginning of the last message sent to the room. Upon receiving real-time message we re-order rooms to put the one with latest message on top.  ","version":"v5","tagName":"h3"},{"title":"Chat Room Search Screen​","type":1,"pageTitle":"App layout and behavior","url":"/docs/tutorial/layout#chat-room-search-screen","content":" This screen allows the user to discover new rooms to join. In our app we decided to not provide a functionality for the user to create chat rooms. Rooms must be pre-created by admin – it's actually possible to do using Django built-in admin web UI - so to keep tutorial shorter (not the ideal justification for this tutorial which is freaking large) we decided to skip it for now.We distinguish rooms current user joined or not joined by using color scheme. The information about current user membership is synchronized between browser tabs and different devices. After user joins the room – it appears on Chat Room List Screen on every user's device.  ","version":"v5","tagName":"h3"},{"title":"Chat Room Detail Screen​","type":1,"pageTitle":"App layout and behavior","url":"/docs/tutorial/layout#chat-room-detail-screen","content":" Finally, a page with room name, list of messages and a possibility to send a new one:Of course messages are sent in real-time to all users participating in chat. Also, the counter with number of users right to the room name is also updated in real-time.  ","version":"v5","tagName":"h3"},{"title":"2-column layout in mind​","type":1,"pageTitle":"App layout and behavior","url":"/docs/tutorial/layout#2-column-layout-in-mind","content":" Often in messenger apps you can see the layout where a list of chats is the left column, and chat details shown on the right. Like this one in Slack:    While we use a slightly simplified layout in the app with a separate chat room list and chat detail screens (more often seen on mobile devices), we keep in mind the possibility to switch to the 2-column layout if needed - just with a change of React component arrangement and some CSS. With our implementation user may be theoretically a member of hundreds or thousands of rooms and receive updates from all of them on one screen. Like in Telegram, Discord or Slack messengers.  This predetermined the fact we are using individual user channels in the app to receive real-time updates from all the rooms, instead of subscribing to each individual chat room channel. We will talk about this decision later, for now let's say simply: using individual real-time channels drastically simplifies frontend implementation, leaving the complexity for the backend side. ","version":"v5","tagName":"h2"},{"title":"Broadcast using transactional outbox and CDC","type":0,"sectionRef":"#","url":"/docs/tutorial/outbox_cdc","content":"","keywords":"","version":"v5"},{"title":"Transactional outbox for publishing events​","type":1,"pageTitle":"Broadcast using transactional outbox and CDC","url":"/docs/tutorial/outbox_cdc#transactional-outbox-for-publishing-events","content":" The first approach involves using the Transactional outbox pattern. When you make database changes, you open a transaction, make the required changes, and write an event into a special outbox table. This event will be written to the outbox table only if the transaction is successfully committed. Then, a separate process reads the outbox table and sends events to the external system — in our case, to Centrifugo.  You can implement this approach yourself to publish events to Centrifugo. However, here we will showcase Centrifugo's built-in feature to consume the PostgreSQL outbox table.  All you need to do is create an outbox table in a predefined format (expected by Centrifugo) and point Centrifugo to it.  Moreover, to reduce the latency of outbox processing, Centrifugo supports parallel processing of the outbox table using a configured partition number. Additionally, Centrifugo can be configured to use the PostgreSQL LISTEN/NOTIFY mechanism, significantly reducing the latency of event processing.  First of all, let's create the Outbox model inside chat Django app which describes the required outbox table:  class Outbox(models.Model): method = models.TextField(default=&quot;publish&quot;) payload = models.JSONField() partition = models.BigIntegerField(default=0) created_at = models.DateTimeField(auto_now_add=True)   And make migrations to create it in PostgreSQL:  docker compose exec backend python manage.py makemigrations docker compose exec backend python manage.py migrate   Now, instead of using Centrifugo HTTP API after successful commit, you can create Outbox instance with the required broadcast method and payload:  # In outbox case we can set partition for parallel processing, but # it must be in predefined range and match Centrifugo PostgreSQL # consumer configuration. partition = hash(room_id)%settings.CENTRIFUGO_OUTBOX_PARTITIONS # Creating outbox object inside transaction will guarantee that Centrifugo will # process the command at some point. In normal conditions – almost instantly. Outbox.objects.create(method='broadcast', payload=broadcast_payload, partition=partition)   Also, add the following to Centrifugo configuration:  { ... &quot;consumers&quot;: [ { &quot;name&quot;: &quot;postgresql&quot;, &quot;type&quot;: &quot;postgresql&quot;, &quot;postgresql&quot;: { &quot;dsn&quot;: &quot;postgresql://grandchat:grandchat@db:5432/grandchat&quot;, &quot;outbox_table_name&quot;: &quot;chat_outbox&quot;, &quot;num_partitions&quot;: 1, &quot;partition_select_limit&quot;: 100, &quot;partition_poll_interval&quot;: &quot;300ms&quot;, &quot;partition_notification_channel&quot;: &quot;centrifugo_partition_change&quot; } } ] }   That's it! Now if you save some model and write an event to outbox table insde transaction – you don't need to worry - an event will be delivered to Centrifugo.  But, if you take a look at the configuration above you will see it has option &quot;partition_poll_interval&quot;: &quot;300ms&quot;. This means the outbox approach may add delay for the real-time message. It's possible to reduce this polling interval – but this would mean increasing number of queries to PostgreSQL database. We can do slightly better.  Centrifugo supports LISTEN/NOTIFY mechanism of PostgreSQL to be notified about new data in the outbox table. To enable it you need first create a trigger in PostgreSQL:  CREATE OR REPLACE FUNCTION centrifugo_notify_partition_change() RETURNS TRIGGER AS $$ BEGIN PERFORM pg_notify('centrifugo_partition_change', NEW.partition::text); RETURN NEW; END; $$ LANGUAGE plpgsql; CREATE OR REPLACE TRIGGER centrifugo_notify_partition_trigger AFTER INSERT ON chat_outbox FOR EACH ROW EXECUTE FUNCTION centrifugo_notify_partition_change();   To do this you can connect to PostgreSQL with this command:  docker compose exec db psql postgresql://grandchat:grandchat@localhost:5432/grandchat   And then update consumer config – add &quot;partition_notification_channel&quot; option to it:  { ... &quot;consumers&quot;: [ { &quot;name&quot;: &quot;postgresql&quot;, ... &quot;postgresql&quot;: { ... &quot;partition_poll_interval&quot;: &quot;300ms&quot;, &quot;partition_notification_channel&quot;: &quot;centrifugo_partition_change&quot; } } ] }   After doing that restart everything – and enjoy instant event delivery!  ","version":"v5","tagName":"h2"},{"title":"Using Kafka Connect for CDC​","type":1,"pageTitle":"Broadcast using transactional outbox and CDC","url":"/docs/tutorial/outbox_cdc#using-kafka-connect-for-cdc","content":" Let's also look at another approach - usually known as CDC - Change Data Capture (you can learn more about it from this post, for example). We will use Kafka Connect with Debezium connector to read updates from PostgreSQL WAL and translate them to Kafka. Then we will use built-in Centrifugo possibility to consume Kafka topics.  The CDC approach with reading WAL has an advantage that in most cases it comes with a very low overhead for the database. In the outbox shown case above we constantly polling PostgreSQL for changes, which may be less effective for the database.  To configure CDC flow we must first configure PostgreSQL to use logical replication. To do this let's update db service in docker-compose.yml:  docker-compose.yml db: image: postgres:15 volumes: - ./postgres_data:/var/lib/postgresql/data/ healthcheck: test: [ &quot;CMD&quot;, &quot;pg_isready&quot;, &quot;-U&quot;, &quot;grandchat&quot; ] interval: 1s timeout: 5s retries: 10 environment: - POSTGRES_USER=grandchat - POSTGRES_PASSWORD=grandchat - POSTGRES_DB=grandchat expose: - 5432 ports: - 5432:5432 command: [&quot;postgres&quot;, &quot;-c&quot;, &quot;wal_level=logical&quot;, &quot;-c&quot;, &quot;wal_writer_delay=10ms&quot;]   Note – added command field where postgres is launched with wal_level=logical option. We also tune wal_writer_delay to be faster.  Then let's add Kafka Connect and Kafka itself to our docker-compose.yml:  docker-compose.yml zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 kafka: image: confluentinc/cp-kafka:latest depends_on: - zookeeper ports: - &quot;29092:29092&quot; expose: - 9092 healthcheck: test: [&quot;CMD&quot;, &quot;kafka-topics&quot;, &quot;--list&quot;, &quot;--bootstrap-server&quot;, &quot;localhost:9092&quot;] interval: 2s timeout: 5s retries: 10 environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 KAFKA_MAX_REQUEST_SIZE: &quot;10485760&quot; # max.request.size KAFKA_MESSAGE_MAX_BYTES: &quot;10485760&quot; # message.max.bytes KAFKA_MAX_PARTITION_FETCH_BYTES: &quot;10485760&quot; # max.partition.fetch.bytes connect: image: debezium/connect:latest depends_on: db: condition: service_healthy kafka: condition: service_healthy ports: - &quot;8083:8083&quot; environment: BOOTSTRAP_SERVERS: kafka:9092 GROUP_ID: 1 CONFIG_STORAGE_TOPIC: connect_configs OFFSET_STORAGE_TOPIC: connect_offsets STATUS_STORAGE_TOPIC: connect_statuses   Kafka uses Zookeeper, so we added it here too. Next, we need to configure Debezium to use PostgreSQL plugin:  debezium/debezium-config.json { &quot;name&quot;: &quot;grandchat-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.postgresql.PostgresConnector&quot;, &quot;database.hostname&quot;: &quot;db&quot;, &quot;database.port&quot;: &quot;5432&quot;, &quot;database.user&quot;: &quot;grandchat&quot;, &quot;database.password&quot;: &quot;grandchat&quot;, &quot;database.dbname&quot;: &quot;grandchat&quot;, &quot;database.server.name&quot;: &quot;db&quot;, &quot;table.include.list&quot;: &quot;public.chat_cdc&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;schema-changes.chat_cdc&quot;, &quot;plugin.name&quot;: &quot;pgoutput&quot;, &quot;tasks.max&quot;: &quot;1&quot;, &quot;producer.override.max.request.size&quot;: &quot;10485760&quot;, &quot;topic.creation.default.cleanup.policy&quot;: &quot;delete&quot;, &quot;topic.creation.default.partitions&quot;: &quot;8&quot;, &quot;topic.creation.default.replication.factor&quot;: &quot;1&quot;, &quot;topic.creation.default.retention.ms&quot;: &quot;604800000&quot;, &quot;topic.creation.enable&quot;: &quot;true&quot;, &quot;topic.prefix&quot;: &quot;postgres&quot;, &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;, &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;, &quot;key.converter.schemas.enable&quot;: &quot;false&quot;, &quot;value.converter.schemas.enable&quot;: &quot;false&quot;, &quot;poll.interval.ms&quot;: &quot;100&quot;, &quot;transforms&quot;: &quot;extractContent&quot;, &quot;transforms.extractContent.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Value&quot;, &quot;transforms.extractContent.field&quot;: &quot;after&quot;, &quot;message.key.columns&quot;: &quot;public.chat_cdc:partition&quot;, &quot;snapshot.mode&quot;: &quot;never&quot; } }   And we should add one more image to docker-compose.yml to apply this configuration on start:  docker-compose.yml connect-config-loader: image: appropriate/curl:latest depends_on: - connect volumes: - ./debezium/debezium-config.json:/debezium-config.json command: &gt; /bin/sh -c &quot; echo 'Waiting for Kafka Connect to start...'; while ! curl -f http://connect:8083/connectors; do sleep 1; done; echo 'Kafka Connect is up, posting configuration'; curl -X DELETE -H 'Content-Type: application/json' http://connect:8083/connectors/chat-connector; curl -X POST -H 'Content-Type: application/json' -v --data @/debezium-config.json http://connect:8083/connectors; echo 'Configuration posted'; &quot;   Here we recreate Kafka Connect configuration on every start of Docker compose, in real life you won't do this - you will create configuration once and update it only if needed. But for development we want to apply changes in file automatically without the need to use REST API of Kafka Connect manually.  Next step here is configure Centrifugo to consume Kafka topic:   ... &quot;consumers&quot;: [ { &quot;name&quot;: &quot;my_kafka_consumer&quot;, &quot;type&quot;: &quot;kafka&quot;, &quot;kafka&quot;: { &quot;brokers&quot;: [&quot;kafka:9092&quot;], &quot;topics&quot;: [&quot;postgres.public.chat_cdc&quot;], &quot;consumer_group&quot;: &quot;centrifugo&quot; } } ] }   We will also create new model in Django called CDC, it will be used for CDC process:  # While the CDC model here is the same as Outbox it has different partition field semantics, # also in outbox case we remove processed messages from DB, while in CDC don't. So to not # mess up with different semantics when switching between broadcast modes of the example app # we created two separated models here. class CDC(models.Model): method = models.TextField(default=&quot;publish&quot;) payload = models.JSONField() partition = models.BigIntegerField(default=0) created_at = models.DateTimeField(auto_now_add=True)   And use it like this:  # In cdc case Debezium will use this field for setting Kafka partition. # We should not prepare proper partition ourselves in this case. partition = hash(room_id) # Creating outbox object inside transaction will guarantee that Centrifugo will # process the command at some point. In normal conditions – almost instantly. In this # app Debezium will perform CDC and send outbox events to Kafka, event will be then # consumed by Centrifugo. The advantages here is that Debezium reads WAL changes and # has a negligible overhead on database performance. And most efficient partitioning. # The trade-off is that more hops add more real-time event delivery latency. May be # still instant enough though. CDC.objects.create(method='broadcast', payload=broadcast_payload, partition=partition)   It seems very similar to what we had with Outbox model, please take a look at comments in the code snippets above to be aware of the difference in the model semantics.  That's how it is possible to use CDC to stream events to Centrifugo. This approach may come with larger delivery latency but it has some important benefits over transactional outbox approach shown above:  it may provide better throughput as we are not limited in predefined number of partitions which is expected to be not very large. Here we can rely on Kafka native partitioning which is more scalable than reading SQL table concurrently by partitionsince we are reading WAL here - the load on the database (PostgreSQL) should be mostly negligible. While in outbox case we are constantly polling tables and removing processed rows.  But we can eliminate latency downside to take best of two worlds.  ","version":"v5","tagName":"h2"},{"title":"Solving CDC latency​","type":1,"pageTitle":"Broadcast using transactional outbox and CDC","url":"/docs/tutorial/outbox_cdc#solving-cdc-latency","content":" To minimize latency in the case of CDC but still ensure reliable event delivery, we can employ a combined approach: broadcasting using the HTTP API upon a successful transaction and saving the Outbox/CDC model as well. Why does this work? Because we use an idempotency key when publishing to Centrifugo. As a result, the second message will be rejected by Centrifugo and will not reach subscribers.  Moreover, on the client side we are using techniques to deal with duplicate messages – we are accurately updating state to prevent duplicate messages in a room, for counters we send the current number of members in a room instead of incrementing/decrementing one by one upon receiving the event. In other words, we ensure idempotent message processing on the client side.  Another technique that helps us distinguish duplicate or outdated messages is using incremental versioning of the room. Each event we send related to the room includes a room version. On the client side, this enables us to compare the current state room version with the event room version and discard processing non-actual messages. This approach addresses the issue of late message delivery, avoiding unnecessary work and UI updates.  In the next chapter, we will examine some actual numbers to illustrate how this combined approach works as expected. ","version":"v5","tagName":"h2"},{"title":"Wrapping up – things learnt","type":0,"sectionRef":"#","url":"/docs/tutorial/outro","content":"Wrapping up – things learnt At this point, we have a working real-time app, so the tutorial comes to an end. We've covered some concepts of Centrifugo, such as: Using channel namespaces to configure channel behavior granularly for a specific real-time feature.Employing user authentication over connection JWT to mitigate the load on your backend's session layer during a mass reconnect scenario.Channel authorization using subscription JWT to ensure users can only subscribe to channels allowed by business logic.Automatic recovery of missed messages from the history cache to restore state upon short-term disconnects.Utilizing the Centrifugo HTTP API for publishing (broadcasting) messages to channels.Implementing a publication idempotency key for safer and more efficient retries.Leveraging Centrifugo's built-in ability to consume events from PostgreSQL and Kafka. It's worth noting that these concepts can be applied beyond building a messenger-like application. Centrifugo is not limited to chats; it serves as a generic-purpose real-time messaging server. Some approaches here may not perfectly suit your specific use case, so be sure to explore the rest of the documentation for additional possibilities. Messenger is quite a complex type of application, for simpler use cases your integration with Centrifugo and handling real-time events may not require all the techniques demonstrated here. Remember that the entire source code of the app is available on GitHub under the permissive MIT license. We would appreciate it if you forked it and adapted it for another tech stack, whether by replacing frontend or backend technologies, or both. If you do so, feel free to share your work in our communities. Please share At the beginning of the tutorial, we promised to go beyond the basics usually shown in chat tutorials. Do you agree we did? If yes, please share the link to the tutorial on your social networks to help Centrifugo grow 🙏","keywords":"","version":"v5"},{"title":"Missed messages recovery","type":0,"sectionRef":"#","url":"/docs/tutorial/recovery","content":"Missed messages recovery At this point, we already have a real-time application with the instant delivery of events to interested messenger users. Now, let's focus on ensuring reliable message delivery. The first step would be enabling Centrifugo's automatic message recovery for personal channels. Enabling this feature allows connections to automatically recover missed messages due to brief network disconnections, such as when moving through areas with limited mobile internet coverage, and it aids in recovering messages after disconnections caused by a Centrifugo node restart (in the case of using the Redis Engine). The most crucial aspect of auto recovery is its ability to handle mass reconnect scenarios. This situation might occur when a load balancer at the infrastructure level is reloaded, causing all connections to your app to be dropped and attempting to re-establish. In cases like our messenger app, clients want to load the latest state, leading to numerous requests to your main database (more connections result in a larger burst of requests in a short time). Centrifugo efficiently recovers from the history cache, helping your backend manage such scenarios. This is particularly valuable if the backend is written in Django, allowing for many WebSocket connections with a still reasonable number of Django app processes. To implement this, we need to extend the Centrifugo personal namespace configuration: { ... &quot;namespaces&quot;: [{ &quot;name&quot;: &quot;personal&quot;, &quot;history_size&quot;: 300, &quot;history_ttl&quot;: &quot;600s&quot;, &quot;force_recovery&quot;: true }] } We set history_size and history_ttl to some reasonable values, also enebled auto recovery for channels in personal namespace. This configuration is enough for recovery to start working in our app - without any changes on the frontend side. Now if client temporary looses internet connection and then comes back online – Centrifugo will redeliver client all the publications from last seen publication offset. It's also possible to set initial offset (if known) when creating subscription object. The feature is described in detail in History and recovery chapter. If client was offline for a long time or there were mode than 300 publications while client was offline Centrifugo understands that it can't recover client's state. In this case Centrifugo sets a special flag to subscribed state event context. We can handle it and suggest client to reload the app: sub.on('subscribed', (ctx: SubscribedContext) =&gt; { if (ctx.wasRecovering &amp;&amp; !ctx.recovered) { setUnrecoverableError('State LOST - please reload the page') } }) So the idea here that in most case Centrifug message history cache will help clients catch up, in some cases though clients still need to load state from scratch from the main database. So that we effectively solve mass reconnect problem. Also note, that message history cache in Centrifugo Memory Engine used in the example does not survive Centrifugo node restarts – so clients will get &quot;recovered&quot;: false upon reconnecting after Centrifugo restart. This is where Redis Engine has an advantage – it allows message hostory to survive Centrifugo node restarts. In the next chapter we will discuss one more aspect of reliable message delivery - on the way between backend and Centrifugo.","keywords":"","version":"v5"},{"title":"Adding Nginx as a reverse proxy","type":0,"sectionRef":"#","url":"/docs/tutorial/reverse_proxy","content":"Adding Nginx as a reverse proxy As mentioned, we are building a single-page frontend application here, and the frontend will be completely decoupled from the backend. This separation is advantageous because Centrifugo users can theoretically swap only the backend or frontend components while following this tutorial. For example, one could keep the frontend part but attempt to implement the backend in Laravel, Rails, or another framework. For general user authentication, we will utilize native Django session authentication, which relies on cookies. For optimal security, we will employ HTTP-only cookies. To make such a setup compatible with the SPA frontend, we should serve both the frontend and backend from the same domain. For more details, you can refer to this excellent tutorial: Django Session-based Auth for Single Page Apps. It provides a thorough explanation of the approach used here, along with other options for configuring Django in SPA scenarios. While any reverse proxy can be used, we will use Nginx, one of the most popular reverse proxies globally. Here is the configuration for Nginx, placed in the nginx/nginx.conf file: nginx/nginx.conf user www-data; worker_processes auto; pid /run/nginx.pid; include /etc/nginx/modules-enabled/*.conf; events { worker_connections 1024; } http { upstream backend { server backend:8000; } upstream frontend { server frontend:5173; } upstream centrifugo { server centrifugo:8000; } server { listen 80; server_name localhost 127.0.0.1; location /api { proxy_pass http://backend; proxy_http_version 1.1; proxy_redirect default; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } location /admin { proxy_pass http://backend; proxy_http_version 1.1; proxy_redirect default; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } location /static { proxy_pass http://backend; proxy_set_header Host $host; proxy_http_version 1.1; proxy_redirect default; } location /connection/websocket { proxy_pass http://centrifugo; proxy_http_version 1.1; proxy_redirect default; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } location / { proxy_pass http://frontend; proxy_http_version 1.1; proxy_redirect default; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } } } And add Nginx to docker-compose.yaml file: docker-compose.yml nginx: image: nginx:1.25 volumes: - ./nginx:/etc/nginx/ ports: - 9000:80 depends_on: - backend As you may noticed we added several locations to Nginx: /api - this is an entrypoint for backend REST API. We will implement it shortly./admin - serves Django built-in admin web UI, it will allow us to create some rooms to interact with/static - to serve Django admin static files/connection/websocket - this is a proxy to Centrifugo service, we will setup Centrifugo later in this tutorial/ – and finally root path serves the frontend app, we will be creating it soon too. Run the app, go to http://localhost:9000/admin – authenticate using superuser credentials created previously and create some rooms.Now it's time to build the frontend part of the app – to display user rooms, join/leave rooms and create new messages.","keywords":"","version":"v5"},{"title":"Scale to 100k cats in room","type":0,"sectionRef":"#","url":"/docs/tutorial/scale","content":"Scale to 100k cats in room Congratulations – we've built an awesome app and we are done with the development within this tutorial! 🎉 But before wrapping up, let's experiment a little. Here we will try to look at some latency numbers for the room with 100, 1k, 10k, 100k members in different scenarios. Not many apps will reach 100k members in one group scale, but we want to show that Centrifugo gives you a way to grow this big keeping reasonable latency times, and also gives answers how to reduce latency and increase a system throughput further. info This chapter is still to be improved. We've included some numbers we were able to get while experimenting with the app – but left configuring Redis Engine out of scope for now. In experiments we used Centrifugo running outside of Docker. See how we did it in Tips and Tricks. In our blog post Million connections with Centrifugo we've shown that on a limited hardware resources, comparable to one modern server machine, delivering 500k messages per second with delivery latency no more than 200ms in 99 percentile is possible with Centrifugo. But this case is different, in this app we want to have large group chats with many members. The difference here is that publishing involves sending a message to each individual channel – so instead of small fan-in and large fan-out we have large fan-in and mostly the same fan-out (mostly – because user may have several connections from different devices). We also have Django on the backend and database communication here – which also makes the use case different as we need to take backend processing timings into account too. Let's create fake users and fill the rooms with members. First, the function to create fake users programatically: backend/app/utils.py from django.contrib.auth.models import User from django.utils.crypto import get_random_string from django.contrib.auth.hashers import make_password def create_users(n): users = [] total = 0 for _ in range(n): username = get_random_string(10) email = f&quot;{username}@example.com&quot; password = get_random_string(50) user = User(username=username, email=email, password=make_password(password, None)) users.append(user) if len(users) &gt;= 100: total += len(users) User.objects.bulk_create(users) users = [] print(&quot;Total users created:&quot;, total) # Create remaining users. if users: total += len(users) User.objects.bulk_create(users) print(&quot;Total users created:&quot;, total) A function to create rooms: from chat.models import Room def create_room(name): return Room.objects.create(name=name) Similar helper script may be used to fill the room with users: from chat.models import RoomMember, Room def fill_room(room_id, limit): members = [] total = 0 room = Room.objects.get(pk=room_id) for user in User.objects.all()[:limit]: members.append(RoomMember(room=room, user=user)) if len(members) &gt;= 100: total += len(members) RoomMember.objects.bulk_create(members, ignore_conflicts=True) members = [] print(&quot;Total members created:&quot;, total) # Create remaining members. if members: total += len(members) RoomMember.objects.bulk_create(members, ignore_conflicts=True) print(&quot;Total members created:&quot;, total) And finally, a function to quickly bootstrap rooms with desired number of members: def setup_dev(): create_users(100_000) r1 = create_room('Centrifugo') fill_room(r1.pk, 100_000) r2 = create_room('Movies') fill_room(r2.pk, 10_000) r3 = create_room('Programming') fill_room(r3.pk, 1_000) r4 = create_room('Football') fill_room(r4.pk, 100) To create users connect to Django shell: docker compose exec backend python manage.py shell And run: from app.utils import setup_dev setup_dev() This may take a while, please see how to speed this app in the comment of create_users in source code. TLDR - it's possible to relax requirements to password a bit. Which is totally OK for experiment purposes and allows creating 100k users in seconds. Now, let's compare some latency numbers for these rooms when broadcasting a message. We will measure: median time of Django handler which processes message creation in every broadcast mode (creation). We have four broadcast modes here: api, outbox, cdc, api_cdc (combined API and CDC)median time Centrifugo spends on broadcast request (broadcast) - this time is spent by Centrifugo on putting each publication to individual channel from the request, saving publication to each channel's historyend-to-end median latency – the time between pressing ENTER by a user till receiving real-time message (delivery). This includes passing data over entire stack: Nginx proxy -&gt; Gunicorn/Django -&gt; [api | outbox | cdc | api_cdc ] -&gt; Centrifugo. In practice, in messenger application, only small part of those members will be online at the moment of message broadcast – in this experiment we will measure the delivery latency while only one client in the room online – it's OK because having more users connected scales very well in Centrifugo with adding more nodes, so numbers achieved here are totally achievable with more online connections in the room just by adding several more Centrifugo nodes. Also note, that in reality there will be some additional overhead due to network latencies missing in this experiment. Our goal here is to show the overhead of technologies used to build the app here. The experiment's goal is to give you the idea of difference, not exact latency values (which may be better or worse depending on the hardware, operating system, etc). All measurements were done on a single local machine – Apple Macbook M1 Pro – not very scientific, but fits the goal. We first start with Centrifugo that uses Memory engine which is the fastest one: api\toutbox\tcdc\tapi_cdc100\tcreation: 50ms broadcast: 2ms delivery 40ms\tcreation: 35ms broadcast: 1ms delivery 70ms\tcreation: 35ms broadcast: 1ms delivery 140ms\tcreation: 50ms broadcast: 1ms delivery 50ms 1k\tcreation: 60ms broadcast: 20ms delivery 50ms\tcreation: 50ms broadcast: 18ms delivery 75ms\tcreation: 50ms broadcast: 18ms delivery 170ms\tcreation: 60ms broadcast: 18ms delivery 55ms 10k\tcreation: 120ms broadcast: 60ms delivery 115ms\tcreation: 55ms broadcast: 55ms delivery 130ms\tcreation: 55ms broadcast: 55ms delivery 250ms\tcreation: 170ms broadcast: 55ms delivery 150ms 100k\tcreation: 620ms broadcast: 520ms delivery 600ms\tcreation: 170ms broadcast: 500ms delivery 600ms\tcreation: 170ms broadcast: 500ms delivery 750ms\tcreation: 900ms broadcast: 500ms delivery 750ms Things to observe: end-to-end latency here includes the time Django processes the request, that's why we can't go below 40ms even in rooms with only 100 members.when broadcasting over Centrifugo API - message delivered even faster than Django handler completes its work (since we are publishing synchronously somewhere inside request processing). I.e. this means your frontend can receive real-time message before publish request completes, this is actually true for all other broadcast mode – just with much smaller probability.using outbox and CDC decreases time of message creation, but latency increases – since broadcastng is asynchronous, and several more stages involved into the flow. It's generally possible to tune to be faster.for 10k members in group latencies are very acceptable for the messenger app, this is already the scale of quite huge organizations which use Slack messenger, and it's not limit as we will show.using API and CDC together provides better latency than just CDC (so we proved it works as expected!), but probably for large groups you may want to only use CDC to keep publication time reasonably small. Now let's use Centrifugo Redis engine. In the tutorial we used in-memory engine of Centrifugo. But with Redis engine it's possible to scale Centrifugo nodes and load balance WebSocket connections over them. We left Redis Engine out of the scope in the tutorial – but you can simply add it by extending docker-compose.yml. Here are results we got for it: api\toutbox\tcdc\tapi_cdc100\tcreation: 55ms broadcast: 6ms delivery 50ms\tcreation: 35ms broadcast: 5ms delivery 55ms\tcreation: 35ms broadcast: 5ms delivery 140ms\tcreation: 55ms broadcast: 5ms delivery 50ms 1k\tcreation: 75ms broadcast: 30ms delivery 60ms\tcreation: 40ms broadcast: 25ms delivery 70ms\tcreation: 40ms broadcast: 25ms delivery 180ms\tcreation: 50ms broadcast: 25ms delivery 60ms 10k\tcreation: 240ms broadcast: 170ms delivery 220ms\tcreation: 65ms broadcast: 160ms delivery 250ms\tcreation: 65ms broadcast: 160ms delivery 300ms\tcreation: 260ms broadcast: 180ms delivery 260ms 100k\tcreation: 1.5s broadcast: 1.4s delivery 1.5s\tcreation: 140ms broadcast: 1.4s delivery 2s\tcreation: 140ms broadcast: 1.4s delivery 2s\tcreation: 2.8ms broadcast: 160ms delivery 2.6s We see that timings went beyond one second for Redis case for group with 100k members. Since we are sending to 100k individual channels here with saving message history for each, the amount of work is significant. But channel is the unit of scalability in Centrifugo. Let's discuss how we can improve timings in Redis engine case. First thing to do is adding more Redis instances. Redis operates using a single core of processor, so on modern server machine we can easily start many Redis processes and point Centrifugo to them. Centrifugo will then shard the work between Redis shards. It's also possible to point Centrifugo to a Redis cluster consisting of many nodes. For example, let's start Redis cluster based on 4 nodes and point Centrifugo to it. We then get the following results (skipped 100, 1k and 10k scenarios here as they already fast enough): api\toutbox\tcdc\tapi_cdc100k\tcreation: 1s broadcast: 900ms delivery 950ms\tcreation: 220ms broadcast: 850ms delivery 1s\tcreation: 200ms broadcast: 850ms delivery 1.3s\tcreation: 1.6s broadcast: 950ms delivery 1.5s We can see that latency of broadcasting to 100k channels dropped: 1.5s -&gt; 900ms. This is because we offloaded some work from a single Redis to several instances. To reduce the latency of massive broadcast further another concern should be taken into account – we need to split broadcast to many Centrifugo nodes. Currently all publications inside broadcast request are processed by one Centrifugo node (since all the channels belong to one broadcast request). If we add more Centrifugo nodes and split one broadcast request to several ones to utilize different Centrifugo nodes – we will parallelize the work of broadcasting the same message to many channels. You may send parallel requests with splitted batches to Centrifugo HTTP broadcast API (though this requires asynchronous model or using thread pool). Or stick with asynchronous broadcast (outbox, CDC, etc). In this case, make sure to construct batches which belong to different partitions to achieve parallel processing. You can reduce the request up to one channel in batch – which makes broadcast equal to Centrifugo's publish API. If you want to keep strict message order then you need to be careful about proper partitioning of processed data. In our app case, we could split channels by user ID. So messages in one broadcast batch belonging to the specific partition may contain stable subset of user IDs (for example, you can apply a hash function to the user ID (or individual channel) and get the reminder of division to some configured number which is much larger that the number of partitions). In that case the order inside one individual user channel will be preserved. After applying these recommendations your requests will be processed in parallel and scale by adding more Centrifugo nodes, more Redis nodes, more partitions. We've made a quick experiment using sth like this in Django code: from itertools import islice def chunks(xs, n): n = max(1, n) iterator = iter(xs) return iter(lambda: list(islice(iterator, n)), []) channel_batches = chunks(channels, 1000) cdc_objects = [] i = 0 for batch in channel_batches: broadcast_payload = { 'channels': batch, 'data': { 'type': 'message_added', 'body': serializer.data }, 'idempotency_key': f'message_{serializer.data[&quot;id&quot;]}' } cdc_objects.append(CDC(method='broadcast', payload=broadcast_payload, partition=i)) i+=1 CDC.objects.bulk_create(cdc_objects) caution Note, this code does not properly partition data, so may result into incorrect ordering - was used just to prove the idea! With this batch approach and running Centrifugo with 8 isolated Redis instances and Centrifugo's client-side Redis sharding feature we were able to quickly achieve 400ms median delivery latency on Digital Ocean 16 CPU-core droplet. So sending a message to a group with 100k members feels almost instant: If we take Slack as an example, this already feels nice to cover messaging needs of some largest organizations in the world. It will also work for Amazon scale, who has around 1.5 million people now – just need more resources for better end-to-end latency or simply trade-off the latency in large messenger groups for reduced resources. To conclude here, scaling messenger apps requires careful thinking. The complexity in the case goes from the fact we are using personal channels for message delivery - thus we have a massive fan-in and need to use broadcast API of Centrifugo. If we had isolated chat rooms (for example, like real-time comments for each video on Youtube web site) – then it could be much easier to implement and to scale. Because we could just subscribe to the specific room channel instead of user individual channel and publish only to one channel on every message sent (using Centrifugo simple publish API). It's a very small fan-in and the scalability with many concurrent users may be simply achieved by adding more Centrifugo nodes. Also, if we had only one-to-one chat rooms in the app, without super-groups with 100k members – again, it scales pretty easily. If you don't need message recovery – than disabling it will provide better performance too. Our experiments with 100k members and a single Nats server as broker showed 300ms delivery latency. But when we design an app where we want to have a screen with all user's rooms, where some rooms have massive number of members, and need to consume updates from all of them – things are becoming harder as we've just shown above. That's an important thing to keep in mind - application specifics may affect Centrifugo channel configuration and performance a lot.","keywords":"","version":"v5"},{"title":"Appendix #2: Tips and tricks","type":0,"sectionRef":"#","url":"/docs/tutorial/tips_and_tricks","content":"","keywords":"","version":"v5"},{"title":"Point to Centrifugo running on host (outside Docker)​","type":1,"pageTitle":"Appendix #2: Tips and tricks","url":"/docs/tutorial/tips_and_tricks#point-to-centrifugo-running-on-host-outside-docker","content":" We did this ourselves while experimenting and measuring latency numbers in different scenarios. If you want to run the example, but need to point backend or Nginx to look at Centrifugo on your machine outside Docker, then you can use:  On Linux run ifconfig and find docker0 interface – use its ip address to point to Centrifugo. In our case it was 172.17.0.1, so we pointed Nginx to 172.17.0.1:8000 upstream (Centrifugo runs on port 8000 by default), and in Django code used http://172.17.0.1:8000/api/broadcast endpoint. Also, make sure you are using &quot;address&quot;: &quot;0.0.0.0&quot; in Centrifugo configurationOn Macos – use host.docker.internal special name, i.e. host.docker.internal:8000 in Nginx and http://host.docker.internal:8000/api/broadcast as API endpoint for Django code.  ","version":"v5","tagName":"h2"},{"title":"Connect to PosgreSQL​","type":1,"pageTitle":"Appendix #2: Tips and tricks","url":"/docs/tutorial/tips_and_tricks#connect-to-posgresql","content":" Run from within example repo root:  docker compose exec db psql postgresql://grandchat:grandchat@localhost:5432/grandchat   ","version":"v5","tagName":"h2"},{"title":"Inspect dockerized Kafka state​","type":1,"pageTitle":"Appendix #2: Tips and tricks","url":"/docs/tutorial/tips_and_tricks#inspect-dockerized-kafka-state","content":" List current Kafka topics (run from within example repo root):  docker compose exec kafka kafka-topics --bootstrap-server kafka:9092 --list   Describe the state of Kafka topic:  docker compose exec kafka kafka-topics --bootstrap-server kafka:9092 --describe --topic postgres.public.chat_cdc   Show state of consumer group – partitions, lag, offsets (centrifugo is a name of consumer group in our case):  docker compose exec kafka kafka-consumer-groups --bootstrap-server kafka:9092 --describe --group centrifugo   Tail new messages in topic:  docker compose exec kafka kafka-console-consumer --bootstrap-server kafka:9092 --topic postgres.public.chat_cdc   ","version":"v5","tagName":"h2"},{"title":"Pause Kafka Connect​","type":1,"pageTitle":"Appendix #2: Tips and tricks","url":"/docs/tutorial/tips_and_tricks#pause-kafka-connect","content":" Or any other service in Docker compose when you need to test failure scenarios (use name of service from docker-compose.yml):  docker compose pause connect   To run again:  docker compose unpause connect  ","version":"v5","tagName":"h2"},{"title":"Creating SPA frontend with React","type":0,"sectionRef":"#","url":"/docs/tutorial/frontend","content":"","keywords":"","version":"v5"},{"title":"App layout​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#app-layout","content":" Describing frontend code will be not so linear like we had for the backend case. First, let's start with the application top-level layout:  frontend/src/App.tsx const App: React.FC = () =&gt; { let localAuth: any = {}; if (localStorage.getItem(LOCAL_STORAGE_AUTH_KEY)) { localAuth = JSON.parse(localStorage.getItem(LOCAL_STORAGE_AUTH_KEY)!) } const [authenticated, setAuthenticated] = useState&lt;boolean&gt;(localAuth.id !== undefined) const [userInfo, setUserInfo] = useState&lt;any&gt;(localAuth) const [csrf, setCSRF] = useState('') const [unrecoverableError, setUnrecoverableError] = useState('') const [chatState, dispatch] = useReducer(reducer, initialChatState); const [realTimeStatus, setRealTimeStatus] = useState('🔴') const [messageQueue, setMessageQueue] = useState&lt;any[]&gt;([]); return ( &lt;CsrfContext.Provider value={csrf}&gt; &lt;AuthContext.Provider value={userInfo}&gt; {authenticated ? ( &lt;ChatContext.Provider value={{ state: chatState, dispatch }}&gt; &lt;Router&gt; &lt;ChatLayout&gt; &lt;Routes&gt; &lt;Route path=&quot;/&quot; element={&lt;ChatRoomList /&gt;} /&gt; &lt;Route path=&quot;/search&quot; element={&lt;ChatSearch /&gt;} /&gt; &lt;Route path=&quot;/rooms/:id&quot; element={&lt;ChatRoomDetail /&gt;} /&gt; &lt;/Routes&gt; &lt;/ChatLayout&gt; &lt;/Router&gt; &lt;/ChatContext.Provider&gt; ) : ( &lt;ChatLogin /&gt; )} &lt;/AuthContext.Provider&gt; &lt;/CsrfContext.Provider&gt; ); }; export default App;   Here we skipped some final code to emphasize the core layout.  First thing to note is that we wrapped the app into two React contexts: CsrfContext and AuthContext. React contexts allow sharing some state without need to pass it over props to children components. CsrfContext allows access to CSRF token everywhere in the app, AuthContext provides authentication information.  We render ChatLogin page if user is not authenticated and one of the chat screens if user authenticated. React Router is used for the navigation.  Authentication information is stored in LocalStorage and we load it from there during the app initial load.  Note, that here we are using React reducer to manage chat state. We do this to serialize changes and thus simplify state management – trust us before we started using the reducer chat state management was a hell. It's not the only approach – there are other techniques to manage state in complex React apps (like Redux, etc), but reducer works for us here. The initial chat state looks like this:  frontend/src/App.tsx const initialChatState = { rooms: [], roomsById: {}, messagesByRoomId: {} };   rooms is an array with room IDs for room sorting during rendering.roomsById keeps room objects by idmessagesByRoomId keeps messages by room id  We are not pretending that the way we show here is the best – it could be organized differently no doubt.  Regarding realTimeStatus and messageQueue – those will be later used for real-time features.  ","version":"v5","tagName":"h2"},{"title":"Login screen​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#login-screen","content":" Let's look at ChatLogin component. To make it we need to render a login form:  title= import React, { useState, useContext } from 'react'; import logo from './assets/centrifugo.svg' import CsrfContext from './CsrfContext'; import { login } from './AppApi'; interface ChatLoginProps { onSuccess: (userId: string) =&gt; void; } const ChatLogin: React.FC&lt;ChatLoginProps&gt; = ({ onSuccess }) =&gt; { const [username, setUsername] = useState(''); const [password, setPassword] = useState(''); const csrf = useContext(CsrfContext); const handleLogin = async () =&gt; { try { const resp = await login(csrf, username, password) onSuccess(resp.user.id.toString()); } catch (err) { console.error('Login failed:', err); } }; return ( &lt;form id=&quot;chat-login&quot; onSubmit={(e) =&gt; { e.preventDefault() handleLogin() }}&gt; &lt;div id=&quot;chat-login-logo-container&quot;&gt; &lt;img src={logo} width=&quot;100px&quot; height=&quot;100px&quot; /&gt; &lt;/div&gt; &lt;div className=&quot;input-container&quot;&gt; &lt;input type=&quot;text&quot; value={username} onChange={(e) =&gt; setUsername(e.target.value)} placeholder=&quot;Username&quot; /&gt; &lt;/div&gt; &lt;div className=&quot;input-container&quot;&gt; &lt;input type=&quot;password&quot; value={password} onChange={(e) =&gt; setPassword(e.target.value)} placeholder=&quot;Password&quot; /&gt; &lt;/div&gt; &lt;div className='login-button-container'&gt; &lt;button&gt;Login&lt;/button&gt; &lt;/div&gt; &lt;/form&gt; ); }; export default ChatLogin;   This is quite a straightforward component. Note the import from ./AppApi - we've put all the API methods to a separate file, where we use axios HTTP client to communicate with the backend API. For example, login call looks like this:  frontend/src/AppApi.tsx import { API_ENDPOINT_BASE } from &quot;./AppSettings&quot;; export const login = async (csrfToken: string, username: string, password: string) =&gt; { const response = await axios.post(`${API_ENDPOINT_BASE}/api/login/`, { username, password }, { headers: { &quot;X-CSRFToken&quot;: csrfToken } }); return response.data }   Other API calls look very similar, so we wan't pay attention to them further – but you can always take a look in source code.  ","version":"v5","tagName":"h2"},{"title":"Chat room list screen​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#chat-room-list-screen","content":" On the root page we show rooms current user is member of. ChatRoomList component renders rooms. But note that rooms are managed outside of this component – it just renders rooms from application chat state.  frontend/src/ChatRoomList.tsx import { useContext } from 'react'; import { Link } from 'react-router-dom'; import ChatContext from './ChatContext' const ChatRoomList = () =&gt; { const { state } = useContext(ChatContext); return ( &lt;div id=&quot;chat-rooms&quot;&gt; {state.rooms.map((roomId: number) =&gt; { const room = state.roomsById[roomId] return &lt;div className=&quot;chat-room-block&quot; key={room.id}&gt; &lt;Link to={`/rooms/${room.id}`}&gt; &lt;div className=&quot;left-column&quot;&gt; &lt;span className=&quot;name&quot;&gt;{room.name}&lt;/span&gt; &lt;span className=&quot;message-content&quot;&gt; {room.last_message? ( &lt;span&gt; &lt;span className='message-content-author'&gt;{room.last_message.user.username}:&lt;/span&gt; &amp;nbsp; {room.last_message.content} &lt;/span&gt; ) : (&lt;&gt;&lt;/&gt;)} &lt;/span&gt; &lt;/div&gt; &lt;div className=&quot;right-column&quot;&gt; &lt;span className=&quot;chat-room-member-counter&quot;&gt;{room.member_count}&amp;nbsp;&lt;span className=&quot;chat-room-member-counter-icon&quot;&gt;🐈&lt;/span&gt;&lt;/span&gt; &lt;/div&gt; &lt;/Link&gt; &lt;/div&gt; })} &lt;/div&gt; ); }; export default ChatRoomList;   Note, we iterate over state.rooms array which only contains IDs of rooms and is a source of truth for the order of rooms on the screen. To remind: we sort rooms on this screen by bumped_at field in the descending order.  ","version":"v5","tagName":"h2"},{"title":"Chat room search screen​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#chat-room-search-screen","content":" Chat rooms search screen shows list of all rooms in the app available to join. What's important to note here – as soon as user joins/leaves the room we update chat state by dispatching ADD_ROOMS or DELETE_ROOM state events. This allows us to synchronize room state – so that after user joins some room, the room appears on room list screen.  frontend/src/ChatSearch.tsx import { useState, useEffect, useContext } from 'react'; import CsrfContext from './CsrfContext'; import ChatContext from './ChatContext'; import { joinRoom, leaveRoom, searchRooms } from './AppApi'; interface ChatSearchProps { fetchRoom: (roomId: string) =&gt; Promise&lt;void&gt; } const ChatSearch: React.FC&lt;ChatSearchProps&gt; = ({ fetchRoom }) =&gt; { const csrf = useContext(CsrfContext); const { state, dispatch } = useContext(ChatContext); const [rooms, setRooms] = useState&lt;any&gt;([]); const [loading, setLoading] = useState&lt;any&gt;({}) const setLoadingFlag = (roomId: any, value: boolean) =&gt; { setLoading((prev: any) =&gt; ({ ...prev, [roomId]: value })); }; const onJoin = async (roomId: any) =&gt; { setLoadingFlag(roomId, true) try { await joinRoom(csrf, roomId) const room = await fetchRoom(roomId) dispatch({ type: &quot;ADD_ROOMS&quot;, payload: { rooms: [room] } }) setRooms(rooms.map((room: any) =&gt; room.id === roomId ? { ...room, is_member: true } : room )) } catch (e) { console.log(e) } setLoadingFlag(roomId, false) }; const onLeave = async (roomId: any) =&gt; { setLoadingFlag(roomId, true) try { await leaveRoom(csrf, roomId) dispatch({ type: &quot;DELETE_ROOM&quot;, payload: { roomId: roomId } }) setRooms(rooms.map((room: any) =&gt; room.id === roomId ? { ...room, is_member: false } : room )) } catch (e) { console.log(e) } setLoadingFlag(roomId, false) }; useEffect(() =&gt; { const fetchRooms = async () =&gt; { const rooms = await searchRooms() setRooms(rooms) }; fetchRooms(); }, []); return ( &lt;div id=&quot;chat-rooms&quot;&gt; {rooms.map((room: any) =&gt; { const roomState = state.roomsById[room.id] let isMember: boolean; if (roomState == null) { isMember = false } else if (roomState !== undefined) { isMember = true } else { isMember = room.is_member } return &lt;div className={`chat-room-block ${(isMember) ? 'member' : 'not-member'}`} key={room.id}&gt; &lt;div className='room-search-item'&gt; &lt;span&gt; {room.name} &lt;/span&gt; &lt;span className=&quot;room-actions&quot;&gt; &lt;button disabled={loading[room.id] === true} className={`${(isMember) ? 'member' : 'not-member'} ${(loading[room.id]) ? 'loading' : ''}`} onClick={() =&gt; { if (isMember) { onLeave(room.id) } else { onJoin(room.id) } }}&gt; {(isMember) ? 'Leave' : 'Join'} &lt;/button&gt; &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; })} &lt;/div&gt; ); }; export default ChatSearch;   ","version":"v5","tagName":"h2"},{"title":"Chat room detail screen​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#chat-room-detail-screen","content":" This screen displays information about the room, renders its messages and provides an input to send new messages.  frontend/src/CharRoomDetail.tsx import { useState, useEffect, useContext, useRef, UIEvent } from 'react'; import { useParams } from 'react-router-dom'; import AuthContext from './AuthContext'; import ChatContext from './ChatContext'; interface ChatRoomDetailProps { fetchRoom: (roomId: string) =&gt; Promise&lt;void&gt; fetchMessages: (roomId: string) =&gt; Promise&lt;any[]&gt; publishMessage: (roomId: string, content: string) =&gt; Promise&lt;boolean&gt; } const ChatRoomDetail: React.FC&lt;ChatRoomDetailProps&gt; = ({ fetchRoom, fetchMessages, publishMessage }) =&gt; { const { id } = useParams() as { id: string }; const userInfo = useContext(AuthContext); const { state, dispatch } = useContext(ChatContext); const [content, setContent] = useState('') const [messagesLoading, setMessagesLoading] = useState(false) const [roomLoading, setRoomLoading] = useState(false) const [sendLoading, setSendLoading] = useState(false) const [notFound, setNotFound] = useState(false); useEffect(() =&gt; { if (messagesLoading) return const init = async () =&gt; { setMessagesLoading(true) if (!state.messagesByRoomId[id]) { const messages = await fetchMessages(id) if (messages === null) { setNotFound(true); } else { setNotFound(false); dispatch({ type: &quot;ADD_MESSAGES&quot;, payload: { roomId: id, messages: messages } }) } } setMessagesLoading(false) } init() }, [id, state.messagesByRoomId, fetchMessages]); useEffect(() =&gt; { if (roomLoading) return const init = async () =&gt; { setRoomLoading(true) if (!state.roomsById[id]) { const room = await fetchRoom(id) if (room === null) { setNotFound(true); } else { setNotFound(false); dispatch({ type: &quot;ADD_ROOMS&quot;, payload: { rooms: [room], } }) } } setRoomLoading(false) } init() }, [id, state.roomsById, fetchRoom]); const room = state.roomsById[id] || {}; const messages = state.messagesByRoomId[id] || []; const messagesEndRef = useRef&lt;any&gt;(null); // Ref for the messages container const scrollToBottom = () =&gt; { const container = messagesEndRef.current; if (container) { const scrollOptions = { top: container.scrollHeight, behavior: 'auto' }; container.scrollTo(scrollOptions); } }; const getTime = (timeString: string) =&gt; { const date = new Date(timeString); const hours = date.getHours().toString().padStart(2, '0'); const minutes = date.getMinutes().toString().padStart(2, '0'); const seconds = date.getSeconds().toString().padStart(2, '0'); return `${hours}:${minutes}:${seconds}`; } const onFormSubmit = async (e: React.FormEvent&lt;HTMLFormElement&gt;) =&gt; { e.preventDefault(); if (sendLoading) { return } setSendLoading(true) try { const message = await publishMessage(id!, content) dispatch({ type: &quot;ADD_MESSAGES&quot;, payload: { roomId: id, messages: [message] } }) setContent('') } catch (e) { console.log(e) } setSendLoading(false) } const handleScroll = (e: UIEvent&lt;HTMLDivElement&gt;) =&gt; { const container = (e.target as HTMLElement); if (!container) return; const threshold = 40; // Pixels from the bottom to be considered 'near bottom' const position = container.scrollTop + container.offsetHeight; const height = container.scrollHeight; setIsAtBottom(position + threshold &gt;= height) }; const [isAtBottom, setIsAtBottom] = useState(true); // Scroll to bottom after layout changes. useEffect(() =&gt; { if (isAtBottom) { scrollToBottom(); } }, [messages, isAtBottom]); // Dependency on messages ensures it runs after messages are updated. return ( &lt;div id=&quot;chat-room&quot;&gt; {notFound ? ( &lt;div id=&quot;room-not-found&quot;&gt; NOT A MEMBER OF THIS ROOM &lt;/div&gt; ) : ( &lt;&gt; &lt;div id=&quot;room-description&quot;&gt; &lt;span id=&quot;room-name&quot;&gt;{room.name}&lt;/span&gt; &lt;span id=&quot;room-member-count&quot;&gt;{room.member_count} &lt;span className='chat-room-member-counter-icon'&gt;🐈&lt;/span&gt;&lt;/span&gt; &lt;/div&gt; &lt;div id=&quot;room-messages&quot; onScroll={handleScroll} ref={messagesEndRef}&gt; {messages.map((message: any) =&gt; ( &lt;div key={message.id} className={`room-message ${(userInfo.id == message.user.id) ? 'room-message-mine' : 'room-message-not-mine'}`}&gt; &lt;div className='message-avatar'&gt; &lt;img src={`https://robohash.org/user${message.user.id}.png?set=set4`} alt=&quot;&quot; /&gt; &lt;/div&gt; &lt;div className='message-bubble'&gt; &lt;div className='message-meta'&gt; &lt;div className='message-author'&gt; {message.user.username} &lt;/div&gt; &lt;div className='message-time'&gt; {getTime(message.created_at)} &lt;/div&gt; &lt;/div&gt; &lt;div className='message-content'&gt; {message.content} &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; ))} &lt;/div&gt; &lt;div id=&quot;chat-input-container&quot; className={`${(sendLoading) ? 'loading' : ''}`}&gt; &lt;form onSubmit={onFormSubmit}&gt; &lt;input type=&quot;text&quot; autoComplete=&quot;off&quot; value={content} placeholder=&quot;Enter message...&quot; onChange={e =&gt; setContent(e.currentTarget.value)} required /&gt; &lt;/form&gt; &lt;/div&gt; &lt;/&gt; )} &lt;/div&gt; ); }; export default ChatRoomDetail;   Let's discuss some important or non-so-obvious things in the implementation.  One interesting thing is how we handle scroll – if user currently in the end of messages area - then we scroll to the end again after adding a new message. If user scrolls top – we prevent automatic scrolling on new message – because user most probably does not want scroll to work at that moment. This is a very common UX decision in messenger apps.  For faking avatars we are using cute pictures of cats generated by robohash.org. Each user gets unique cat picture based on user ID.  The core behaviour is straightforward – we render messages in chat and render an input for sending new messages. As soon as user submits input form – we call the backend API to create a new message in the room.  The thing to note – again, we use calls to modify state here, ADD_ROOMS and ADD_MESSAGES. We will look at reducer and describe state management shortly.  ","version":"v5","tagName":"h2"},{"title":"Chat state reducer​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#chat-state-reducer","content":" To remind, the initial chat state looks like this:  frontend/src/App.tsx const initialChatState = { rooms: [], roomsById: {}, messagesByRoomId: {} };   In the app we have several reducer actions to modify this state:  CLEAR_CHAT_STATEADD_ROOMSDELETE_ROOMADD_MESSAGESSET_ROOM_MEMBER_COUNT  State management in React is not very handy to write to be honest. What we've found though while writing this tutorial is that ChatGPT helps a lot with this task. If you describe the desired behavior properly – ChatGPT answers correctly.  ","version":"v5","tagName":"h2"},{"title":"CLEAR_CHAT_STATE​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#clear_chat_state","content":" Allows dropping the entire chat state, simply returns initialChatState const:  function reducer(state: any, action: any) { switch (action.type) { case 'CLEAR_CHAT_STATE': { return initialChatState; }   ","version":"v5","tagName":"h3"},{"title":"ADD_ROOMS​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#add_rooms","content":" Used to add rooms to the state. This may happen when we load rooms for room list screen, when we got an event from a room which is not yet known, also it's called from search screen when user joins some room:  case 'ADD_ROOMS': { const newRooms = action.payload.rooms; // Update roomsById with new rooms, avoiding duplicates. const updatedRoomsById = { ...state.roomsById }; newRooms.forEach((room: any) =&gt; { if (!updatedRoomsById[room.id]) { updatedRoomsById[room.id] = room; } }); // Merge new room IDs with existing ones, filtering out duplicates. const mergedRoomIds = [...new Set([...newRooms.map((room: any) =&gt; room.id), ...state.rooms])]; // Sort mergedRoomIds based on bumped_at field in updatedRoomsById. const sortedRoomIds = mergedRoomIds.sort((a, b) =&gt; { const roomA = updatedRoomsById[a]; const roomB = updatedRoomsById[b]; // Compare RFC 3339 date strings directly return roomB.bumped_at.localeCompare(roomA.bumped_at); }); return { ...state, roomsById: updatedRoomsById, rooms: sortedRoomIds }; }   ","version":"v5","tagName":"h3"},{"title":"DELETE_ROOM​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#delete_room","content":" This action helps to remove the room from room list screen. Used when current user leaves the room from search screen, or when we received an event that current user left the room – this help us to sync accross different devices.  case 'DELETE_ROOM': { const roomId = action.payload.roomId; // Set the specified room to null instead of deleting it. const newRoomsById = { ...state.roomsById, [roomId]: null // On delete we set roomId to null. This allows to sync membership state of rooms on ChatSearch screen. }; // Remove the room from the rooms array. const newRooms = state.rooms.filter((id: any) =&gt; id !== roomId); // Remove associated messages. const { [roomId]: deletedMessages, ...newMessagesByRoomId } = state.messagesByRoomId; return { ...state, roomsById: newRoomsById, rooms: newRooms, messagesByRoomId: newMessagesByRoomId }; }   ","version":"v5","tagName":"h3"},{"title":"ADD_MESSAGES​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#add_messages","content":" Whenever we send message, got async real-time message, or simply load messages on Chat Detail Screen - we call this action to maintain a proper message list for each known room on room list screen.  case 'ADD_MESSAGES': { const roomId = action.payload.roomId; const newMessages = action.payload.messages; let currentMessages = state.messagesByRoomId[roomId] || []; // Combine current and new messages, then filter out duplicates. const combinedMessages = [...currentMessages, ...newMessages].filter( (message, index, self) =&gt; index === self.findIndex(m =&gt; m.id === message.id) ); // Sort the combined messages by id in ascending order. combinedMessages.sort((a, b) =&gt; a.id - b.id); // Find the message with the highest ID. const maxMessageId = combinedMessages.length &gt; 0 ? combinedMessages[combinedMessages.length - 1].id : null; let needSort = false; // Update the roomsById object with the new last_message if necessary. const updatedRoomsById = { ...state.roomsById }; if (maxMessageId !== null &amp;&amp; updatedRoomsById[roomId] &amp;&amp; (!updatedRoomsById[roomId].last_message || maxMessageId &gt; updatedRoomsById[roomId].last_message.id)) { const newLastMessage = combinedMessages.find(message =&gt; message.id === maxMessageId); updatedRoomsById[roomId].last_message = newLastMessage; updatedRoomsById[roomId].bumped_at = newLastMessage.room.bumped_at; needSort = true; } let updatedRooms = [...state.rooms]; if (needSort) { // Sort mergedRoomIds based on bumped_at field in updatedRoomsById. updatedRooms = updatedRooms.sort((a: any, b: any) =&gt; { const roomA = updatedRoomsById[a]; const roomB = updatedRoomsById[b]; // Compare RFC 3339 date strings directly return roomB.bumped_at.localeCompare(roomA.bumped_at); }); } return { ...state, messagesByRoomId: { ...state.messagesByRoomId, [roomId]: combinedMessages }, roomsById: updatedRoomsById, rooms: updatedRooms, }; }   ","version":"v5","tagName":"h3"},{"title":"SET_ROOM_MEMBER_COUNT​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#set_room_member_count","content":" This reducer is called whenever we are getting events about membership changes – we will add such events soon when talk about Centrifugo integration.  case 'SET_ROOM_MEMBER_COUNT': { const { roomId, version, memberCount } = action.payload; // Check if the roomId exists in roomsById. if (!state.roomsById[roomId]) { console.error(`Room with ID ${roomId} not found.`); return state; } // Check if the version in the event is greater than the version in the room object. if (version &lt;= state.roomsById[roomId].version) { console.error(`Outdated version for room ID ${roomId}.`); return state; } // Update the member_count and version of the specified room. const updatedRoom = { ...state.roomsById[roomId], member_count: memberCount, version: version, }; // Return the new state with the updated roomsById. return { ...state, roomsById: { ...state.roomsById, [roomId]: updatedRoom, }, }; }   ","version":"v5","tagName":"h3"},{"title":"Adding styles​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#adding-styles","content":" For making frontend layout we use flexbox for CSS rules so the app will be fully responsive and look good on different screen sizes. If you are interested to learn more about it: check out this guide. Here we won't pay attention to CSS styles anymore.  ","version":"v5","tagName":"h2"},{"title":"What we have at this point​","type":1,"pageTitle":"Creating SPA frontend with React","url":"/docs/tutorial/frontend#what-we-have-at-this-point","content":" Actually, at this point we have an app which provides messenger functionality.  You can use Django admin web UI to create some rooms and interact with them. Users can join/leave rooms, send messages. But to see new messages in room users need to reload a page. Not a good thing for chat app, right? Counters about number of users in particular room are also not updated until page reload. So finally we are ready to integrate the app with Centrifugo. ","version":"v5","tagName":"h2"}],"options":{"id":"default"}}